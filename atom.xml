<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>记录思考</title>
  
  <subtitle>ML、DL、NLP</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ilewseu.github.io/"/>
  <updated>2018-10-07T15:11:22.416Z</updated>
  <id>https://ilewseu.github.io/</id>
  
  <author>
    <name>luerwei</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Attention 机制学习小结</title>
    <link href="https://ilewseu.github.io/2018/10/07/attention%E6%9C%BA%E5%88%B6%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%BB%93/"/>
    <id>https://ilewseu.github.io/2018/10/07/attention机制学习小结/</id>
    <published>2018-10-07T07:22:20.000Z</published>
    <updated>2018-10-07T15:11:22.416Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在今年1月份期间曾简单翻译了一篇关于Attention机制的文章<a href="https://ilewseu.github.io/2018/02/12/Attention%20Model%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">Attention Model注意力机制</a>。虽然，翻译了这篇文章，但由于当时对RNN、Seq2Seq等知识只处于初步了解阶段，对Attention机制总感觉是一知半解。虽然很努力的去看各种资料，但还是感觉不能够理解。经过最近一段时间的学习，对RNN、Seq2Seq有了一定的理解且进行了一定的实践，再阅读Attention相关的论文和博客就感觉不是那么吃力了，现在对最近学习的Attention机制相关的知识进行简单地总结。</p></blockquote><a id="more"></a><h2 id="Encoder-Decoder简介"><a href="#Encoder-Decoder简介" class="headerlink" title="Encoder-Decoder简介"></a>Encoder-Decoder简介</h2><p>Encoder-Decoder框架是深度学习的一种模型框架，是一个解决问题的通用框架，主要解决Seq2Seq类问题。Encoder-Decoder框架的流程可以理解为：先编码，存储，最后解码。用人脑流程来类比，先看到的是源Sequence，人脑根据自己的知识理解这个Sequence，并将理解的记忆下来，形成记忆（对应Context），这个过程叫Encoder。然后，再根据这个Context，针对不同的问题利于Context进行解答，形成答案并输出，这个过程叫Decoder。NLP的许多经典任务都可以抽象为Encode-Decoder，比如:机器翻译、文本摘要、阅读理解及语音识别等。模型的框架如下图所示：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/181007/bIfbkgD1e3.jpg?imageslim" alt="mark"></p><blockquote><p>图片来自于[3]</p></blockquote><p>上图所示的框架中，<strong>在Encoder阶段</strong>，首先将输入的序列$x_1,x_2,…,x_4$进行编码，编码成语义表示$c$：<br>$$c = f(x_1,x_2,x_3,x_4)$$<br>$f$一般是非线性函数，称为编码器，一般在文本处理和语音识别任务中通常采用RNN模型，图像处理中一般采用CNN模型。<br><strong>在Decoder阶段</strong>，则是根据语义表示$c$和之前的的生成历史信息$y_1,y_2,…,y_{i-1}$来生成i时刻要生成的$y_i$:<br>$$<br>y_i = g(C, y_1,y_2,…,y_{i-1})<br>$$<br>每个$y_i$都依次产生。Encoder-Decoder模型结果不限制输入和输出的序列长度，因此应用的范围非常广泛。</p><h2 id="引入Attention机制"><a href="#引入Attention机制" class="headerlink" title="引入Attention机制"></a>引入Attention机制</h2><p>使用Encode-Decoder时，一般会使用RNN模型对输入的序列进行学习，将输入序列编码成固定长度的向量表示。它存在一个问题：<strong>输入序列不论长短都会被编码成一个固定长度的向量表示，而解码器则受限于该固定长度的向量表示。这个问题限制了模型的性能，尤其是当输入序列较长时，模型的学习效果会很差（解码效果很差）。</strong></p><p><strong>为什么要引入Attention?</strong></p><ol><li><strong>当输入序列非常长时，会损失一些信息，模型难以学到合理的向量表示；</strong></li><li><strong>序列输入时，随着序列的不断增长，原始根据时间步的方式表现越来越差。</strong>这是由于原始的这种时间步模型设计的结构有缺陷，即所有的上下文输入信息都被限制到固定的长度，整个模型的能力都同样受到限制；</li><li>编码器的结构无法解释，也就导致了其无法设计；</li></ol><p><strong>Attention机制就是根据当前的某个状态，从已有的大量信息中选择性的关注部分信息的方法。Attention机制打破了传统Encoder-Decoder结构在解码时都依赖于内部一个固定长度向量的限制。基本思想是：通过保留编码器对输入序列编码的中间结果，然后训练一个模型来对这些输入进行选择性的学习并且在输出时将输出序列与之进行关联，不再是使用固定的向量$c$，而是在模型输出时会选择性地专注考虑输入中的对应相关的信息。</strong></p><p>Attention机制可以理解成：当我们人在看一样东西的时候，我们当前时刻关注的一定是我们当前正在看的这样东西的某一地方。换句话说，当我们目光移到别处时，注意力随着目光的转移也在转移。这意味着，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。</p><p><strong>个人对Attention的理解是，在Encoder-Decoder过程中，在Decoder阶段不是使用固定的Context向量，而是在解码阶段，动态地根据当前输出，利用特定的方式从众多的输入信息中选择对当前任务目标更关键的信息。</strong>不同论文提出的Attention方法一般的区别就是在于从输入信息中选择信息的方式。下面就来看一些论文中提出的Attention机制。</p><h2 id="一些Attention"><a href="#一些Attention" class="headerlink" title="一些Attention"></a>一些Attention</h2><p>基础seq2seq模型中，Decoder每次的输入是$[y_{i-1},s_{i-1},c]$，这里的前两者是上一个cell的输出和隐藏层状态，$c$是Encoder最后一个cell的隐状态。Attention就是将固定的$c$变为$c^{‘}(h(1),h(2),…,h(t))$。不同的Attention，$c^{‘}(h(1),h(2),…,h(t))$的计算方式是不同的。</p><h3 id="Bahdanau-Attention"><a href="#Bahdanau-Attention" class="headerlink" title="Bahdanau Attention"></a>Bahdanau Attention</h3><p>这篇论文中的Attention可以看成是Soft Attention的一种，其上下文向量$c$的计算方式如下：<br>$$\begin{align}<br>&amp;e_{ij} = v_a^T tanh(W_as_{i-1}+U_ah_j)\\<br>&amp;\alpha_{ij} = \frac {exp(e_{ij})}{\sum_j exp(e_{ij})}\\<br>&amp;c_{i} = \sum_j \alpha_{ij}h_j<br>\end{align}<br>$$<br>相当于构建了一个以所有Encoder隐藏层向量为输入的全连接层，通过BP在训练的时候训练好权重$W_a$、$h_j$和$U_a$。$\alpha_{ij}$表示Encoder隐藏层向量$h_j$的权重。$c_{ij}$为语义向量。$v_a$是一个输入长度的one-hot向量，用来表示输入的词汇。<br>现在Decoder隐藏层向量与前一时刻状态，输出以及语义向量有关：<br>$$s_i = f(s_{i-1},y_{i-1}, c_i)$$<br>计算出$s_i$隐层状态后，解码过程根据$c_i$、上一步的输出$y_{i-1}$、当前的RNN隐向量$s_i$计算得到当前时间步的输出概率分布，g为非线性函数：<br>$$p(y_i|y_1,…,y_{i-1},x)=g(y_{i-1},s_i,c_i)$$</p><p>整个解码的流程如下：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180921/hfHkbmgk27.jpg?imageslim" alt="mark"></p><h3 id="Luong-Attention"><a href="#Luong-Attention" class="headerlink" title="Luong Attention"></a>Luong Attention</h3><p>论文中的$e_{ij}$计算方式如下：<br>$$<br>\begin{align}<br>&amp;e_{ij} = h_j^T W_a s_{i-1}\\<br>&amp;\alpha_{ij}=\frac {exp(e_{ij})}{\sum_j exp(e_{ij})}\\<br>&amp;c_{ij} = \sum_j \alpha_{ij}h_j<br>\end{align}<br>$$</p><p>Decoder的过程如下：<br>$$<br>\begin{align}<br>    &amp;p(y_i|y_1,…,y_{i-1},x) = softmax(W_s\hat h_i)\\<br>    &amp;\hat h_i = tanh(W_c[c_i;s_i])\\<br>    &amp;s_i = f(s_{i-1}, y_{i-1},\hat h_{i-1})\\<br>    &amp;c_{ij}=align(s_i, h_j)=\sum_j \alpha_{ij}h_j<br>\end{align}<br>$$</p><p>Luong中和了Soft Attention和Hard Attention的思想，提出了折中的方法，只考虑附近窗口内的单词贡献度。在选取$e_{ij}$上，尝试了点乘和拼接两种，点乘简化了运算过程，提高了计算效率。</p><blockquote><p>Hard Attention指只考虑Attention分数最高的词，其余词权重归为0。Hard Attention有两种实现方式：（一）选取最相似的输入信息；（二）通过在注意力分布式上随机采样；Hard Attention的优点是节约时间，缺点是丢失了非常多的信息。在图像中表现不错，但在NLP中很少用。基于最大采样或随机采样的方式来选择信息，因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用反向传播算法进行训练。</p></blockquote><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h3><p>Self Attention也被称为Intra Attention(内部Attention)，在一般任务Encoder-Decoder框架中，输出和输出的内容是不一致的，而Self Attention，指的不是输入和输出之间的Attention，而是输入内部元素之间或输出内部元素之间发生Attention。Self Attention其实就是除了$s$和$h$还加入了输出$v$，三者一起进行Attention Score的打分。计算方式如下：<br>$$<br>\begin{align}<br>&amp;u_t = tanh(Wh_t)\\<br>&amp;\alpha_t = \frac {exp(score(u_t,u))} {\sum_{t^{‘}=1}^Texp(score(u_{t^{‘}},u))}\\<br>&amp;c_t = \sum_{t=1}^T \alpha_t h_t<br>\end{align}<br>$$<br>公式中的score可以选择任意的公式；$u$是一个可训练的参数，随机初始化初始状态。</p><p><strong>引入Self Attention后能够捕获句子中长距离的相互依赖的特征，因为如果是RNN或者是LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远被捕获的可能性越小。但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离极大缩短，有利于有效地利用这些特征。除此外，Self Attention 对于增加计算的并行性也有直接帮助作用。[3]</strong></p><p>除了上面介绍的Attention，对于CNN也可以进行Attention的设计，具体可以参考论文《ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs》。</p><h2 id="Attention机制的通用理解"><a href="#Attention机制的通用理解" class="headerlink" title="Attention机制的通用理解"></a>Attention机制的通用理解</h2><p>Attention机制可以通用的理解为一个查询（query)到一系列（键key-值value)的映射，如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/181007/lciD6GGA0a.jpg?imageslim" alt="mark"></p><blockquote><p>图片来自于[3]</p></blockquote><p>可以这样理解Attention机制，将输入中的构成元素想象成由一系列的<key, value="">数据对构成，此时给定的输出中为某个元素Query,通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对输入元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数[3]。即可以将其本质思想改写为如下公式：<br>$$Attention(Query,input) = \sum_{i=1} Score(Query, Key_i)*Value_i$$</key,></p><p>在Encoder-Decoder中，Decode的第t步的Hidden State可以看成Query，Encoder的Hidden states是values。换句话说，Attention机制也是一种根据一些其他向量表达（Query），从向量表达集合（values）中获得特定的向量表达的方法。加权和是已有信息（Values）的选择性摘要信息（selective summary）,其中摘要信息是查询（Query）根据自己需要选择确定要关注的信息；在机器翻译的例子中，因为在计算Attention的过程中，输入中的Key和Value合二为一，指向的是同一个东西，也就是输入句子中每个单词对应的语义编码。</p><p>Attention机制的计算过程，对目前大多数方法进行抽象，可以归纳为三个过程：第一个阶段是根据Query和每个Key计算两者之间的相似性或者相关性；第二个阶段是对第一个阶段的原始分值进行归一化处理，一般采用Softmax计算出权重系数；第三个阶段是根据权重系数对Value进行加权求和。整体流程如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/181007/75E49jaF66.jpg?imageslim" alt="mark"></p><blockquote><p>图片来自于[3]</p></blockquote><p>计算Query和每个Key之间的相似性或相关性一般有三种方式：</p><ul><li>$dot: Q^TK_i$</li><li>$General: Q^TW_aK_i$</li><li>$Concat:W_a[Q;K_i]$</li><li>$Preceptron:V_a^Ttanh(W_aQ + U_aK_i)$</li></ul><p>第二个阶段和第三个阶段的计算如下面的公式所示：<br>$$<br>\begin{align}<br>&amp;a_i = softmax(f(Q,K_i)) = \frac {exp(f(Q,K_i))}{\sum_j exp(f(Q, K_j))}\\<br>&amp;Attention(Q,K,V) = \sum_i a_iV_i<br>\end{align}<br>$$</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>Attention机制其实就是一种加权求和机制;</li><li>Attention的权重由当前的Hidden state和需要计算的Hidden state通过一定的方式计算出Score，然后在通过Softmax得到；</li><li>Attention可以用来做什么：主要是关注长序列中的关键信息；</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://www.cnblogs.com/robert-dlut/p/5952032.html" target="_blank" rel="external">注意力机制（Attention Mechanism）在自然语言处理中的应用</a></li><li><a href="https://www.cnblogs.com/robert-dlut/p/8638283.html" target="_blank" rel="external">自然语言处理中的自注意力机制（Self-attention Mechanism）</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzA4Mzc0NjkwNA==&amp;mid=2650783542&amp;idx=1&amp;sn=3846652d54d48e315e31b59507e34e9e&amp;chksm=87fad601b08d5f17f41b27bb21829ed2c2e511cf2049ba6f5c7244c6e4e1bd7144715faa8f67&amp;mpshare=1&amp;scene=1&amp;srcid=1113JZIMxK3XhM9ViyBbYR76#rd" target="_blank" rel="external">深度学习中的注意力机制</a></li><li><a href="https://blog.csdn.net/bvl10101111/article/details/78470716" target="_blank" rel="external">Attention Model（mechanism)的套路</a> </li><li>Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate. Iclr 2015 1–15 (2014).</li><li>Luong, M. &amp; Manning, C. D. Effective Approaches to Attention-based Neural Machine Translation. 1412–1421 (2015).</li><li>Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在今年1月份期间曾简单翻译了一篇关于Attention机制的文章&lt;a href=&quot;https://ilewseu.github.io/2018/02/12/Attention%20Model%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/&quot;&gt;Attention Model注意力机制&lt;/a&gt;。虽然，翻译了这篇文章，但由于当时对RNN、Seq2Seq等知识只处于初步了解阶段，对Attention机制总感觉是一知半解。虽然很努力的去看各种资料，但还是感觉不能够理解。经过最近一段时间的学习，对RNN、Seq2Seq有了一定的理解且进行了一定的实践，再阅读Attention相关的论文和博客就感觉不是那么吃力了，现在对最近学习的Attention机制相关的知识进行简单地总结。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>RNN札记</title>
    <link href="https://ilewseu.github.io/2018/09/23/RNN%E6%9C%AD%E8%AE%B0/"/>
    <id>https://ilewseu.github.io/2018/09/23/RNN札记/</id>
    <published>2018-09-23T08:22:20.000Z</published>
    <updated>2018-10-07T15:14:26.624Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基本RNN"><a href="#基本RNN" class="headerlink" title="基本RNN"></a>基本RNN</h3><a id="more"></a><p>$$<br>h_t = \sigma(W_{xh} \cdot x_t+W_{hh}h_{t-1}+b)<br>$$</p><p>其中，$h_t$表示当前时刻的隐状态;$h_{t-1}$表示上一时刻的隐状态;$x_t$表示当前时刻的输入；$W_{xh},W_{hh}$为要学习的参数，所有时刻的权重矩阵都是共享的。这是循环神经网络相对于前馈网络而言最为突出的优势。</p><h3 id="梯度消失及梯度爆炸"><a href="#梯度消失及梯度爆炸" class="headerlink" title="梯度消失及梯度爆炸"></a>梯度消失及梯度爆炸</h3><p>RNN会存在梯度爆炸或消失的问题，尤其是当time_step越长的时候，建模的序列越长，它就越容易遗忘，就会出现梯度问题。本质上它每一时刻之间的梯度传递也是连乘的，激活函数和权值影响也是一样的。<br>一般在两种情况下可能出现梯度消失：（1）深层网络，网络层数多；（2）采用了不合适的激活函数，比如Sigmoid；梯度爆炸一般出现在深层网络和权值太大的情况下。出现梯度消失的现象后，接近输出层的参数会基本上收敛，收敛后基本上就结束了，但是前面接近input的参数基本上还是随机的，导致学习不充分。</p><p>一般的解决方案有如下几种方案：</p><ul><li>选择其他的激活函数，如ReLu；</li><li>梯度剪切（针对梯度爆炸）；</li><li>正则化；</li><li>BatchNorm。</li></ul><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM网络是专门设计用来避免长期依赖的，它可以说是一种优化的RNN，它和RNN的区别在于隐藏层的设计。</p><p>LSTM在隐藏层引入：细胞单元（Cell)和门（Gate)，细胞单元是利用<strong>先前状态$h_{t-1}$</strong>和当前的输入$x_t$产生新的信息；门其实就是一个开关，它决定哪些信息通过或通过多少，门并不提供额外的信息，门只是起到限制信息的量的作用，因为门起到的是过滤器的作用，所以用的激活函数是sigmoid，而不是tanh。<br>LSTM的各种门：</p><ul><li><strong>输入门：</strong>$i_t=\sigma(W_{xi}x_t+W_{hi}h_{t-1} + b_i)$</li><li><strong>遗忘门：</strong>$f_t=\sigma(W_{xf}x_t+W_{hf}h_{t-1} + b_f)$</li><li><strong>输出门：</strong>$o_t=\sigma(W_{xo}x_t+W_{ho}h_{t-1} + b_o)$</li><li><strong>cell记忆细胞更新：</strong>$c_t=f_t\circ c_{t-1}+i_t\circ tanh(W_{xc}x_t+W_{hc}h_{t-1}+b_c)$</li><li><strong>隐层更新：</strong>$h_t = o_t\cdot tanh(c_t)$</li></ul><p><strong>输入门</strong>，控制多少信息可以进入到memory cell；<strong>遗忘门</strong>，控制有多少上一时刻的mermory cell的信息可以累积到当前时刻的memory cell；<strong>输出门</strong>，控制多少当前时刻的memory cell中的信息可流入到当前隐藏状态；</p><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>GRU(Gated Recurrent Unit,GRU)是在2014年提出的，是一种更简单的变种模型，它不但可以有效避免梯度消失，而且有着比LSTM更简单的网络结构。它引入了更新门$z_t$和重置门$r_t$，隐藏层的状态更新如下：</p><ul><li><strong>更新门：</strong>$z_t=\sigma(W_{xz}x_t+W_{hz}h_{t-1})$</li><li><strong>重置门：</strong>$r_t=\sigma(W_{xr}x_t+W_{hr}h_{t-1})$</li><li><strong>新的记忆单元:</strong>$\hat h_t= tanh(Wx_t+ U(r_t\circ h_{t-1}))$</li><li><strong>隐层更新：</strong>$h_t=(1-z_t)\circ\hat h_t + z_t \circ h_{t-1}$</li></ul><p><strong>更新门</strong>，决定先前的状态有多少会传输到新的状态，类似遗忘门的功能，决定擦除前一个时刻的多少信息；<strong>重置门</strong>，决定先前状态有多少会影响到新的记忆单元；</p><p>从直观上来说，重置门决定了如何将新的输入信息与前面的记忆相结合，更新门定义了前面记忆保存到当前时间步的量。如果将重置门设置为1，更新门设置为0，那么将再次获得标准RNN模型。重置门其实强制隐藏状态遗忘一些历史信息，并利用当前输入的信息。这可以令隐藏状态遗忘任何在未来发现与预测不相关的信息，同时也允许构建更加紧致的表征。而更新门将控制前面隐藏状态的信息有多少会传递到当前隐藏状态，这与 LSTM 网络中的记忆单元非常相似，它可以帮助 RNN 记住长期信息。由于每个单元都有独立的重置门与更新门，每个隐藏单元将学习不同尺度上的依赖关系。那些学习捕捉短期依赖关系的单元将趋向于激活重置门，而那些捕获长期依赖关系的单元将常常激活更新门。使用门控制机制学习长期依赖关系的基本思想和LSTM一致，但还是有一些关键区别：</p><ul><li>GRU有两个门（重置门与更新门），而LSTM有三个门（输入门、遗忘门和输出门）；</li><li>GRU并不会控制保留内部记忆($c_t$)，且没有LSTM中的输出门；</li><li>LSTM中的输入与遗忘门对应于GRU的更新门，重置门直接作用于前面的隐藏状态。 </li></ul><p>尽管GRU简化了参数，但实际上和LSTM基本上是等价的，区别在于LSTM专门用了一个遗忘门来控制前一个时刻隐藏层的影响，输出门控制mermory cell传输到下一个状态，而GRU使用重置门进行控制前一个时刻的影响，而对new memory cell传输不做控制。在GRU中，只存在一个更新门来控制旧的信息和新的信息的组合，而在LSTM里使用遗忘门和输入门共同控制。LSTM里存在显式的memory cell，而GRU则没有显式的memory，隐层为旧的隐层状态和新的输入的线性组合。总体来说，LSTM和GRU表现相差不大，GRU在收敛速度上更具优势，这也是参数少的优势。</p><p>虽然LSTM和GRU的原始目的是为了解决长距离依赖的问题，但一定程度上两者都能优化梯度消失的现象。原因是由于引入了new memory，$h_t$由两部分做加法得到，那么在计算梯度的时候，梯度也会变大，从而缓解了梯度消失的问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;基本RNN&quot;&gt;&lt;a href=&quot;#基本RNN&quot; class=&quot;headerlink&quot; title=&quot;基本RNN&quot;&gt;&lt;/a&gt;基本RNN&lt;/h3&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>CNN for NLP</title>
    <link href="https://ilewseu.github.io/2018/09/01/CNN%20for%20NLP/"/>
    <id>https://ilewseu.github.io/2018/09/01/CNN for NLP/</id>
    <published>2018-09-01T06:22:20.000Z</published>
    <updated>2018-09-10T15:17:35.140Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近一段时间一直在学习CNN在NLP方面的应用，主要是学习如何利用CNN解决NLP的传统任务，如文本分类、句子相似度计算等。在此期间阅读了一些论文和博客，在此系统整理一下，以便加强理解。</p></blockquote><a id="more"></a><h2 id="CNN简介"><a href="#CNN简介" class="headerlink" title="CNN简介"></a>CNN简介</h2><p>卷积神经网络（Convolutional Neural Network, CNN或ConvNet)是一种具有局部连接、权重共享等特性的<strong>前馈神经网络</strong>。经典的CNN的架构LeNet-5如下图所示:</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180901/G7DDl7JcGG.jpg?imageslim" alt="mark"></p><p>整个网络由卷积层、池化层以及全连接层等组成。下面简要介绍不同层的操作及作用。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积，是一种重要的数学运算， 在信号处理或图像处理中，经常使用一维或二维卷积。一维卷积常用在信号处理中，用于计算信号的延迟累积。在图像处理中经常使用的是二维卷积，以图像作为输入，比较容易理解卷积的方法就是把卷积想象成作用于矩阵的一个滑动窗口函数。如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180901/H6jf9Ilma2.jpg?imageslim" alt="mark"></p><p>滑动窗口矩阵称为卷积核（convolution kernel)或滤波器(filter)。在上图中，使用3<em>3的卷积核与矩阵对应的部元素相乘，然后相加。然后，按照一定的步长从上到下，从左到右不断执行卷积操作，就会得到如右边所示的输出矩阵。<em>*在图像处理中，卷积经常作为特征提取的有效方法。</em></em></p><p><strong>卷积操作是一种局部操作，其作用是提取一个局部区域的特征</strong>，一个卷积层可以设置多个卷积核，可以提取不同的特征。卷积核参数可以通过网络训练学习出来。除了可以学到类似横向、纵向边缘滤波器，还可以学到任意角度的边缘滤波器。通过组合这些滤波器以及后续的操作进行，基本而一般的模式会逐渐被抽象为具有高层语义的概念表示，并以此对应到样本类别。和盲人摸象类似，首先摸到比较粗的腿、长长的鼻子、大大的耳朵等<strong>局部区域特征</strong>，然后将这些特征组合在一起，判断是像。而卷积在此处的作用就是摸的过程中提取腿、鼻子、耳朵等<strong>局部区域特征。</strong></p><h3 id="池化层-pooling"><a href="#池化层-pooling" class="headerlink" title="池化层(pooling)"></a>池化层(pooling)</h3><p>卷积神经网络中的另一个重要的概念就是池化层，一般卷积层之后都会接一个池化层。池化层对输入数据做降采样操作。例如，下图就是2*2窗口的max-pooling，及其输出结果：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180901/dfhgL8L3l2.jpg?imageslim" alt="mark"></p><p>池化操作也可以看成是一个p-范数作为非线性映射的“卷积”操作，特别当趋近于正无穷时，就是常见的max pooling。常见的池化操作有如下几种：</p><ul><li><strong>Max-pooling</strong>:最大池化，取整个区域内的最大值作为特征，在自然语言处理中，<strong>常用于分类问题，希望观察到特征是强特征</strong>，以便可以区分哪一个类别。</li><li><strong>Averge-pooling</strong>：平局池化。取整个区域内的均值作为特征。常用于主题模型，一个句子可能不止一个主题标签，如果使用Max-pooling的话只保留最强的特征，获取信息较少，使用Average可以广泛反映这个区域的特征；</li><li><strong>K-max pooling</strong>:k-最大池化，选取整个区域内前k个最大的特征。</li></ul><p><strong>pooling操作具有如下作用：</strong></p><ul><li><strong>特征不变性，pooling操作使模型更关注是否存在某些特征，而不是特征的具体位置。</strong>可以看做是一种很强的先验，使特征学习包含某种程度自由度，能容忍一些特征微小的位移，使得网络的鲁棒性增强，有一定的抗干扰的作用；</li><li><strong>特征降维</strong>，由于pooling操作为降采样操作，因此相当于在空间范围内做降维操作，从而使模型可以抽取更广泛的特征。同时，减小了下一层的输入大小，进而减小计算量和参数的个数；</li><li>在一定程度上防止过拟合，更方便优化。</li></ul><p>关于CNN的更多详细内容，可以参考[1]《解析卷积神经网络—深度学习实践手册》。</p><h2 id="CNN-在NLP应用上的一些论文"><a href="#CNN-在NLP应用上的一些论文" class="headerlink" title="CNN 在NLP应用上的一些论文"></a>CNN 在NLP应用上的一些论文</h2><h3 id="TextCNN-2"><a href="#TextCNN-2" class="headerlink" title="TextCNN[2]"></a>TextCNN[2]</h3><p>在图像处理中卷积核通过对图像的一小块区域进行卷积操作。但是，文本与图像不同，一个句子构成的词向量作为输入。每一行代表一个词的词向量，在处理文本时，卷积核通常覆盖上下几行的词，此时卷积核的宽度与输入数据的宽度相同，通过这样的方式，能够捕捉到多个连续词之间的特征，并且能够在同一类特征计算时中共享权重。Kim Y’s 论文讲的是如何利用CNN做文本分类，论文中的模型结构如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180901/fh72LhkC62.jpg?imageslim" alt="mark"></p><p>如上图所示，句子长度为9，embeding大小为6。卷积核的宽度为6，和词embeding的大小相同，滑动窗口的大小为2和3。使用卷积核进行卷积，在第一层学到的卷积核捕捉的特征与n-gram非常相似，（但不局限于），但是以更紧凑的方式表征。</p><p>通过卷积操作，每个卷积核都可以提取出响应的Feature Map，不同的句子长度，产生的Feature Map的行数不同。但是，在池化层，采样Max Pooling，可以将每个Feature Map的维度全部降为1。所以，pooling之后，得到的向量维度，就是卷积层Feature Map的数量。因此，<strong>解决了不同文本长度不统一的问题</strong>。</p><ul><li><strong>输入层</strong></li></ul><p>如上图所示，输入层是句子中词语对应的向量表示形式，从上到下表示。假设句子有n个词，词向量的维度是k，则输入矩阵为<strong>n*k</strong>。输入矩阵的类型分为静态和动态两种形式，静态表示在训练过程中固定不变，动态表示模型训练过程中，词向量也当做是可优化的参数。通常把反向传播导致词向量中值发生变化的这一过程称为Fine tune。对于未登录词，用0或者随机小的正数填充。</p><ul><li><strong>卷积层</strong></li></ul><p>输入层经过卷积操作得到若干Feature Map，卷积窗口大小为h*k，h表示滑动窗口大小，k表示词向量大小；通过这样一个卷积核，将得到若干个列数为1的Feature Map。</p><ul><li><strong>池化层</strong></li></ul><p>池化层采用的是max-pooling，这种pooling方式可以解决可变长度句子输入问题。最终池化层的输出为各个Feature Map的最大值，即一个一维向量。</p><ul><li><strong>全连接+Softmax层</strong></li></ul><p>池化层的一维向量的输出通过全连接的方式，连接一个Softmax层，Softmax层可根据任务的需要设置。最终实现时，可以在倒数第二层的全连接部分上使用Dropout技术，以及L2正则化防止过拟合。</p><p><strong>一些结论：</strong></p><ul><li>CNN-static比CNN-rand好，说明预训练的词向量确实有较大的提升作用；</li><li>CNN-non-static比大部分的CNN-static好，说明适当的Fine tune也是有利的，使得词向量更加贴近于具体的任务；</li><li>CNN-multi channel较与CNN-single在小规模的数据集上有更好的表现，实际上CNN-multichannel体现了一种折中思想，即，既不希望Fine tuned的词向量距离原始值太远，但同时保留一定的变化空间。</li></ul><h3 id="DCNN-3"><a href="#DCNN-3" class="headerlink" title="DCNN[3]"></a>DCNN[3]</h3><p>这篇论文提出了一种DCNN(Dynamic Convolutional Neural Network)的模型，该模型提出了动态Pooling的方法。<br>论文中首先对句子语义建模，在底层通过组合临近的词语信息，逐步向上传递，上层则又组合新的语义信息，从而使得句子中相离较远的词语也有交互的行为。从直观上看，这个模型能够通过词语的组合，再通过池化层提取出句子中重要的语义信息。</p><p>该模型的架构如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180901/7dl0HbcFAf.jpg?imageslim" alt="mark"></p><p>网络架构中的卷积层使用的是宽卷积（Wide Convolutional)的方式，紧接着是动态k-max pooling。中间卷积层的输出即Feature Map的大小会根据输入句子的长度而变化。</p><p>首先从卷积层看，卷积层采用的是宽卷积，句子长度为7，词向量维度为5，宽卷积窗口大小为3。经过卷积操作之后，为pooling操作，这里pooling采用的是k-max pooling。</p><p>k-max pooling的好处在于，既能提取出来句子的较为重要的信息，同时保留了它们的次序信息（相对位置）。同时，由于应用在最后的卷积层上只需要提取出k个值，所以这种方法允许不同长度的输入。然而，对于中间的卷积层而言，参数k也不是固定的。k的计算公式如下：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180901/2f6098j1AH.jpg?imageslim" alt="mark"></p><p>其中，l表示当前卷积的层数，L为网络中总共卷积的层数，$k_top$为最顶层卷积层pooling的k值，是一个固定的值。</p><p>经过pooling操作之后，从在一个Folding操作，这个操作是吧pooling的输出结果相邻的两行相加。这种操作则是考虑相邻两行之间的某种联系。最后，在经过一个k-max pooling操作，进行全连接操作。</p><p>该模型具有如下特点：</p><ul><li>保留了句子中词序信息和词语之间的相对位置；</li><li>宽卷积的结果是传统卷积的一个扩展，某种意义上，也是n-gram的一个扩展；</li><li>模型不需要任何的先验知识，例如句法依存树等，并且模型考虑了句子中相隔较远的词语之间的语义信息。</li></ul><h3 id="RCNN-4"><a href="#RCNN-4" class="headerlink" title="RCNN[4]"></a>RCNN[4]</h3><p>这篇论文首先介绍了传统的文本分类任务，并比较了一些常用方法的优缺点。首先是介绍文本分类任务。首先介绍文本的表示方法，传统的方法如Bag of words，比如n-gram、pattern等。用词频、MI、pLSA、LDA作为特征选择的方法。传统的特征表达方法经常忽略上下文的信息和词序信息，以及语义信息；高阶的n-gram、tree kernels也可以应用在特征表达，但是也存在稀疏的缺点，影响准确性。</p><p><strong>Recursive Neural Network</strong>：<br>效果完全依赖于文本树的构建，并且构建文本树需要的时间为$O(n^2)$。同时，两个句子的关系也不能通过一颗树表现出来。因此，不适合长句子或长文本建模。</p><p><strong>Recurrent Neural Network</strong>：<br>循环神经网络能够保留一个句子的上下文信息。但是，其是一个有偏的模型，后面的词占得重要性更大。因此，如果用它来捕捉整个文档的语义，其效果可能不太好。因为，整个文档关键的部分可能分布在文档的各个部位，而不仅仅是文档的末尾。</p><p><strong>Computational Neural Network</strong>：</p><p>CNN是一个无偏的模型，能够通过最大池化获得最重要的特征。它的缺点是卷积核大小是固定的，如果卷积核选小了容易造成信息的丢失；如果选大了，参数空间会变得很大。</p><p>针对上面不同模型存在的缺陷，论文提出Recurrent Convolutional Neural Network (RCNN)。模型采用双向循环结构：相比传统的基于窗口的神经网络噪声要更小，能够最大化提取上下文信息。模型采用max-pooling自动决策哪个特征有更加重要的作用。模型的架构如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180902/B35id3lGA9.jpg?imageslim" alt="mark"></p><p>首先，定义一些符号：</p><ul><li>$c_l(w_i)$:表示词$w_i$左边的上下文表示；</li><li>$c_r(w_i)$:表示词$w_i$右边的上下文表示；</li><li>$e(w_i)$:表示词$w_i$的词向量表示；</li></ul><p>$c_l(w_i)$和$c_r(w_i)$可以通过下面的公式计算：</p><p>$$<br>c_l(w_i) = f(W^{(l)}c_l(w_{i-1})+W^{(sl)}e(w_{i-1})) \\<br>c_r(w_i) = f(W^{(r)}c_r(w_{i+1})+W^{(sr)}e(w_{i+1}))<br>$$</p><p>其中，$W^{(l)}$、$W^{(r)}$、$W^{(sl)}$、$W^{(sr)}$为相应的参数矩阵，全局共享相同的参数。</p><p>$c_l(w_i)$和$c_r(w_i)$计算完成之后，词i可以用下面的公式表示：</p><p>$$x_i =[c_l(w_i);e(w_i);c_r(w_i)]$$</p><p>得到词i的表示$x_i$之后，经过线性变换，加tanh激活函数，输出到下一层：</p><p>$$y_i^{(2)} = tanh(W^{(2)}x_i+b^{(2)})$$</p><p>其中，$y_i^{(2)}$是一个latent semantic向量，用来分析计算最有用的文本表示。</p><p>到目前为止，第2层的操作已经完成。接下来是max-pooling层，其操作如下：</p><p>$$y^{(3)} = max_{i=1}^n y_i^{(2)}$$</p><p>max操作是一个element-wise的函数。$y^{(3)}$的第k个元素，是$y_i^{(2)}$第k列最大的元素。</p><p>max-pooling操作后，输出层就是传统的全连接+Softmax操作。</p><h3 id="分段pooling-5"><a href="#分段pooling-5" class="headerlink" title="分段pooling[5]"></a>分段pooling[5]</h3><p>这篇提出了一个分段pooling的操作解决关系抽取问题。关系抽取问题可以简单的理解分析一段文本中两个实体之间的关系。一般可以将关系抽取问题转化为分类问题（个人理解）。<br>比如，下面的一句话，涉及两个实体，实体之间的关系</p><blockquote><p>The  [fire]  inside WTC was caused by exploding [fuel].</p><p><strong>关系</strong>: Cause-Effect</p></blockquote><p>论文中提出的模型如下所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180902/a90DkKD0J0.jpg?imageslim" alt="mark"></p><p>首先，卷积操作，和之前介绍的卷积操作相同，但是，模型中的卷积，将文本分为3部分，每一部分分别进行卷积，和池化。对于上图中的示例，句子中涉及到两个实体，Kojo Annan和Kofi Annan，以这两个实体为分界线可以将句子分为3部分：</p><ul><li>[…,hired, Kojo Annan]</li><li>[,the,son,of]</li><li>[Kofi Annan,in,…]</li></ul><p>模型对这三部分分别进行卷积，每部分卷积后的输出，再分别进行pooling操作，具体操作可以简单表示为如下。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180902/EJ23L02lg0.jpg?imageslim" alt="mark"></p><p>如图中所示，在pooling阶段，$max(c_{11})$、$max(c_{12})$、和$max(c_{13})$表示第一个Feature Map的分段pooling的结果，图中存在3个Feature Map。同时，模型中也加入了对位置的embedding，具体可以参考论文。</p><h2 id="CNN参数如何调"><a href="#CNN参数如何调" class="headerlink" title="CNN参数如何调"></a>CNN参数如何调</h2><p>论文[6]中提供了一些CNN相关调参策略。相关的终结如下：</p><h3 id="使用什么样的词向量？"><a href="#使用什么样的词向量？" class="headerlink" title="使用什么样的词向量？"></a>使用什么样的词向量？</h3><ul><li>使用预训练的词向量比随机初始化的效果要好；</li><li>采取微调策略（non-static)的效果比固定词向量（static)的效果要好；</li><li>如果无法确定用哪种预训练词向量（word2vec/Glove等）更好，不同的任务结果不同，应该针对当前的任务进行试验。</li></ul><h3 id="Filter窗口大小和数量设置"><a href="#Filter窗口大小和数量设置" class="headerlink" title="Filter窗口大小和数量设置"></a>Filter窗口大小和数量设置</h3><ul><li>每次使用一种类型的Filter进行试验，表明Filter的窗口大小设置在1到10之间是一个比较合理的选择；</li><li>首先，在一种类型的Filter大小上搜索，以找到当前数据集的“最佳”大小，然后探索这个最佳大小附近的多种Filter大小的组合；</li><li>每种窗口类型的Filter对应的“最好”的Filter个数（feature map数量）取决于具体数据集；</li><li>通过可以看出，当Feature Map数量超过600时，performance提高有限，甚至会损害performance。这可能是过多的feature Map数量导致过拟合；</li><li>在实践中，100到600是一个比较合理的搜索空间；</li></ul><h3 id="激活函数选择"><a href="#激活函数选择" class="headerlink" title="激活函数选择"></a>激活函数选择</h3><ul><li>Sigmoid,Cube,Tanh Cube相较于Relu和Tanh，表现很糟糕；</li><li>Tanh比Sigmoid好，这可能是由于Tanh具有zero-centering性质；</li><li>与Sigmoid相比，ReLU具有非饱和形式(a-non-saturating form)的优点，并能够加速SGD的收敛；</li><li>对于某些数据集，线性变换（Iden,即不使用非线性激活函数）足够捕获词嵌入与输出标签之间的相关性。（但是如果有多个隐藏层，相较于非线性激活函数，Iden就不太合适了，因为完全用线性激活函数，即使有多个隐藏层，组合后整个模型还是线性的，表达能力可能不足，无法捕获足够信息）；</li><li>因此，建议首先考虑ReLU和Tanh，也可以尝试Iden;</li></ul><h3 id="Pooling的选择"><a href="#Pooling的选择" class="headerlink" title="Pooling的选择"></a>Pooling的选择</h3><ul><li>对于句子分类任务，max-pooling往往比其他池化策略要好。这可能是因为上下文的具体位置对于预测Label可能并不是很重要，而句子某个具体的n-gram（1-max pooling后Filter提取出的特征）可能更可以刻画整个句子的某些含义，对于预测类别更有帮助；</li><li>但是在其他任务，如释义识别，k-max pooling可能更好；</li></ul><h3 id="正则化参数"><a href="#正则化参数" class="headerlink" title="正则化参数"></a>正则化参数</h3><ul><li>0.1到0.5之间的非零dropout rates能够提高一些performance(尽管提升幅度很小)，具体的最佳设置取决于具体数据集；</li><li>对于l2 norm加上一个约束往往不会提高performance;</li><li>当Feature Map的数量大于100时，可能会导致过拟合，影响performance，而dropout将减轻这些影响；</li><li>在卷积层上进行dropout帮助很小，而且较大dropout rate对performance有坏的影响；</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前，对CNN在NLP相关任务上的应用已经有了一个基本的了解。CNN能够捕捉到局部的特征，在自然语言处理中能够起到n-gram的作用，通过将捕捉到的局部特征进行组合，组合的结果可以看成是对输入文本的文本表示，在这个文本表示的基础之上可以完成各种NLP任务。整体的框架和其在图像方面的应用相差不大。但是，有时需要针对特定的任务对模型进行一定程度的修改，比如卷积层的输入，除了词的向量组成的矩阵之外、还可以加入位置的embeding以及采用non-static和static的向量组成多通道。在pooling层可以选择max-pooling、k-max pooling以及动态k-max pooling等多种类型的pooling操作。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://lamda.nju.edu.cn/weixs/book/CNN_book.pdf" target="_blank" rel="external">《解析卷积神经网络—深度学习实践手册》</a></li><li>Kim Y. Convolutional neural networks for sentence classification[J]. arXiv preprint arXiv:1408.5882, 2014.</li><li>Kalchbrenner N, Grefenstette E, Blunsom P. A convolutional neural network for modelling sentences[J]. arXiv preprint arXiv:1404.2188, 2014.</li><li>Recurrent Convolutional Neural Network for Text Classification, Siwei Lai etc.</li><li>Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks. Daojian Zeng, Kang Liu, Yubo Chen and Jun Zhao.</li><li>Zhang Y, Wallace B. A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification[J]. arXiv preprint arXiv:1510.03820, 2015.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近一段时间一直在学习CNN在NLP方面的应用，主要是学习如何利用CNN解决NLP的传统任务，如文本分类、句子相似度计算等。在此期间阅读了一些论文和博客，在此系统整理一下，以便加强理解。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepNLP" scheme="https://ilewseu.github.io/tags/DeepNLP/"/>
    
  </entry>
  
  <entry>
    <title>词向量</title>
    <link href="https://ilewseu.github.io/2018/07/08/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    <id>https://ilewseu.github.io/2018/07/08/词向量/</id>
    <published>2018-07-08T13:08:20.000Z</published>
    <updated>2018-08-03T15:56:48.556Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>看了多篇关于词向量的论文及博文，特此整理一下，以加强理解。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在NLP的相关任务中，要将自然语言交给机器学习中的算法来处理，通常需要首先将语言数学化，将语言转化为机器能够识别的符号，然后再进行各种计算。词向量就是用来将语言中的词进行数学化的一种方式。顾名思义，词向量就是把一个词表示成一个向量。<br><a id="more"></a></p><p>一种最简单的词向量是One-hot Representation，它利用一个很长的向量表示一个词，向量的长度为词典D的大小$|V|$，向量的分量只有一个1，其他全是0,1的位置对应该词在词典中的索引。这种词向量表示有如下缺点：</p><ul><li>存在语义鸿沟，不能很好地刻画词与词之间的相似性；</li><li>维数灾难、稀疏；</li><li>无法表示新词。</li></ul><p>另一种就是Distributed Representation，它最早是Hinton在1986年提出的，可以克服one-hot Representation的缺点。其基本想法是：通过训练将某种语言中的每一个词映射为一个固定长度的向量，所有的向量构成一个词向量空间，可以在这个词向量空间上进行各种NLP任务。其词向量表示的核心是：利用上下文信息进行词表示；（具有相同（类似）上下文信息的词应该具有相同的词表示）。根据建模的不同，主要可以分为三类：</p><ul><li>基于矩阵的分布表示；</li><li>基于聚类的分布表示；</li><li>基于神经网络的分布表示；</li></ul><p>三种类型的分布表示使用了不同的技术手段获取词向量，它们的核心思想分为两步：（1）选择一种方式描述上下文；（2）选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系。<br><strong>下面就介绍几种常见的词向量</strong>:</p><h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><h3 id="CBOW和Skip-gram介绍"><a href="#CBOW和Skip-gram介绍" class="headerlink" title="CBOW和Skip-gram介绍"></a>CBOW和Skip-gram介绍</h3><p>Word2Vec是Google于2013年提出的一种高效训练词向量的模型,核心的思想是<strong>上下文相似的两个词,它们的词向量也应该相似</strong>。比如，香蕉和苹果经常出现在相同的上下文中，因此这两词的表示向量应该比较相似。word2vec模型利用这个基本思想提出了两种基本的模型<strong>CBOW</strong>和<strong>Skip-gram</strong>，其架构如下：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180722/aigI1fibjB.png?imageslim" alt="mark"></p><p>word2vec中比较重要的概念是词的上下文，其实就是一个词其周围的词，比如$w_t$的范围为1的上下文就是$w_{t-1}$和${w_{t+1}}$。可以看出，两个模型都包含三层：<strong>输入层、投影层和输出层</strong>。CBOW是在已知当前词$w_t$的上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$的前提下预测当前词$w_t$；Skip-gram是在已知当期词$w_t$的前提下，预测上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$。</p><p>基于神经网络的语言模型的目标函数通常取为如下<strong>对数似然函数</strong>:</p><p>$$L=\sum_{w\in C}log  p(w|Context(w))$$</p><p>其中，$w$表示当前词，$C$表示语料库中的所有的词。其中<strong>关键是条件概率p(w|Context(w))的构建</strong>。word2vec中的CBOW模型，其优化的目标函数形如上式，即已知上下文是使当前词出现概率最大，而Skip-gram模型的优化目标则形如:<br>$$L=\sum_{w\in C}log  p(Context(w)|w)$$<br>即已知当前词使上下文出现的概率最大。</p><p><strong>为什么这样做就能够得到词向量？</strong></p><p>模型的目标函数可以理解为对于语料中的词及其上下文，最大化当前词和上下文同时出现的概率，而通过训练模型中的参数能够使优化的目标函数达到最优，即最优化后的模型的参数已经学习到语料中语义信息，能够对于给定的上下文给出最有可能的当前词，而对于相同的上下文，能够给出比较相似的当前词。这在一定程度上满足了上下文相同的词，其语义也是相似的假设。所以，可以把学习到的参数作为词的向量表示。<strong>（个人理解）</strong></p><h3 id="基于Softmax的梯度更新推导"><a href="#基于Softmax的梯度更新推导" class="headerlink" title="基于Softmax的梯度更新推导"></a>基于Softmax的梯度更新推导</h3><p>无论是CBOW和Skip-gram都可以看做是一个分类问题，即对于给定的输入(当前词或上下文)，预测可能的词（上下文或当前词）。CBOW和Skip-gram的架构均可以看成是一个只包含一个隐层的神经网络，只不过没有激活函数，其梯度更新推导可以按照BP神经网络的过程来进行，本文按照[3]中的方式，从上下文只有一个词的情况开始，然后扩展到CBOW和Skip-gram，分别介绍模型的参数更新方式。</p><p><strong>One-word Context model</strong></p><p>首先，对于输入只有一个词,且输出只有一个词，如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180722/5lH294EAA6.jpg?imageslim" alt="mark"></p><p>其中：</p><ul><li>$V$：语料库中词汇个数；</li><li>$N$：隐层神经元个数，同时也是词向量的维度；</li><li>$W\in R^{V*N}$：输出层到隐层的权重矩阵，每一行代表一个词的词向量；</li><li>$W^{‘}$ R^{N*V}$:隐层到输出层权重矩阵，其中每一列可以看作是额外的一种词向量；</li></ul><p>模型表示用输入词预测输出的词，输入层的词$w_I$使用One-hot表示，即在上图输入层中的结点$x_1,x_2,…,x_V$只有$x_k$为1，其余为0，其中$k$可以是输入的词在词汇表中的索引下标。从输入层到隐藏层,输入的词$w_I$经过与矩阵$W$相乘，相当于取出$W$中的第k行，实际也就是输入词$w_I$的$N$维向量，使用$v_{w_I}$表示，以此作为隐藏层的输入。与神经网络不同的是，<strong>word2vec隐层并没有激活函数</strong>:<br>$$h = W^T\cdot X=v_{w_I}^T$$<br>然后，从隐层的$h$到输出层$Y$，$h$与矩阵$W^{‘}$相乘，得到一个$V*1$的向量$u$:<br>$$u=W^{‘}\cdot h$$</p><p>其中：$u$每个元素$u_j$就是$W^{‘}$的第$j$列用$v_{w_j}^{‘}$表示，与$h$做内积得到：$u_j=v_{w_j}^{‘}\cdot h$，表示词汇表中第$j$个词的得分。</p><p>因为是对于给定的词预测输出词，由于词汇表中的词是多个，因此使用softmax将u归一化到[0,1]之间，从而作为输出词的概率，即：<br>$$<br>p(w_j|w_I)=y_j = \frac {exp(u_j)}{\sum_{k\in V}exp(u_k)} = \frac {exp(v_{w_j}^{‘T}\cdot v_{w_I})}{\sum_{k\in V}exp(v_{w_k}^{‘T}\cdot v_{w_I})}<br>$$<br>其中$v_w$与$v_w^{‘}$都称为词$w$的词向量，一般使用前者作为词向量。</p><p>上面的模型可以看成是对于给定的一个输出词，给出一个输出词。因此，训练数据中一个训练样本可以是$(w_I,w_O)$的形式，$w_I$和$w_O$分别是One-Hot表示，因此模型的目标函数可以表示为如下形式：<br>$$<br>\begin{align}<br>L &amp;= max  P(w_O|w_I)=max_{y_{j*}}=max  log y_{j^*}\\&amp;=max  log(\frac {exp(u_{j^*})}{\sum exp(u_k)})\\&amp;=max  u_{j^*} - log \sum_{k=1}^V exp(u_k)<br>\end{align}<br>$$</p><p>将最大化目标函数，转换为最小化目标函数，因此，损失函数可以定义为:<br>$$E = -u_{j^*}+log \sum_{k=1}^V exp(u_k)$$<br>其中，$j^*$是真实单词在词汇表中的下标。</p><p>损失函数已经给定，剩下的就是结合反向传播算法，使用梯度下降法来更新参数。</p><p><strong>隐层到输出层矩阵$W^{‘}$梯度更新计算</strong>：</p><p>$$<br>\begin{align}<br>\frac {\partial E}{\partial w_{ij}^{‘}}&amp;=\frac {\partial E}{\partial u_j} \frac {\partial u_j}{\partial w_{ij}^{‘}}=(y_j - t_j)h_i<br>\end{align}<br>$$<br>其中，$\frac {\partial E}{\partial u_j}=y_j - t_j$,$y_j$表示模型的输出，即归一化后的第$j$项概率值，$t_j$真实值的第$j$项$y_j - t_j$可以理解为输出值第j项和真实值的差值。</p><p>因此，隐层到输出层参数$W^{‘}$更新公式为：<br>$$w_{ij}^{‘}=w_{ij}^{‘} - \eta(y_j - t_j)h_i=w_{ij}^{‘} - \eta e_j h_i\\e_j=y_j-t_j$$<br>写成向量的形式为：<br>$$v_{w_j}^{‘}=v_{w_j}^{‘} - \eta e_j h,  j\in {1,2,…,V}$$</p><p><strong>输入层到隐层参数W的梯度更新计算</strong>:</p><p>首先，看一下隐层结点$h_i$的梯度计算：<br>$$<br>\frac {\partial E}{\partial h_i} = \sum_{j=1}^V \frac {\partial E}{\partial u_j}\frac {\partial u_j}{\partial h_i} = \sum_{j=1}^V e_j w_{ij}^{‘}=EH_i$$</p><p>其中,$h_i$是隐层的第$i$个结点的输出。<br>定义$EH$为一个$N$维向量，由$E_i,i=1,2,…,N$组成，则整个隐藏结点$h$关于损失函数的梯度为：<br>$$\frac {\partial E}{\partial h}=EH^T$$<br>从上面的介绍可以看出$h$就是$W$的一行，但是因为输入中只有一个1，因此每次只能更新一行，其余行的梯度为0，所以$v_{W_I}$的更新公式为：<br>$$v_{W_I}^T =v_{W_I}^T - \eta EH^T $$</p><p>到此为止，一个训练样本的反向传播训练过程就计算完了。相比传统的神经网络，在这里没有引入激活函数，在一定程度上能够加速训练，但是每次更新需要变量整个词表计算一遍概率，计算量还是很大的。</p><p><strong>CBOW</strong></p><p>上面介绍的是对于输入只有一个词，输出也只有一个词的模型，本部分将word2vec的第一种模型：CBOW，即给定上下文预测当前词，其模型如下所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180722/cGkka55GHf.jpg?imageslim" alt="mark"></p><p>和上面介绍的模型不同的是，输入不再是一个词，而是多个词，上图中一个C个词：$x_{1k},x_{2k},…,x_{Ck}$每个$x$都是One-Hot表示，这些词就是上下文$Context(x)$。这样，计算隐层的输入$h$时，不再是直接复制某一行，而是将输入的C个词对应$W$中的$C$行取出，然后取均值，作为隐层的输入$h$:<br>$$<br>h = \frac {1}{C} W^T(x_1+x_2+…+x_C)=\frac {1}{c}(v_{w_1}+v_{w_2}+…+v_{w_C})^T<br>$$<br>这样损失函数变成下面的写公式：<br>$$<br>E = -log  p(w_O|w_{I,1},…,w_{I,C})=-u_{j^*} + log \sum_{k=1}^V exp(u_k)<br>$$</p><p>从模型结构可以看出，隐层到输出层与上面介绍的模型是一样的，因此采用反向传播训练，$W^{‘}$的参数更新公式是一样的，即：<br>$$v_{w_j}^{‘}=v_{w_j}^{‘} - \eta e_j h,  j\in {1,2,…,V}$$</p><p>同样，隐层神经元的梯度也是一样的：<br>$$\frac {\partial E}{\partial h}=EH^T$$</p><p>从输入层到隐层的更新，因为输入变为$C$个词，所以，每个词对应的向量都应该更新，更新公式如下：<br>$$<br>v_{w_{I,c}} = v_{w_{I,c}} - \frac {1}{C} \cdot \eta \cdot EH^T,  c = 1,2,…,C<br>$$<br>可以看出，CBOW的参数更新方式与一个词的基本一样。</p><p><strong>Skip-gram</strong></p><p>Skip-gram是word2vec的第二种模型，是根据当前词预测上下文，这个模型与第一个介绍的模型比，Skip-gram的输出有多个词，而不是一个词，这样输出层就不是一个多项式分布了，而是$C$个多项式分布，模型架构图如下所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180722/G22GmBJaGG.jpg?imageslim" alt="mark"></p><p>我们还是使用$v_{w_I}$表示输出词的向量，从输入层到隐层与One-word Context Model相同，隐层神经元的计算方式如下：</p><p>$$h = W_{(k,.)}^T=v_{w_I}^T$$</p><p>输出层不在是计算一个多项式分布，而是计算$C$个多项式分布$y_1,y_2,…,y_C$，因此前向计算的过程也需要分开计算。因此，计算第$c$个输出单词的预测的多项式分布中的第$j$项相比One-word Context Model多一个$c$参数:<br>$$<br>p(w_{c,j}=w_{O,c}|w_I)=y_{c,j} = \frac {exp(u_{c,j})}{\sum_{k=1}^V exp(u_{c,k})}<br>$$<br>需要注意的是这$C$个输出向量是相互独立的，可以看成是$C$个独立的One-word Context Model中的输出向量，相互之间没有影响。虽然，输出是相互独立的，但隐层到输出层的参数$W^{‘}$是共享的，所以：<br>$$<br>u_{c,j} = u_j = {v_{w_j}^{‘}}^T<br>$$</p><p>所以，从前向后，根据上述公式计算出$C$个输出向量之后，在每个$V$维向量中，选取概率最大的作为输出的单词，这样就根据输入单词$w_I$得到$C$个输出单词，也就达到根据单词预测上下文的目的了。</p><p>Skip-gram的损失函数和One-word Context Model的损失函数有所不同，其损失函数如下：<br>$$<br>\begin{align}<br>E &amp;= -log p(w_1w_2,…,w_C|w_I)\\&amp;=-log \prod_{c=1}^C p(w_c|w_I)\\&amp;=-log \prod_{c=1}^C \frac {exp(u_{c,j})} {\sum_{k=1}^V exp(u_{c,k})}\\&amp;=- \sum_{c=1}^C u_{j_c^*} + C \cdot \sum_{k=1}^V exp(u_k)<br>\end{align}<br>$$<br>其中，$j_c^*$的含义同One-word Context Model中的$u_{j^*}$一样，都表示训练样本真实输出单词在词汇表中的下标。</p><p>下面从后向前介绍参数更新方式，对第$c$个词对应的多项式分布的第$j$项的梯度为：<br>$$<br>\frac {\partial E}{\partial u_{c,j}} = y_{c,j} - t_{c,j}=e_{c,j}<br>$$<br>表示输出结点的预测误差，我们用一个$V$维的向量表示该输出误差$EI={EI_1,…,EI_V}$为所有上下文词预测输出的误差之和：</p><p>$$<br>EI_j = \sum_{c=1}^C e_{c,j}<br>$$<br>下一步，我们就可以计算隐层到输入层的参数$W^{‘}$的梯度：<br>$$<br>\frac {\partial E}{\partial w_{ij}^{‘}}=\sum_{c=1}^C \frac {\partial E}{\partial u_{c,j}} \frac {\partial u_{c,j}}{\partial w_{ij}^{‘}}=EI_j \cdot h_i<br>$$<br>因此，可以得到参数的更新方式：<br>$$<br>w_{ij}^{‘} = w_{ij}^{‘} - \eta \cdot EI_j \cdot h_i<br>$$<br>或者写成向量的形式：<br>$$v_{w_j}^{‘} = v_{w_j}^{‘} - \eta \cdot EI_j \cdot h , for j=1,2,…,V $$</p><p>接着计算隐层神经元的梯度：<br>$$<br>\frac {\partial E}{\partial h_i} = \sum_{c=1}^C\sum_{j=1}^V \frac {\partial E}{\partial u_{c,j}} \frac {\partial u_{c,j}}{\partial h_i}= \sum_{c=1}^C\sum_{j=1}^V e_{c,j}W_{ij}^{‘}=\sum_{j=1}^V EI_j w_{ij}^{‘} = W_i^{‘} \cdot EI<br>$$<br>同样，整理成向量的形式：<br>$$<br>\frac {\partial E}{\partial h} = W^{‘}\cdot EI<br>$$</p><p>因为输出只有一个词，所以，每次训练更新只更新$W$的一行：<br>$$<br>v_{w_I}^T = v_{w_I}^T - \eta W^{‘}\cdot EI<br>$$<br>至此，Skip-gram的参数更新方式也介绍完毕。</p><h3 id="两种加速训练方法"><a href="#两种加速训练方法" class="headerlink" title="两种加速训练方法"></a>两种加速训练方法</h3><p>到目前为止，我们讨论的模型都是原始形式，没有使用任何的优化方法，训练效率还是很低的。对于所有讨论过的模型，首先需要学习两个词向量矩阵$W$和$W^{‘}$。对于每一个训练样本而言，CBOW更新$W$的$C$行，Skip-gram更新W其中的一行；但是对于$W^{‘}$，无论是CBOW还是Skip-gram，对于每个训练样本，需要对$W^{‘}$的所有元素进行更新，考虑到词汇表中的词汇一般是几十万甚至千万级别，这个计算量还是很大的。除了，参数更新计算量比较大，在输出层softmax函数计算输出层V个元素，计算量也是很大。<br>针对此存在两种优化策略<strong>Hierarchical Softmax</strong>和<strong>Negative Sampling</strong>。二者的出发点一致，就是在每个训练样本中，不再完全计算或者更新$W^{‘}$这个矩阵。二者都不再显示使用$W^{‘}$这个矩阵。下面就分别介绍这两种加速训练方法：</p><h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>Hierarchical Softmax是Bengio在2005年最早提出来专门为了加速计算神经语言模型中的Softmax的一种方式，使用一个哈夫曼树表示词汇表中的$V$个词，$V$个词分布在树的叶节点，可以证明非叶节点有$V-1$个。如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180722/Cj6hKjj8aI.jpg?imageslim" alt="mark"></p><p>由于在隐层和输出层的Softmax计算量较大，层次Softmax避免要计算所有词的Softmax，采用哈夫曼树来代替从隐层到输出Softmax层的映射，计算Softmax概率只要沿着哈夫曼树结构进行。首先，定义如下符号：（以下部分来自4）</p><ul><li>$p^w$:表示从根节点到w对应的叶子结点的路径；</li><li>$l^w$:表示路径$p^w$上包含的结点个数；</li><li>$p_1^w,p_2^w,…,p_{l^w}^w$:表示路径$p^w$上对应的结点；</li><li>$d_2^w,d_3^w,…,d_{l^w}^w\in {0,1}$:表示路径上的Huffman编码，根节点不对应编码；</li><li>$\theta_1^w,\theta_2^w,…,\theta_{l^w}^w \in R^m$:表示路径上非叶子结点对应的向量；</li></ul><p>从根节点出发到某个叶子结点的路径上，每个分支都可视为进行了一次二分类。默认左边（编码为0）是父类，右边（编码为1）是正类。</p><ul><li>分为正类的概率为：$\sigma(X_w^T\theta)=\frac {1}{1+e^{-X_w^T\theta}}$</li><li>分为负类的概率为：$1-\sigma(X_w^T\theta)$</li></ul><p>其中，$\theta$为当前非叶子结点对应的词向量，$X_w^T$表示隐藏的输出；</p><p>所以，Hierarchical Softmax的思想就是：</p><p><strong>对于词典中的任意词$w$，Huffman树中必存在一条从根节点到词$w$对应叶子节点$p^w$的路径。路径$p^w$上存在$l^w-1$个分支，将每个分支看做一次二分类，每次分类就产生一个概率，将这些概率连乘，即$p(w|Context(w))$。</strong></p><p><strong>对于CBOW</strong></p><p>根据上下文$Context(w)$，预测当前词$w$,基于Hierarchical Softmax的思想，可以得出:</p><p>$$p(w|Context(w))=\prod_{j=2}^{l^w}p(d_j^w|X_w,\theta_{j-1}^w) \\<br>其中 \ <br>p(d_j^w|X_w,\theta_{j-1}^w) =<br>\begin{cases}<br>\sigma(X_w^T\theta_{j-1}^w) &amp; d_j^w=0 \ <br>1-\sigma(X_w^T\theta_{j-1}^w) &amp; d_j^w=1<br>\end{cases} \\<br>= [\sigma(X_w^T\theta_{j-1}^w)]^{1-d_j^w} \cdot [1-\sigma(X_w^T\theta_{j-1}^w)]^{d_j^w}<br>$$<br><strong>带入最大似然函数，得到要优化的目标函数：</strong>:<br>$$<br>\mathcal{L}=\sum_{w\in C}log\prod_{j=2}^{l^w}{[\sigma(X_w^T\theta_{j-1}^w)]^{1-d_j^w} \cdot [1-\sigma(X_w^T\theta_{j-1}^w)]^{d_j^w}} \\<br>= \sum_{w\in C}\sum_{j=2}^{l^w}{(1-d_j^w) \cdot log[\sigma(X_w^T\theta_{j-1}^w)]+d_j^w \cdot log[1-\sigma(X_w^T\theta_{j-1}^w)] }<br>$$<br>上面的公式即是CBOW要优化的目标函数，可以采用梯度上升法求解，具体梯度计算就不再一一推导，请参看[4]中的详细推导。</p><p><strong>对于Skip-gram</strong></p><p>已知当前词是$w$，对其上下文$Context(w)$中的词进行预测。同样，基于Hierarchical Softmax的思想，可以得出<br>$$<br>\begin{align}<br>   &amp;p(Context(w)|w)=\prod_{u\in Context(w)}p(u|w) \ <br>&amp;p(u|w)=\prod_{j=2}^{l^u}p(d_j^u|v(w),\theta_{j-1}^u)<br>= [\sigma(v(w)^T\theta_{j-1}^u)]^{1-d_j^u}\cdot [1-\sigma(v(w)^T\theta_{j-1}^u)]^{d_j^u}\end{align}$$ </p><p><strong>带入最大似然函数，得到要优化的目标函数：</strong><br>$$<br>\mathcal{L}=\sum_{w\in C}log\prod_{u\in Context(w)}\prod_{j=2}^{l^u}{[\sigma(v(w)^T\theta_{j-1}^u)]^{1-d_j^u}\cdot [1-\sigma(v(w)^T\theta_{j-1}^u)]^{d_j^u}} \\<br>= \sum_{w\in C}log\sum_{u\in Context(w)}\sum_{j=2}^{l^u}{(1-d_j^u)\cdot log[\sigma(v(w)^T\theta_{j-1}^u)] + d_j^u \cdot log[1-\sigma(v(w)^T\theta_{j-1}^u)]}<br>$$</p><h4 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h4><p>Negative Sampling是另一种加速训练的方式，我们分别介绍CBOW和Skip-gram利用Negative Sampling后其要优化的目标函数是什么样子的。</p><p><strong>对于CBOW</strong></p><p>我们已知词$w$的上下文$Context(w)$,需要预测$w$。假设我们已经选好一个关于$w$的负样本子集$NEG(w)$，并且定义了对于词典中的任意词$w’$，都有：<br>$$<br>L^w(w’)=\begin{cases}<br>1 &amp; w’=w \ <br>0 &amp; w’\neq w<br>\end{cases}<br>$$</p><p>对于一个给定的正样本$(Context(w),w)$，我们希望最大化：<br>$$<br>g(w)=\prod_{u\in {w}\bigcup NEG(w)} p(u|Context(w)) \\<br>其中\\<br>p(u|Context(w))=<br>\begin{cases}<br>\sigma(X_w^T\theta^u) &amp; L^w(u)=1 \ <br>1-\sigma(X_w^T\theta^u) &amp; L^w(u)=0<br>\end{cases} \\<br>= [\sigma(X_w^T\theta^u)]^{L^w(u)} \cdot [1-\sigma(X_w^T\theta^u)]^{1-L^w(u)}<br>$$</p><p>所以，</p><p>$$<br>g(w)=\sigma(X_w^T\theta^w)\prod_{u\in NEG(w)} [1-\sigma(X_w^T\theta^u)]<br>$$<br><strong>为什么要最大化$g(w)$?</strong></p><p>因为$\sigma(X_w^T\theta^w)$表示的是上下文为$Context(w)$时，预测中心词$w$的概率，而$\sigma(X_w^T\theta^u)$表示的是上下文为$Context(w)$时，预测中心词为$u$的概率。因此，最大化$g(w)$即相当于增大正样本的概率，同时降低负样本的概率，而这就是我们所期望的。</p><p>所以，对于给定的语料库$C$来说，整体的优化目标即为最大化$G=\prod_{w\in C}g(w)$，所以目标函数为：</p><p>$$<br>\begin{align}<br>\mathcal{L}&amp;=log G=log\prod_{w\in C}g(w)=\sum_{w \in C} log\,g(w) \\<br>&amp;= \sum_{w \in C} log \prod_{u\in {w}\bigcup NEG(w)}  {[\sigma(X_w^T\theta^u)]^{L^w(u)} \cdot [1-\sigma(X_w^T\theta^u)]^{1-L^w(u)}} \ <br>&amp;= \sum_{w \in C} \sum_{u\in {w}\bigcup NEG(w)}  {L^w(u) \cdot log[\sigma(X_w^T\theta^u)] + [1-L^w(u)] \cdot log[1-\sigma(X_w^T\theta^u)]}<br>\end{align}<br>$$</p><p><strong>对于Skip-gram</strong></p><p>和上面介绍的CBOW模型所用的思想是一样的。因此，可以直接从目标函数出发。首先，对于一个给定的样本$(w,Context(w))$，我们希望最大化：<br>$$<br>g(w)=\prod_{\hat{w} \in Context(w)} \prod_{u \in {w}\bigcup NEG^{\hat {w}}(w)}<br>其中\\<br>p(u|\hat{w})=<br>\begin{cases}<br>\sigma(v(\hat {w})^T\theta^u) &amp; L^w(u)=1 \ <br>1-\sigma(v(\hat{w})^T\theta^u) &amp; L^w(u)=0<br>\end{cases}<br>$$<br>或者，写成整体的表达式：<br>$$<br>p(u|\hat{w}) = [\sigma(v(\hat{w})^T \theta^u)]^{L^w(u)}\cdot[1-\sigma(v(\hat{w})^T\theta^u)]^{1-L^w(u)}<br>$$<br>这里，$NEG^{\hat {w}}(w)$表示处理词$\hat{w}$时生成的负样本子集。所以，对于一个给定的语料库$C$,函数：<br>$$<br>G=\prod_{w\in C} g(w)<br>$$<br>就可以作为整体优化的目标。同样，我们取$G$的对数，最终的目标函数是：<br>$$<br>\begin{align}<br>\mathcal{L}&amp;=logG=log \prod_{w \in C}g(w)=\sum_{w\in C}log  g(w)\\<br>&amp;=\sum_{w \in C} log \prod_{\hat {w} \in Context(w)}  \prod_{w\in {w} \bigcup NEG^{\hat{w}}(w)} { [\sigma(v(\hat{w})^T \theta^u)]^{L^w(u)}\cdot[1-\sigma(v(\hat{w})^T\theta^u)]^{1-L^w(u)} } \\<br>&amp;=\sum_{w \in C} \sum_{\hat{w}\in Context(w)} \sum_{u\in {w}\bigcup NEG^{\hat{w}}(w)}{L^w(u)\cdot log[\sigma(v(\hat{w})^T \theta^u)]+[1-L^w(u)]\cdot log[1-\sigma(v(\hat{w})^T\theta^u)]}<br>\end{align}<br>$$<br>同样，最大化目标函数也同样可以利用梯度上升法进行计算梯度，这里就不在一一推导了，具体请参考：[4]。</p><h2 id="Glove"><a href="#Glove" class="headerlink" title="Glove"></a>Glove</h2><p>上面介绍的word2vec是一种基于预测的词向量模型。除了基于预测的词向量模型，还存在基于统计的词向量模型，以基于SVD分解技术的LSA模型为代表，通过构建一个共现矩阵，然后对矩阵进行分解得到隐层的语义向量，<strong>充分利用了全局的统计信息</strong>。然而，这类模型得到的语义向量很难获取到词与词之间的线性关系。基于预测的词向量模型，比如Skip-gram模型，通过预测一个词出现在上下文里的概率得到词向量，这类模型的缺陷在于对统计信息的利用不充分，训练时间与语料大小息息相关，其得到的词向量能够很好捕捉到词与词之间的线性关系，因此在很多任务上的表现都要优于SVD模型。</p><p>Glove模型综合了两者的优点，<strong>即使用了语料库的全局统计特征，也使用了局部的上下文特征（即滑动窗口）</strong>，模型的代价函数如下：</p><p>$$J = \sum_{i,j}^N f(X_{i,j})(v_i^Tv_j+b_i+b_j - log(X_{i,j}))^2$$</p><p>其中，$v_i,v_j$是单词i和j的词向量，$b_i,b_j$是两个标量，$f$是权重函数，$N$表示词汇表的大小(共现矩阵的维度为$N*N$)。</p><p>Glove模型首先基于语料库构建词的共现矩阵，然后基于共现矩阵学习词向量。设共现矩阵为$X$，其元素为$X_{i,j}$,表示在整个语料库中，词$i$和词$j$共同出现在一个窗口中的次数，比如对于语料库：</p><blockquote><p>glove model is very nice</p></blockquote><p>语料只有一个句子，涉及到5个单词：glove、model、is、very、nice。如果采用窗口宽度为3，左右长度都为1的统计窗口，那么就有以下窗口内容：</p><table><thead><tr><th>窗口编号</th><th style="text-align:center">中心词</th><th style="text-align:center">窗口内容</th></tr></thead><tbody><tr><td>0</td><td style="text-align:center">glove</td><td style="text-align:center">glove model</td></tr><tr><td>1</td><td style="text-align:center">model</td><td style="text-align:center">glove model is</td></tr><tr><td>2</td><td style="text-align:center">is</td><td style="text-align:center">model is very</td></tr><tr><td>3</td><td style="text-align:center">very</td><td style="text-align:center">is very nice</td></tr><tr><td>4</td><td style="text-align:center">nice</td><td style="text-align:center">very nice</td></tr></tbody></table><p>扫描语料，可以生成上面所示的句子片段，这就包含了上下文特征，然后统计所有窗口内容中不同词的共现次数，比如$X_{glove,model}=2$，可以统计出如下的共现矩阵。</p><table><thead><tr><th></th><th style="text-align:center">glove</th><th style="text-align:center">model</th><th style="text-align:center">is</th><th style="text-align:center">very</th><th style="text-align:center">nice</th></tr></thead><tbody><tr><td><strong>glove</strong></td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td><strong>model</strong></td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td><strong>is</strong></td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">1</td></tr><tr><td><strong>very</strong></td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">2</td></tr><tr><td><strong>nice</strong></td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">0</td></tr></tbody></table><p>Glove模型就是基于这个词共现矩阵来计算词向量的。首先，$X$元素$X_{i,j}$是语料库中出现在词$i$上下文中的词$j$的次数。如下，引入一些变量：<br>$$X_i = \sum_{j=1}^N X_{i,j}\\<br>P_{i,k} = \frac {X_{i,k}}{X_i}<br>$$<br>$P_{i,k}$表示条件概率，表示单词k出现在单词i上下文中的概率。<br>$$<br>ratio_{i,j,k} = \frac {P_{i,k}}{P_{j,k}}<br>$$<br>表示两个条件概率的比值。Glove模型的作者发现$ratio_{i,j,k}$这个指标有如下规律：</p><table><thead><tr><th>$ratio_{i,j,k}$的值</th><th style="text-align:center">单词j,k相关</th><th style="text-align:center">单词j,k不相关</th></tr></thead><tbody><tr><td><strong>单词i,k相关</strong></td><td style="text-align:center">趋近1</td><td style="text-align:center">很大</td></tr><tr><td><strong>单词i,k不相关</strong></td><td style="text-align:center">很小</td><td style="text-align:center">趋近1</td></tr></tbody></table><p>可以看出ratio的值能够反映词之间的相关性，而Glove模型就是利用这个Ratio的值进行建模。如果已经得到词向量，词i,词j和词k的词向量分别用$v_i,v_j,v_k$表示，通过某种函数计算$ratio_{i,j,k}$能够得到同样的规律，那么说明词向量与共现矩阵具有很好的一致性，也就说明得到的词向量中蕴涵了共现矩阵中所蕴含的信息。假设这个未知的函数是f，则：<br>$$F(v_i,v_j,v_k)=ratio_{i,j,k}=\frac {P_{ik}}{P_{j,k}}$$<br>即$F(v_i,v_j,v_k)$和$ratio_{i,j,k}$应该尽可能的接近即可。因此，可以用两者的差来作为代价函数：<br>$$<br>J = \sum_{i,j,k}^N (\frac {P_{i,k}}{P_{j,k}} - F(v_i,v_j,v_k))^2<br>$$<br>如果函数F的形式确定下来，就可以通过优化算法求解词向量了。那么Glove模型的作者是怎样将F确定下来的呢？具体形式如下：</p><ol><li>$\frac {P_{ik}}{P_{jk}}$考察了$i,j,k$三个两两之间的相似关系，不妨单独考察$i,j$两个词和他们词向量$v_i,v_j$，线性空间中的相似关系自然想到的是两个向量的差$(v_i - v_j)$，所以F的函数形式可以是：$F(w_i-w_j, w_k)=\frac {P_{ik}}{P_{jk}}$</li><li>$\frac {P_{ik}}{P_{jk}}$是一个标量，而F是作用在两个向量上，向量和标量之间的关系自然想到用内积的方式。所以，F函数的形式可以进一步确定为：$F((v_i-v_j)^Tv_k)=F(v_iv_k - v_jv_k)=\frac {P_{ik}}{P_{jk}}$</li><li>可以看到F的形式为$F(v_iv_k  - v_jv_k)=\frac {P_{ik}}{P_{jk}}$，左边是差的形式，右边是商的形式，模型通过将F取exp来将差和商关联起来：$exp(v_iv_k-v_jv_k)=\frac {exp(v_i^Tv_k)}{exp(v_j^Tv_k)} = \frac {P_{ik}}{P_{jk}}$</li><li>现在只需要让分子分母分别相等上式即可成立，所以：$exp(v_iv_k)=P_{ik},exp(v_jv_k)=P_{jk}$</li><li>所以，只需要整个语料库中考察$exp(v_i,v_j)=P_{ij}=\frac{X_{ij}}{X_i}$，即：$v_i^Tv_j=log(\frac {X_{ij}}{X_i})=log X_{ij}-logX_i$</li><li>由于$i$和$j$都是随机选取的，即交换$i$和$j$的顺序$v_i^Tv_j$和$v_j^Tv_i$应该是相等的，但是等式右边将$i$和$k$交换顺序$logX_{ij} - logX_i \ne logX_{ji} - logX_j$。为了解决这个问题，作者在模型中引入两个偏置项$b_i,b_j$，从而将模型变成了：$log X_{ij}=v_i^Tv_j+b_i+b_j$,即添加了一个偏置项$b_j$,并将$log(X_i)$吸收到偏置项$b_i$中。</li><li>最后，代价函数变成了如下的形式：$J=\sum_{i,j}^N (v_i^Tv_j + b_i+b_j-log(X_{i,j}))^2$</li><li>由于出现频率越高的词对权重应该越大，所有在代价函数中加入权重项，于是代价函数进一步完善：$$J = \sum_{i,j}^N f(X_{i,j})(v_i^Tv_j+b_i+b_j - log(X_{i,j}))^2$$</li></ol><p>模型认为权重函数f应该符合以下三个特点：</p><ul><li>$f(0)=0$,如果两个词没有共同出现过，权重为0；</li><li>$f(x)$必须是非减函数，即两个词出现的次数越多，不能权重反而变小了；</li><li>$f(x)$对于较大的值$x$不能取太大的值，类似于停用词。<br>论文中给出的权重函数为：</li></ul><p>$$<br>f(x)=\begin{cases}<br>   (\frac{x}{x_{max}})^{0.75}, &amp; if x<x_{max} \\="" 1,="" &="" if="" x="">=x_{max}<br>\end{cases}\\<br>$$</x_{max}></p><p>以上就是Glove模型是怎样构造的过程，可以看出有很多技巧在里面。</p><h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><p>FastText是Facebook于2016年开源的一个词向量计算和文本分类工具，典型的应用场景是“有监督的文本分类问题”。提供简单而高效的文本和特征学习的方法，性能比肩神深度学习而且速度更快。</p><p>FastText方法包含三部分：<strong>模型架构、层次SoftMax和N-gram特征</strong>。下面就分别简要介绍各个部分。</p><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>FastText模型的输入是一个词的序列（一段文本或者一句话），输出这个词序列属于不同类别的概率。序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。FastText在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。FastText模型架构和Word2Vec中的CBOW模型很相似，不同之处在于，FastText预测的是标签，而CBOW模型预测的是中间词。FastText模型的架构如下：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180728/igjm0C5jCA.jpg?imageslim" alt="mark"></p><h3 id="层次SoftMax"><a href="#层次SoftMax" class="headerlink" title="层次SoftMax"></a>层次SoftMax</h3><p>将输入层中的词和词组构成的特征向量，再将特征向量通过线性变换映射到隐藏层，隐藏层通过求解最大似然函数，然后根据每个类别的权重和模型参数构建Huffman树，将Huffman树作为输出。对于有大量类别的数据集，FastText使用了一个分层分类器，层次Softmax建立在Huffman编码的基础上，利用类别不均衡对标签进行编码，缩小模型预测目标的数量。当类别数为K，word embedding大小为$d$时，计算复杂度可以从$O(Kd)$降到$O(dlog(K))$。</p><h3 id="N-gram特征"><a href="#N-gram特征" class="headerlink" title="N-gram特征"></a>N-gram特征</h3><p>如果仅仅是以词和词组构成的向量作为特征向量，FastText隐藏层通过简单的求和取平均得到，会损失次序信息。为了弥补这个不足，FastText增加了N-gram特征。具体的做法是把N-gram当成一个词，也用embedding向量来表示，在计算隐层时，把N-gram的embedding向量也加进去求和取平均。例如，假设某篇文章只有3个词，$w_1,w_2,w_3$，N-gram的N取2，$w_1、w_2、w_3$以及$w_{12}、w_{23}$分别表示词$w_1、w_2$和bigram$w_1w_2、w_2w_3$的word embidding向量，那么文章的隐层可表示为：<br>$$<br>h = \frac {1}{5}(w_1+w_2+w_3+w_{12}+w_{23})<br>$$<br>在具体实现上，由于N-gram的量远比word大的多，完全存下所有的n-gram也不现实。FastText采样了Hash桶的方式，把所有的n-gram都Hash到buckets个桶中。Hash到同一个桶的所有n-gram共享一个embedding向量。如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180728/Jj9cJ1Afm1.png?imageslim" alt="mark"><br><strong>(注：图片来自[8])</strong></p><p>图中每行代表一个word或N-gram的embeddings向量，其中前V行是word embeddings，后Buckets行是n-grams embeddings。每个N-gram经过Hash函数Hash到0~bucket-1的位置，得到对应的embedding向量。</p><h3 id="FastText对比word2vec"><a href="#FastText对比word2vec" class="headerlink" title="FastText对比word2vec"></a>FastText对比word2vec</h3><p>FastText=word2vec中的CBOW+层次softmax的灵活使用，主要体现在如下两个方面：</p><ul><li><strong>模型的输入层</strong>：word2vec是context window内的word，而FastText是整个sentence的内容，包括word以及n-gram的内容；</li><li><strong>模型的输出层</strong>：wore2vec对应的是每个word，计算某个word的概率最大，而FastText的输出层对应的是分类的label。</li><li><strong>层次Softmax的使用</strong>：word2vec的目的是得到词向量，该词向量最终是在输入层得到，输出层对应的层次Softmax也会生成一些列的词向量，但最终都被抛弃，不会使用；FastText则使用了层次Softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label(一个或多个)。</li></ul><h2 id="词向量开源工具"><a href="#词向量开源工具" class="headerlink" title="词向量开源工具"></a>词向量开源工具</h2><p>word2vec、Glove和FastText都提供了开源的代码及工具包供我们使用，具体地址如下：</p><p><strong>word2vec</strong></p><ul><li>C版本：<a href="https://github.com/svn2github/word2vec" target="_blank" rel="external">https://github.com/svn2github/word2vec</a></li><li>C++版本：<a href="https://github.com/jdeng/word2vec" target="_blank" rel="external">https://github.com/jdeng/word2vec</a></li><li>python版本可使用gensim包</li></ul><p><strong>GloVe</strong></p><ul><li>C版本：<a href="http://github.com/stanfordnlp/glove" target="_blank" rel="external">http://github.com/stanfordnlp/glove</a></li><li>python版本：<a href="https://github.com/maciejkula/glove-python" target="_blank" rel="external">https://github.com/maciejkula/glove-python</a></li></ul><p><strong>FastText</strong></p><ul><li>GitHub地址：<a href="https://github.com/facebookresearch/fastText" target="_blank" rel="external">https://github.com/facebookresearch/fastText</a></li></ul><h2 id="如何训练出比较好的词向量？"><a href="#如何训练出比较好的词向量？" class="headerlink" title="如何训练出比较好的词向量？"></a>如何训练出比较好的词向量？</h2><p>在<strong>来博士</strong>的论文中，对不同的词向量在不同的任务上做了相关的实验并对比，给出了训练词向量的一些建议，这部分主要来自于该博士论文。</p><p><strong>模型比较</strong></p><ul><li>对于评价语言学特性的任务，通过上下文预测目标词的模型，比上下文与目标词联合打分的C&amp;B模型效果更好。</li><li>对于实际的自然语言处理任务，各模型的差异不大，选用简单的模型即可；</li><li>简单模型在小语料上整体表现更好，复杂的模型需要更大的语料作支撑。</li></ul><p><strong>语料影响</strong></p><ul><li>同领域的语料，一般语料越大效果越好；</li><li>领域内的语料对相似领域任务的效果提升非常明显，但在领域不契合时甚至会有负面作用。</li></ul><p><strong>规模和领域的权衡</strong></p><ul><li>语料的领域纯度比语料的规模更重要。</li></ul><p><strong>迭代次数</strong></p><ul><li>根据词向量的损失函数选择迭代次数不合适；</li><li>条件允许的话，选择目标任务的验证集性能作为参考标准；</li><li>具体任务性能指标趋势一样，可以选简单任务的性能表现最好的点停止迭代（比如同义词检测任务）；</li></ul><p><strong>词向量的维度</strong></p><ul><li>对于分析词向量语言学特性的任务，维度越大效果越好；</li><li>对于提升自然语言处理任务而言，50维词向量通常足够好；</li></ul><p><strong>总结</strong></p><ol><li>选择一个合适的模型。复杂的模型相比简单的模型，在较大的语料中才有优势；</li><li>选择一个合适领域的语料，在此前提下，语料规模越大越好。使用大规模的语料进行训练，可以普遍提升词向量的性能，如果使用领域内的语料，对同领域的任务会显著的提升；</li><li>训练时，迭代优化的终止条件最好根据具体任务的验证集来判断，或者近似地选取其他任务作为指标，但是不应该选用训练词向量时的损失函数；（一般可以根据训练语料大小，一般选用10~25次）；</li><li>词向量的维度一般需要选择50维及以上，特别当衡量词向量的语言学特性时，词向量的维度越大，效果越好。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><strong>word2vec原论文</strong>：Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean：Efficient estimation of word representations in vector space</li><li><strong>来斯惟</strong>：基于神经网络的词和文档语义向量表示方法研究博士论文</li><li><strong>Xin Rong</strong>:word2vec Parameter Learning Explained</li><li><a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="external">word2vec中的数学原理</a></li><li><a href="http://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="external">word2vec原理</a></li><li><a href="https://blog.csdn.net/coderTC/article/details/73864097" target="_blank" rel="external">理解Glove</a></li><li><a href="http://www.cnblogs.com/robert-dlut/p/5617466.html" target="_blank" rel="external">如何产生好的词向量</a></li><li><a href="http://albertxiebnu.github.io/fasttext/" target="_blank" rel="external">玩转Fasttext</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;看了多篇关于词向量的论文及博文，特此整理一下，以加强理解。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;在NLP的相关任务中，要将自然语言交给机器学习中的算法来处理，通常需要首先将语言数学化，将语言转化为机器能够识别的符号，然后再进行各种计算。词向量就是用来将语言中的词进行数学化的一种方式。顾名思义，词向量就是把一个词表示成一个向量。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理基础-句法分析</title>
    <link href="https://ilewseu.github.io/2018/07/08/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <id>https://ilewseu.github.io/2018/07/08/自然语言处理基础-句法分析/</id>
    <published>2018-07-08T13:08:20.000Z</published>
    <updated>2018-08-03T15:33:00.451Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要是对宗成庆老师《自然语言理解》讲义，第9章的一个学习笔记，同时，也参考了刘群老师《计算语言学》的句法分析讲义，所有的配图均来自这两个讲义。通过该讲义能够对句法分析是做什么的以及怎么做，有一个基本的认识。</p></blockquote><a id="more"></a><h2 id="句法分析介绍"><a href="#句法分析介绍" class="headerlink" title="句法分析介绍"></a>句法分析介绍</h2><p>句法分析(syntactic parsing)的任务是识别句子的句法结构(syntactic structure)，是从单词串得到句法结构的过程。不同的语法形式，对应的句法分析算法也不尽相同。句法分析的类型可以分为两类：</p><ul><li><strong>短语结构分析</strong>(Phrase parsing)<ul><li>完全句法分析（Full parsing)—&gt;针对整个句子</li><li>局部句法分析（Partial parsing)—&gt;针对句子中的一部分</li></ul></li><li><strong>依存句法分析</strong></li></ul><p>由于短语结构语法（特别是上下文无关语法）应用得最为广泛，因此以短语结构树为目标的句法分析器研究得最为彻底。很多其他形式语法对应的句法分析器都可以通过对短语结构语法的句法分析器进行简单的改造得到。</p><p>人们在正常交流中所使用的语言，放在特定的环境下看，一般是没有歧义的，否则人们将无法交流（某些特殊情况如幽默或双关语除外）。如果不考虑语言所处的环境和语言单位上下文，将发现语言的歧义现象无所不在。但是，一般来说，语言单位的歧义现象在引入更大的上下文范围或者语言环境时总是可以被消解的。<strong>句法分析的核心任务就是消解一个句子在句法结构上的歧义。</strong>例如，对于下面的句子,可以得到两种句法分析结果。</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180708/ki1Kg7BG9i.jpg?imageslim" alt="mark"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180708/1a3Fm3k3F1.jpg?imageslim" alt="mark"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180708/a4gch1JFK7.jpg?imageslim" alt="mark"><br></div><h2 id="句法分析策略"><a href="#句法分析策略" class="headerlink" title="句法分析策略"></a>句法分析策略</h2><p>目前，比较成熟的句法分析主要是基于上下文无关的分析方法，本文也主要介绍基于上下文无关的句法分析方法。句法分析策略指的是句法分析的过程中按照何种方式进行分析，通常采用的策略有：</p><ul><li>自顶向下分析法；</li><li>自底向上分析法；</li><li>左角分析法；</li><li>其他策略。</li></ul><p>常见的上下文无关语法的句法分析算法有：</p><ul><li>线图分析法（Chart parsing）</li><li>CYK算法；</li><li>Earley算法；</li><li>Tomita算法；</li><li>…</li></ul><p>句法分析的过程也可以理解为句法树的构造分析过程。所谓自顶向下分析法也就是先构造句法树的根节点，再逐步向下扩展，直到叶结点；所谓自底向上分析法就是先构造句法树的叶节点，再逐步上上合并， 直到根节点。左角分析法是一种自顶向下和自底向上相结合的方法。</p><p>自顶向下的方法又称为基于预测的方法，也就是说，这种方法产生对后面将要出现的成分的预期，然后再通过逐步吃进待分析的字符串来验证预期。如果预期得到了证明，就说明待分析的字符串可以被分析为所预期的句法结构。如果某一个环节上出现了差错，那就要用另外的预期来替换（即回溯）。如果所有环节上所有可能的预期都被吃进的待分析字符串所“反驳”，那就说明待分析的字符串不可能是一个合法的句子，分析失败。</p><p>自底向上的方法也叫基于规约的方法。就是说，这种方法是先逐步吃进待分析字符串，把它们从局部到整体层层规约为可能的成份。如果整个待分析字符串被规约为开始字符号S，那么分析成功。如果在某个局部证明不可能有任何从这里把整个待分析字符串规约为句子的方案，那么就需要回溯。</p><p>例如，对于下面的一个句子及语法规则：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180709/FCEgBEkA5E.jpg?imageslim" alt="mark"></p><p><strong>采用自顶向下分析出句法结构的步骤如下：</strong><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180709/GbBBLb2GgI.jpg?imageslim" alt="mark"><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180709/cLf5KJ2BIa.jpg?imageslim" alt="mark"><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180709/63kalm0J86.jpg?imageslim" alt="mark"><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180709/KG8CKf8dbh.jpg?imageslim" alt="mark"><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180709/1GfmDG7agJ.jpg?imageslim" alt="mark"><br>自底向上分析法及左角分析法与自顶向下分析方法类似，具体就不一一在举例说明。</p><h2 id="句法分析算法"><a href="#句法分析算法" class="headerlink" title="句法分析算法"></a>句法分析算法</h2><p>基于上下文无关的句法分析有多种，本文简要介绍线图分析算法、CYK算法和概率上下文无关算法PCFG。</p><h3 id="线图-Chart-分析算法"><a href="#线图-Chart-分析算法" class="headerlink" title="线图(Chart)分析算法"></a>线图(Chart)分析算法</h3><p>线图分析法同样有3种策略来构建句法分析树，分别是：</p><ul><li>自底向上；</li><li>自顶向下；</li><li>从上到下和从下到上结合。</li></ul><p>本文介绍基于自底向上的Chart分析算法。<strong>算法的具体情况如下</strong>：</p><ul><li>给定一组CFG规则：$XP \rightarrow \alpha_1…\alpha_n(n \geq 1)$</li><li>给定一个句子的词性序列：$S=W_1W_2…W_n$</li><li>构造一个线图：一组结点和边的集合<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180710/a5Eikhlm8e.jpg?imageslim" alt="mark"></li><li>建立一个二维表：记录每一条边的起始位置和终止位置。</li></ul><p><strong>执行如下操作：</strong>查看任意相邻几条边上的词性串是否与某条重写规则的右部相同，如果相同，则增加一条新的边跨越原来的相应的边，新增加边上的标记为这条重写规则的头（左部）。重复这个过程，直到没有新的边产生。</p><p>线图分析法中引入点规则，表示规则右部被规约（匹配）的程度。点规则的具体定义如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180710/2ag1GJb3gI.jpg?imageslim" alt="mark"><br>线图分析法中用到如下数据结构：</p><ul><li><strong>线图(Chart)</strong>:保存分析过程中已经建立的成份（包括终结符和非终结符）、位置（包括起点和终点）。通常以n*n的数组表示（n为句子包含的词数）；</li><li><strong>代理表（代处理表）（Agenda)</strong>:记录刚刚得到的一些重写规则所代表的成分，这些重写规则的右端符号串与输入的词性串（或短语标志串）中的一段完全匹配，通常以栈或线性队列表示；</li><li><strong>活动边集（ActiveArc）</strong>:记录那些右端符号串与输入串的某一段相匹配，但还未完全匹配的重写规则，通常以数组或列表存储。</li></ul><p>基于如上定义的点规则和数据结构，<strong>线图分析法的算法描述如下</strong>：<br>从输入串的起始位置到最后位置，循环执行如下步骤：</p><p><strong>(1)</strong> 如果待处理表（Agenda）为空，则找到下一个位置上的词，将该词对应的（所有）词类X附以(i,j)作为元素放到待处理表中，即X(i,j)。其中，i,j分别是该词的起始位置和终止位置，$j&gt;i$,j-i为该词的长度。</p><p><strong>(2)</strong> 从Agenda中取出一个元素X(i,j)</p><p><strong>(3)</strong> 对于每条规则$A \rightarrow X\gamma$，将$A \rightarrow X \cdot \gamma(i,j)$加入到活动边集ActiveArc中，然后调用<strong>扩展弧子程序</strong></p><p><strong>扩展弧子程序</strong>:</p><p><strong>(a)</strong> 将X插入图表(Chart)的(i，j)位置中。</p><p><strong>(b)</strong> 对于活动边集(ActiveArc)中每个位置为$(k,i)(i \leq k &lt; i)$的点规则，如果该规则具有如下形式：$A \rightarrow \alpha \cdot X$，如果A=S,则把S(1,n+1)加入到Chart中，并给出一个完整的分析结果；否则，将A(k,j)加入到Agenda表中。</p><p><strong>(c)</strong> 对于每个位置为(k,i)的点规则:$A \rightarrow \alpha \cdot X \beta$，则将$A \rightarrow \alpha X \cdot \beta(k,j)$加入到活动边集中。</p><p><strong>整体的流程大致可以描述为：从Agenda弹出元素，如果Agenda为空，则找下一个位置上的词，形成X(i,j)放入到Agenda中，然后对X(i,j)匹配所有的规则，形成点规则，加入到ActiveArc中，并扩展弧。扩展弧操作即将X(i,j)加入到Chart中，然后，检查活动边集上位置为(k,i)的点规则，进行合并，将合并出的点规则加入到ActiveArc中，并将(k,j)加入到Agenda表中，然后从Agenda中弹出元素，匹配规则，加入到Archive，然后在扩展弧，这样不断循环。</strong></p><p>线图分析法的优点是算法简单，容易实现，开发周期短；但是，其算法效率低，时间复杂度为$Kn^3$且需要高质量的规则，分析结果与规则质量密切相关；也难以区分歧义结构。</p><h3 id="CYK算法"><a href="#CYK算法" class="headerlink" title="CYK算法"></a>CYK算法</h3><p>CYK算法的全称是Coke-Younger-Kasami(CYK)算法，是一种自下而上的分析方法，其构造一个(n+1)*(n+1)的识别矩阵，n为输入句子长度。假设输入句子为$x=w_1w_2…w_n,w_i$为构成句子的单词，$n=|x|$。识别矩阵具有如下特点：</p><ul><li>方阵对角线以下全部为0；</li><li>主对角线以上的元素由文法G的非终结符（句法成分）构成；</li><li>主对角线上的元素由输入句子的终结符构成（单词）构成；</li></ul><p>下图为识别矩阵的示例图：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/AI50jLdhGl.jpg?imageslim" alt="mark"></p><p><strong>识别矩阵构造的步骤如下：</strong></p><p><strong>(1)</strong> 首先构造主对角线，另$t_{0,0}=0$，然后，从$t_{1,1}$到$t_{n,n}$在主对角线的位置上依次放入输入句子x的单词$w_i$。</p><p><strong>(2)</strong> 构造主对角线上紧靠主对角线的元素$t_{i,i+1}$，其中，$i=0,1,2,…,n-1$。对于输入句子$x=w_1w_2…w_n$，从$w_1$开始分析。<br>如果在文法G的产生式集中有一条规则：$A \rightarrow w_1$，则$t_{0,1}=A$。以此类推，如果有$A \rightarrow w_{i+1}$则$t_{i,i+1}=A$。即，对于主对角线的每一个终结符$w_i$，所有可能推导出它的非终结符写在它的右边主对角线方的位置上。</p><p><strong>(3)</strong> 按平行于主对角线的方向，一层一层地向上填写矩阵的各个元素$t_{i,j}$，其中，$i=0,1,…,n-d,j=d+i, d=2,3,…,n$。如果存在一个正整数k，$i+1 \leq k \leq j-1$，在文法G的规则集中有产生式$A \rightarrow BC$，并且，$B \in t_{i,k},C \in t_{k,j}$，那么，将A写到矩阵$t_{i,j}$位置上。</p><p>算法的示意图如下：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/l8DBdl07il.jpg?imageslim" alt="mark"></p><p>下面就用具体的实例，来看CYK算法是如何进行句法分析的，对于给定的如下文法：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/BE3DGbfd10.jpg?imageslim" alt="mark"></p><p>分析句子”他喜欢读书”的句法结构。</p><p><strong>(1)</strong> 首先，进行分词和词性标注：</p><blockquote><p>他/P喜欢/V 读/v 书/N  n=4</p></blockquote><p><strong>(2)</strong> 构造识别矩阵，并执行分析过程，具体如下：</p><p>首先，由规则$VP \rightarrow V V$，产生如下结果：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/24bKJmLm2f.jpg?imageslim" alt="mark"></p><p>然后，继续执行分析过程，将没有匹配的直接复制到右边或者上面一个位置，如下图的P和N：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/EJHJ8mjdjm.jpg?imageslim" alt="mark"></p><p>然后，由规则$S \rightarrow P VP$，由于S不在最右上角的位置，说明匹配失败，不能执行这条规则的匹配，如下图：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/3LL9Fe7Cc6.jpg?imageslim" alt="mark"></p><p>接着，可以执行规则$VP \rightarrow VP N$，如下图：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/IlkJgmke86.jpg?imageslim" alt="mark"></p><p>然后，将没有匹配的直接复制到右边或上面一个位置，结果如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/HAFcGAHceF.jpg?imageslim" alt="mark"></p><p>最后，匹配规则$S \rightarrow P  VP$，完成句法分析过程：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/hAIIhHE8g5.jpg?imageslim" alt="mark"></p><p>最终，句法分析的结果如下：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/e9Jk6bc6F0.jpg?imageslim" alt="mark"></p><p>CYK算法要比Chart算法要容易理解一些，其优点是：简单易行，执行效率高；但是，必须对文法进行范式化处理，且无法区分歧义。</p><h3 id="概率上下文无关文法"><a href="#概率上下文无关文法" class="headerlink" title="概率上下文无关文法"></a>概率上下文无关文法</h3><p>上面介绍的句法分析算法都是基于规则的句法分析方法，其具有一定的缺陷：</p><ul><li>很难穷尽所有的句法规则；</li><li>很难保证规则间的一致性；</li><li>通常只能处理有限的比较规范的句子；</li><li>对真实语料的处理能力不够；</li><li>分析效率通常不高；</li></ul><p>概率上下文无关文法，PCFG即Probabilistic CFG，直观的意义就是基于概率的短语结构分析。乔姆斯基的短语结构文法表示为一个四元组：G=(X,V,S,R)。而PCFG中增加了一个概率信息，变为五元组PCFG=(X,V,S,R,P)，其中：</p><ul><li>X：是一个有限词汇的集合(词典)。它的元素被称为词汇或终结符；</li><li>V：是一个有限标注的集合，叫做非终结符集合。它的元素称为变量或非终结符；</li><li>$S \in V$：称为文法的开始符号；</li><li>R：是一个有序偶对$(\alpha, \beta)$的集合，也就是产生的规则集；</li><li>P：代表每个产生规则的统计概率。</li></ul><p>如果把形如”-&gt;”看作一个运算符，PCFG可以写成如下形式：</p><p><strong>形式</strong>： $A-&gt;\alpha, P$</p><p><strong>约束</strong>： $\sum_{\alpha} P(A-&gt;\alpha)=1$</p><p>比如，对于如下给定的规则集：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/Dadc8fb2LH.jpg?imageslim" alt="mark"></p><p>对于给定句子 S: <strong>Astronomers saw stars with ears。</strong>采用基于规则的句法分析会分析出如下两种结果：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/3m42gjFAA9.jpg?imageslim" alt="mark"></p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/Hfi3614b0j.jpg?imageslim" alt="mark"></p><p>利用PCFG能够计算出$t_1$和$t_2$产生的概率。最后，选择概率最大的作为句法分析的结果。PCFG计算分析树的概率作出了三个基本假设：</p><ul><li><p><strong>位置不变性</strong>：子树的概率与其管辖的词在整个句子中所处的位置无关，即对于任意的k,$p(A_{k(k+C)} \rightarrow w)$一样。</p></li><li><p><strong>上下文无关性</strong>： 子树的概率与子树管辖范围以外的词无关，即$p(A_{kl} \rightarrow w| 任何超出$k~l$范围的上下文)=p(A_{kl} \rightarrow w)$。</p></li><li><strong>祖先无关性</strong>：子树的概率与推导出该子树的祖先结点无关，即$p(A_{kl}\rightarrow w|任何除A以外的祖先结点)=p(A_{kl}\rightarrow w)$。</li></ul><p>下图为$t_1$基于三种基本假设下的计算过程：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/IE5mJ2jj8k.jpg?imageslim" alt="mark"></p><p>依据三个基本假设，可以得到$t_1$的产生的概率：</p><div></div><p>$$\begin{align}<br>P(t_1)&amp;=S*NP*VP*V*NP*NP*PP*P*NP\\&amp;=1.0*0.1*0.7*1.0*0.4*0.18*1.0*1.0*0.18\\&amp;= 0.000 9072\\<br>P(t_2)&amp;=S*NP*VP*VP*VP*V*NP*PP*P*NP\\&amp;=1.0*0.1*0.3*0.7*1.0*0.18*1.0*1.0*1.0*0.18\\&amp;=0.000 6804<br>\end{align}<br>$$</p><p>因此，对于给定的句子S，两棵句法分析树的概率不等$P(t_1)&gt;P(t_2)$，因此，可以得出结论：分析结果$t_1$正确的可能性大于$t_2$。</p><p>根据上述的例子，可以很自然地想到关于PCFG算法的如下三个基本问题：</p><p>(1) 给定句子$W=w_1w_2…w_n$和PCFG G，<strong>如何快速计算$p(W|G)$?</strong></p><p>(2) 给定句子$W=w_1w_2…w_n$和PCFG G，<strong>如何快速地选择最佳句法结构树？</strong></p><p>(3) 给定句子$W=w_1w_2…w_n$和CFG G，<strong>如何调节G的参数，使得$p(w|G)$最大？</strong></p><h3 id="内向算法与外向算法"><a href="#内向算法与外向算法" class="headerlink" title="内向算法与外向算法"></a>内向算法与外向算法</h3><p>内向算法与外向算法是解决第一个问题——计算句子的句法树概率的方法。假设文法G(S)的规则只有两种形式：<br>$$<br>A \rightarrow w, w\in V_{T}\\<br>A \rightarrow BC,B,C \in V_{N}<br>$$<br>可以通过规范式化处理，使CFG规则满足上述形式。这种假设的文法形式称为乔姆斯基范式（Chomsky Normal Form）。</p><p><strong>内向算法</strong>的基本思想是从给定字符串的底层向上逐次推导出句子的全部概率。利用动态规划法计算非终结符A推导出的某个字串片段$w_1w_{i+1}…w_j$的概率$\alpha_{ij}(A)$。语句$W=w_1w_2…w_n$的概率即为文法G(S)中S推导出的子串概率$\alpha_{1n}(S)$。</p><p>定义： 内向变量$\alpha_{ij}(A)$是由非终结符A推导出的语句W中子字串$w_iw_{i+1}…w_j$的概率：<br>$$<br>\alpha_{ij}(A) = p(A \rightarrow w_iw_{i+1}…w_j)<br>$$<br>计算$\alpha_{ij}(A)$的递推公式：<br>$$\begin{align}<br>&amp;\alpha_{ii}(A) = p(A \rightarrow w_i)\\<br>&amp;\alpha_{ij}(A) = \sum_{B,C\in V_N}\sum_{i \leq k \leq j}p(A \rightarrow BC)\alpha_{ik}(B)\alpha_{(k+1)j}(C)<br>\end{align}<br>$$</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/I6e3JhJI9F.jpg?imageslim" alt="mark"></p><ul><li>当$i==j$时，字符串$w_iw_{i+1}…w_j$只有一个字$w_{ii}$，可以简单记作$w_i$，由A推导出$w_i$的概率就是产生式$A \rightarrow w_i$的概率$p(A \rightarrow w_i)$;</li><li>当$i \neq j$时，也就是说，字符串$w_iw_{i+1}…w_j$至少有两个词，根据约定，A要推导出该词串，必须首先运用产生式$A \rightarrow BC$，那么，可用B推导出前半部$w_i…w_k$，用C推导出后半部$w_{k+1}…w_j$。由这一推导过程产生$w_iw_{i+1}…w_j$的概率为：$p(A \rightarrow BC)\alpha_{ik}(B)\alpha_{(k+1)j}(C)$。考虑到B、C和k取值的任意性，应计算各个概率的总和。</li></ul><p>内向算法的描述如下：</p><p><strong>输入：</strong>文法G(S)，语句$W=w_1w_2…w_n$</p><p><strong>输出：</strong>$p(S\rightarrow w_1w_2…w_n)$</p><p>(1) 初始化：$\alpha_{ij}(A) = p(A \rightarrow w_i)  A\in V_N,  1 \leq i \leq j \leq n$</p><p>(2) 归纳计算：$j=1…n,i=1…n-j$，重复下列计算：<br>$$<br>\alpha_{ij}(A) = \sum_{B,C\in V_N}\sum_{i \leq k \leq i+j}p(A \rightarrow BC)\alpha_{ik}(B)\alpha_{(k+1)j}(C)<br>$$</p><p>(3) 终结：$p(S\rightarrow w_1w_2…w_n)=\alpha_{1n}(S)$</p><p><strong>外向算法</strong>的基本思想是从给定字符串的顶层向下逐次推导出句子的概率。</p><p>定义：外向变量$\beta_{ij}(A)$是由文法初始符号S推导出语句$W=w_1w_2…w_n$的过程中，到达扩展符号串$w_1…w_{i-1}Aw_{j+1}…w_n$的概率$A=w_i…w_j$:<br>$$<br>\beta_{ij}(A) = p(S\rightarrow w_1…w_{i-1}Aw_{j+1}…w_n)<br>$$<br>同样，$\beta_{ij}(A)$可由动态规划算法求得，其递推公式为：<br>$$<br>\begin{align}<br>    &amp;\beta_{1n}=\delta(A,S)     (初始化) \\<br>    &amp;\beta_{ij}(A)=\sum_{B,C}\sum_{k&gt;j} \beta_{ik}(B)p(B \rightarrow AC)\alpha_{(j+1)k}(C)+\sum_{B,C}\sum_{k_i}\beta_{kj}(B)p(B\rightarrow CA)\alpha_{k(i-1)}(C)<br>\end{align}<br>$$</p><p>具体的解释如下：</p><p>(1) 当$i=1,j=n$时，即$w_iw_{i+1}…w_j$是整个语句W时，根据乔姆斯基语法范式的约定，不可能有规则$S \rightarrow A$，因此，由S推导出W的过程中，如果$A \neq S$的话，A推导出W的概率为0，即$\beta_{1n}(A)=0$。如果$A=S,\beta_{1n}(A)$为由初始符S推导出W的概率，因此，$\beta_{1n}(A)=1$</p><p>(2) 当$i\neq 1$或者$j \neq n$时，如果在S推导出W的过程中出现字符串$w_1…w_{i-1}Aw_{j+1}…w_n$，则推导过程必定使用规则$B \rightarrow AC$或$B \rightarrow CA$。假定运用了规则$B \rightarrow AC$推导出$w_i…w_j w_{j+1}…w_k$，则该推导可以分解为以下三步：</p><ul><li>由S推导出$w_1…w_{i-1}Bw_{k+1}…w_n$，其概率为$\beta_{ik}(B)$;</li><li>运用产生式$B \rightarrow AC$扩展非终结符B，其概率为$p(B \rightarrow AC)$;</li><li>由非终结符C推导出$w_{j+1}…w_k$，其概率为$\alpha_{(j+1)k}(C)$。</li></ul><p>考虑到B、C和k的任意性，在计算$\beta_{ik}(B)$时，必须考虑所有可能的B、C和k，因此，计算概率时必须要考虑所有情况下的概率之和。同样方法，可以计算出运用产生式$B \rightarrow CA$推导出$w_i…w_jw_{j+1}…w_k$的概率。</p><p>其示意图如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180714/hFH7fJlgeD.jpg?imageslim" alt="mark"></p><p>外向算法描述如下：</p><p><strong>输入：</strong>PCFG G=(S,N,T,P)，语句$W=w_1w_2…w_n$<br><strong>输出：</strong>$\beta_{ij},A,A\in N, 1\leq i \leq j \leq n$</p><p>(1) 初始化： $\beta_{1n}(A) = \delta(A,S),A \in N$;<br>(2) 归纳： j从n-1到0，i从1到n-j，重复计算：<br>$$<br>\beta_{i(i+j)}(A) = \sum_{B,C}\sum_{i+j\leq k \leq n}p(B \rightarrow AC)\alpha_{(i+j+1)k}(C)\beta_{ik}(B)+\sum_{B,C}\sum_{1 \leq k \leq i}p(B \rightarrow CA)\alpha_{k(i-1)}(C)\beta_{k(i+j)}(B)<br>$$<br>(3) 终结： $p(S \rightarrow w_1w_2…w_n)=\beta_{1n}(S)$</p><h3 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h3><p>第二个问题是如何选择最佳的句法树，其解决方法是使用Viterbi算法，在短语结构文法中，Viterbi的定义如下：</p><ul><li>Viterbi变量的$\gamma_{ij}(A)$是由非终结符A推导出语句W中子字串$w_iw_{i+1}…w_j$的最大概率;</li><li>变量$\psi_{i,j}$用于记忆字串$W=w_1w_2…w_n$的Viterbi语法分析结果。</li></ul><p><strong>Viterbi算法描述如下</strong>：</p><p><strong>输入</strong>：文法G(S)，语句$W=w_1w_2…w_n$<br><strong>输出</strong>：$\gamma_{1n}(S)$</p><p>(1) 初始化： $\gamma_{ii}A=p(A\rightarrow w_i),A\in V_N, 1\leq i \leq j \leq n$</p><p>(2) 归纳计算： j=1…n, i=1…n-j，重复下列计算：<br>$$<br>\begin{align}<br>&amp;\gamma_{i(i+j)}(A) = max_{B,C\in V_N;i\leq k \leq i+j}p(A\rightarrow BC)\gamma_{ik}(B)\gamma_{(k+1)(i+j)}(C)\\<br>&amp;\psi_{i(i+j)}(A) = max_{B,C\in V_N;i\leq k \leq i+j}p(A\rightarrow BC)\gamma_{ik}(B)\gamma_{(k+1)(i+j)}(C)<br>\end{align}<br>$$<br>(3) 终结：$p(S\rightarrow w_1w_2…w_n)=\gamma_{1n}(S)$</p><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>如果有大量已经标注语法结构的训练语料，则可以通过语料直接计算每个语法规则的使用次数。已知训练语料中的语法结构，记录每个语法规则的使用次数，使用最大似然估计来计算PCFG的参数。</p><p>一般来说，树库的标注需要投入较大的人力，时间和资金投入都比较大。而且，树库也不可能覆盖所有的词汇信息，在NLP中，使用有限的资源预估目标的概率分布，这种情况是常态。所谓参数估计，就是对未知现象出现概率的一种计算方法。PCFG中常用的方式是EM算法。</p><p>EM算法的基本思想是：初始时随机地给参数赋值，得到语法$G_0$，依据$G_0$和训练语料，得到语法规则使用次数的期望值，以期望次数用于最大似然估计，得到语法参数新的估计值，由此得到新的语法$G_1$，由$G_1$再次得到语法规则的使用次数的期望值，然后又可以重新估计语法参数。循环这个过程，语法参数将收敛与最大似然估计值。</p><p>PCFG中使用的方法称为内向、外向算法：给定CFG G和训练数据$W=w_1w_2…w_n$，语法规则$A\rightarrow BC$使用次数的期望值为：<br>$$<br>\begin{align}<br>Count(A \rightarrow BC) &amp;=\sum_{1\leq i\leq k \leq j \leq n}P(A_{ij},B_{ik},C_{(k+1)j}|w_1…w_n,G)\\&amp;=\frac {1}{p(w_1…w_n|G)}\sum_{1\leq i\leq k \leq j \leq n} p(A_{ij},B_{ik},C_{(k+1)j},w_1,…w_n|G)\\&amp;=\frac {1}{p(w_1…w_n|G)}\sum_{1\leq i\leq k \leq j \leq n} \beta_{ij}(A)p(A\rightarrow BC)\alpha_{ik}(B)\alpha_{(k+1)j}(C)<br>\end{align}<br>$$</p><p>上面的公式的解释：给定了语句$W=w_1…w_n$PCFG G中产生$A\rightarrow BC$在产生的W的过程中被使用次数的期望值为：在所有可能的情况下，即在条件$1\leq i\leq k \leq j \leq n$下，W的语法分析结构中$w_i…w_k$由B导出，$w_{k+1},…w_{j}$由C导出，$w_i…w_j$由A导出的概率总和。</p><p>类似的，语法规则$A\rightarrow a$的使用次数的期望值为：<br>$$<br>\begin{align}<br>   Count(A\rightarrow a) &amp;=\sum_{1 \leq i \leq n} p(A_{ii}|w_1…w_n,G)\\&amp;=\frac {1}{p(w_1…w_n|G)}\sum_{1 \leq i \leq n}p(A_{ii},w_1…w_n|G)\\&amp;= \frac {1}{p(w_1…w_n|G)}\sum_{1 \leq i \leq n}\beta_{ii}(A)p(A\rightarrow a)\delta(a,w_i)<br>\end{align}<br>$$<br>G的参数可由如下公式重新估计：<br>$$<br>\hat p(A\rightarrow \mu)= \frac {C(A\rightarrow \mu)}{\sum_{\mu}C(A \rightarrow \mu)}<br>$$<br>其中，$\mu$要么为终结符号，要么为两个非终结符号串，即$A\rightarrow \mu$为乔姆斯基语法范式要求的两种形式。</p><p><strong>内向、外向算法的步骤如下：</strong></p><p>(1) 初始化：随机地给出$P(A\rightarrow \mu)$的赋值，使得$\sum_{\mu}P(A\rightarrow \mu)=1$，由此得到语法$G_0$,令$i=0$。</p><p>(2) EM算法步骤如下：</p><ul><li>E步：根据$G_i$的公式，计算期望值$Count(A\rightarrow BC)$和$Count(A \rightarrow a)$；</li><li>M步:用E-步所得的期望值，根据公式重新估计$p(A\rightarrow \mu)$，得到$G_(i+1)$</li></ul><p>(3) 循环：i=i+1，重复EM步骤，直至$p(A\rightarrow \mu)$值收敛。</p><h2 id="依存句法分析"><a href="#依存句法分析" class="headerlink" title="依存句法分析"></a>依存句法分析</h2><blockquote><p>这一部分暂时先挖个坑</p></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>宗成庆《自然语言理解》讲义-第9章</li><li>刘群《计算语言学》讲义-第7章</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要是对宗成庆老师《自然语言理解》讲义，第9章的一个学习笔记，同时，也参考了刘群老师《计算语言学》的句法分析讲义，所有的配图均来自这两个讲义。通过该讲义能够对句法分析是做什么的以及怎么做，有一个基本的认识。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理基础-中文分词</title>
    <link href="https://ilewseu.github.io/2018/06/16/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"/>
    <id>https://ilewseu.github.io/2018/06/16/中文分词/</id>
    <published>2018-06-16T08:48:20.000Z</published>
    <updated>2018-06-18T14:14:45.282Z</updated>
    
    <content type="html"><![CDATA[<h2 id="中文分词问题介绍"><a href="#中文分词问题介绍" class="headerlink" title="中文分词问题介绍"></a>中文分词问题介绍</h2><p>词是自然语言中能够独立运用的最小单位，是信息处理的基本单位。自然语言处理的对象是一个个的句子，拿到句子之后一般要对句子进行分词。分词就是利用计算机识别出文本中词的过程。大部分的印欧语言，词与词之间有空格之类的显示标志指示词的边界。因此，利用很容易切分出句子中的词。而与大部分的印欧语言不同，中文语句中词与词之间没有空格标志指示，所以，需要专门的方法去实现中文分词。分词是文本挖掘的基础，通常用于自然语言处理、搜索引擎、推荐等领域。<br><a id="more"></a><br><strong>中文分词的难点主要有</strong>：</p><ul><li><strong>歧义无处不在</strong></li><li><strong>新词层出不穷</strong></li><li><strong>需求多种多样</strong></li></ul><p><strong>(1) 歧义无处不在</strong></p><p>分词的歧义主要包括如下几个方面：</p><ul><li><p><strong>交集型歧义</strong>，例如：</p><p>  研究/生命/的/起源</p><p>  研究生/命/的/起源</p></li><li><p><strong>组合型歧义</strong>，例如：</p><p>  门/把/手/弄/坏/了</p><p>  门/把手/弄/坏/了</p></li><li><p><strong>真歧义</strong>，例如：</p><p>  乒乓球/拍/卖/完了</p><p>  乒乓球/拍卖/完了</p></li></ul><p><strong>(2) 新词层出不穷</strong></p><ul><li>人名、地名、机构名</li><li>网名</li><li>公司名、产品名</li></ul><p><strong>(3) 需求多种多样</strong></p><ul><li>切分速度:搜索引擎vs单集版语音合成</li><li>结果呈现：<ul><li>切分粒度的要求不同；</li><li>分词重点要求不同；</li><li>唯一结果vs多结果；</li><li>新词敏感程度不同；</li></ul></li><li>处理对象：书面文本(规范/非规范)vs口语文本</li><li>硬件平台：嵌入式vs单机版vs服务器版</li></ul><p>本文只介绍分词相关的基本算法原理及思想，其他的暂时不介绍。</p><h2 id="中文分词算法"><a href="#中文分词算法" class="headerlink" title="中文分词算法"></a>中文分词算法</h2><p>目前，中文分词技术已经非常成熟，学者们研究出了很多的分词方法，这些方法大致可以分为三类。第一类是基于字符串匹配的，即扫描字符串，如果发现字符串的子串和词典中的词相同，就算匹配，比如机械分词方法。这类分词方法通常会加入一些启发式规则，例如，正向最大匹配、反向最大匹配、长词优先等。第二类时基于统计的分词方法，它们基于人工标注的词性和统计特征，对中文进行建模，即根据观测到的数据（标注好的语料）对模型参数进行训练，在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词结果作为最终结果。常见的序列标注模型有HMM和CRF。这类分词算法能够很好的处理歧义和未登录词问题，效果要比前一类效果好，但是需要大量的人工标注数据，且分词速度较慢。第三类是理解分词，通过让计算机模拟人对句子的理解，达到识别词的效果。由于汉语语义的复杂性，难以将各种语言信息组织成机器能够识别的形式，目前这种分词系统还处于试验阶段。</p><h3 id="基于字符串匹配的方法"><a href="#基于字符串匹配的方法" class="headerlink" title="基于字符串匹配的方法"></a>基于字符串匹配的方法</h3><p>这一类方法也叫基于词表的分词方法，基本思路是首先存在一份字典，对于要分词的文本从左至右或从右至左扫描一遍，遇到字典里有最长的词就标识出来，遇到不认识的字串就分割成单字词。常见的方法有：</p><ul><li>正向最大匹配法(forward maximum matching method,FMM)</li><li>逆向最大匹配法(backward maximum matching method, BMM)</li><li>N-最短路径方法</li></ul><p><strong>正向最大匹配法</strong>指从左到右对一个字符串进行匹配，所匹配的词越长越好，比如“中国科学院计算机研究所”，按照词典中最长匹配的切分原则切分的结果是：“中国科学院/计算研究所”，而不是”中国/科学院/计算/研究所”。但是，正向匹配会存在一些bad case，常见的例子如：”他从东经过我家”，使用正向最大匹配会得到错误的结果：”他/从/东经/过/我/家”</p><p><strong>逆向最大匹配法</strong>是从右向左倒着匹配，如果能够匹配到更长的词，则优先选择，上面的”他从东经过我家”逆向最大匹配能够得到正确的结果，”他/从/东/经过/我/家”。但是，逆向最大匹配同样也存在bad case：“他们昨日本应该回来”，逆向匹配会得到错误的结果”他们/昨/日本/应该/回来”</p><p>针对正向逆向匹配的问题，将双向切分的结果进行比较，选择切分词语数量最少的结果。但是最少切分结果同样有bad case，比如：”他将来上海”，正确的切分结果是”他/将/来/上海”，有4个词，而最少切分结果“他/将来/中国”只有3个词。</p><p>基于字符串匹配的方法的优点如下：</p><ul><li>程序简单易行，开发周期短；</li><li>没有任何复杂计算，分词速度快；</li></ul><p><strong>不足</strong>：</p><ul><li>不能处理歧义；</li><li>不能识别新词；</li><li>分词准确率不高，不能满足实际的需要；</li></ul><p><strong>N-最短路径方法</strong>的基本思想是：设待分字串$S=c_1c_2…c_n$，其中$c_i(i=1,2,…,n)$为单个字，n为串的长度，$n \geq 1$。建立一个结点数为n+1的切分有向无环图G，各结点编号依次为$V_0,V_1,V_2,…,V_n$。如下图所示：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180616/dlcc2jcgAd.jpg?imageslim" alt="mark"></p><p>其算法如下：</p><p>（1） 相邻结点$v_{k-1},v_k$之间建立有向边$<v_{k-1},v_k>$，边对应的词默认为$c_k(k=1,2,..,n)$</v_{k-1},v_k></p><p>（2） 如果$w=c_ic_{i+1}…c_j(0 &lt; i <j \leq="" n)$是一个词，则结点$v_{i-1},v_j$之间建立有向边，$<v_{i-1},v_j="">$，边对应的词为w。</j></p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180616/6JLiabm6eH.jpg?imageslim" alt="mark"></p><p>（3） 重复上述步骤(2)，直到没有新的路径（词序列）产生。</p><p>（4） 从产生的所有路径中，选择路径最短的（词数最少的）作为最终的分词结果。</p><p>例如：输入的字串：他只会诊断一般的疾病。</p><p>可能的输出： </p><ul><li>他/只会/诊断/一般/的/疾病。</li><li>他/只/会诊/断/一般/的/疾病。</li></ul><p>最终的分词结果为：他/只会/诊断/一般/的/疾病。</p><p>N-最短路径方法，采用的原则（切分出来的次数最少）符合汉语自身的规律；同时，需要的语言资源（词表）也不多。但是，它对许多歧义字段难以区分，最短路径有多条时，选择最终的输出结果缺乏应有的标准。当字符串长度较大和选取的最短路径数增大时，长度相同的路径数急剧增加，选择最终正确的结果困难越来越大。</p><h3 id="基于N-gram语言模型的分词方法"><a href="#基于N-gram语言模型的分词方法" class="headerlink" title="基于N-gram语言模型的分词方法"></a>基于N-gram语言模型的分词方法</h3><p>基于N-gram语言模型的方法是一个典型的生成式模型，早期很多统计分词均是以它为基本模型，然后配合其他未登录词识别模块进行扩展。其基本思想是：首先根据词典对句子进行简单匹配，找出所有可能的词典词，然后将它们和所有单个字作为结点，构造n元切分词图，图中的结点表示可能的此候选，边表示路径，边上的n元概率表示代价，最后利用相关搜索算法从中找到代价最小的路径作为最后的分词结果。</p><p>假设随机变量S为一个汉字序列，W是S上所有可能切分路径，对于分词，实际上就是求解使条件概率$P(W|S)$最大的切分路径$\hat W$，即:<br>$$\hat W=arg max_W P(W|s)$$<br>根据贝叶斯公式：<br>$$\hat W =arg max_W \frac {P(W)P(S|W)}{P(S)} $$<br>由于P(S)为归一化因子，P(S|W)恒为1，因此只需要求解P(W)。P(W)使用N-gram语言模型建模，定义如下(以Bi-gram为例)：<br>$$P(W) = P(w_1w_2…w_T)=P(w_1)P(w_2|w_1)…P(w_T|w_{T-1})$$<br>这样，各种切分路径的好坏程度(条件概率P(W|S))可以求解。简单的，可以根据DAG枚举全路径，暴力求解最优路径；也可以使用动态规划的方法的求解，jieba分词中不带HMM新词发现的分词，就是DAG+Uni-gram语言模型+后向动态规划的方式进行求解的。</p><p>基于N-gram语言模型的分词方法的优点如下：</p><ul><li>减少了很多手工标注知识库（语义词典、规则等）的工作；</li><li>在训练语料规模足够大和覆盖领域足够多的情况下，可以获得较高的切分正确率；</li></ul><p>缺点：</p><ul><li>训练语料的规模和覆盖领域不好把握；</li><li>计算量较大；</li><li>很难进行分词发现；</li></ul><h3 id="基于序列标注的分词方法"><a href="#基于序列标注的分词方法" class="headerlink" title="基于序列标注的分词方法"></a>基于序列标注的分词方法</h3><p>针对基于词典的机械分词所面对的问题，尤其是未登录词识别问题，使用基于统计模型的分词方式能够取得更好的结果。基于统计模型的分词方法，简单来说就是一个序列标注问题。在一句文本中，可以将每个字按照它们在词中的位置进行标注，常用的标记有如下四种：</p><ul><li>B：表示词开始的字；</li><li>M：表示词中间的字；</li><li>E：表示词结尾的字；</li><li>S：表示单字成词</li></ul><p>分词的过程就是序列标注的过程，将一句文本输入，然后得到相应的标记序列，然后根据标记序列进行分词。举例来说“他只会诊断一般的疾病”，经过模型后得到的理想标注序列是：“SBEBEBESBE”，最终还原分词结果是“他/只会/诊断/一般/的/疾病”。<br>在NLP领域中，解决序列标注问题的常见模型主要有HMM模型和CRF模型。</p><h4 id="基于HMM模型分词"><a href="#基于HMM模型分词" class="headerlink" title="基于HMM模型分词"></a>基于HMM模型分词</h4><p>一般而言，一个HMM模型可以用一个5元组表示$\mu=(Q,V,A,B,\pi)$，其中：</p><ul><li>Q：所有可能的状态集合；</li><li>V：所有可能的观测集合；</li><li>A：状态转移矩阵；</li><li>B：发射概率；</li><li>$\pi$：初始概率分布；</li></ul><p>HMM模型的三个基本问题：</p><ul><li><strong>概率计算问题</strong>：给定一个观察序列$O=O_1,O_2,…,O_t$和模型$\mu=(A,B,\pi)$，计算观察序列O的概率；</li><li><strong>学习问题</strong>：给定一个观察序列$O=O_1,O_2,…,O_t$，估计模型$\mu=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\mu)$最大，即用极大似然估计方法估计参数；</li><li><strong>预测问题</strong>：也称为解码问题。已知模型$\mu=(A,B,\pi)$和观测序列$O=O_1,O_2,…,O_t$，求出对于给定观测序列条件概率$P(I|O)$最大的状态序列$I={i_1,i_2,…,i_t}$，即给定观察序列，求最有可能对应的状态序列。</li></ul><p>HMM模型概率计算问题可以通过前向、后向的动态规划算法来求解；学习问题可以通过EM算法求解；预测问题可以通过viterbi算法求解。通过足够的语料数据，可以方便快速地学习HMM模型。</p><p>利用HMM模型求解分词的基本思路是根据观测值（词序列）找到真正的隐藏状态值序列。在分词中，一句文本的每个字符可以看做是一个观测值，而这个字符的标记可以看做是隐藏状态。使用HMM分词，通过对标注语料进行统计，可以得到模型的：初始概率分布、转移概率矩阵、发射概率矩阵、观察值集合、状态值集合。在概率矩阵中，起始概率矩阵表示序列的第一个状态值的概率，在中文分词中，理论上M和E的概率为0,。转移概率矩阵表示状态间的概率，比如B-&gt;M的概率，E-&gt;S的概率等等。而发射概率是一个条件概率，表示当前这个状态下，出现某个字的概率，比如P(人|B)表示在状态为B的情况下”人”字的概率。</p><p>HMM模型的参数从标注数据学习好之后，HMM问题最终转化成求解隐藏状态序列最大值的问题，求解这个问题使用的是Viterbi算法。具体可以参照<a href="http://www.52nlp.cn/itenyh%E7%89%88-%E7%94%A8hmm%E5%81%9A%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E4%B8%80%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%87%86%E5%A4%87" target="_blank" rel="external">Itenyh版-用HMM做中文分词</a>。</p><h4 id="基于CRF模型分词"><a href="#基于CRF模型分词" class="headerlink" title="基于CRF模型分词"></a>基于CRF模型分词</h4><p>CRF也是求解序列标注问题的一种比较常用的模型，和HMM模型不同，CRF是一种判别式模型，CRF模型通过定义条件概率$P(Y|X)$来描述模型。基于CRF的分词方法与传统的分类模型求解很相似，即给定feature（字级别的各种信息）输出label。具体的CRF的基本原理就不具体介绍了。分词所使用的是Linear-CRF，它由一组特征函数组成，包括权重$\lambda$和特征函数$f$，特征函数$f$的输入是整个句子S、当前位置i、前一个字的标记$y_{i-1}$和当前字的标记$y_i$。对于分词这个任务，同样是使用BMES来标记一句话，X表示观测到句子，通过利用CRF模型的解码来，来求出最大的$P(Y|X)$，Y即是BMES组成的序列，然后用Y序列得到实际的分词结果。与HMM模型相比，CRF存在以下优点：</p><ul><li>CRF可以使用输入文本的全局特征，而HMM只能看到输入文本在当前位置的局部特征；</li><li>CRF是判别式模型，直接对序列标注建模，HMM则引入了不必要的先验信息。</li></ul><p><strong>采用HMM模型的分词方法，属于生成式分词，其优点如下</strong>：</p><ul><li>在训练语料规模足够大和覆盖领域足够多的情况下，可以获得较高的切分正确率；</li></ul><p><strong>不足：</strong></p><ul><li>需要很大的训练语料；</li><li>新词识别能力弱；</li><li>解码速度相对较慢；</li></ul><p><strong>基于CRF的分词方法，属于判别式的分词，其优点如下</strong>：</p><ul><li>解码速度快；</li><li>分词精度高；</li><li>新词识别能力强；</li><li>所需学习样本少；</li></ul><p><strong>不足：</strong></p><ul><li>训练速度慢；</li><li>需要高配置的机器训练；</li></ul><h3 id="基于深度学习端到端的分词方法"><a href="#基于深度学习端到端的分词方法" class="headerlink" title="基于深度学习端到端的分词方法"></a>基于深度学习端到端的分词方法</h3><p>基于深度神经网络的序列标注算法在词性标注、命名实体识别问题上取得了不错的结果。词性标注、命名实体识别都属于序列标注问题，这些端到端的方法可以迁移到分词问题上，免去CRF的特征模板配置问题。但与所有深度学习的方法一样，它需要较大的训练语料才能体现优势。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180616/1g3aBAd38j.jpg?imageslim" alt="mark"></p><p>BiLSTM-CRF的网络结构如上图所示，输入层是一个embedding层，经过双向LSTM网络编码，输出层是一个CRF层。下图是BiLSTM-CRF各层的物理含义，可以看见经过双向LSTM网络输出的实际上是当前位置对于各词性的得分，CRF层的意义是对词性得分加上前一位置的词性概率转移的约束，其好处是引入一些语法规则的先验信息。利用此网络结构可以完成序列标注问题，同样分词任务也可以尝试这个思路。</p><p><strong>总结一下：</strong></p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180617/Lf8f5AimlK.jpg?imageslim" alt="mark"></p><h2 id="中文分词工具"><a href="#中文分词工具" class="headerlink" title="中文分词工具"></a>中文分词工具</h2><p>目前，存在多种中文分词工具供使用。比较具有代表性的有jieba分词、SnowNLP、THULAC、NLPIR和LTP，这些分词工具均提供Python库，可以很方便的使用。</p><h3 id="jieba分词"><a href="#jieba分词" class="headerlink" title="jieba分词"></a>jieba分词</h3><p>结巴分词是我比较常用的分词工具，Github库为：<a href="https://github.com/fxsjy/jieba" target="_blank" rel="external">https://github.com/fxsjy/jieba</a>，分词效果较好。它支持三种分词模式：</p><ul><li><strong>精确模式</strong>：试图将句子最精确地切开，适合文本分析；</li><li><strong>全模式</strong>：将句子中所有的可能成词的词语都扫描出来，速度非常快，但是不能解决歧义问题；</li><li><strong>搜索引擎模式</strong>：在精确模式的基础之上，对长词再次切分，提高召回率，适用于搜索引擎分词；</li></ul><p>另外，jieba分词还支持繁体分词，支持用户自定义词典。其使用的算法是基于统计的分词方法，主要有如下几种：</p><ul><li>基于前缀词典实现高效的词图扫描，生成句子中所有可能成词情况构成有向无环图(DAG);</li><li>采用了动态规划查找最大概率路径，找出基于词频的最大切分组合；</li><li>对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法；</li></ul><p>其使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># encoding=utf-8</span></div><div class="line"><span class="keyword">import</span> jieba</div><div class="line"></div><div class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>, cut_all=<span class="keyword">True</span>)</div><div class="line">print(<span class="string">"Full Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 全模式</span></div><div class="line"></div><div class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>, cut_all=<span class="keyword">False</span>)</div><div class="line">print(<span class="string">"Default Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 精确模式</span></div><div class="line"></div><div class="line">seg_list = jieba.cut(<span class="string">"他来到了网易杭研大厦"</span>)  <span class="comment"># 默认是精确模式</span></div><div class="line">print(<span class="string">", "</span>.join(seg_list))</div><div class="line"></div><div class="line">seg_list = jieba.cut_for_search(<span class="string">"小明硕士毕业于中国科学院计算所，后在日本京都大学深造"</span>)  <span class="comment"># 搜索引擎模式</span></div><div class="line">print(<span class="string">", "</span>.join(seg_list))</div></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学</div><div class="line"></div><div class="line">【精确模式】: 我/ 来到/ 北京/ 清华大学</div><div class="line"></div><div class="line">【新词识别】：他, 来到, 了, 网易, 杭研, 大厦    (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)</div><div class="line"></div><div class="line">【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造</div></pre></td></tr></table></figure><p>除了基本的分词功能，jieba分词还提供了关键词抽取、词性标注等功能，具体的使用参照github上的文档。<a href="https://blog.csdn.net/rav009/article/details/12196623" target="_blank" rel="external">Python中文分词模块结巴分词算法过程的理解和分析</a>对于jieba分词的源码进行了解析，有兴趣的可以看一下。</p><h3 id="SnowNLP"><a href="#SnowNLP" class="headerlink" title="SnowNLP"></a>SnowNLP</h3><p>SnowNLP:Simplified Chinese Text Processing，可以方便的处理中文文本内容，是受到TextBlob的启发而写的，由于大部分的自然语言处理库基本上是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现，并且自带了一些训练好的词典。GitHub地址为：<a href="https://github.com/isnowfy/snownlp" target="_blank" rel="external">https://github.com/isnowfy/snownlp</a>。SnowNLP的分词是基于Character-Based Generative Model 来实现的，论文地址：<a href="http://aclweb.org/anthology//Y/Y09/Y09-2047.pdf" target="_blank" rel="external">http://aclweb.org/anthology//Y/Y09/Y09-2047.pdf</a></p><h3 id="THULAC"><a href="#THULAC" class="headerlink" title="THULAC"></a>THULAC</h3><p>THULAC(THU Lexical Analyzer for Chinese)是由清华大学自然语言与社会人文计算实验室推出的一套中文分词工具包。Github的地址为：<a href="https://github.com/thunlp/THULAC-Python" target="_blank" rel="external">https://github.com/thunlp/THULAC-Python</a>，具有中文分词和词性标注功能。THULAC具有如下几个特点：</p><ul><li><strong>能力强</strong>:利用集成的目前世界上规模最大的人工分词和词性标注中文语料（约含5800万字）训练而成，模型标注能力强大；</li><li><strong>准确率高</strong>:该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当；</li><li><strong>速度较快</strong>:同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s；</li></ul><h3 id="NLPIR"><a href="#NLPIR" class="headerlink" title="NLPIR"></a>NLPIR</h3><p>NLPIR 分词系统，前身为2000年发布的 ICTCLAS 词法分析系统，Github地址：<a href="https://github.com/NLPIR-team/NLPIR" target="_blank" rel="external">https://github.com/NLPIR-team/NLPIR</a>，是由北京理工大学张华平博士研发的中文分词系统，经过十余年的不断完善，拥有丰富的功能和强大的性能。NLPIR是一整套对原始文本集进行处理和加工的软件，提供了中间件处理效果的可视化展示，也可以作为小规模数据的处理加工工具。主要功能包括：中文分词，词性标注，命名实体识别，用户词典、新词发现与关键词提取等功能。另外对于分词功能，它有 Python 实现的版本，Github地址为：<a href="https://github.com/tsroten/pynlpir" target="_blank" rel="external">https://github.com/tsroten/pynlpir</a>。</p><h3 id="LTP"><a href="#LTP" class="headerlink" title="LTP"></a>LTP</h3><p>语言技术平台（Language Technology Platform，LTP）是哈工大社会计算与信息检索研究中心历时十年开发的一整套中文语言处理系统。LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于动态链接库（Dynamic Link Library, DLL）的应用程序接口、可视化工具，并且能够以网络服务（Web Service）的形式进行使用。<br>LTP 有 Python 版本，GitHub地址：<a href="https://github.com/HIT-SCIR/pyltp" target="_blank" rel="external">https://github.com/HIT-SCIR/pyltp</a>，另外运行的时候需要下载模型，模型还比较大，下载地址：<a href="">http://ltp.ai/download.html</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前，分词算法已经非常成熟，并且存在很多的分词工具。虽然，不用我们自己去实现各种分词算法，但是还是有必要去了解各种分词算法的原理的。从最初的基于字符串匹配的分词方法到把分词任务转化为序列标注问题，以及将深度学习用于分词等，这些基本的研究方法可以为自然语言处理的其他任务的解决提供了思路，值的我们去学习和思考。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li>中文分词一席谈；</li><li><a href="https://zhuanlan.zhihu.com/p/33261835" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/33261835</a></li><li><a href="http://www.cnblogs.com/sxron/articles/6391926.html" target="_blank" rel="external">http://www.cnblogs.com/sxron/articles/6391926.html</a></li><li>统计自然语言处理-宗庆成</li><li>Lample G, Ballesteros M, Subramanian S, et al. Neural Architectures for Named Entity Recognition[J]. 2016:260-270.</li><li><a href="http://www.52nlp.cn/itenyh%E7%89%88-%E7%94%A8hmm%E5%81%9A%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E4%B8%80%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%87%86%E5%A4%87" target="_blank" rel="external">Itenyh版-用HMM做中文分词</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;中文分词问题介绍&quot;&gt;&lt;a href=&quot;#中文分词问题介绍&quot; class=&quot;headerlink&quot; title=&quot;中文分词问题介绍&quot;&gt;&lt;/a&gt;中文分词问题介绍&lt;/h2&gt;&lt;p&gt;词是自然语言中能够独立运用的最小单位，是信息处理的基本单位。自然语言处理的对象是一个个的句子，拿到句子之后一般要对句子进行分词。分词就是利用计算机识别出文本中词的过程。大部分的印欧语言，词与词之间有空格之类的显示标志指示词的边界。因此，利用很容易切分出句子中的词。而与大部分的印欧语言不同，中文语句中词与词之间没有空格标志指示，所以，需要专门的方法去实现中文分词。分词是文本挖掘的基础，通常用于自然语言处理、搜索引擎、推荐等领域。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理基础-条件随机场(Conditional Random Fields,CRFs)学习笔记</title>
    <link href="https://ilewseu.github.io/2018/05/21/CRF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://ilewseu.github.io/2018/05/21/CRF学习笔记/</id>
    <published>2018-05-21T12:57:20.000Z</published>
    <updated>2018-06-18T15:05:17.120Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇笔记来自李航老师的《统计学习方法》一书</p></blockquote><p>条件随机场最早由Lafferty等人2001年提出，其模型思想的主要来源是最大熵模型，模型的三个基本问题的解决用到了HMMs模型中的方法如forward-backward和Viterbi。我们可以把条件随机场看成是一个无向图模型或马尔科夫随机场，它是一种用来标记和切分序列化数据的统计模型。<a id="more"></a>该模型在给定需要标记观察序列的条件下，计算整个标记序列的联合概率，而不是在给定当前状态条件下，定义一个状态的分布。标记序列的分布条件属性，可以让CRFs很好的拟合现实数据，而在这些数据中，标记序列的条件概率依赖于观察序列中非独立的、相互作用的特征，并通过赋予特征不同权值来表示特征的重要程度。</p><h2 id="概率无向图模型"><a href="#概率无向图模型" class="headerlink" title="概率无向图模型"></a>概率无向图模型</h2><h3 id="概率图模型-Graphical-Models"><a href="#概率图模型-Graphical-Models" class="headerlink" title="概率图模型(Graphical Models)"></a>概率图模型(Graphical Models)</h3><p>概率图模型是一类用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的集合。图中的节点表示随机变量，缺少边表示条件独立假设。根据图中边有无方向，分为有向图和无向图。<strong>概率无向图</strong>，又称为马尔科夫随机场，是一个可以由无向图表示的联合概率分布。</p><p>图是由节点以及连接节点的边组成的集合。节点和边分别记作为v和e，节点和边的集合分别记作V和E，图记作G=(V,E)。无向图是指边没有方向的图。概率图模型是由图表示的概率分布。设有联合概率分布$P(Y),Y \in \cal Y$是一组随机变量。由无向图G=(V,E)表示概率分布P(Y)，即在图G中，结点$v\in V$表示一个随机变量$Y_v，Y=(Y_v)_{v\in V}$；边$e\in E$表示随机变量之间的概率依赖关系。</p><p>给定一个联合概率分布P(Y)和表示它的无向图G。首先定义无向图表示的随机变量之间存在的成对马尔科夫性(pairwise Markov property)、局部马尔科夫性(local Markov property)和全局马尔可夫性(global Markov property)。</p><p><strong>成对马尔科夫性</strong></p><p>设u和v是无向图G中任意两个没有边连接的结点，结点u和v分别对应随机变量$Y_u$和$Y_v$。其他所有结点为O,对应的随机变量组是$Y_O$，成对马尔科夫性是指给定随机变量组$Y_O$的条件下随机变量$Y_u$和$Y_v$是条件独立的，即：<br>$$<br>P(Y_u,Y_v|Y_O)=P(Y_u|Y_O)P(Y_v|Y_O)<br>$$</p><p><strong>局部马尔科夫性</strong></p><p>设$v\in V$是无向图G中任意一个结点，W是与v有边连接的所有结点，O是v，W以外的其他所有结点。v表示的随机变量是$Y_v$，W表示的随机变量是$Y_W$，O表示的随机变量组是$Y_O$。局部马尔科夫性是指在给定的随机变量组$Y_W$的条件下随机变量$Y_v$与随机变量组$Y_O$是独立的，即：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/Chi3EKLL2B.jpg?imageslim" alt="mark"><br></div><br>(注：图引自:<a href="http://www.cnblogs.com/Determined22/p/6915730.html" target="_blank" rel="external">http://www.cnblogs.com/Determined22/p/6915730.html</a>)<br><br>$$P(Y_v,Y_O|Y_W)=P(Y_v|Y_w)P(Y_O|Y_W)$$<br>在$P(Y_O|Y_W)&gt;0$时，等价地：$P(Y_v|Y_W)=P(Y_v|Y_W,Y_O)$<br><br><strong>全局马尔科夫性</strong><br><br>设结点集合A、B是在无向图G中被结点集合C分开的任意结点集合，全局马尔科夫性指：在给定$Y_C$的条件下，$Y_A$和$Y_B$条件独立，即:<br><div><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/22mb282if1.jpg?imageslim" alt="mark"></div><br>$$P(Y_A,Y_B|Y_C) = P(Y_A|Y_C)P(Y_B|Y_C)$$<br><br>上述成对的、局部的、全局的马尔科夫性定义是等价的。<br><div></div><h2 id="概率无向图模型定义"><a href="#概率无向图模型定义" class="headerlink" title="概率无向图模型定义"></a>概率无向图模型定义</h2><p>设有联合概率分布P(Y)，由无向图G(V,E)表示，在图G中，结点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率分布P(Y)满足成对、局部和全局马尔科夫性，就称此联合概率分布为概率无向图模型或马尔科夫随机场。<br>以上是<strong>概率无向图模型的定义</strong>，实际上，我们更关心如何求其联合概率分布。对于给定的概率无向图模型，我们希望将整体的联合概率写成若干子联合概率乘积的形式，也就是将联合概率进行因子分解，这样便于模型的学习与计算。事实上，概率无向图模型的最大特点就是易于因子分解，下面介绍这一知识点。</p><p>首先给出无向图中的团和最大团的定义：<br>无向图G中任何两个结点，均有边连接的结点子集称为团(clique)。若C是无向图G的一个团，并且不能再加进任何一个G的结点使其成为一个更大的团，则称此C为最大团(maximal clique)。<br>例如，下图表示由4个结点组成的无向图。图中的2个结点组成的团有5个：${Y_1,Y_2},{Y_2,Y_3},{Y_3,Y_4},{Y_4,Y_2},{Y_1,Y_3}$。有2个最大团：${Y_1,Y_2,Y_3},{Y_2,Y_3,Y_4}$。而${Y_1,Y_2,Y_3,Y_4}$不是一个团，因为$Y_1$和$Y_4$没有边连接</p><div align="center"><br>     <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/B76cDHL286.jpg?imageslim" alt="mark"><br></div><br>将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解(factorization)。给定概率无向图模型，设其无向图为G，C为G上的最大团，$Y_C$表示C对应的随机变量。那么概率无向图的联合概率分布$P(Y)$可以写作图中的所有最大团C上的函数$\Psi_C(Y_C)$的乘积形式，即:<br>$$<br>P(Y) = \frac {1}{Z} \prod_C \Psi_C(Y_C)<br>$$<br>其中，Z是规范化因子(normalization factror)，由公式：<br>$$<br>Z = \sum_Y \prod_C \Psi_C(Y_C)<br>$$<br>计算得出。规范化因子保证P(Y)构成一个概率分布。函数$\Psi_C(Y_C)$称为势函数(potential function)。这里要求势函数$\Psi_C(Y_C)$是严格正的，通常定义为指数函数：$$<br>\Psi_C(Y_C) = exp{-E(Y_C)}<br>$$<br>概率无向图模型的因子分解是由下述定理来保证的。<br><strong>(Hammersley-Clifford 定理)</strong>:概率无向图模型的联合概率分布P(Y)可以表示为如下形式：<br>$$<br>P(Y)=\frac {1}{Z}\prod_C \Psi_C(Y_C) \\<br>Z=\sum_Y \prod_C \Psi_C(Y_C)<br>$$<br>其中，C是无向图的最大团，$Y_C$是C的结点对应的随机变量，$\Psi_C(Y_C)$是C上定义的严格正函数，乘积是在无向图所有的最大团上进行的。<br><div></div><h2 id="条件随机场定义"><a href="#条件随机场定义" class="headerlink" title="条件随机场定义"></a>条件随机场定义</h2><p>条件随机场(conditional random field)是在给定随机变量X条件下，随机变量Y的马尔科夫随机场。这里主要介绍定义在线性链上的特殊的条件随机场，称为线性链条件随机场(linear chain conditional random field)。线性链条件随机场可以用于标注等问题。这时，在条件概率模型P(Y|X)中，Y是输出变量，表示标记序列，X是输入变量，表示需要标注的观测序列。也把标记序列称为状态序列。学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型$\hat P(Y|X)$;预测时，对于给定的输入序列x，求出条件概率$\hat P(y|x)$最大的输出序列$\hat y$。</p><p>首先定义一般的条件随机场，然后定义线性链条件随机场。</p><p>(<strong>条件随机场</strong>)，设X与Y是随机变量，P(Y|X)是在给定X的条件下Y的条件概率分布。若随机变量Y构成一个由无向图G=(V,E)表示的马尔科夫随机场，即：<br>$$P(Y_v|X, w\neq v)=P(Y_v|X,Y_w, w \sim v)$$<br>对任意结点v成立，则称条件概率分布P(Y|X)为条件随机场。式中$w \sim v$表示在图G=(V,E)中与结点v有结点v有边连接的所有结点w，$w \neq v$结点v以外的所有结点，$Y_v,Y_u与Y_w$为结点v,u与w对应的随机变量。</p><p>在定义中没有要求X和Y具有相同的结构。现实中，一般假设X和Y有相同的图结构。本书主要考虑无向图为下图所示的线性链的情况，即：<br>$$G=(V={1,2,…,n},E={(i,i+1)}), i=1,2,…, n-1$$<br>在此情况下，$X=(X_1,X_2,…,X_n)，Y=(Y_1,Y_2,…,Y_n)$，<strong>最大团是相邻两个结点的集合</strong>。</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/bLF9b1Bl8m.jpg?imageslim" alt="mark"><br></div><p><strong>线性链条件随机场有下面的定义</strong>：</p><p>设$X=(X_1,X_2,…,X_n)，Y=(Y_1,Y_2,…,Y_n)$均为线性链表示的随机变量序列，若下给定随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔科夫性：<br>$$<br>P(Y_i|X,Y_1,..,Y_{i-1},…,Y_n)= P(Y_i|X, Y_{i-1},Y_{i+1})\\<br>i=1,2,…,n(在i=1和n时只考虑单边)<br>$$<br>则称P(Y|X)为线性链条件随机场。在标注问题中，X表示输入观测序列，Y表示对应的输出标记序列或状态序列。</p><h3 id="参数化形式"><a href="#参数化形式" class="headerlink" title="参数化形式"></a>参数化形式</h3><p>根据Hammersley-Clifford定理,可以给出线性链条件随机场P(Y|X)的因子分解式，各因子是定义在相邻两个结点上的函数。<br><strong>定理(线性链条件随机场的参数化形式)</strong> 设P(Y|X)为线性链条件随机场，则在随机变量X取值为x的条件下，随机变量Y取值为y的条件概率具有如下形式：<br>$$<br>p(y|x)= \frac {1}{Z(x)} exp {\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_{l}s_l(y_i, x,i)}<br>$$<br>其中，<br>$$<br>Z(x)=\sum_y exp ({\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_{l}s_l(y_i, x,i)})<br>$$<br>式中，$t_k和s_l$是特征函数，$\lambda_k和\mu_l$是对应的权值。$Z(x)$是规范化因子，求和是在所有可能的输出序列上进行的。</p><p>上面的两个式子是线性链条件随机场模型的基本形式，表示给定输入序列x，对输出序列y预测的条件概率。$t_k$是定义在边上的特征函数，称为转移特征，依赖于当前和前一个位置，$s_l$是定义在结点上的特征函数，称为状态特征，依赖于当前位置。$t_k和s_l$都依赖于位置，是局部特征函数。通常，特征函数$t_k和s_l$取值为1或0;当满足特征条件时取值为1，否则为0。条件随机场完全由特征函数$t_k,s_l$和对应的权值$\lambda_k,\mu_l$确定。<br>线性链条件随机场也是对数线性模型(log linear model)。</p><h3 id="简化形式"><a href="#简化形式" class="headerlink" title="简化形式"></a>简化形式</h3><p>条件随机场还可以由简化形式表示。注意到条件随机场式中同一特征在各个位置都有定义，可以对同一个特征在各个位置求和，将局部特征函数转化为一个全局特征函数，这样就可以将条件随机场写成权值向量和特征向量的内积形式，即条件随机场的简化形式。</p><p>为简便起见，首先将转移特征和状态特征及其权值用统一的符号表示。设有$K_1$个转移特征，$K_2$个状态特征，$K=K_1+K_2$，记：<br>$$<br>f_k(y_{i-1},y_i, x,i) = \begin{cases}<br>t_k(y_{i-1},y_i, x, i)&amp; k=1,2,…,K_1\\<br>s_l(y_i, x, i)&amp; k=K_1+l; l=1,2,…,K_2<br>\end{cases}<br>$$<br>然后，对转移与状态特征在各个位置i求和，记作：<br>$$f_k(y,x) = \sum_{i=1}^n f_k(y_{i-1}, y_i, x,i), k=1,2,..,K$$<br>用$w_k$表示特征$f_k(y,x)$的权值，即：<br>$$<br>w_k = \begin{cases}<br>   \lambda_k, &amp; k=1,2,…,K_1\\<br>   \mu_l, &amp; k=K_1+l; l=1,2,…,K_2<br>\end{cases}<br>$$<br>于是，条件随机场可以表示为：<br>$$<br>P(y|x) = \frac {1}{Z(x)}exp \sum_{k=1}^K w_kf_k(y,x)\\<br>Z(x) = \sum_y exp \sum_{k=1}^K w_kf_k(y,x)<br>$$<br>若以w表示权值向量，即：<br>$$w = (w_1,w_2,…,w_K)^T$$<br>以F(y,x)表示全局特征向量，即：<br>$$F(y,x) = (f_1(y,x),f_2(y,x),…,f_k(y,x))^T$$<br>则条件随机场可以写成向量w与F(y,x)的内积形式：<br>$$<br>P_w(y|x) = \frac {exp(w\cdot F(y,x))}{Z_w(x)}<br>$$<br>其中，<br>$$Z_w(x) = \sum_y exp(w\cdot F(y,x))$$</p><h3 id="矩阵形式"><a href="#矩阵形式" class="headerlink" title="矩阵形式"></a>矩阵形式</h3><p>条件随机场还可以由矩阵表示。假设$P_w(y|x)$是由内积形式给出的线性链条件随机场，表示对给定观测序列x，相应的标记序列y的条件概率。引进特殊的起点和终点状态标记$y_0=start,y_{n+1}=stop$，这时$P_w(y|x)$可以通过矩阵的形式表示。</p><p>对观察序列x的每个位置i=1,2,…,n+1，定义一个m阶矩阵(m是标记$y_i$取值个数)<br>$$<br>M_i(x) = [M_i(y_{i-1},y_i|x)]\\<br>M_i(y_{i-1},y_i|x)=exp(W_i(y_{i-1},y_i|x))\\<br>W_i(y_{i-1},y_i|x) = \sum_{i=1}^K w_kf_k(y_{i-1},y_i,x,i)<br>$$</p><p>这样，给定观测序列x，标记序列y的非规范化概率可以通过n+1个矩阵的乘积$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$表示，于是，条件概率$P_w(y|x)$是:<br>$$<br>P_w(y|x)=\frac {1}{Z_w(x)}\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)<br>$$<br>其中，$Z_w(x)$为规范化因子，是n+1个矩阵的乘积(start,stop)元素：<br>$$<br>Z_w(x)= (M_1(x)M_2(x)…M_{n+1}(x))_{start,stop}<br>$$<br>注意，$y_0=start与y_{n+1}=stop$表示开始状态与终止状态，规范化因子$Z_w(x)$是以start为起点stop为终点通过状态的所有路径$y_1y_2,…,y_m$的非规范化概率$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$之和。</p><p>这个M矩阵和一阶HMM中的转移概率矩阵，因为链式CRF中只有相邻两个结点之间才有连接边。</p><h2 id="三个问题"><a href="#三个问题" class="headerlink" title="三个问题"></a>三个问题</h2><h3 id="概率计算问题"><a href="#概率计算问题" class="headerlink" title="概率计算问题"></a>概率计算问题</h3><p>条件随机场的概率计算问题是给定条件随机场P(Y|X)，输入序列x和输出序列y，计算条件概率$<br>P(Y_i=y_i|x),P(Y_{i-1}=y_{i-1},Y_i=y_i|x)$以及相应的数学期望问题。为了方便起见，像隐马尔科夫模型那样，引进前向-后向向量，递归地计算以上概率及期望值。像这样的算法称为前向-后向算法。</p><p><strong>前向-后向算法</strong><br>对每个指标$i=0,1,…,n+1$，定义前向向量$\alpha_i(x)$:<br>$$<br>\alpha_0(y|x)=\begin{cases}<br>   1, &amp; y=start \\<br>   0, &amp; 否则<br>\end{cases}<br>$$<br>递推公式为：<br>$$<br>\alpha_i^T(y_i|x)=\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x),i=1,2,…,n+1<br>$$<br>又可以表示为：<br>$$<br>\alpha_i^T(x) = \alpha_{i-1}^T(x)M_i(x)<br>$$<br>$\alpha_i(y_i|x)$表示在位置i的标记是$y_i$并且到位置i的前面部分标记序列的非规范化概率，$y_i$可取的值有m个，所以$\alpha_i(x)$是m维列向量。</p><p>同样，对于每个位置$i=0,1,…,n+1$，定义后向向量$\beta_i(x)$:<br>$$<br>\beta_{n+1}(y_{n+1}|x)=\begin{cases}<br>   1, &amp; y=stop \\<br>   0, &amp; 否则<br>\end{cases}\\<br>\beta_i(y_i|x)=M_{i+1}(y_i,y_{i+1}|x)\beta_{i+1}(y_{i+1}|x),i=1,2,…,n+1<br>$$<br>又可以表示为：<br>$$<br>\beta_i(y_i|x)=M_{i+1}(x)\beta_{i+1}(x)<br>$$<br>$\beta_i(y_i|x)$表示在位置i的标记$y_i$并且从i+1到n的后面标记序列的非规范化概率。</p><p>由前向-后向向量定义不难得到：<br>$$<br>Z(x) = \alpha_n^T(x)\cdot 1 = 1^T \cdot \beta_1(x)<br>$$<br>这里的1是元素均为1的m维列向量。</p><p><strong>概率计算</strong></p><p>按照前向-后向向量的定义，很容易计算标记序列在为位置i是标记$y_i$的条件概率和在位置$i-1$与i的标记$y_{i-1}和y_i$的条件概率。<br>$$<br>\begin{align}<br>&amp;P(Y_i=y_i|x) =\frac {\alpha_i^T(y_i|x)\beta_i(y_i|x)}{Z(x)}\\<br>&amp;P(Y_{i-1}=y_{i-1},Y_i=y_i|x)=\frac {\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i,|x)}{Z(x)}\\&amp;其中，Z(x)=\alpha_n^T(x)\cdot 1<br>\end{align}<br>$$</p><p><strong>期望计算</strong></p><p>利用前向-后向向量，可以计算特征函数关于联合分布P(X,Y)和条件分布P(Y|X)的数学期望。</p><p>特征函数$f_k$关于条件分布P(Y|X)的数学期望是：<br>$$<br>\begin{align}E_{P(Y|X)}[f_k]<br>&amp;= \sum_y P(y|x)f_k(y,x)\\<br>&amp;=\sum_{i=1}^{n+1} \sum_{y_{i-1},y_i}f_k(y_{i-1},y_i,x,i)\frac {\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i,|x)}{Z(x)}\\<br>&amp; k=1,2,…,K<br>\end{align}$$<br>其中，<br>$$<br>Z(x) = \alpha_n^T(x)\cdot 1<br>$$</p><p>假设经验分布$\widetilde P(X)$，特征函数$f_k$关于联合分布P(X,Y)的数学期望是:<br>$$<br>\begin{align}E_{P(X,Y)}[f_k]<br>&amp;=\sum_{x,y}P(x,y)\sum_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\\<br>&amp;=\sum_x \widetilde P(x)\sum_y P(y|x)\sum_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\\<br>&amp;=\sum_x \widetilde p(x)\sum_{i=1}^{n+1}\sum_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)\frac {\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}\\k=1,2,…,K<br>\end{align}<br>$$<br>其中，<br>$$<br>Z(x)=\alpha_n^T(x) \cdot 1<br>$$<br>对于转移特征$t_k(y_{i-1},y_i,x,i),k=1,2,…,K_1$，可以将式中的$f_k$替换成$t_k$；对于状态特征，可以将式中的$f_k$替换为$s_i$，表示为$s_l(y_i,x,u),k=K_1+l,l=1,2,…,K_2$。</p><p>有了上面的公式，对于给定的观测序列x与标记序列y，可以通过一次前向扫描计算$\alpha_i及Z(x)$，通过一次后向扫描计算$\beta_i$，从而计算所有的概率和特征的期望。</p><h3 id="学习方法"><a href="#学习方法" class="headerlink" title="学习方法"></a>学习方法</h3><p>条件随机场在时序数据上的对数线性模型，使用MLE或带正则的MLE来训练。类似于最大熵模型，可以采用改进的迭代尺度法(IIS)和拟牛顿法(如BFGS算法)来训练。</p><p>训练数据${(x^{j},y^{j})}_{j=1}^N$的对数似然函数为：<br>$$<br>\begin{align}<br>L(w) &amp;= L_{\widetilde P}(P_w)\\<br>&amp;=ln \prod_{j=1}^N P_w(Y=y^{(j)}|x^{(j)})\\<br>&amp;=\sum_{j=1}^N ln P_w(Y=y^{(j)}| x^{(j)})\\<br>&amp;=\sum_{j=1}^N ln \frac {exp \sum_{k=1}^K w_kf_k(y^{(j)},x^{(j)}) }{Z_w(x^{(j)})}\\<br>&amp;=\sum_{j=1}^N (\sum_{k=1}^K w_kf_k(y^{(j)},x^{(j)}) - ln Z_w(x^{(j)}) )<br>\end{align}<br>$$</p><p>或者可以写成这样：<br>$$<br>\begin{aligned}L(\textbf w)=L_{\widetilde P}(P_\textbf w)&amp;=\ln\prod_{x,y}P_{\textbf w}(Y=y|x)^{\widetilde P(x,y)}\\&amp;=\sum_{x,y}\widetilde P(x,y)\ln P_{\textbf w}(Y=y|x)\\&amp;=\sum_{x,y}\widetilde P(x,y)\ln \frac{\exp\sum_{k=1}^Kw_kf_k(y,x)}{Z_{\textbf w}(x)}\\&amp;=\sum_{x,y}\widetilde P(x,y)\sum_{k=1}^Kw_kf_k(y,x)-\sum_{x,y}\widetilde P(x,y)\ln Z_{\textbf w}(x)\\&amp;=\sum_{x,y}\widetilde P(x,y)\sum_{k=1}^Kw_kf_k(y,x)-\sum_{x}\widetilde P(x)\ln Z_{\textbf w}(x)\end{aligned}<br>$$</p><p>最后一个等号是因为$\sum_y P(Y=y|x)=1$。顺便求个导：<br>$$<br>\begin{aligned}\frac{\partial L(\textbf w)}{\partial w_i}&amp;=\sum_{x,y}\widetilde P(x,y)f_i(x,y)-\sum_{x,y}\widetilde P(x)P_{\textbf w}(Y=y|x)f_i(x,y)\\&amp;=\mathbb E_{\widetilde P(X,Y)}[f_i]-\sum_{x,y}\widetilde P(x)P_{\textbf w}(Y=y|x)f_i(x,y)\end{aligned}<br>$$<br>似然函数中的$ln Z_w(x)$项是一个指数函数的和的对数的形式。</p><h3 id="预测算法"><a href="#预测算法" class="headerlink" title="预测算法"></a>预测算法</h3><p>条件随机场的预测问题是给定条件随机场P(Y|X)和输入序列（观测序列）x,求条件概率最大的输出序列（标记序列）$y^*$，即对观测序列进行标注。条件随机场预测算法是采用维特比算法。</p><p>由$P_w(y|x)=\frac {exp(w\cdot F(y,x))}{Z_w(x)}$可得：</p><p>$$\begin{align}<br>y^{*}&amp;=arg max_yP_w(y|x)\\<br>&amp;=arg max_y \frac {exp(w \cdot F(y,x))}{Z_w(x)}\\<br>&amp;=arg max_y exp(w \cdot F(y,x))\\<br>&amp;=arg max_y(w \cdot F(y,x))<br>\end{align}$$</p><p>于是，条件随机场预测问题就成为求非规范化概率最大的最优路径问题：<br>$$<br>max_y(w \cdot F(y,x))<br>$$<br>这里，路径表示标记序列，其中，<br>$$\begin{align}<br>&amp;w = (w_1,w_2,…,w_K)^T\\<br>&amp;F(y,x) = (f_1(y,x),f_2(y,x),…,f_K(y,x))^T\\<br>&amp;f_k(y,x)=\sum_{i=1}^n f_k(y_{i-1},y_i, x,i), k=1,2,…,K<br>\end{align}$$</p><p>注意，这时只需计算非规范化概率，而不必计算概率，可以大大提高效率。为了求解路径，将$max_y(w \cdot F(y,x))$写成如下形式：<br>$$max_y \sum_{i=1}^n w\cdot F_i(y_{i-1,y_i,x})$$<br>其中，<br>$$<br>F_i(y_{i-1},y_i,x) = (f_1(y_{i-1},y_i,x,i), f_2(y_{i-1}, y_i, x,i), … , f_K(y_{i-1},y_i,x,i))^T<br>$$<br>是局部特征向量。<br>下面叙述维特比算法，首先求出位置1的各个标记j=1,2,…,m的非规范化概率：<br>$$<br>\delta_1(j) = w \cdot F_1(y_0=start, y_1=j,x), j=1,2,…,m<br>$$<br>一般地，由地推公式，求出到位置i的各个标记l=1,2,..,m的非规范化概率的最大值，同时记录非规范化概率最大值的路径：<br>$$<br>\delta_i(l) = max_{1\le j \le m} {\delta_{i-1}(j)+w \cdot F_i(y_{i-1}=j, y_i=l,x)},l=1,2,…,m\\<br>\Psi_i(l)  = arg max_{1 \le j \le m} { \delta_{i-1}(j) + w \cdot F_i(y_{i-1}=j, y_i=l,x)}, l =1,2,…,,m<br>$$<br>直到i=n时终止。这时求得非规范化概率的最大值为：<br>$$<br>max_y (w \cdot F(y,x)) = max_{1 \le j \le m}\delta_n(j)<br>$$<br>及最优的终点：<br>$$<br>y_n^<em> = \Psi_{i+1}(y_{i+1}^</em>),i = n-1,n-2,…,1<br>$$<br>求得最优路径$y^<em> = (y_1^</em>, y_2^<em>, …, y_n^</em>)^T$<br>维特比算法如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/9eFHIE0ha7.jpg?imageslim" alt="mark"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本篇笔记来自李航老师的《统计学习方法》一书&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;条件随机场最早由Lafferty等人2001年提出，其模型思想的主要来源是最大熵模型，模型的三个基本问题的解决用到了HMMs模型中的方法如forward-backward和Viterbi。我们可以把条件随机场看成是一个无向图模型或马尔科夫随机场，它是一种用来标记和切分序列化数据的统计模型。
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理基础-最大熵模型</title>
    <link href="https://ilewseu.github.io/2018/05/13/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"/>
    <id>https://ilewseu.github.io/2018/05/13/最大熵模型/</id>
    <published>2018-05-13T06:46:20.000Z</published>
    <updated>2018-06-18T14:32:50.143Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><h3 id="熵和条件熵"><a href="#熵和条件熵" class="headerlink" title="熵和条件熵"></a>熵和条件熵</h3><p>熵(entropy)原是一个热力学中的概念，后由香农引入到信息论中。在信息论和概率统计中，熵用来表示随机变量的不确定性的度量。其定义如下：<br><a id="more"></a><br>设$X \in {x_1,x_2,…,x_n}$为一个离散随机变量，其概率分布为$p(X=x_i)=p_i,i=1,2…,n$，则X的熵为：<br>$$<br>H(X)=-\sum_{i=1}^n p_ilogp_i<br>$$<br>其中，当$p_i=0$时，定义$0log0=0$。<br>上式中的对数log以2为底或者以e为底，分别对应bit或nat，熵的两种单位。由熵的公式可知H(X)仅依赖于X的分布，而与X的具体取值无关，因此，常将H(X)记为H(p)。<br><strong>熵的意义：</strong>可以认为熵是描述事物无序性的参数，熵越大则无序性不确定性越大。一个孤立系统的熵，自发性地趋于极大，随着熵的增加，有序状态逐步变为混沌状态，不可能自发的产生新的有序结构。当熵处于最小值，即能量集中程度最高、有效能量处于最大值时，那么整个系统也处于最有序的状态，相反为最无序状态。熵增原理预示着自然界越变越无序。s<br>由熵的计算公式可知熵的取值范围为：<br>$$<br>0 \leq H(X) \leq  log n<br>$$</p><ul><li>左边的等号在X为确定值的时候成立，即X没有变化的可能；</li><li>右边的等号在X为均匀分布的时候成立；</li></ul><p><strong>条件熵的定义</strong>：<br>设$X \in {x_1,x_2,…,x_n},Y \in {y_1,y_2,…,y_n)}$为离散的随机变量。在已知X的条件下，Y的条件熵（conditional entropy）可以定义为：<br>$$<br>H(Y|X)=\sum_i^n p(x_i)H(Y|X=x_i)=-\sum_{i=1}^n p(x_i) \sum_{j=1}^m p(y_j|x_i)log p(y_j|x_i)<br>$$<br>它表示已知X的情况下，Y的条件概率分布的熵对X的数学期望。</p><h2 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h2><p>最大熵的理论基础是熵增理论，即在无外力作用下，事物总是朝着最混乱的方向发展。同时，事物是约束和自由的统一体。事物总是在约束下争取最大的自由权，这其实也是自然界的根本原则。<strong>在已知条件下，熵最大的事物，最可能接近它的真实状态</strong>。<br>最大熵原理是在1957年由E.T.Jaynes提出的，其主要思想是，在只掌握关于未知分布的部分知识时，应该选取符合这些知识但熵值最大的概率分布。因为在这种情况下符合已知知识的概率分布可能不止一个。常在[3]中写道：“我们知道熵定义的实际上是一个随机变量的不确定性，熵最大的时候，说明随机变量最不确定，换句话说，也就是随机变量最随机，对其行为做准确的预测最困难。从这个意义上讲，那么最大熵原理的实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的唯一不偏不倚的选择，任何其他的选择都意味着我们增加了其他的约束和假设，这些约束和假设根据我们掌握的信息无法做出。”<br>吴军老师的《数学之美》第20章中提到了一个掷骰子的例子可以很好的解释最大熵原理。对于一个骰子，每面向上的概率是多少，可能我们会不加思索会说是$\frac {1}{6}$，但是，如果说骰子的其中四点被做过特殊处理，四点向上的概率为$\frac {1}{3}$，那么其他点向上的概率则变为$\frac {2}{15}$。虽然，这是一个很简单的问题，却隐含着最大熵的原理。首先，在骰子没做任何处理之前，我们认为骰子的各个面出现的概率是相同的，即符合均匀分布，此时熵最大。然后，当增加四点被做过特殊处理后，其他面的向上的概率变为$\frac {2}{15}$，在这里四点被做过特殊处理，即所谓的约束条件，满足约束条件之后，而对其它则不做任何假设其他面向上的概率是相同的，即为$\frac {2}{15}$，也是熵最大的。在这个例子中，不作任何假设就是使用“等概率”，这个时候概率分布最均匀，从而使得概率分布的熵最大，即最大熵原理。</p><h2 id="最大熵模型的定义"><a href="#最大熵模型的定义" class="headerlink" title="最大熵模型的定义"></a>最大熵模型的定义</h2><p>最大熵原理是统计学学习的一般原理，将它应用到分类得到最大熵模型。<br>假设分类模型是一个条件概率分布$P(Y|X)$，$X \in \cal X \subseteq R^*$表示输入，$Y \in \cal Y $表示输出，$\cal X和\cal Y$表示输入和输出的集合。这个模型表示的是对于给定的输入X，以条件概率$P(Y|X)$输出Y。<br>给定义一个训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，学习的目标是用最大熵原理选择最好的分类模型。首先，考虑模型应该满足的条件。给定训练数据集，可以确定联合概率分布P(X,)的经验分布和边缘分布P(X)经验分布，分别以$\hat P(X,Y)$和$\hat P(X)$表示，这里：<br>$$<br>\begin{align}<br>    &amp;\hat P(X=x,Y=y)=\frac {v(X=x,Y=y)}{N} \\<br>    &amp;\hat P(X=x)=\frac {v(X=x)}{N}<br>\end{align}<br>$$<br>其中，$v(X=x,Y=y)$表示训练数据中样本(x,y)出现的频数，$v(X=x)$表示训练数据中输入x出现的频数，N表示训练样本的容量。</p><p>用特征函数(feature function)$f(x,y)$描述输入x和输出y之间的某一事实，其定义为：<br>$$<br>f(x,y)=\begin{cases}<br>   1, &amp; x与y满足某一事实 \\<br>   0, &amp; 否则<br>\end{cases}\\<br>$$<br>它是一个二值函数，当x和y满足这个事实时取值为1，否则取值为0。<br>特征函数$f(x,y)$在训练数据集上关于经验分布$\hat P(X,Y)$的期望值，用$E_{\hat p}(f)$表示：<br>$$E_{\hat p}(f) = \sum_{x,y} \hat P(x, y)f(x,y)$$<br>特征函数$f(x,y)$关于模型上关于$p(x,y)$的数学期望为：<br>$$E_{p}(f) = \sum_{x,y}p(x,y)f(x,y)$$<br>$p(y|x)$与经验分布$\hat p(x)$的期望值用$E_p(f)$表示。需要注意的是$p(x,y)$是未知的，而且建模的目标是$p(y|x)$。因此，我们希望将排$p(x,y)$表示成$p(y|x)$的函数。利用贝叶斯定理有，$p(x,y)=p(x)p(y|x)$，但$p(x)$依然是未知的，此时，只好利用$\hat p(x)$来近似了。这样$E_p(f)$可以重新定义为：<br>$$E_p(f) = \sum_{x,y}\hat P(x)P(y|x)f(x,y)$$<br>对于概率分布$p(y|x)$，我们希望特征$f$的期望值应该和从训练数据中得到的特征期望值是一致的，因此，提出约束：<br>$$E_p(f) = E_{\hat p}(f)$$<br>即：<br>$$<br>\sum_{x,y}\hat P(x)P(y|x)f(x,y)=\sum_{x,y}p(x,y)f(x,y)<br>$$<br>假设从训练数据集中抽取了n个特征，相应地，便有n个特征函数$f_i(i=1,2,…,n)$以及n个约束条件：<br>$$<br>C_i:E_p(f_i)=E_{\hat p}(f_i),i=1,2,…,n.<br>$$<br><strong>最大熵模型定义</strong>：<br>利用最大熵原理选择一个最好的分类模型，即对于任意一个给定的输入$x \in \cal X$，可以以概率$p(y|x)$输出$y \in \cal Y$。前面已经讨论了特征函数和约束条件，要利用最大熵原理，还差一个熵的定义，注意，我们的目标是获取一个条件分布，因此，这里也采用相应的条件熵，如下：<br>$$H(p(y|x))= \sum_{x,y} \hat p(x)p(y|x)log p(y|x)$$<br>下文将$H(p(y|x))$简记为$H(p)$，为了后面计算方便，上式中的对数为自然对数，即以e为底。至此，可以给出最大熵模型的完整描述了。对于给定的训练数据T，特征函数$f_i(x,y),i=1,2,…,n$，最大熵模型就是求解：<br>$$<br>\begin{align}<br>&amp;max_{p\in C}   H(p)=(-\sum_{x,y} \hat p(x)p(y|x)log p(y|x)),\\<br>&amp;s.t.       \\<br>&amp;\sum_y p(y|x)=1<br>\end{align}<br>$$<br>或者:<br>$$<br>\begin{align}<br>&amp;min_{p\in C}   -H(p)=(-\sum_{x,y} \hat p(x)p(y|x)log p(y|x)),\\<br>&amp;s.t.       \sum_y p(y|x)=1<br>\end{align}<br>$$<br>则模型集合C中条件熵$H(p)$最大的模型称为最大熵模型。</p><h2 id="最大熵模型的学习"><a href="#最大熵模型的学习" class="headerlink" title="最大熵模型的学习"></a>最大熵模型的学习</h2><p>最大熵模型的学习过程就是最大熵模型的求解过程。最大熵模型的学习可以形式化为约束最优化问题，主要思路和步骤如下：</p><ol><li>利用拉格朗日乘子法将最大熵模型由一个带约束的最优化问题转化为一个与之等价的无约束的最优化问题，它是一个<strong>极小极大问题</strong>（min max)；</li><li>利用对偶问题等价性，转化为求解上一步得到的极小极大问题的对偶问题，它是一个<strong>极大极小问题</strong>（max min）</li></ol><p>在求解内层的极小问题时，可以导出最大熵模型的解具有指数形式，而在求解最外层的极大问题时，还将意外地发现其与最大似然估计的等价性。<br>对于给定的训练数据$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$以及特征函数$f_i(x,y),i=1,2,…n$，最大熵模型的学习等价于约束最优化问题：<br>$$<br>\begin{align}<br>&amp;max_{p\in C}   H(p)=-\sum_{x,y}\hat p(x)p(y|x)logp(y|x)\\<br>&amp;s.t.       E_p(f_i)=E_{\hat p}(f_i),i=1,2,…,n\\<br>&amp;            \sum_yp(y|x)=1<br>\end{align}<br>$$<br>按照最优化问题的习惯，将求<strong>最大值问题转换为求最小值的问题</strong>：<br>$$<br>\begin{align}<br>&amp;min_{p\in C}   -H(p)=\sum_{x,y}\hat p(x)p(y|x)logp(y|x)\\<br>&amp;s.t.       E_p(f_i)-E_{\hat p}(f_i)=0,i=1,2,…,n\\<br>&amp;            \sum_yp(y|x)=1<br>\end{align}<br>$$<br>求解约束最优化问题，所得出的解，就是最大熵模型学习的解，下面给出具体的推导。这里先将约束最优化的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题求解原始问题。<br>首先，引入拉格朗日乘子$\lambda_{0},\lambda_{1},…,\lambda_n$，记为$\lambda=(\lambda_{0},\lambda_{1},…,\lambda_n)^T$，定义拉格朗日函数为$L(p,\lambda)$，则：<br>$$\begin{align}<br>L(p,\lambda)&amp;=-H(p)+\lambda_0(1-\sum_yp(y|x))+\sum_{i=1}^n \lambda_{i}(E_{\hat p}(f_i)-E_p(f_i))\\<br>&amp;=\sum_{x,y}\hat p(x)p(y|x)log p(y|x)+\lambda_0(1-\sum_y p(y|x))+\sum_{i=1}^n(E_{\hat p}(f_i)-E_p(f_i))<br>\end{align}<br>$$<br>利用拉格朗日对偶性，可知最大熵模型等价于求解：<br>$$<br>min_{p\in C} max_{\lambda} L(p,\lambda)<br>$$<br>我们把上式称为<strong>原始问题</strong>（primary problem)极小极大问题；通过交换极大和极小的位置，可以得到原始问题的<strong>对偶问题</strong>（dual problem）为<br>$$max_{\lambda}min_{p\in C}L(p,\lambda)$$<br>由于$H(p)$是关于p的凸函数，因此，原始问题和对偶问题是等价的。这样可以通过求解对偶问题来求解原始问题。首先，对于对偶问题内层的极小问题$min_{p\in C}L(p,\lambda)$是关于参数$\lambda$的函数，将其记为$\psi(\lambda)$，即：<br>$$<br>\psi(\lambda)=min_{p\in C}L(p,\lambda)=L(p_{\lambda},\lambda)<br>$$<br>其中，<br>$$<br>p_{\lambda} = arg min_{p\in C}L(p,\lambda)<br>$$<br>首先，计算拉格朗日函数$L(p,\lambda)$对$p(y|x)$的偏导数。<br>$$\begin{align}<br>\frac {\partial L(p,\lambda)}{\partial p(y|x)}<br>&amp;=\sum_{x,y}\hat p(x)(logp(y|x)+1)-\sum_y \lambda_0 - \sum_{i=1}^n \lambda_i(\sum_{x,y}\hat p(x)f_i(x,y))\\<br>&amp;=\sum_{x,y}\hat p(x)(logp(y|x)+1)-\sum_x \hat p(x)\sum_y \lambda_0 - \sum_{x,y}\hat p(x)\sum_{i=1}^n \lambda_i f_i(x,y)\\<br>&amp;=\sum_{x,y} \hat p(x)(log p(y|x)+1) - \sum_{x,y}\hat p(x)\lambda_0 - \sum_{x,y}\hat p(x)\sum_{i=1}^n \lambda_i f_i(x,y)\\<br>&amp;=\sum_{x,y}\hat p(x)(log p(y|x)+1 - \lambda_0 - \sum_{i=1}^n \lambda_if_i(x,y))<br>\end{align}<br>$$<br>另偏导数等于0，在$\hat p(x)&gt;0$的情况下，进一步，令$\frac {\partial L(p, \lambda)}{\partial p(y|x)}=0$可得：<br>$$<br>log p(y|x)+1 - \lambda_0 - \sum_{i=1}^n \lambda_if_i(x,y)=0<br>$$<br>从而得到：<br>$$p(y|x)=e^{\lambda_0 - 1} \cdot e^{\sum_{i=1}^n \lambda_i f_i(x,y)}$$<br>将上式代入约束条件$\sum_y p(y|x)=1$，即：<br>$$<br>\sum_y p(y|x)=e^{\lambda_0 - 1} \cdot \sum_y e^{\sum_{i=1}^n \lambda_i f_i(x,y)=1}<br>$$<br>可得：<br>$$<br>e^{\lambda_0-1} = \frac {1}{\sum_y e^{\sum_{i=1}^n \lambda_i f_i(x,y)}}<br>$$<br>将上式代入到$p(y|x)=e^{\lambda_0 - 1} \cdot e^{\sum_{i=1}^n \lambda_i f_i(x,y)}$得到：<br>$$<br>p_{\lambda} = \frac {1}{Z_{\lambda}(x)}e^{\sum_{i=1}^n \lambda_if_i(x,y)}<br>$$<br>其中，<br>$$Z_{\lambda}(x)=\sum_{y} e^{\sum_{i=1}^n \lambda_if_i(x,y)}$$<br>称为规范化因子，注意，此时的$\lambda=(\lambda_1,\lambda_2,…,\lambda_n)^T$，不再包含$\lambda_0$了。</p><p>$p_{\lambda}$就是最大熵模型的解，它具有指数形式，其中$\lambda_1,\lambda_2,…,\lambda_n$为参数，分别对应$f_1,f_2,…,f_n$的权重，$\lambda_i$越大，表示特征$f_i$越重要。<br>之后求解对偶问题外部的极大化问题：<br>$$max_{\lambda} \psi (\lambda)$$<br>设其解为：<br>$$<br>\lambda^{*} = argmax_{\lambda} \psi (\lambda)<br>$$<br>则，最大熵模型的解为$p^* = p_{\lambda^*}$<br>为此，首先要给出$\psi(\lambda)$的表达式：<br>$$\begin{align}<br>   \psi(\lambda)<br>   &amp;=L(p_{\lambda},\lambda)\\<br>   &amp;=\sum_{x,y}\hat p(x)p_{\lambda}(y|x)logp_{\lambda}(y|x)+\sum_{i=1}^n \lambda_{i}(E_{\hat p}(f_i) - E_{p}(f_i))\\<br>   &amp;=\sum_{x,y}\hat p(x)p_{\lambda}(y|x)logp_{\lambda}(y|x)+\sum_{i=1}^n \lambda_{i}(\sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_{x,y}\hat p(x)p_{\lambda}(y|x)f_i(x,y))\\<br>   &amp;=\sum_{i=1}^n \lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) + \sum_{x,y} \hat p(x)p_{\lambda}(y|x)(log p_{\lambda}(y|x) - \sum_{i=1}^n \lambda_if_i(x,y))<br>\end{align}<br>$$<br>由于，<br>$$<br>log p_{\lambda}(y|x) = \sum_{i=1}^n \lambda_if_i(x,y) - log Z_{\lambda}(x)<br>$$<br>带入前面的$\psi (\lambda)$可得：<br>$$<br>\begin{align}<br>\psi(\lambda)&amp;=\sum_{i=1}^n\lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_{x,y}\hat p(x)p_{\lambda}(y|x)log Z_{\lambda}(x)\\<br>&amp;=\sum_{i=1}^n\lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_x \hat p(x)log Z_{\lambda}(x)\sum_y p_{\lambda}(y|x)\\<br>&amp;=\sum_{i=1}^n\lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_x \hat p(x)log Z_{\lambda}(x)<br>\end{align}<br>$$<br>有了$\psi(\lambda)$的具体表达式，就可以求解其最大值点$\lambda^*$了。<br>对$\psi (\lambda)$的最大化等价于最大熵模型的极大似然估计。已知训练数据的经验分布为$\hat p(x,y)$，条件概率分布$p(y|x)$的对数似然函数表示为：<br>$$<br>L_{\hat p}(p) = log \prod_{x,y}p(y|x)^{\hat p(x,y)}=\sum_{x,y} \hat p(x,y)logp(y|x)<br>$$<br>将$p_{\lambda}$带入似然函数，可得：<br>$$<br>\begin{align}<br>L_{\hat p}(p_{\lambda})&amp;=\sum_{x,y}\hat p(x,y)log p_{\lambda}(y|x)\\<br>&amp;=\sum_{x,y}\hat p(x,y)(\sum_{i=1}^n \lambda_if_i(x,y) - logZ_{\lambda}(x))\\<br>&amp;=\sum_{x,y}\hat p(x,y)\sum_{i=1}^n \lambda_if_i(x,y) - \sum_{x,y}logZ_{\lambda}(x)\\<br>&amp;=\sum_{i=1}^n \lambda_i(\sum_{x,y} \hat p(x,y)f_i(x,y)) - \sum_{x,y} \hat p(x,y)log Z_{\lambda}(x)\\<br>&amp;=\sum_{i=1}^n \lambda_i \sum_{x,y} \hat p(x,y)f_i(x,y) - \sum_x \hat p(x)log Z_{\lambda}(x)<br>\end{align}<br>$$<br>可以看到公式25和公式30是相等的，这说明最大化$\psi(\lambda)$和最大化似然估计是等价的。<br>最大熵模型最优化方法有多种，比如GIS算法、IIS算法以及梯度下降法等方法，这里就不具体介绍了，具体可参考《统计学习》方法。</p><h2 id="最大熵模型的优缺点"><a href="#最大熵模型的优缺点" class="headerlink" title="最大熵模型的优缺点"></a>最大熵模型的优缺点</h2><p>最大熵模型的优点如下：</p><ul><li>建模时，实验者只需要集中精力选择特征，而不需要花费精力考虑如何使用这些特征；</li><li>特征选择灵活，且不需要额外的独立假定或者内在约束；</li><li>模型应用在不同领域时的可移植性强；</li><li>可结合更丰富的信息；</li></ul><p>缺点：</p><ul><li>时空开销大；</li><li>数据稀疏问题严重；</li><li>对语料的依赖性较强；</li></ul><h2 id="特征引入算法"><a href="#特征引入算法" class="headerlink" title="特征引入算法"></a>特征引入算法</h2><p>在使用最大熵模型的时候，往往会构建很多特征，但并不是所有的特征都是有用的，所以需要作特征选择。尤其对于自然语言处理问题，这些特征的选取往往具有很大的主观性。同时，因为特征数量往往很多，必须引入一个客观标准自动计算以引入特征到模型中，同时针对特征进行参数估计。Della Pietra et al[6]对自然语言处理中随机域的特征选择进行了描述，在进行特征选取时，是由特征的信息增益作为衡量标准的。一个特征对所处理问题带来的信息越多，该特征越适合引入到模型中。一般我们首先形式化一个特征空间，所有可能的特征都为后补特征，然后从这个后补特征集内选取对模型最为有用的特征集合。特征引入的算法如下：</p><p>算法： 特征引入算法<br>输入：候选特征集合F，经验分布$\hat p(x,y)$<br>输出：模型选用的特征集合S，集合这些特征的模型$P_s$</p><ol><li>初始化：特征集合S为空，它所对应的模型$P_s$均匀分布，n=0;</li><li>对于候选特征集合F中的每一个特征$f\in F$，计算加入该特征后为模型带来的增益值$G_f$；</li><li>选择具有最大增益值$G(S,f)$的特征$f_n$;</li><li>把特征$f_n$加入到集合S中，$S=(f_1,f_2,…,f_n)$;</li><li>重新调整参数值，使用GIS算法计算模型$P_s$;</li><li>n=n+1，返回到第2步；</li></ol><p>在算法的第2步中，要计算每个特征的增益值，这里是根据Kullback-Leibler（简称KL）距离来计算的。衡量两个概率分布p和q的KL距离，公式如下：<br>$$<br>D(p||q)  = \sum_x p(x)ln \frac {p(x)}{q(x)}<br>$$<br>距离的大小与两种分布的相似程度成反比，距离越小表示两种分布越逼近。因此，在加入第n个特征前后，求的模型分布与样本分布之间的KL距离为：<br>$$<br>D(\hat p||p^{(n-1)}) = \sum_x \hat p(x) ln \frac {\hat p(x)}{p^{(n-1)}(x)}\\<br>D(\hat p||p^{(n)}) = \sum_x \hat p(x) ln \frac {\hat p(x)}{p^{(n)}(x)}<br>$$<br>这样，我们定义引入第n个特征$f^{(n)}$后的增益值为：<br>$$<br>f^{(n)} = D(\hat p||p^{(n-1)})  - D(\hat p||p^{(n)})<br>$$<br>因此，选择第n个特征为：<br>$$<br>f^{(n)} = arg max_{f \in F}G(p, f^{(n)})<br>$$</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://www.cnblogs.com/pinard/p/6093948.html" target="_blank" rel="external">https://www.cnblogs.com/pinard/p/6093948.html</a>；</li><li><a href="https://blog.csdn.net/itplus/article/details/26550597" target="_blank" rel="external">https://blog.csdn.net/itplus/article/details/26550597</a>；</li><li>自然语言处理的最大熵模型-常宝宝；</li><li>统计学习方法第6章；</li><li>自然语言处理技术中的最大熵模型方法-李素建、刘群等；</li><li>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty, Inducing features of random fields, IEEE Transactions on Pattern Analysis and Machine Intelligence 19:4, pp.380–393, April, 1997</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本知识&quot;&gt;&lt;a href=&quot;#基本知识&quot; class=&quot;headerlink&quot; title=&quot;基本知识&quot;&gt;&lt;/a&gt;基本知识&lt;/h2&gt;&lt;h3 id=&quot;熵和条件熵&quot;&gt;&lt;a href=&quot;#熵和条件熵&quot; class=&quot;headerlink&quot; title=&quot;熵和条件熵&quot;&gt;&lt;/a&gt;熵和条件熵&lt;/h3&gt;&lt;p&gt;熵(entropy)原是一个热力学中的概念，后由香农引入到信息论中。在信息论和概率统计中，熵用来表示随机变量的不确定性的度量。其定义如下：&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>统计语言模型</title>
    <link href="https://ilewseu.github.io/2018/05/07/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>https://ilewseu.github.io/2018/05/07/语言模型/</id>
    <published>2018-05-07T11:06:20.000Z</published>
    <updated>2018-05-12T15:29:18.900Z</updated>
    
    <content type="html"><![CDATA[<h2 id="语言模型概述"><a href="#语言模型概述" class="headerlink" title="语言模型概述"></a>语言模型概述</h2><p>语言模型(Language Model)，就是用来计算一个句子概率的模型。从统计的角度看，自然语言中的一个句子可以由任何词串构成。不过P(s)有大有小。比如：<br><a id="more"></a></p><ul><li>s1 = 我 刚 吃 过 晚饭</li><li>s2 = 刚 我 过 晚饭 吃</li></ul><p>可以看出P(s1)&gt;P(s2)。对于给定的句子而言，通常P(s)是未知的。对于一个服从某个概率分布P的语言L，根据给定的语言样本估计P的过程被称作语言建模。<br>根据语言样本估计出的概率分布P就称为语言L的语言模型。<br>$$\sum_{s\in L}P(s)=1$$<br>语言建模技术首先在语音识别研究中提出，后来陆续用到OCR、手写体识别、机器翻译、信息检索等领域。在语音识别中，如果识别结果有多个，则可以根据语言模型计算每个识别结果的可能性，然后挑选一个可能性较大的识别结果。语言模型也可以用于汉语歧义消解。<br>那么如何计算一个句子的概率呢？对于给定的句子:<br>$$S=w_1w_2,…,w_n$$<br>它的概率可以表示为：<br>$$P(S)=P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,w_2,…,w_{n-1})$$<br>由于上面的式子参数过多，因此需要近似的计算方法，常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机场(CRF)方法、神经网络方法等。本篇文章主要记录n-gram模型方法。</p><h2 id="n-gram模型"><a href="#n-gram模型" class="headerlink" title="n-gram模型"></a>n-gram模型</h2><p>对于给定的句子$S=w_1w_2…w_n,$,根据链式规则:<br>$$<br>P(S)=P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,w_2,…,w_{n-1})=\prod_{i=1}^np(w_i|w_1…w_{i-1})<br>$$<br>P(S)就是语言模型，即用来计算一个句子S概率的模型。<br>那么，如何计算$p(w_i|w_1,w_2,…,w_{i-1})$呢？最简单、直接的方法是计数后做除法，即最大似然估计(Maximum Likelihood Estimate，MLE)，如下：<br>$$<br>p(w_i|w_1,w_2,…,w_{i-1})=\frac {count(w_1,w_2,…,w_{i-1},w_i)}{count(w_1,w_2,…,w_{i-1})}<br>$$<br>其中，$count(w_1,w_2,…,w_{i-1},w_i)$表示次序列$(w_1,w_2,…,w_{i-1},w_i)$在预料库中出现的频率。</p><p>这里面临两个重要的问题：数据稀疏严重和参数空间过大，导致无法实用。实际中，我们一般较长使用N语法模型(n-gram)，它采用了马尔科夫假设，即认为语言中的每个词只与其前面长度为n-1的上下文有关。</p><ul><li><p>假设下一个词的出现不依赖前面的词，即为uni-gram，则有:$$ \begin {aligned}<br>  P(S)&amp;=P(w_1)P(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_1,w_2,…,w_{n-1})\\\\&amp;=p(w_1)p(w_2)…p(w_n)<br>\end{aligned}$$</p></li><li><p>假设下一个词的出现只依赖前面的一个词，即为bi-gram，则有：$$ \begin {aligned}<br>  P(S)&amp;=P(w_1)P(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_1,w_2,…,w_{n-1})\\\\&amp;=p(w_1)p(w_2|w_1)p(w_3|w_2)…p(w_n|w_{n-1})<br>\end{aligned}$$</p></li><li>假设下一个词的出现依赖它前面的两个词，即为tri-gram，则有：<br>$$<br>\begin {aligned} P(S)&amp;=P(w_1)P(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_1,w_2,…,w_{n-1})\\\\&amp;=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)…p(w_n|w_{n-2},w_{n-1})<br>\end {aligned}<br>$$</li></ul><p>实际上常常在对数空间里面计算概率，原因有两个：</p><ul><li>防止溢出；如果计算的句子很长，那么最后得到的结果将非常小，甚至会溢出，比如计算得到的概率是0.001，那么假设以10为底取对数的结果就是-3，这样就不会溢出；</li><li>对数空间里面加法可以代替乘法，因为log(p1p2) = logp1 +logp2，在计算机内部，显然加法比乘法执行更快；</li></ul><h3 id="n-gram中的n如何选择？"><a href="#n-gram中的n如何选择？" class="headerlink" title="n-gram中的n如何选择？"></a><strong>n-gram中的n如何选择？</strong></h3><ul><li><strong>n较大时</strong>：提供了更多的上下文语境信息，语境更具有区别性；但是，参数个数多、计算代价大、训练预料需要多，参数估计不可靠；</li><li><strong>n较小时</strong>：提供的上下文语境少，不具有区别性；但是，参数个数小、计算代价小、训练预料无须太多、参数估计可靠；</li></ul><p>理论上，n越大越好，经验上tri-gram用的最多，尽管如此，原则上，<strong>能用bi-gram解决，绝不使用tri-gram</strong>。</p><h3 id="建立n-gram语言模型"><a href="#建立n-gram语言模型" class="headerlink" title="建立n-gram语言模型"></a>建立n-gram语言模型</h3><p>构建n-gram语言模型，通过计算最大似然估计构造语言模型。一般的过程如下：</p><ul><li><strong>1、数据准备：</strong><ul><li>确定训练语料</li><li>对语料进行tokenization或切分</li><li>句子边界，增加特殊的词<bos>和<eos>开始和结束</eos></bos></li></ul></li><li><strong>2、参数估计：</strong><ul><li>利用训练语料，估计模型参数</li></ul></li></ul><p>令$c(w_1,…,w_n)$表示n-gram $w_1,…,w_n$在训练语料中出现的次数，则：<br>$$<br>    P_{MLE}(w_n|w_1,…,w_{n-1})=\frac {c(w_1,…,w_n)}{c(w_1,…,w_{n-1})}<br>$$</p><h3 id="语言模型效果评估"><a href="#语言模型效果评估" class="headerlink" title="语言模型效果评估"></a>语言模型效果评估</h3><p>目前主要有两种方法判断建立的语言模型的好坏：</p><ul><li>实用方法：通过查看该模型在实际应用（如拼写检查、机器翻译）中的表现来评价，优点是直观、实用，缺点是缺乏针对性、不够客观；</li><li>理论方法：困惑度(preplexity)，其基本思想是给测试集赋予较高概率值的语言模型较好；</li></ul><h2 id="平滑方法"><a href="#平滑方法" class="headerlink" title="平滑方法"></a>平滑方法</h2><p>最大似然估计给训练样本中未观察到的事件赋以0概率。如果某个n-gram在训练语料中没有出现，则该n-gram的概率必定是0。这就会使得在计算某个句子S的概率时，如果某个词没有在预料中出现，那么该句子计算出来的概率就会变为0，这是不合理的。<br>解决的办法是扩大训练语料的规模。但是无论怎样扩大训练语料，都不可能保证所有的词在训练语料中均出现。由于训练样本不足而导致估计的分布不可靠的问题，称为数据稀疏问题。在NLP领域中，稀疏问题永远存在，不太可能有一个足够大的语料，因为语言中的大部分词都属于低频词。</p><h3 id="Zipf定律"><a href="#Zipf定律" class="headerlink" title="Zipf定律"></a>Zipf定律</h3><p>Zipf定律描述了词频以及词在词频表中的位置之间的关系。针对某个语料库，如果某个词w的词频是f，并且该词在词频表中的序号为r(即w是所统计的语料中第r常用词)，则：<br>$$f*r=k(k是一个常数)$$<br>若$w_i$在词频表中的排名50，$w_j$在词频表中排名为150，则$w_i$的出现频率大约是$w_j$的频率的3倍。<br>Zipf定律告诉我们：</p><ul><li>语言中只有很少的常用词</li><li>语言中的大部分词都是低频词（不常用的词)</li></ul><p>Zipf的解释是Principle of Lease effort </p><ul><li>说话的人只想使用少量的常用词进行交流</li><li>听话的人只想使用没有歧义的词（量大低频）进行交流</li></ul><p>Zipf定律告诉我们：</p><ul><li>对于语言中的大多数词，它们在语料中出现是稀疏的</li><li>只有少量的词语料库可以提供它们规律的可靠样本</li></ul><h3 id="平滑技术"><a href="#平滑技术" class="headerlink" title="平滑技术"></a>平滑技术</h3><p>对于语言而言，由于数据稀疏的存在，MLE不是一种很好的参数估计方法。为了解决数据稀疏问题，人们为理论模型实用化而进行了众多的尝试，出现了一系列的平滑技术，<strong>它们的基本思想是降低已出现n-gram的条件概率分布，以使未出现的n-gram条件概率分布为非零，且经过平滑后保证概率和为1。</strong>目前，已经提出了很多数据平滑技术，如下：</p><ul><li>Add-one平滑</li><li>Add-delta平滑 </li><li>Good-Turing平滑</li><li>Interpolation平滑</li><li>回退模型-Katz平滑</li><li>…</li></ul><h4 id="Add-one平滑"><a href="#Add-one平滑" class="headerlink" title="Add-one平滑"></a>Add-one平滑</h4><p>加一平滑法，又称为拉普拉斯定律，其规定任何一个n-gram在训练预料中至少出现一次（即规定没有出现过的n-gram在训练预料中出现了1次）</p><p>$$P_{Add1}(w_1,w_2,…,w_n)=\frac {C(w_1,w_2,…,w_n)+1}{N+V}$$</p><ul><li>N:为训练预料中所所有的n-gram的数量(token);</li><li>V:所有的可能的不同的n-gram的数量(type);<br>下面两幅图分别演示了未平滑和平滑后的bi-gram示例：</li></ul><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180505/d6ljdKdckd.jpg?imageslim" alt="mark"></p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180505/EG0fA48HjC.jpg?imageslim" alt="mark"></p><p>(注：上图均来自《计算语言学-常宝宝》的课件)</p><p>训练语料中未出现的n-gram的概率不再为0，而是一个大于0的较小的概率值。但是，由于训练预料中未出现的n-gram数量太多，平滑后，所有未出现的n-gram占据了整个概率分布中的一个很大的比例。因此，在NLP中，Add-one给训练预料中没有出现过的n-gram分配太多的概率空间。同时，它认为所有未出现的n-gram概率相等，这是否合理？出现在训练语料中的哪些n-gram，都增加同样频度值，不一定合理。</p><h4 id="Add-delta平滑"><a href="#Add-delta平滑" class="headerlink" title="Add-delta平滑"></a>Add-delta平滑</h4><p>Add-delta平滑，不是加1，而是加一个比1小的整数$\lambda$:<br>$$<br>P_{AddD}(w_1,w_2,…,w_n)=\frac {C(w_1,w_2,…,w_n)+\lambda}{N+\lambda V}<br>$$<br>通常$\lambda =0.5$，此时又称为Jeffreys-Perks Law或ELE。它的效果要比Add-one好，但是仍然不理想。</p><h4 id="Good-Turing平滑"><a href="#Good-Turing平滑" class="headerlink" title="Good-Turing平滑"></a>Good-Turing平滑</h4><p>其基本思想是利用频率的类别信息对频率进行平滑。假设N是样本数据的大小，$n_r$是在样本中正好出现r次的事件的数目(在这里，事件为n-gram $w_1,w_2,…,w_n$)。即：出现1的$n_1$个，出现2次的$n_2$个,…。那么：<br>$$<br>N=\sum_{r=1}^{\infty} n_r r<br>$$<br>由于，$$N=\sum_{r=0}^{\infty}n_r r^*=\sum_{r=0}^{\infty}(r+1)n_{r+1}$$，所以，<br>$$<br>r^* = (r+1)\frac {n_{r+1}}{n_r}<br>$$</p><p>那么，Good-Turing估计在样本中出现r次的事件的概率为:<br>$$<br>P_r = \frac {r^*}{N}<br>$$<br>实际应用中，一般直接使用$n_{r+1}$代替$E(n_{r+1})$，$n_r$代替$E(n_r)$。这样，样本中所有事件的概率之和为：<br>$$<br>\sum_{r&gt;0} n_r * P_r = 1 - \frac {n_1}{N} &lt;1<br>$$<br>因此，有$\frac {n_1}{N}$的剩余的概率量就可以均分给所有未出现事件(r=0)。<br>Good-Turing估计适用于大词汇集产生的符合多项式分布的大量的观察数据。<br>在估计频度为r的n-gram的概率$p_r$时，如果数据集中没有频度为r+1的n-gram怎么办？此时，$N_{r+1}=0$导致$p_r=0$。解决的办法是对$N_r$进行平滑，设S(.)是平滑函数，S(r)是$N_r$的平滑值。<br>$$<br>r^* = (r+1)\frac {S(r+1)}{S(r)}<br>$$</p><h4 id="Interpolation平滑"><a href="#Interpolation平滑" class="headerlink" title="Interpolation平滑"></a>Interpolation平滑</h4><p>不管是Add-one，还是Good Turing平滑技术，对于未出现的n-gram都一视同仁，难免存在不合理性。所以介绍一种线性差值的的平滑技术，其基本思想是将高阶模型和低阶模型作线性组合，利用低阶n-gram模型对高阶n-gram模型进行线性差值。因为没有足够的数据对高阶n-gram模型进行概率估计时，低阶的n-gram模型通常可以提供有用的信息。因此，可以把不同阶的n-gram模型组合起来产生一个更好的模型。</p><p>把不同阶别的n-gram模型线性加权组合：<br>$$P(w_n|w_{n-1},w_{n-2})=\lambda_1P(w_n)+\lambda_2P(w_n|w_{n-1})+\lambda_3P(w_n|w_{n-1}w_{n-2})$$<br>其中，$0&lt;=\lambda_i&lt;=1,\sum_i \lambda_i=1$。$\lambda_i$可以根据实验凭经验设定，也也可以通过应用某些算法确定，例如EM算法。</p><h4 id="回退模型-Katz平滑"><a href="#回退模型-Katz平滑" class="headerlink" title="回退模型-Katz平滑"></a>回退模型-Katz平滑</h4><p>回退模型-Katz平滑，其基本思想是：当某一事件在样本中出现的概率大于K(通常K为0或1)，运用最大似然估计减值来估计其概率，否则使用低阶的，即(n-1)gram概率代替n-gram概率。而这种替代必须受归一化因子$\alpha$的作用。回退模型的一般形式如下：<br>$$<br>p_{smooth}(w_i|w_{i-n+1}^{i-1})=\begin{cases}<br>             \hat p(w_i|w_{i-n+1}^{i-1}), &amp;  if c(w_{i-n+1}^i)&gt;0 \\<br>             \alpha(w_{i-n+1}^{i-1})\cdot p_{smooth}(w_i|w_{i-n+2}^{i-1}), &amp; if c(w_{i-n+1}^{i-1})=0<br>            \end{cases}$$<br>参数$\alpha(w_{i-n+1}^{i-1})$是归一化因子，以保证$$\sum_{w_i}p_{smooth}(w_i|w_{i-n+1}^{i-1})=1$$<br>以bi-gram为例，令$r=c(w_{i-1}w_i)$，如果r&gt;0，则$p_{katz}(w_i|w_{i-1})=d_r\cdot p_{ML}(w_i|w_{i-1})$，$d_r$称为折扣率，给定$w_{i-1}$，从r&gt;0的bi-grams中折除的概率为：$$<br>S(w_{i-1}) = 1 - \sum_{w_i \in M(w_{i-1})} p_{katz}(w_i|w_{i-1}) \\其中，M(w_{i-1})={w_i|c(w_{i-1}w_i)&gt;0}<br>$$</p><p>对于给定的$w_{i-1}$，令：$$<br>Q(w_{i-1}) = {w_i|c(w_{i-1}w_i)=0}<br>$$<br><strong>如何把$S(w_{i-1})$分配给集合$Q(w_{i-1})$中的那些元素？</strong></p><p>对于$w_i \in Q(w_{i-1})$，如果$p_{ML}(w_i)$比较大，则应该分配更多的概率给它。所以，若r=0，则：$$<br>p_{katz}(w_i|w_{i-1})=\frac {p_{ML}(w_i)}{\sum_{w_j\in Q}p_{ML}(w_j)} \cdot S(w_{i-1})<br>$$<br>对于bi-gram模型，Katz平滑为：<br>$$p_{katz}(w_i|w_{i-1})=<br>             \begin{cases}<br>             d_r\cdot p_{ML}(w_i|w_{i-1}), &amp;  if r&gt;0 \\<br>             \alpha(w_{i-1}) \cdot p_{ML}(w_i), &amp; if r=0<br>             \end{cases}<br>\\<br>其中，\alpha(w_{i-1}) = \frac {1 - \sum_{w_j\in M} p_{katz}(w_j|w_{j-1}) }{\sum_{w_j\in Q} p_{ML}(w_j)}<br>$$</p><p><strong>如何计算$d_r$?</strong></p><ul><li>如果$r&gt;k$，不折扣，即$d_r=1$(Katz提出k=5)</li><li>如果$0&lt;r \leq k$，按照和Good-Turing估计同样的方式折扣，即按照$\frac {r^{*}}{r}$进行折扣。严格说，要求$d_r$满足，$1-d_r=u(1-\frac {r^{*}}{r})$</li><li>根据Good-Turing估计，未出现的n元组估计出现频次是$n_1$，$\sum_{r=1}^k n_r(1-d_r)r=n$</li><li>具体而言，若$0&lt;r \leq k$，有$$<br>d_r = \frac {\frac {r^*}{r} - \frac {(k+1)n_{k+1}}{n_1}} {1 - \frac {(k+1)n_{k+1}}{n!}}<br>$$</li></ul><p>big-gram的Katz平滑模型最终可描述为：<br>$$p_{katz}(w_i|w_{i-1})=\begin{cases}<br>             c(w_{i-1}w_i)/c(w_{i-1}), &amp;  if r&gt;k \\<br>             d_rc(w_{i-1}w_i)/c(w_{i-1}), &amp; if k \geq r &gt; 0 \\<br>             \alpha (w_{i-1})p_{katz}(w_i) &amp; r=0<br>             \end{cases}<br>$$<br>n-gram模型的Katz平滑可以此类推。<br>在回退模型和线性插值模型中，当高阶n-gram未出现时，使用低阶n-gram估算高阶n-gram的概率分布。在回退模型中，高阶n-gram一旦出现，就不再使用低阶n-gram进行估计。在线性插值模型中，无论高阶n-gram是否出现，低阶n-gram都会被用来估计高阶n-gram的概率分布。</p><h2 id="大规模n-gram的优化"><a href="#大规模n-gram的优化" class="headerlink" title="大规模n-gram的优化"></a>大规模n-gram的优化</h2><p>如果不想自己动手实现n-gram语言模型，推荐几款开源的语言模型项目：</p><ul><li>SRILM(<a href="http://www.speech.sri.com/projects/srilm/" target="_blank" rel="external">http://www.speech.sri.com/projects/srilm/</a>)</li><li>IRSTLM(<a href="http://hlt.fbk.eu/en/irstlm" target="_blank" rel="external">http://hlt.fbk.eu/en/irstlm</a>)</li><li>MITLM(<a href="http://code.google.com/p/mitlm/" target="_blank" rel="external">http://code.google.com/p/mitlm/</a>)</li><li>BerkeleyLM(<a href="http://code.google.com/p/berkeleylm/" target="_blank" rel="external">http://code.google.com/p/berkeleylm/</a>)</li></ul><p>在使用 n-gram 语言模型时，也有一些技巧在里面。例如，面对 Google N-gram 语料库，压缩文件大小为 27.9G，解压后 1T 左右，如此庞大的语料资源，使用前一般需要先剪枝（Pruning）处理，缩小规模，如仅使用出现频率大于 threshold 的 n-gram，过滤高阶的 n-gram（如仅使用 n&lt;=3 的资源），基于熵值剪枝，等等。</p><p>另外，在存储方面也需要做一些优化:</p><ul><li>采样Trie数的数据结构，可以优化时间复杂度为$O(log_{|V|}m)$|V|为字母的个数；</li><li>借助Bloom filter辅助查询，把String映射为int类型处理；</li><li>利用郝夫曼树对词进行编码，将词作为索引值而不是字符串进行存储，能将所有词编码成包含在2个字节内的索引值；</li><li>优化概率值存储，概率值原使用的数据类型是（float），用4-8bit来代替原来8Byte的存储内容；</li></ul><h2 id="N-gram模型的缺陷"><a href="#N-gram模型的缺陷" class="headerlink" title="N-gram模型的缺陷"></a>N-gram模型的缺陷</h2><ul><li>数据稀疏问题：利用平滑技术解决；</li><li>空间占用大；</li><li>长距离依赖问题；</li><li>多义性；</li><li>同义性；如 “鸡肉”和“狗肉”属于同一类词，p(肉|鸡)应当等于p(肉|狗)，而在训练集中学习到的概率可能相差悬殊；</li></ul><h2 id="语言模型应用"><a href="#语言模型应用" class="headerlink" title="语言模型应用"></a>语言模型应用</h2><h3 id="n-gram距离"><a href="#n-gram距离" class="headerlink" title="n-gram距离"></a>n-gram距离</h3><p>假设有一个字符串s，那么该字符串的n-gram就表示按长度n切分原词得到的词段，也就是s中长度为n的子串。假设有两个字符串，然后分别求它们的n-gram，那么就可以从它们的共有子串的数量这个角度定义两个字符串间的n-gram距离。但是仅仅是简单地对共有子串进行计数显然也存在不足，这种方案忽略了两个字符长度的差异，可能导致的问题。比如，字符串girl和girlfriend，二者拥有的公共子串数量显然与girl和其自身所拥有的公共子串数量相等，但是不能认为girl和girlfriend是两个等同的匹配。为解决该问题，有学者便提出以非重复的n-gram分词为基础来定义n-gram距离这一概念，可以用下面的公式来表述：</p><p>$$|G_N(s)|+|G_N(t)|-2*|G_N(s)\cap G_N(t)|$$</p><p>例如，字符串s=”ABC”，t=”AB”，分别在字符串首尾加上begin和end，采用二元语言模型，字符串s产生的bi-gram为：(begin,A),(A,B),(B,C),(C,end)；字符串t产生的bi-gram为：(begin,A),(A,B),(B,end)。<br>采用上面公式定义:4+3 - 2*2 = 3<br>显然，字符串之间的距离越小，它们就越接近。当两个字符串完全相等的时候，它们之间的距离就是0。</p><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>分词是NLP中一项比较基础且重要的任务。对于X=”我爱中国”这样一句话，有多种切分方案，对于$y_i$这种分词方案，如,$Y_0=(“我”，“爱”，“中国”),Y_1=(“我”，“爱中”，“国”)$，利用贝叶斯公式可以计算出每种切分的概率:$$<br>P(Y_i|X)=\frac {P(X|Y_i)P(Y_i)}{P(X)}\propto P(X|Y_i)P(Y_i),i=1,2,3,…<br>$$<br>无论在哪种$Y_i$下，最终都能生成句子X，因此$P(X|Y_i)=1$，所以$P(Y_i|S)\propto P(Y_i),i=1,2,3…$。所以，只需要最大化$P(Y_i)即可$。例如，根据bi-gram语言模型，$P(Y_0|X)\propto P(Y_0)=P(我)P(爱|我)P(中国|爱)$，$P(Y_1|X)\propto P(Y_1)=P(我)P(爱中|我)P(国|爱中)$，然后利用计算出的概率，选择最大的作为分词方案。</p><h3 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h3><p>词性标注（POS tagging）是一个典型的多分类问题，将每个词标注为名词、动词、形容词、副词、代词等。例如，在“我/爱/中国”句话中，“爱”有很多词性，比如，名词、动词。最简单的标注其语义的方案就是，看语料库中“爱”出现的次数，以及其词性，即：<br>$$<br>P(POS_i|爱)=\frac {c(“爱”作为POS_i )}{c(爱),i=1,2,…,k，k为词性总数}<br>$$<br>但是，这种简单的词性标注的方案依赖人工，且未考虑上下文。考虑到在一个句子中当前词的词性和前面一两个词关系比较大。因此，可以借用n-gram模型的思路进行求解。比如，在考虑“爱”的词性时，以前面一个词的词性作为参考，即“我”的词性，则，当前这个“爱”的词性概率分布为:<br>$$P(POS_i|我，爱)=P(POS_i|Pron.,爱)=\frac {前面被“副词”修饰的“爱”的POS_i}{c(前面被“副词”修饰的“爱”)},i=1,2,…,k,k为词性总数$$</p><p>计算这个概率需要对语料库进行统计。但是，前提是先判断好“我”的词性。因为，采用的是bi-gram模型，由于“我”已经是第一个词，在二元模型中主需要级简的方案判断即可。</p><h3 id="n-gram作为文本特征"><a href="#n-gram作为文本特征" class="headerlink" title="n-gram作为文本特征"></a>n-gram作为文本特征</h3><p>在处理文本的特征的时候，通常一个关键词作为一个特征。但是，在某些场景下可能词的表达能力不够，需要提取更多的特征，n-gram就是一个很好的特征。以bi-gram为例，在原始文本中，每个关键词可以作为一个特征，将每个关键词两两组合，得到一个bi-gram组合，在根据n-gram语言模型，计算各个bi-gram组合的概率，作为新的特征。</p><h3 id="英语介词短语消歧"><a href="#英语介词短语消歧" class="headerlink" title="英语介词短语消歧"></a>英语介词短语消歧</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>我们是这样理解语言的；</li><li>《计算语言学-常宝宝》的课件</li><li><a href="https://www.cnblogs.com/ljy2013/p/6425277.html" target="_blank" rel="external">https://www.cnblogs.com/ljy2013/p/6425277.html</a></li><li><a href="https://blog.csdn.net/TiffanyRabbit/article/details/72654180" target="_blank" rel="external">https://blog.csdn.net/TiffanyRabbit/article/details/72654180</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;语言模型概述&quot;&gt;&lt;a href=&quot;#语言模型概述&quot; class=&quot;headerlink&quot; title=&quot;语言模型概述&quot;&gt;&lt;/a&gt;语言模型概述&lt;/h2&gt;&lt;p&gt;语言模型(Language Model)，就是用来计算一个句子概率的模型。从统计的角度看，自然语言中的一个句子可以由任何词串构成。不过P(s)有大有小。比如：&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP基础-熵</title>
    <link href="https://ilewseu.github.io/2018/05/02/NLP%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E7%86%B5/"/>
    <id>https://ilewseu.github.io/2018/05/02/NLP中的各种熵/</id>
    <published>2018-05-02T12:52:20.000Z</published>
    <updated>2018-05-12T14:46:25.352Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一些基础"><a href="#一些基础" class="headerlink" title="一些基础"></a>一些基础</h2><h3 id="互斥"><a href="#互斥" class="headerlink" title="互斥"></a>互斥</h3><p>如果事件A和B不可能同时发生，即$AB=\Phi$，则称A与B是互斥的。<br><a id="more"></a></p><h3 id="对立"><a href="#对立" class="headerlink" title="对立"></a>对立</h3><p>如果A与B互斥，又在每次试验中不是出现A就是出现B，即$AB=\Phi$且$A+B=\Omega$，则称B是A的对立事件。</p><h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>在事件B发生的条件下，事件A发生的概率称为事件A在事件B已发生的条件下的条件概率，记作P(A|B)。当P(B)&gt;0时，规定:<br>$$<br>P(A|B)=\frac {P(AB)}{P(B)}<br>$$<br>当P(B)=0时，规定P(A|B)=0。由条件概率的定义，可以得到<strong>乘法公式</strong>：<br>$$\begin {aligned}<br>&amp;P(AB)=P(A)P(B|A)\\\\<br>&amp;P(A_1A_2…A_n)=P(A_1)P(A_2|A_1)P(A_3|A_2A_1)…P(A_n|A_{n-1}A_{n-2}…A_1)=\prod_i^n P(A_i|A_{i-1}A_{i-2}…A_1)<br>\end {aligned}$$<br>一般而言，条件概率P(A|B)与概率P(A)是不等的。但在某些情况下，它们是相等的。根据条件概率的定义和乘法公式有:$$P(AB)=P(A)P(B)$$这时，称事件A与B是相互独立的。</p><h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>根据乘法公式，可以的得到下面重要的公式，该公式称为贝叶斯公式：<br>$$P(A|B)=\frac {P(B|A)(A)}{P(B)}$$<br>一般地，事件$A_1,A_2,…,A_n$两两互斥，事件B满足$B\subset A_1+A_2+…+A_n$且$P(A_i)&gt;0(i=1,2,…,n),P(B)&gt;0$，贝叶斯公式可以推广为：$$<br>p(A_j|B)=\frac{P(A_j)P(B|A_j)}{P(A_1)P(B|A_1)+…+P(A_n)P(B|A_n)}=\frac {P(A_j)P(B|A_j)}{\sum_i^n P(A_i)P(B|A_i)}<br>$$<br>实用上称，$P(A_1),P(A_2),…,P(A_n)$的值称为先验概率，称$P(A_1|B),P(A_2|B),…,P(A_n|B)$的值称为后验概率，贝叶斯公式便是从先验概率计算后验概率的公式。</p><h2 id="各种熵-entropy"><a href="#各种熵-entropy" class="headerlink" title="各种熵(entropy)"></a>各种熵(entropy)</h2><p>在信息论中，如果发送一个消息所需要的编码的长度较大，则可以理解为消息所蕴涵的信息量较大，如果发送一个消息所需要的编码长度较小，则该消息所蕴涵的信息量较小，平均信息量即为发送一个消息的平均编码长度，可以用<strong>熵</strong>的概念来描述。</p><h3 id="自信息"><a href="#自信息" class="headerlink" title="自信息"></a>自信息</h3><p>自信息是熵的基础，自信息表示某一事件发生时所带来的信息量的多少。当事件发生的概率越大，则自信息越小。当一件事发生的概率非常小，并且实际上也发生了（观察结果），则此时的自信息较大。某一事件发生的概率非常大，并且实际上也发生了，则此时的自信息较小。如何度量它？现在要寻找一个函数，满足条件：事件发生的概率越大，则自信息越小；自信息不能是负值，最小是0；自信息应该满足可加性，并且两个对立事件的自信息应该等于两个事件单独的自信息。自信息的公式如下：<br>$$I(p_i)=-log(p_i)$$<br>其中，$p_i$表示随机变量的第i个事件发生的概率，自信息单位是bit,表征描述该信息需要多少位。可以看出，自信息的计算和随机变量本身数值没有关系，只和其概率有关。</p><h3 id="熵的定义"><a href="#熵的定义" class="headerlink" title="熵的定义"></a>熵的定义</h3><p>设X是取有限个值的随机变量，它的分布密度为$p(x)=P(X=x),且x\in X$，则X的熵的定义为：$$H(x)=-\sum _{x \in X}p(x)log_ap(x)$$<br>熵描述了随机变量的不确定性。一般也说，熵给出随机变量的一种度量。对于数底a可以是任何正数，对数底a决定了熵的单位，如果a=2，则熵的单位称为比特(bit)。</p><h3 id="熵的基本性质"><a href="#熵的基本性质" class="headerlink" title="熵的基本性质"></a>熵的基本性质</h3><ul><li>$H(x)&lt;=log|X|$，其中等号成立当且仅当$p(x)=\frac {1}{|x|}$，这里|X|表示集合X中的元素个数。该性质表明等概场具有的最大熵；</li><li>$H(X)&gt;=0$，其中等号成立的条件当且仅当对某个i,$p(x_i)=1$，其余的$p(x_k)=0 (k!=i)$。这表明确定场(无随机性)的熵最小；</li><li>熵越大，随机变量的不确定性就越大，分布越混乱，随机变量状态数越多；</li></ul><h3 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h3><p>设X,Y是两个离散随机变量，它们的联合分布密度为p(x,y)，则给定X时Y的条件熵定义为：<br>$$\begin{aligned}<br>H(Y|X) &amp;=-\sum_{x\in X}p(x)H(Y|X=x)\\\\<br>&amp;=\sum_{x\in X}p(x)[-\sum_{y \in Y}p(y|x)log p(y|x)]\\\\<br>&amp;=-\sum_{x \in X}\sum_{y \in Y} p(x,y)log p(y|x)<br>\end{aligned}<br>$$<br>联合熵和条件熵的关系可以用下面的公式来描述，该关系一般也称为链式规则：$$<br>H(X,Y)=H(X)+H(Y|X)$$<br>信息量的大小随着消息的长度增加而增加，为了便于比较，一般使用熵率的概率。熵率一般也称为字符熵(per-letter entropy)或词熵(per-word entropy)。</p><h3 id="熵率"><a href="#熵率" class="headerlink" title="熵率"></a>熵率</h3><p>对于长度为n的消息，熵率的定义为：<br>$$<br>H_{rate}=\frac{1}{n}H(x_{1n}) = -\frac {1}{n}\sum_{x_{1n}}p(x_{1n})log p(x_{1n})<br>$$<br>这里的$x_{1n}$表示随机变量序列$X_1,X-2…X_n,p(x_{1n})表示分布密度p(x_1,x_2,…,x_n)$。<br>可以把语言看做一系列语言单位构成的一个随机变量序列$L={X_1X_2…X_n}$，则语言L的熵可以定义这个随机变量序列的熵率:<br>$$<br>H_{rate}=\lim_{x \to +\infty}\frac{1}{n}H(H_1,H_2,…,H_n)<br>$$</p><h3 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h3><p>根据链式规则，有:$$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$可以推导出：$$<br>H(X)-H(X|Y)=H(Y)-H(Y|X)<br>$$<br>H(X)与H(X|Y)的差称为互信息，一般记作I(X;Y)。I(X;Y)描述了包含在X中的有关Y的信息量，或包含在Y中的有关X的信息量。下图很好的描述了互信息和熵之间的关系。<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180503/7Cceem39Dg.jpg?imageslim" alt="mark"><br>$$\begin{aligned}<br> I(X;Y)&amp;=H(X)-H(X|Y)\\\\&amp;=H(X)+H(Y)-H(X,Y)\\\\<br> &amp;=\sum_x p(x)log \frac {1}{p(x)}+\sum_x p(y)log \frac {1}{p(y)}+\sum_{x,y} p(x,y)log p(x, y)\\\\&amp;=\sum_{x,y}log \frac {p(x,y)}{p(x)p(y)}<br>\end{aligned}<br>$$<br><strong>互信息(mutual information),</strong>随机变量X,Y之间的互信息定义为：$$<br>I(X;Y)=\sum_{x,y}p(x,y)log \frac {p(x,y)}{p(x)p(y)}<br>$$<br><strong>互信息的性质</strong>：</p><ul><li>$I(X;Y)&gt;=0$，等号成立当且仅当X和Y相互独立。</li><li>$I(X;Y)=I(Y;X)$说明互信息是对称的。</li></ul><p>互信息相对于相对熵的区别就是，互信息满足对称性；<br>互信息的公式给出了两个随机变量之间的互信息。在计算语言学中，更为常用的是两个具体事件之间的互信息，一般称之为<strong>点式互信息</strong>。<br><strong>点式互信息(pointwise mutual information)</strong>，事件x,y之间的互信息定义为：$$<br>I(x,y) = log \frac {p(x,y)}{p(x)p(y)}<br>$$<br>一般而言，点间互信息为两个事件之间的相关程度提供一种度量，即：</p><ul><li>当$I(x,y)&gt;&gt;0$时，x和y是高度相关的；</li><li>当$I(x,y)=0$时，x和y是高度相互独立；</li><li>当$I(x,y)&lt;&lt;0$时，x和y呈互补分布；</li></ul><h3 id="交叉熵-cross-entropy"><a href="#交叉熵-cross-entropy" class="headerlink" title="交叉熵(cross entropy)"></a>交叉熵(cross entropy)</h3><p>交叉熵的概念是用来衡量估计模型与真实概率分布之间差异情况的，其定义为，设随机变量X的分布密度p(x)，在很多情况下p(x)是未知的，人们通常使用通过统计的手段得到X的近似分布q(x),则随机变量X的交叉熵定义为：<br>$$<br>H(p,q) = -\sum_{x \in X} p(x)log q(x)<br>$$<br>其中，p是真实样本的分布，q为预测样本分布。在信息论中，其计算数值表示：如果用错误的编码方式q去编码真实分布p的事件，需要多少bit数，是一种非常有用的衡量概率分布相似的数学工具。</p><h3 id="相对熵-relative-entropy"><a href="#相对熵-relative-entropy" class="headerlink" title="相对熵(relative entropy)"></a>相对熵(relative entropy)</h3><p>相对熵，设p(x),q(x)是随机变量X的两个不同的分布密度，则它们的相对熵定义为:$$<br>D(p||q)=\sum_{x \in X}p(x) log \frac {p(x)}{q(x)}=H(p,q)-H(p)<br>$$<br>相对熵较交叉熵有更多的优异性质，主要为：</p><ul><li>当p分布和q分布相等的时候，KL散度值为0；</li><li>可以证明是非负的；</li><li>KL散度是非对称的，通过公式可以看出，KL散度是衡量两个分布的不相似性，不相似性越大，则值越大，当完全相同时，取值为0；</li></ul><p>对比交叉熵和相对熵，可以发现仅仅差一个H(p)，如果从最优化的角度来看，p是真实分布，是固定值，最小化KL散度的情况下，H(p)可以省略，此时交叉熵等价于KL散度。</p><p><strong>在机器学习中，何时需要使用相对熵，何时使用交叉熵？</strong><br>在最优化问题中，最小化相对熵等价于最小化交叉熵；相对熵和交叉熵的定义其实可以从最大似然估计得到。最大化似然函数，等价于最小化负对数似然，等价于最小化交叉熵，等价于最小化KL散度。交叉熵大量应用在Sigmoid函数和SoftMax函数中，而相对熵大量应用在生成模型中，例如，GAN、EM、贝叶斯学习和变分推导中。从这可以看出：如果想通过算法对样本数据进行概率分布建模，那么通常都是使用相对熵，因为需要明确知道生成的分布和真实分布的差距，最好的KL散度值应该是0；而在判别模型中，仅仅只需要评估损失函数的下降值即可，交叉熵可以满足要求，其计算量比KL散度小。在《数学之美》一书中是这样描述它们的区别：<strong>交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除新系统的不确定性所需要付出的努力的大小；相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异。</strong></p><h3 id="困惑度-perplexity"><a href="#困惑度-perplexity" class="headerlink" title="困惑度(perplexity)"></a>困惑度(perplexity)</h3><p>对于语言$L=(x_i)~p(x)$，与其模型q的交叉熵定义为：$$<br>H(L,p)=-\lim_{x \to \infty} \frac {1}{n} \sum_{x_1^n}p(x_1^n)log q(x_1^n)<br>$$<br>其中，$x_1^n=x_1,…,x_n$为语言L的语句，$p(x_1^n)$为L中语句的概率，$q(x_1^n)$为模型q对$x_1^n$的概率估计。<br>我们可以假设这种语言是“理想”的，即n趋于无穷大时，其全部“单词”的概率和为1。就是说，根据信息论的定理：假定语言L是稳态(stationary) ergodic随机过程， L与其模型q的交叉熵计算公式就变为：<br>$$H(L,q)=-\lim_{x \to \infty} \frac {1}{n} log q(x_1^n)$$<br>由此，我们可以根据模型q和一个含义大量数据的L的样本来计算交叉熵。在设计模型q时，我们的目的是使交叉熵最小，从而使模型最接近真实的概率分布p(x)。</p><p>在设计语言模型时，通常用困惑度来代替交叉熵衡量语言模型的好坏。给定语言L的样本$l_1^n=l_1,,,,l_n$，L的困惑度为$PP_q$定义为：<br>$$<br>PP_q = 2^{H(L,q)}   \approx 2^{-\frac {1}{n}log q(l_1^n)} = [q(l_1^n)]^{- \frac {1}{n}}<br>$$<br>于是语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实语言的情况。从perplexity的计算式可以看出来，它是对于样本句子出现的概率，在句子长度上Normalize一下的结果。它越小，说明出现概率越大，所得模型就越好。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>机器学习各种熵：从入门到全面掌握：<a href="https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw##" target="_blank" rel="external">https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw##</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一些基础&quot;&gt;&lt;a href=&quot;#一些基础&quot; class=&quot;headerlink&quot; title=&quot;一些基础&quot;&gt;&lt;/a&gt;一些基础&lt;/h2&gt;&lt;h3 id=&quot;互斥&quot;&gt;&lt;a href=&quot;#互斥&quot; class=&quot;headerlink&quot; title=&quot;互斥&quot;&gt;&lt;/a&gt;互斥&lt;/h3&gt;&lt;p&gt;如果事件A和B不可能同时发生，即$AB=\Phi$，则称A与B是互斥的。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的激活函数</title>
    <link href="https://ilewseu.github.io/2018/04/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>https://ilewseu.github.io/2018/04/21/神经网络中的激活函数/</id>
    <published>2018-04-21T06:16:20.000Z</published>
    <updated>2018-05-12T15:19:03.139Z</updated>
    
    <content type="html"><![CDATA[<h2 id="激活函数简介"><a href="#激活函数简介" class="headerlink" title="激活函数简介"></a>激活函数简介</h2><p>神经网络是目前比较流行深度学习的基础，神经网络模型模拟人脑的神经元。人脑神经元接收一定的信号，对接收的信号进行一定的处理，并将处理后的结果传递到其他的神经元，数以亿计的神经元组成了人体复杂的结构。在神经网络的数学模型中，神经元节点，将输入进行加权求和，加权求和后再经过一个函数进行变换，然后输出。这个函数就是激活函数，神经元节点的激活函数定义了对神经元输入的映射关系。<br><a id="more"></a></p><div align="center"><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/k641bmB33m.png?imageslim"><br></div><h3 id="激活函数的定义"><a href="#激活函数的定义" class="headerlink" title="激活函数的定义"></a>激活函数的定义</h3><p>在ICML2016的一篇论文：Noisy Activation Functions中给出的了激活函数的定义：激活函数是实数到实数的映射，且几乎处处可导。激活函数一般具有以下性质：</p><ul><li>非线性：弥补线性模型的不足；</li><li>几乎处处可导：反向传播时需要计算激活函数的偏导数，所以要求激活函数除个别点外，处处可导；</li><li>计算简单</li><li>单调性：当激活函数是单调的时候，单层网络能够保证是凸函数；</li><li>输出值范围有限：当激活函数的输出值有限的时候，基于梯度的优化方法会更加稳定；因为特定的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的learning rate。</li></ul><h3 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h3><ul><li>神经网络中的激活函数能够引入非线性因素，提高模型的表达能力；<br>网络中仅有线性模型的话，表达能力不够。比如一个多层的线性网络，其表达能力和单层的线性网络是相同的。网络中卷积层、池化层和全连接层都是线性的。所以，需要在网络中加入非线性的激活函数层。</li><li>一些激活函数能够起到特征组合的作用；<br>例如，对于Sigmoid函数$\sigma(x) = \frac {1}{1+e^(-x)}$，根据泰勒公式展开:$$<br>e^x = 1+ \frac {1}{1!}x + \frac {1}{2!}x^2 + \frac {1}{3!}x^3+O(x^3)<br>$$<br>对于输入特征为$x_1,x_2$，加权组合后如下：<br>$$<br>x = w_1x_1+w_2x_2<br>$$<br>将x带入到$e^x$泰勒展开的平方项，$$<br>x^2=(w_1x_1+w_2x_2)^2 = ((w_1x_1)^2+(w_2x_2)^2 + 2w_1x_1*w_2x_2)<br>$$<br>可以看出，平方项起到了特征两两组合的作用，更高阶的$x^3,x^4$等，则是更复杂的特征组合。</li></ul><h2 id="常见非线性激活函数"><a href="#常见非线性激活函数" class="headerlink" title="常见非线性激活函数"></a>常见非线性激活函数</h2><p>在介绍常见的激活函数之前，先介绍一下饱和(Saturated)的概念。</p><ul><li>左饱和：当函数h(x)满足，$\lim \limits_{x \to +\infty}h^{‘}(x)=0$;</li><li>右饱和：当函数h(x)满足，$\lim \limits_{x \to -\infty}h^{‘}(x)=0$;</li><li>饱和：当函数h(x)既满足左饱和又满足右饱和，称h(x)是饱和的;</li></ul><p>当激活函数是饱和的，对激活函数进行求导计算梯度时，计算出的梯度趋近于0，导致参数更新缓慢。</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>Sigmoid函数：</p><ul><li>定义：$\sigma(x) = \frac {1}{1+e^{-x}}$</li><li>值域：(0,1)</li><li>导数：$\sigma^{‘}(x) = \sigma(x)(1-\sigma(x))$<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/hB715EhmHJ.png?imageslim" alt="mark"></li></ul><p>从数学上看，非线性的Sigmoid函数对中央区域的信号增益较大，对两侧区域的信号增益较小，在信号的特征空间映射上，有很好的效果。从神经科学上来看，中央神经区酷似神经元的兴奋态，两侧区酷似神经元的抑制状态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。</p><p>Sigmoid的有以下优点：</p><ul><li>输出值域在(0,1)之间，可以被表示为概率；</li><li>输出范围有限，数据在传递的过程中不容易发散；</li><li>求导比较方便；</li></ul><p>Sigmoid的缺点如下：</p><ul><li>Sigmoid函数是饱和的，可能导致梯度消失(两个原因:(1)Sigmoid导数值较小；(2)Sigmoid是饱和的)，导致训练出现问题；</li><li>输出不以0为中心，可能导致收敛缓慢(待思考原因)；</li><li>指数计算，计算复杂度高；</li></ul><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>Tanh函数的表达式为：</p><p>$$tanh(x)=\frac {e^x-e^{-x}}{e^x+e^{-x}}=2\sigma(2x) - 1$$<br>它将输入值映射到[-1,1]区间内，其函数图像为：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/3jhB57gBEi.png?imageslim" alt="mark"></p><p>它的导数为$tanh^{‘}(x)=1-tanh^2(x)$<br>Tanh函数是Sigmoid函数的一种变体；与Sigmoid不同的是，Tanh是0均值的。因此，在实际应用中，Tanh会比Sigmoid更好，但Tanh函数现在也很少使用，其优缺点总结如下：</p><ul><li>相比Sigmoid函数，收敛速度更快；</li><li>相比Sigmoid函数，其输出是以0为中心的；</li><li>没有解决由于饱和性产生的梯度消失问题；</li></ul><h3 id="ReLU-Rectified-Linear-Units"><a href="#ReLU-Rectified-Linear-Units" class="headerlink" title="ReLU(Rectified Linear Units)"></a>ReLU(Rectified Linear Units)</h3><p>ReLU函数为现在使用比较广泛的激活函数，其表达式为：<br>$$<br>f(x)=max(0,x)=\begin{cases}<br>             x, &amp;  if x&gt;0 \\<br>             0  &amp; if x\leq0<br>             \end{cases}<br>$$<br>其函数图像如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/2HBL5Fa5ld.png?imageslim" alt="mark"></p><p>导数图像如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/dHg6L8J47d.png?imageslim" alt="mark"></p><p><strong>ReLU的优点如下:</strong></p><ul><li>相比Sigmoid和Tanh，ReLU在SGD中收敛速度要相对快一些；</li><li>Sigmoid和Tanh涉及到指数运算，计算复杂度高，ReLU只需要一个阈值就可以得到激活值，加快正向传播的计算速度；</li><li>有效的缓解了梯度消失的问题；</li><li>提供了神经网络的稀疏表达能力；</li></ul><p><strong>ReLU的缺点如下：</strong></p><ul><li>ReLU的输出不是以0为中心的；</li><li>训练时，网络很脆弱，很容易出现很多神经元值为0，从而再也训练不动；</li></ul><h3 id="ReLU的变体"><a href="#ReLU的变体" class="headerlink" title="ReLU的变体"></a>ReLU的变体</h3><p>为了解决上面的问题，出现了一些变体，这些变体的主要思路是将x&gt;0的部分保持不变，$x\leq0$的部分不直接设置为0，设置为$\alpha x$，如下三种变体:</p><ul><li>L-ReLU(Leaky ReLU):$\alpha$固定为比较小的值，比如：0.01，0.05；</li><li>P-ReLU(Parametric ReLU):$\alpha$作为参数，自适应地从数据中学习得到；</li><li>R-ReLU(Randomized ReLU):先随机生成一个$\alpha$，然后在训练过程中再进行修正；</li></ul><h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><p>ELU的函数形式如下：<br>$$<br>f(x)=\begin{cases}<br>             x, &amp;  if x&gt;0 \\<br>             \alpha(e^x-1)  &amp; if x\leq0<br>             \end{cases}<br>$$</p><p>其函数图像如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/4aIakGg1G8.png?imageslim" alt="mark"></p><p>ELU也是为了解决ReLU存在的问题而提出的，它具有ReLU的基本所有优点，以及：</p><ul><li>不会有神经元死亡的问题；</li><li>输出的均值接近于0，zero-centered;</li><li>计算量稍大，理论上虽然好于ReLU，但在实际使用中，目前并没有好的证据证明ELU总是优于ReLU;</li></ul><h3 id="MaxOut"><a href="#MaxOut" class="headerlink" title="MaxOut"></a>MaxOut</h3><p>MaxOut函数定义如下：$$<br>y=f(x)=max_{j\in[1,k]}z_j \\<br>z_j = w_jx+b_j<br>$$<br>一个比较容易的介绍，如下图：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/GFGj5LcC7a.png?imageslim" ,width="600" height="400"><br></div><p>假设w是二维的，那么有:<br>$$<br>f(x)=max(w_1^Tx+b_1,w_2^Tx+b_2)<br>$$<br>可以看出，ReLU及其变体都是它的一个变形(当$w_1,b_1=0的时候，就是ReLU$)</p><h2 id="激活函数使用建议"><a href="#激活函数使用建议" class="headerlink" title="激活函数使用建议"></a>激活函数使用建议</h2><ul><li>如果想让结果在(0,1)之间，使用Sigmoid(如LSTM的各种Gates);</li><li>如果想神经网络训练的很深，不要使用S型的激活函数；</li><li>如果使用ReLU，要注意初始化和Learning Rates的设置；</li><li>如果使用ReLU，出现很多神经元死亡的问题，且无法解决，可以尝试使用L-ReLU、P-ReLU等ReLU的变体；</li><li>最好不要使用Sigmoid，可以尝试使用Tanh;</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;激活函数简介&quot;&gt;&lt;a href=&quot;#激活函数简介&quot; class=&quot;headerlink&quot; title=&quot;激活函数简介&quot;&gt;&lt;/a&gt;激活函数简介&lt;/h2&gt;&lt;p&gt;神经网络是目前比较流行深度学习的基础，神经网络模型模拟人脑的神经元。人脑神经元接收一定的信号，对接收的信号进行一定的处理，并将处理后的结果传递到其他的神经元，数以亿计的神经元组成了人体复杂的结构。在神经网络的数学模型中，神经元节点，将输入进行加权求和，加权求和后再经过一个函数进行变换，然后输出。这个函数就是激活函数，神经元节点的激活函数定义了对神经元输入的映射关系。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://ilewseu.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 变量管理</title>
    <link href="https://ilewseu.github.io/2018/03/11/Tensorflow%E5%8F%98%E9%87%8F%E7%AE%A1%E7%90%86/"/>
    <id>https://ilewseu.github.io/2018/03/11/Tensorflow变量管理/</id>
    <published>2018-03-11T02:47:20.000Z</published>
    <updated>2018-03-11T05:37:11.102Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>摘自：《TensorFlow实战Google深度学框架》一书，5.3节。</p></blockquote><p>Tensorflow提供了通过变量名称来创建或者获取一个变量的机制。通过这个机制，在不同的函数中可以直接通过变量的名字来使用变量，而不需要将变量通过参数的形式到处传递。TensorFlow中通过变量名获取变量的机制主要是通过tf.get_variable和tf.variable_scope函数实现的。下面将分别介绍如何使用这两个函数。<br><a id="more"></a><br>通过tf.Variable()函数可以创建一个变量。除了tf.Variable()函数，TensorFlow还提供了tf.get_variable函数来创建或者获取变量。当tf.get_variable用于创建变量时，它和tf.Variable()的功能是基本等价的。下面的代码给出通过这两个函数创建同一个变量的示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 下面这两个定义是等价的</span></div><div class="line">v = tf.get_variable(<span class="string">"v"</span>,shape=[<span class="number">1</span>],initializer=tf.constant_initializer(<span class="number">1.0</span>))</div><div class="line">v = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]),name=<span class="string">'v'</span>)</div></pre></td></tr></table></figure></p><p>从上面的代码可以看出，通过tf.Variable和tf.get_variable函数创建变量的过程基本上是一样的。tf.get_variable函数调用时提供的维度(shape)信息以及初始化方法(initializer)的参数和tf.Variable函数调用时提供的初始化过程中的参数也类似。TensorFlow中提供的initializer函数和随机数以及常量生成函数大部分是一一对应的。比如，在上面的样例程序中使用的常数初始化函数tf.constant_initializer和常数生成的函数tf.constant功能上是一致的。Tensorflow提供了7种不同的初始化函数，如下表所示：</p><table><thead><tr><th>初始化函数</th><th>功能</th><th>主要参数</th></tr></thead><tbody><tr><td>tf.constant_initializer</td><td>将变量初始化为给定常量</td><td>常量的取值</td></tr><tr><td>tf.random_normal_initializer</td><td>将变量初始化为满足正态分布的随机值</td><td>正态分布的均值和标准差</td></tr><tr><td>tf.truncated_normal_initializer</td><td>将变量初始化为满足正态分布的随机值，但如果随机出来的值<br>偏离平均值超过2个标准差，那么这个数将会被重新随机</td><td>正态分布的均值和标准差</td></tr><tr><td>tf.random_uniform_initializer</td><td>将变量初始化为满足均匀分布的随机值</td><td>最大值、最小值</td></tr><tr><td>tf.uniform_unit_scaling_initializer</td><td>将变量初始化为满足均匀分布但不影响输出数量级的随机值</td><td>factor(产生随机数时<br>乘以的系数)</td></tr><tr><td>tf.zeros_initializer</td><td>将被变量设置为0</td><td>变量维度</td></tr><tr><td>tf.ones_initializer</td><td>将变量设置为1</td><td>变量的维度</td></tr></tbody></table><p><strong>tf.get_variable函数与tf.Variable函数最大的区别在于指定变量名称的参数</strong>。对于tf.Variable函数， 变量名称是一个可选的参数，通过name=“v”的形式给出。但是对于tf.get_variable函数，变量名称是一个必填的参数。tf.get_variable会根据这个名字去创建或者获取变量。在上面的示例程序中，tf.get_variable首先会试图创建一个名字为v的参数，如果创建失败（比如已经有同名的参数），那么这个程序会报错。这是为了避免无意识的变量复用造成的错误。比如在定义神经网络参数时，第一层网络的权重已经叫weights了，如果创建第二层的神经网络时，如果参数名仍然叫weights，那么就会触发变量重用的错误。否则两层神经网络公用一个权重会出现一些比较难以发现的错误。<strong>如果需要通过tf.get_variable获取一个已经创建的变量，需要通过tf.variable_scope函数来生成一个上下文管理器，并明确指定在这个上下文管理器中，tf.get_variable将直接获取已经生成的变量。</strong>下面给出一段代码说明如何通过tf.variable_scope函数来控制tf.get_variable函数获取已经创建过的变量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 在名字为foo的命名空间内创建名字为v的变量</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</div><div class="line"></div><div class="line"><span class="comment"># 因为在命名空间foo中已经存在名为v的变量，所以下面的代码将会报错</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="comment"># 在生成上下文管理器时，将参数reuse设置为True。这样tf.get_vaiable函数将直接获取</span></div><div class="line"><span class="comment"># 已经声明的变量。</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>,reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line">    <span class="keyword">print</span> v==v1 <span class="comment">#输出为True，v和v1代表的是相同的变量</span></div><div class="line"></div><div class="line"><span class="comment"># 将参数reuse设置为True时，tf.variable_scope将只能获取已经创建过的变量。因为在命名</span></div><div class="line"><span class="comment"># 空间bar中还没有创建变量v，所以下面的代码将会报错</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>,reuse=<span class="keyword">True</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div></pre></td></tr></table></figure></p><p>上面的样例简单地说明了通过tf.variable_scope函数可以控制tf.get_variable函数的语义。<strong>当tf.variable_scope函数使用参数reuse=True生成上下文管理器时，这个上下文管理器内所有的tf.get_variable函数会直接获取已经创建的变量。如果变量没有被创建，则tf.get_variable将会报错；相反如果tf.variable_scope函数使用参数reuse=None或者reuse=False创建上下文管理器，tf.get_variable操作将创建新的变量。</strong>如果同名变量已经存在，则tf.get_variable函数将会报错。TensorFlow中tf.variable_scope函数是可以嵌套的。下面的程序说明了当tf.variable_scope函数嵌套时，reuse参数的取值时如何确定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"root"</span>):</div><div class="line">    <span class="comment"># 可以通过tf.get_variable_scope().reuse函数来获取当前上下文管理器中reuse参数的取值</span></div><div class="line">    <span class="keyword">print</span> tf.get_variable_scope().reuse   <span class="comment">#输出False，即最外层reuse是False</span></div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>,reuse=<span class="keyword">True</span>): <span class="comment"># 新建一个嵌套的上下文管理器，</span></div><div class="line">                                              <span class="comment"># 并指定reuse为True</span></div><div class="line">        <span class="keyword">print</span> tf.get_variable_scope().reuse   <span class="comment"># 输出为True</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):        <span class="comment"># 新建一个嵌套的上下文管理器</span></div><div class="line">                                              <span class="comment"># 但不指定reuse的取值，和外层的保持一致</span></div><div class="line">            <span class="keyword">print</span> tf.get_variable_scope().reuse <span class="comment"># 输出为True</span></div><div class="line">    <span class="keyword">print</span> tf.get_variable_scope().reuse      <span class="comment"># 输出False，退出reuse设置为True</span></div><div class="line">                                             <span class="comment"># 的上下文之后，reuse的值又回到了False</span></div></pre></td></tr></table></figure><p>tf.variable_scope函数生成的上下文管理器也会创建一个TensorFlow中的命名空间，在命名空间内创建的变量名称都会带上这个命名空间名作为前缀。所以，tf.variable_scope函数除了控制tf.get_variable执行的功能之外，这个函数也提供了一个管理变量命名空间的方式。下面的代码显示如何通过tf.variable_scope来管理变量的名称。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">v1 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line"><span class="keyword">print</span> v1.name    <span class="comment"># 输出v:0,"v"为变量名称，“：0”表示这个变量时生成变量这个运算的第一个结果</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v2 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line">    <span class="keyword">print</span> v2.name  <span class="comment"># 输出为foo/v:0。在tf.variable_scope中创建的变量，名称前面会</span></div><div class="line">                   <span class="comment"># 加入命名空间的名称，通过/来分隔命名空间的名称和变量的名称。</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">        v3 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line">        <span class="keyword">print</span> v3.name <span class="comment"># 输出为foo/bar/v:0。命名空间可以嵌套，同时变量的名称也会</span></div><div class="line">                      <span class="comment"># 加入所有命名空间的名称作为前缀。</span></div><div class="line">    v4 = tf.get_variable(<span class="string">"v1"</span>,[<span class="number">1</span>])</div><div class="line">    <span class="keyword">print</span> v4.name  <span class="comment"># 输出foo/v1:0。当命名空间退出之后，变量名称也就不会再被加入其前缀了。</span></div><div class="line"></div><div class="line"><span class="comment"># 创建一个名称为空的命名空间，并设置为reuse=True</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">""</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    v5 = tf.get_variable(<span class="string">"foo/bar/v"</span>,[<span class="number">1</span>]) <span class="comment"># 可以直接通过带命名空间名称的变量名来</span></div><div class="line">                                          <span class="comment"># 获取其他命名空间下的变量</span></div><div class="line">    <span class="keyword">print</span> v5 == v3 <span class="comment"># 输出为True</span></div><div class="line">    v6 = tf.get_variable(<span class="string">"foo/v1"</span>,[<span class="number">1</span>])</div><div class="line">    <span class="keyword">print</span> v6 == v4 <span class="comment"># 输出为True</span></div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;摘自：《TensorFlow实战Google深度学框架》一书，5.3节。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tensorflow提供了通过变量名称来创建或者获取一个变量的机制。通过这个机制，在不同的函数中可以直接通过变量的名字来使用变量，而不需要将变量通过参数的形式到处传递。TensorFlow中通过变量名获取变量的机制主要是通过tf.get_variable和tf.variable_scope函数实现的。下面将分别介绍如何使用这两个函数。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="TensorFlow" scheme="https://ilewseu.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Attention Model 注意力机制</title>
    <link href="https://ilewseu.github.io/2018/02/12/Attention%20Model%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>https://ilewseu.github.io/2018/02/12/Attention Model 注意力机制/</id>
    <published>2018-02-12T06:23:20.000Z</published>
    <updated>2018-02-13T15:12:11.778Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文是对:<a href="https://blog.heuritech.com/2016/01/20/attention-mechanism/" target="_blank" rel="external">https://blog.heuritech.com/2016/01/20/attention-mechanism/</a> 的翻译。这篇文章对Attention Model原理进行了一个比较清晰的阐述，所以记录一下。由于本人英语能力有限，翻译不周的地方，还请见谅。<br><a id="more"></a><br>在2015年，随着DeepLearning和AI的发展，神经网络中的注意力机制引起了许多研究者的兴趣。这篇博文的目的是从一个高的层次上对注意力机制进行解释，以及详细介绍attention的一些计算步骤。如果你要更多关于attention的公式或例子，文后的参考文献提供了一下，特别注意Cho et al[3]这篇文章。不幸的是，这些模型往往你自己实现不了，仅仅有一些开源的实现。</p></blockquote><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>神经科学和计算神经科学中的neural processes已经广泛研究了注意力机制[1,2]。视觉注意力机制是一个特别值得研究的方向：许多动物专注于视觉输入的特定部分，去计算适当的反映。这个原理对神经计算有很大的影响，因为我们需要选择最相关的信息，而不是使用所有可用的信息，所有可用信息中有很大一部分与计算神经元反映无关。一个类似于视觉专注于输入的特定部分，也就是注意力机制已经用于深度学习、语音识别、翻译、推理以及视觉识别。</p><h2 id="Attention-for-Image-Captioning"><a href="#Attention-for-Image-Captioning" class="headerlink" title="Attention for Image Captioning"></a>Attention for Image Captioning</h2><p>我们通过介绍一个例子，去解释注意力机制。这个任务是我们想实现给图片加标题：对于给定的图片，根据图片中的内容给图片配上标题(说明/描述)。一个典型的image captioning系统会对图片进行编码，使用预训练的卷积神经网络产生一个隐状态h。然后，可以使用RNN对这个隐状态h进行解码，生成标题。这种方法已经被不少团队采用，包括[11]，如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/27GEF0IlE8.png?imageslim" alt="mark"></p><p>这种方法的问题是：当模型尝试去产生标题的下一个词时，这个词通常是描述图片的一部分。使用整张图片的表示h去调节每个词的生成，不能有效地为图像的不同部分产生不同的单词。这正是注意力机制有用的地方。<br>使用注意力机制，图片首先被划分成n个部分，然后我们使用CNN计算图像每个部分的表示$h_1,…,h_n$，也就是对n个部分的图像进行编码。当使用RNN产生一个新的词时，注意力机制使得系统只注意图片中相关的几个部分，所以解码仅仅使用了图片的特定的几个部分。如下图所示，我们可以看到标题的每个词都是用图像(白色部分)的一部分产生的。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/cIDeG9Jgl9.png?imageslim" alt="mark"></p><p>更多的例子如下图所示，我们可看到图片相关的部分产生标题中划线的单词。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/IiaGamlfj5.png?imageslim" alt="mark"></p><p>现在我们要解释注意力机制是怎么工作的，文献[3]详细介绍了基于注意力机制的Encoder-Decoder Network的实现。</p><h2 id="What-is-an-attention-model"><a href="#What-is-an-attention-model" class="headerlink" title="What is an attention model"></a>What is an attention model</h2><p>在一般情况下，什么是注意力机制？</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/K49GLi96C2.png?imageslim" alt="mark"></p><p>一个attention model通常包含n个参数$y_1,..,y_n$(在前面的例子中，$y_i$可以是$h_i$) ,和一个上下文c。它返回一个z向量，这个向量可以看成是对$y_i$的总结，关注与上下文c相关联的信息。更正式地说是，它返回的$y_i$的加权算术平均值，并且权重是根据$y_i$与给定上下文c的相关性来选择的。</p><p>在上面的例子中，上下文是刚开始产生的句子，$y_i$是图像的每个部分的表示($h_i$)，输出是对图像进行一定的过滤（例如：忽略图像中的某些部分）后的表示，通过过滤将当前生成的单词的重点放在感兴趣的部分。<br>注意力机制有一个有趣的特征：计算出的权重是可访问的并且可以被绘制出来。这正是我们之前展示的图片，如果权重越高，则对应部分的像素越白。</p><p>但是，这个黑盒子做了什么？下图能够清晰的表示Attention Model的原理：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/mmcbj2eK5C.png?imageslim" alt="mark"></p><p>可能这个网络图看起来比较复杂，我们一步一步来解释这个图。<br>首先，我们能够看出 一个输入c，是上下文，$y_i$是我们正在研究的“数据的一部分”。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/591aa4eId3.png?imageslim" alt="mark"></p><p>然后，网络计算$m_1,…,m_n$通过一个tanh层。这意味着我们计算$y_i$和c的一个”聚合”。重要的一点是，每个$m_i$的计算都是在不考虑其他$y_j,j \neq i$的情况下计算出来的。它们是独立计算出来的。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/fejdk9cA73.png?imageslim" alt="mark"></p><p>$$m_i = tanh(W_{cm}c+W_{ym}y_i)$$</p><p>然后，我们计算每个weight使用softmax。softmax，就像他的名字一样，它的行为和argmax比较像，但是稍有不同。$argmax(x_1,…,x_n)=(0,..,0,1,0,..,0)$,在输出中只有一个1，告诉我们那个是最大值。但是，softmax的定义为：$softmax(x_1,…,x_n)=(\frac {e^{x_i}}{\sum_j e^{x_j}})_i$。如果其中有一个$x_i$比其他的都大，$softmax(x_1,…,x_n)将会非常接近argmax(x_1,…,x_n)$。<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/g3JgH62d65.png?imageslim" alt="mark"></p><p>$$s_i 正比于 exp(w_m, m_i)\\\ \sum_i s_i=1$$<br>这里的$s_i$是通过softmax进行归一化后的值。因此，softmax可以被认为是“相关性”最大值的变量，根据上下文。<br>输出$z$是所有$y_i$的算术平均，每个权重值表示$y_i,..,y_n$和上下文c的相关性。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/43eLlm4gDb.png?imageslim" alt="mark"></p><p>$$z = \sum_i s_iy_i$$</p><h2 id="An-other-computation-of-“relevance”"><a href="#An-other-computation-of-“relevance”" class="headerlink" title="An other computation of “relevance”"></a>An other computation of “relevance”</h2><p>上面介绍的attention model是可以进行修改的。首先，tanh层可以被其他网络或函数代替。重要的是这个网络或函数可以综合c和$y_i$。比如可以只使用点乘操作计算c和$y_i$的内积，如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/8b2LBfc266.png?imageslim" alt="mark"></p><p>这个版本的比较容易理解。上面介绍的Attention是softly-choosing与上下文最相关的变量（$y_i$）。据我们所知，这两种系统似乎都能产生类似的结果。<br>另外一个比较重要的改进是hard attention。</p><h2 id="Soft-Attention-and-Hard-Attention"><a href="#Soft-Attention-and-Hard-Attention" class="headerlink" title="Soft Attention and Hard Attention"></a>Soft Attention and Hard Attention</h2><p>上面我们描述的机制称为Soft Attention，因为它是一个完全可微的确定性机制，可以插入到现有的系统中，梯度通过注意力机制进行传播，同时它们通过网络的其余部分进行传播。<br>Hard Attention是一个随机过程：系统不使用所有隐藏状态作为解码的输入，而是以概率$s_i$对隐藏状态$y_i$进行采样。为了进行梯度传播，使用蒙特卡洛方法进行抽样估计梯度。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/5c1A061BL6.png?imageslim" alt="mark"></p><p>这两个系统都有自己的优缺点，但是研究的趋势是集中于Soft Attention，因为梯度可以直接计算，并不是通过随机过程来估计的。</p><h2 id="Return-to-the-image-captioning"><a href="#Return-to-the-image-captioning" class="headerlink" title="Return to the image captioning"></a>Return to the image captioning</h2><p>现在，我们来理解一下image captioning 系统是怎样工作的。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/E6AGi93bJ9.png?imageslim" alt="mark"></p><p>从上面的图我们可以看到image captioning的典型模型，但是添加了一个新的关于attention model的层。当我们想要预测标题的下一个单词时，发生了什么？如果我们要预测第i个词，LSTM的隐藏状态是$h_i$。我们选择图像相关的部分通过把$h_i$作为上下文。然后，attention model的输出是$z_i$，这是被过滤的图像的表示，只有图像的相关部分被保留，用作LSTM的输入。然后，LSTM预测一个下一个词，并返回一个隐藏状态$h_{i+1}$。</p><h2 id="Learning-to-Align-in-Machine-Translation"><a href="#Learning-to-Align-in-Machine-Translation" class="headerlink" title="Learning to Align in Machine Translation"></a>Learning to Align in Machine Translation</h2><p>Bahdanau, et al[5]中的工作提出了一个神经翻译模型将句子从一种语言翻译成另一种语言，并引入注意力机制。在解释注意力机制之前，vanillan神经网络翻译模型使用了编码-解码(Encoder-Decoder)框架。编码器使用循环神经网络(RNN,通常GRU或LSTM)将用英语表示的句子进行编码，并产生隐藏状态h。这个隐藏状态h用于解码器RNN产生正确的法语的句子。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/JL6JJK2g25.png?imageslim" alt="mark"></p><p>编码器不产生与整个句子对应的单个隐藏状态，而是产生与一个词对应的隐藏状态$h_j$。每当解码器RNN产生一个单词时，取决于每个隐藏状态作为输入的贡献，通常是一个单独的参数（参见下图）。这个贡献参数使用Softmax进行计算：这意味着attention weights $a_j$在$\sum a_j=1$的约束下进行计算并且所有的隐藏状态$h_j$给解码器贡献的权重为$a_j$。<br>在我们的例子中，注意力机制是完全可微的，不需要额外的监督，它只是添加在现有的编码-解码框架的顶部。<br>这个过程可以看做是对齐，因为网络通常在每次生成输出词时都会学习集中于单个输入词。这就意味着大多数的注意力权重是0(黑)，而一个单一的被激活(白色)。下面的图像显示了翻译过程中的注意权重，它揭示了对齐方式，并使解释网络所学的内容成为可能（这通常是RNNs的问题！）。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/fbb3Gee98c.png?imageslim" alt="mark"></p><h2 id="Attention-without-Recurrent-Neural-Networks"><a href="#Attention-without-Recurrent-Neural-Networks" class="headerlink" title="Attention without Recurrent Neural Networks"></a>Attention without Recurrent Neural Networks</h2><p>到现在为止，我们仅介绍了注意力机制在编码-解码框架下的工作。但是，当输入的顺序无关紧要时，可以考虑独立的隐藏状态$h_j$。这个在Raffel et Al[10]中进行了介绍，这里attention model是一个前向全连接的网络。同样的应用是Mermory Networks[6]（参见下一节）。</p><h2 id="From-Attention-to-Memory-Addressing"><a href="#From-Attention-to-Memory-Addressing" class="headerlink" title="From Attention to Memory Addressing"></a>From Attention to Memory Addressing</h2><p>NIPS 2015会议上提出了一个非常有趣的工作叫做 RAM for Reasoning、Attention and Memory。它的工作包含Attention，但是也包括Memory Networks[6],Neural Turing Machines[7]或 Differentiable Stack RNNS[8]以及其他的工作。这些模型都有共同之处，它们使用一种外部存储器的形式，这种存储器可以被读取（最终写入）。<br>比较和解释这些模型是超出了这个本文的范围, 但注意机制和记忆之间的联系是有趣的。例如，在Memory Networks中，我们认为外部存储器-一组事实或句子$x_i$和一个输入q。网络学习对记忆的寻址，这意味着选择哪个事实$x_i$去关注来产生答案。这对应了一个attention model在外部存储器上。In Memory Networks, the only difference is that the soft selection of the facts (blue Embedding A in the image below) is decorrelated from the weighted sum of the embeddings of the facts (pink embedding C in the image).（PS：实在读不懂了）。在Neural Turing Machine中，使用了一个Soft Attention机制。这些模型将会是下个博文讨论的对象。<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/cHmIh83088.png?imageslim" alt="mark"></p><h2 id="Final-Word"><a href="#Final-Word" class="headerlink" title="Final Word"></a>Final Word</h2><p>注意力机制和其他完全可微寻址记忆系统是目前许多研究人员广泛研究而的热点。尽管它们仍然年轻, 在现实世界系统中没有实现, 但它们表明,它们可以处理在编码-解码框架下以前遗留的许多问题，它们可以被用来击败最先进的系统。<br>在Heuritech，几个月前，我们对注意力机制产生了兴趣，组织了一个小组，去实现带注意力机制的编码-解码器。虽然我们还没有在生产中使用注意机制,但我们设想它在高级文本理解中有一个重要的作用, 在某些推理是必要的, 以类似的方式，Hermann et al[9]中的工作和此类似。<br>在另一个单独的博客帖子中,我将详细阐述我们在研讨会上所学到的内容以及在RAM研讨会上提出的最新进展。<br>Léonard Blier et Charles Ollion</p><h2 id="Acknowledgments"><a href="#Acknowledgments" class="headerlink" title="Acknowledgments"></a>Acknowledgments</h2><p>在此感谢Mickael Eickenberg 和 Olivier Grisel的有益的讨论。</p><h2 id="Bibliography"><a href="#Bibliography" class="headerlink" title="Bibliography"></a>Bibliography</h2><p>[1] Itti, Laurent, Christof Koch, and Ernst Niebur. « A model of saliency-based visual attention for rapid scene analysis. » IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 11 (1998): 1254-1259.</p><p>[2] Desimone, Robert, and John Duncan. « Neural mechanisms of selective visual attention. » Annual review of neuroscience 18.1 (1995): 193-222.</p><p>[3] Cho, Kyunghyun, Aaron Courville, and Yoshua Bengio. « Describing Multimedia Content using Attention-based Encoder–Decoder Networks. » arXiv preprint arXiv:1507.01053 (2015)</p><p>[4] Xu, Kelvin, et al. « Show, attend and tell: Neural image caption generation with visual attention. » arXiv preprint arXiv:1502.03044 (2015).</p><p>[5] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. « Neural machine translation by jointly learning to align and translate. » arXiv preprint arXiv:1409.0473 (2014).</p><p>[6] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. « End-to-end memory networks. » Advances in Neural Information Processing Systems. (2015).</p><p>[7] Graves, Alex, Greg Wayne, and Ivo Danihelka. « Neural Turing Machines. » arXiv preprint arXiv:1410.5401 (2014).</p><p>[8] Joulin, Armand, and Tomas Mikolov. « Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets. » arXiv preprint arXiv:1503.01007 (2015).</p><p>[9] Hermann, Karl Moritz, et al. « Teaching machines to read and comprehend. » Advances in Neural Information Processing Systems. 2015.</p><p>[10] Raffel, Colin, and Daniel PW Ellis. « Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems. » arXiv preprint arXiv:1512.08756 (2015).</p><p>[11] Vinyals, Oriol, et al. « Show and tell: A neural image caption generator. » arXiv preprint arXiv:1411.4555 (2014).</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是对:&lt;a href=&quot;https://blog.heuritech.com/2016/01/20/attention-mechanism/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://blog.heuritech.com/2016/01/20/attention-mechanism/&lt;/a&gt; 的翻译。这篇文章对Attention Model原理进行了一个比较清晰的阐述，所以记录一下。由于本人英语能力有限，翻译不周的地方，还请见谅。&lt;br&gt;
    
    </summary>
    
      <category term="翻译文章" scheme="https://ilewseu.github.io/categories/%E7%BF%BB%E8%AF%91%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="Attention Model" scheme="https://ilewseu.github.io/tags/Attention-Model/"/>
    
  </entry>
  
  <entry>
    <title>GRU神经网络</title>
    <link href="https://ilewseu.github.io/2018/01/20/GRU%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://ilewseu.github.io/2018/01/20/GRU神经网络/</id>
    <published>2018-01-20T05:58:20.000Z</published>
    <updated>2018-01-20T15:38:11.786Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GRU神经网络"><a href="#GRU神经网络" class="headerlink" title="GRU神经网络"></a>GRU神经网络</h2><p>GRU(Gated Recurrent Unit)是LSTM的一种变体，它对LSTM做了很多简化，同时却保持着和LSTM几乎相同的效果。因此，GRU最近变得非常流行。下图是GRU的网络架构图。<br><a id="more"></a><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180113/Ja3I15iABg.jpg?imageslim" alt="mark"><br>GRU对LSTM做了两个大得改动：</p><ul><li>将<strong>输入门、遗忘门和输出门</strong>改变为两个门：<strong>更新门（Update Gate)$z_t$</strong>和<strong>重置门(Reset Gate)$r_t$</strong>。</li><li><strong>将单元状态与输出合并为一个状态：h</strong>。</li></ul><p>根据上图的架构图可以得出GRU的前向计算公式：<br>$$\begin{aligned}<br>&amp;r_t = \sigma(W_r \cdot [h_{t-1},x_t])\\\\<br>&amp;z_t =\sigma(W_z \cdot [h_{t-1},x_t])\\\\<br>&amp;\hat {h_t}=tanh(W_{\hat {h}} \cdot [r_t \bigodot h_{t-1},x_t])\\\\<br>&amp;h_t =(1-z_t)\bigodot h_{t-1}+z_t \bigodot \hat {h_t}\\\\<br>&amp;y_t=\sigma(W_o \cdot h_t)<br>\end{aligned}$$</p><h2 id="GRU的反向传播梯度计算"><a href="#GRU的反向传播梯度计算" class="headerlink" title="GRU的反向传播梯度计算"></a>GRU的反向传播梯度计算</h2><p>GRU的参数更新方式同样是基于沿时间反向传播的算法（BPTT）,为了为了更清晰的推导GRU反向传播梯度计算，对上文的GRU前向计算公式进行一定的改写，实质上还是一样的，只不过是将参数分开写而已。具体如下：假设，对于t时刻，GRU的输出为$\hat {y_t}$，输入为$x_t$，前一时刻的状态为$s_{t-1}$，则可以得出如下的前向计算的公式：<br>$$\begin{aligned}<br>    &amp;z_t = \sigma(U_zx_t+W_zs_{t-1}+b_z)\\\\<br>    &amp;r_t = \sigma(U_r x_t+W_rs_{t-1}+b_r)\\\\<br>    &amp;h_t = tanh(U_hx_t+W_h(s_{t-1}\bigodot  r_t)+b_h)\\\\<br>    &amp;s_t = (1-z_t)\bigodot  h_t + z_t \bigodot  s_{t-1}\\\\<br>    &amp;\hat {y_t}=softmax(Vs_t+b_V)<br>\end{aligned}<br>$$<br>其中，$\bigodot$表示向量的点乘；$z_t$表示更新门；$r_t$表示重置门；$\hat {y_t}$表示t时刻的输出。<br>如果采用交叉熵损失函数，那么在t时刻的损失为$L_t$:$$<br>L_t =sumOfAllElements(-y_t\bigodot log(\hat {y_t}))<br>$$<br>为了训练GRU，需要把所有时刻的损失加在一起，并最小化损失$L=\sum_{t=1}^T L_t$:<br>$$argmin_{\Theta}L$$<br>其中，$\Theta={U_z,U_r,U_c,W_z,W_r,W_c,b_z,b_r,b_c,V,b_V}$。</p><p>这是一个非凸优化问题，通常采用随机梯度下降法去解决问题。因此，需要计算$\partial L/ \partial U_z,\partial L/ \partial U_r,\partial L/ \partial U_h,\partial L/ \partial W_z,\partial L/ \partial W_r,\partial L/ \partial W_h,\partial L/ \partial b_z,\partial L/ \partial b_r,\partial L/ \partial b_h,\partial L/ \partial V,\partial L/ \partial b_v$。计算上面的梯度，最好的方式是利用链式法则从输出到输入一步一步去计算，为了更好得看清输入、中间值以及输出之间的关系，画了一张GRU的计算图，如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180120/bHHG1j84eF.jpg?imageslim" alt="mark"><br></div><br>根据计算图，利用链式法则计算梯度，需要从上至下沿着边进行计算。如果节点X有多条出边和目标节点T相连，如果要计算$\partial T / \partial X$，需要分别计算每条边对X的梯度，并将梯度进行相加。</p><p>以计算$\frac {\partial L}{\partial U_z}$为例，其他的计算方式和其相似。因为$L=\sum_{t=1}^T L_t$，所以，$\frac {\partial L}{\partial U_z}=\sum_{t=1}^T \frac {\partial L_t}{\partial U_z}$，因此，可以先计算$\frac {\partial L_t}{\partial U_z}$，然后将不同时刻结果加起来就可以。</p><p>根据链式法则:$$\frac {\partial L_t}{\partial U_z} = \frac {\partial L_t}{\partial s_t} \frac{\partial s_t}{\partial U_z}    (公式1)<br>$$<br>公式1右边的第一个式子的计算如下：$$\frac {\partial L_t}{\partial s_t}=V(\hat {y_t}-y_t)    (公式2)<br>$$<br>对于$\frac {\partial z}{\partial U_z}$，一些人可能会直接进行如下的求导计算<br>$$<br>\frac {\overline{\partial s_t}}{\partial U_z}=((s_{t-1}-h_t)\bigodot z_t \bigodot (1-z_t))x_t^T     (公式3)<br>$$<br>在$s_t$的计算公式里有$1-z$和$z\bigodot s_{t-1}$两个公式都会影响到$\frac {\partial s_t}{\partial U_z}$。正确的方法是分别计算每条边的偏导数，并将它们相加，因此，需要引入$\frac {\partial s_t}{\partial s_{t-1}}$。但是，公式3只计算了部分的梯度，因此用$\frac {\overline{\partial s_t}}{\partial U_z}$表示。</p><p>因为$s_{t-1}$同样依赖于$U_z$，因此我们不能把$s_{t-1}$作为一个常量处理。$s_{t-1}$同样会受到$s_i,i=1,…,t-2$的影响，因此，需要将公式1进行扩展，如下：<br>$$\begin{aligned}<br>    \frac {\partial L_t}{\partial U_z} = &amp;\frac {\partial L_t}{\partial s_t} \frac{\partial s_t}{\partial U_z}\\\\<br>    =&amp;\frac {\partial L_t}{\partial s_t}\sum_{i=1}^t(\frac{\partial s_t}{\partial s_i}\frac{\overline {\partial s_i}}{\partial U_z})\\\\<br>    =&amp;\frac {\partial L_t}{\partial s_t}\sum_{i=1}^t ((\prod_{j=i}^{t-1} \frac {\partial s_{j+1}}{\partial s_j})\frac{\overline {\partial s_i}}{\partial U_z})<br>\end{aligned}     (公式4)<br>$$<br>其中，$\frac{\overline {\partial s_i}}{\partial U_z}$是$s_i$对$U_z$的梯度，其计算公式如公式3所示。<br>$\frac {\partial s_t}{\partial s_{t-1}}$的计算和$\frac {\partial s_t}{\partial z}$的计算相似。因为从$s_{t-1}$到$s_t$有四条边，直接或间接相连，通过$z_t,r_t和h_t$，因此，需要计算这四条边上的梯度，然后进行相加，计算公式如下：$$\begin{aligned}<br>\frac {\partial s_t}{\partial s_{t-1}}=&amp;\frac {\partial s_t}{\partial h_t} \frac {\partial h_t}{\partial s_{t-1}}+\frac{\partial s_t}{\partial z_t}\frac{\partial z_t}{\partial s_{t-1}} + \frac {\overline {\partial s_t}}{\partial s_{t-1}}\\\\<br>=&amp;\frac {\partial s_t}{\partial h_t}(\frac {\partial h_t}{\partial r_t}\frac {\partial r_t}{\partial s_{t-1}}+\frac {\overline {\partial h_t}}{\partial s_{t-1}}) + \frac {\partial s_t}{\partial z_t}\frac{\partial z_t}{\partial s_{t-1}}+\frac {\overline {\partial s_t}}{\partial s_{t-1}}<br>\end{aligned}    (公式5)$$<br>其中，$\frac {\overline {\partial s_t}}{\partial s_{t-1}}$是对$s_t$关于$s_{t-1}$的导数，并将$h_t,z_t$看做常量。同样，$\frac {\overline {\partial h_t}}{\partial s_{t-1}}$是$h_t$关于$s_{t-1}$的导数，将$r_t$看做常量。最终，可以得到:<br>$$ \frac {\partial s_t}{\partial s_{t-1}}=(1-z_t)(W_r^T((W_h^T(1-h\bigodot h))\bigodot s_{t-1}\bigodot r \bigodot (1-r))+((W_h^T(1-h \bigodot h))\bigodot r_t))+\\\\W_z^T((s_{t-1}-h_t)\bigodot z_t \bigodot (1-z_t))+z     (公式6)$$<br>到此为止，$\frac {\partial L}{\partial U_z}$的计算已经完成，而其余的参数的计算和它的计算方式类似，沿着计算图一步一步计算，这里就不一一计算了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>A Tutorial On Backward Propagation Through Time (BPTT) In The Gated Recurrent Unit (GRU) RNN</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;GRU神经网络&quot;&gt;&lt;a href=&quot;#GRU神经网络&quot; class=&quot;headerlink&quot; title=&quot;GRU神经网络&quot;&gt;&lt;/a&gt;GRU神经网络&lt;/h2&gt;&lt;p&gt;GRU(Gated Recurrent Unit)是LSTM的一种变体，它对LSTM做了很多简化，同时却保持着和LSTM几乎相同的效果。因此，GRU最近变得非常流行。下图是GRU的网络架构图。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="GRU" scheme="https://ilewseu.github.io/tags/GRU/"/>
    
  </entry>
  
  <entry>
    <title>LSTM参数更新推导</title>
    <link href="https://ilewseu.github.io/2018/01/06/LSTM%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E6%8E%A8%E5%AF%BC/"/>
    <id>https://ilewseu.github.io/2018/01/06/LSTM参数更新推导/</id>
    <published>2018-01-06T07:45:20.000Z</published>
    <updated>2018-01-20T15:22:34.868Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转自：<a href="https://zybuluo.com/hanbingtao/note/581764，对其进行一定的整理。" target="_blank" rel="external">https://zybuluo.com/hanbingtao/note/581764，对其进行一定的整理。</a></p></blockquote><h2 id="LSTM前向计算"><a href="#LSTM前向计算" class="headerlink" title="LSTM前向计算"></a>LSTM前向计算</h2><a id="more"></a><p>在<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>一文中，介绍了LSTM的基本原理。LSTM网络使用了门(gete)的概念，门实际上就是一层全连接，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，b是偏置项，那么门可以表示为：$$<br>g(x) =\sigma(Wx+b)<br>$$<br>门的使用，就是用门的输出向量按元素乘以要控制的向量。因为门的输出是0到1之间的实数向量。所以，当门输出为0时，任何向量与之相乘都会得到0向量，这就相当于不能通过；当输出为1时，任何向量与之相乘都不会有任何改变，相当都通过。因为$\sigma$函数的值域是(0,1)，所以门的状态都是半开半闭的。</p><p>典型的LSTM的网络架构图如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180106/49m8miGdIC.jpg?imageslim" alt="mark"><br></div><br>相比RNN网络，LSTM新增加状态C，称为单元状态(cell state)，如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180106/adGab6A7GF.png?imageslim" alt="mark"></div></p><pre><code>图引自：https://zybuluo.com/hanbingtao/note/581764</code></pre><p><br>从图中可以看出，LSTM的输入有三个：<strong>当前时刻网络的输入$x_t$、上一时刻LSTM的输出值$h_{t-1}$以及上一时刻的单元状态$c_{t-1}$</strong>。LSTM的输出有两个：<strong>当前时刻LSTM输出值$h_t$和当前时刻的单元状态$c_t$</strong>。在这里x、h、c都是向量。</p><p>LSTM中引入了三个门：<strong>遗忘门(forget gate)、输入门(input gate)和输出门(output gate)</strong>。</p><ul><li><strong>遗忘门</strong>：它决定了上一时刻的单元状态$c_{t-1}$有多少保留到当前时刻$c_t$;</li><li><strong>输入门</strong>：它决定了当前时刻网络的输入$x_t$有多少保留到单元状态$c_t$。</li><li><strong>输出门</strong>：来控制单元状态$c_t$有多少输出到LSTM的当前输出值$h_t$。</li></ul><p><strong>遗忘门计算</strong><br>$$f_t = \sigma(W_f\cdot[h_{t-1},x_t]+b_f)        (公式1)$$<br>其中，$W_f$是遗忘门的权重矩阵，$[h_{t-1},x_t]$是表示把两个向量连接成一个更长的向量，$b_f$是遗忘门的偏置项，$\sigma$是sigmoid函数。事实上权重矩阵$W_f$是由两个矩阵拼接而成的，一个是$W_{fh}$，它对应着输入项$h_{t-1}$，一个是$W_{fx}$。$W_f$可以写为：$$<br>[W_f]\begin{bmatrix}h_{t-1}\\\\x_t\end{bmatrix}=\begin{bmatrix}<br>    W_{fh}&amp;W_{fx}<br>\end{bmatrix}\begin{bmatrix}h_{t-1}\\\\x_t\end{bmatrix}=W_{fh}h_{t-1}+W_{fx}x_t<br>$$</p><p><strong>输出门计算</strong><br>$$i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)    (公式2)$$</p><p><strong>单元状态$\hat{c_t}$</strong><br>$$\hat{c_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)    (公式3)$$<br>单元状态$c_t$,它是有上一时刻的单元状态$c_{t-1}$按元素乘以遗忘门$f_t$，再用当前输入的单元状态$\hat{c_t}$按元素乘以输入门$i_t$，再将两个积加和产生的：<br>$$c_t = f_t \bigodot c_{t-1}+i_t \bigodot \hat{c_t}    (公式4)$$<br>其中$\bigodot$表示按位相乘。这样就把LSTM关于当前的记忆$\hat{c_t}$和长期的记忆$c_{t-1}$组合在一起，形成了新的状态单元$c_t$。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，又可以避免当前无关紧要的内容进入记忆。下面看一下输出门，它控制了长期记忆对当前输出的影响：<br><strong>输出门</strong><br>$$o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)    (公式5)$$</p><p>LSTM最终的输出，是由输出门和单元状态共同确定的：<br>$$h_t = o_t \bigodot tanh(c_t)     (公式6)$$<br>从公式1到公式6就是LSTM的前向计算的全部公式。<br>LSTM前向传播的更新过程如下：</p><ol><li><strong>更新遗忘门的输出：</strong>$$f_t = \sigma(W_f\cdot[h_{t-1},x_t]+b_f)$$</li><li><strong>更新输入门的两部分输出:</strong>$$i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\\\\hat{c_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)$$</li><li><strong>更新细胞状态:</strong>$$c_t = f_t \bigodot c_{t-1}+i_t \bigodot \hat{c_t}$$</li><li><strong>更新输出门状态:</strong>$$o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\\\h_t = o_t \bigodot tanh(c_t)$$</li><li><strong>更新当前序列输出</strong>:$$\hat {y_t}=\sigma(Vh_t+b_y)$$</li></ol><h2 id="LSTM反向传播"><a href="#LSTM反向传播" class="headerlink" title="LSTM反向传播"></a>LSTM反向传播</h2><p>LSTM的训练算法，仍然是反向传播算法，主要有三个步骤：</p><ol><li>前向计算每个神经元的输出值，对于LSTM来说，即$f_t、i_t、c_t、o_t、h_t$ 5组向量。</li><li>反向计算每个神经元的误差项$\delta$。与循环神网络一样，LSTM误差项的反向传播也是包括两个方向：一个是沿时间的反向传播，即从当前时刻t开始，计算每个时刻的误差项；一个是将误差项向上一层传播。</li><li>根据相应的误差项，计算每个权重的梯度。</li></ol><p>LSTM需要学习的参数共有8组，分别是：遗忘门的权重矩阵$W_f$和偏置项$b_f$、输入门的权重矩阵$W_i$和偏置项$b_i$、输出门的权重矩阵$W_o$和偏置项$b_o$以及计算单元状态的权重矩阵$W_c$和偏置项$b_c$。因为权重矩阵的两部分在反向传播中使用不同的公式，因此，权重矩阵$W_f$、$W_i$、$W_c$、$W_o$都将被写为分开的两个矩阵:$W_{fh}$、$W_{fx}$、$W_{ih}$、$W_{ix}$、$W_{ch}$、$W_{cx}$、$W_{oh}$、$W_{ox}$。</p><p>在t时刻，LSTM的输出值为$h_t$，定义t时刻的误差项为$\delta_t$为:$$<br>\delta_t = \frac {\partial E}{\partial h_t}$$</p><p>因为LSTM有四个加权输入，分别为$f_t、i_t、c_t、o_t、h_t$，定义这四个加权输入，以及他们对应的误差项。$$<br>net_{f,t} = W_f[h_{t-1},x_t]+b_f=W_{fh}h_{t-1}+W_{fx}x_t+b_f\\\\<br>net_{i,t} = W_i[h_{t-1},x_t]+b_i=W_{ih}h_{t-1}+W_{ix}x_t+b_i\\\\<br>net_{\hat{c},t} = W_c[h_{t-1},x_t]+b_f=W_{ch}h_{t-1}+W_{cx}x_t+b_c\\\\<br>net_{o,t} = W_o[h_{t-1},x_t]+b_o=W_{oh}h_{t-1}+W_{ox}x_t+b_o\\\\<br>\delta_{f,t}=\frac {\partial E}{\partial net_{f,t}}\\\\<br>\delta_{i,t}=\frac {\partial E}{\partial net_{i,t}}\\\\<br>\delta_{\hat{c},t}=\frac {\partial E}{\partial net_{\hat{c},t}}\\\\<br>\delta_{o,t}=\frac {\partial E}{\partial net_{o,t}}<br>$$</p><h3 id="误差项沿时间反向传递"><a href="#误差项沿时间反向传递" class="headerlink" title="误差项沿时间反向传递"></a>误差项沿时间反向传递</h3><p>沿时间反向传递误差项，就是要计算出t-1时刻的误差项$\delta_{t-1}$。<br>$$<br>\delta_{t-1}^T=\frac {\partial E}{\partial h_{t-1}}=\frac {\partial E}{\partial h_{t}}\frac {\partial h_t}{\partial h_{t-1}}=\delta_t^T\frac  {\partial h_t}{\partial h_{t-1}}<br>$$<br>其中，$\frac {\partial h_t}{\partial h_{t-1}}$是一个Jacobian矩阵。如果隐藏层h的维度是N的话，那么它就是一个N*N的矩阵。为了求出它，先列出$h_t$的计算公式，即公式6和公式4：<br>$$<br>h_t=o_t\bigodot tanh(c_t)\\\\c_t=f_t \bigodot c_{t-1}+i_t \bigodot \hat {c_t}<br>$$<br>可以看出，$o_t、f_t、i_t、\hat{c_t}$都是$h_{t-1}$的函数，那么利用全导数公式可得：$$<br>\delta_t^T\frac {\partial h_t}{\partial h_{t-1}}=\delta_t^T\frac {\partial h_t}{\partial o_t}\frac {\partial o_t}{\partial net_{o,t}}\frac {\partial net_{o,t}}{\partial h_{t-1}}+\delta_t^T\frac {\partial h_t}{\partial c_t}\frac {\partial c_t}{\partial f_t}\frac{\partial f_t}{\partial net_{f,t}}\frac {\partial net_{f,t}}{\partial h_{t-1}}+\delta_t^T\frac{\partial h_t}{\partial c_t}\frac {\partial c_t}{\partial i_t}\frac {\partial i_t}{\partial net_{i,t}}\frac {\partial net_{i,j}}{\partial h_{t-1}}\\\\=\delta_{o,t}^T\frac {\partial net_{o,t}}{\partial h_{t-1}}+\delta_{f,t}^T\frac {\partial net_{f,t}}{\partial h_{t-1}}+\delta_{i,t}^T\frac {\partial net_{i,t}}{\partial h_{t-1}}+\delta_{\hat{c_t},t}^T\frac{\partial net_{\hat{c_t},t}}{\partial h_{t-1}}\\\(公式7)<br>$$<br>下面要把公式7中的每个偏导数都求出来，根据公式6，我们可以求出：$$<br>\frac {\partial h_t}{\partial o_t}= diag[tanh(c_t)]\\\\<br>\frac {\partial h_t}{\partial c_t}= diag[o_t\bigodot(1-tanh(c_t)^2)]<br>$$<br>根据公式4，可以求出：$$<br>\frac {\partial c_t}{\partial f_t} = diag[c_{t-1}]\\\\<br>\frac {\partial c_t}{\partial i_t} = diag[\hat{c_t}]\\\\<br>\frac {\partial c_t}{\partial \hat{c_t}} =diag[i_t]<br>$$<br>因为：$$\begin{aligned}<br>    &amp;o_t = \sigma(net_{o,t})\\\\<br>&amp;net_{o,t} = W_{oh}h_{t-1}+W_{ox}x_t+b_o\\\ <br>&amp;f_t = \sigma(net_{f,t})\\\ <br>&amp;net_{f,t} = W_{ft}h_{t-1}+W_{fx}x_t+b_f\\\ <br>&amp;i_t = \sigma(net_{i,t})\\\\<br>&amp;net_{i,t}=W_{ih}h_{t-1}+W_{ix}x_t+b_i\\\ <br>&amp;\hat{c_t}=tanh(net_{\hat{c},t})\\\\<br>&amp;net_{\hat{c},t}=W_{ch}h_{t-1}+W_{cx}x_t+b_c<br>\end{aligned}<br>$$<br>很容易得出:<br>$$\begin{aligned}<br>&amp;\frac {\partial o_t}{\partial net_{o,t}}= diag[o_t\bigodot(1-o_t)]\\\\<br>&amp;\frac {\partial_{o,t}}{\partial h_{t-1}}=W_{oh}\\\\<br>&amp;\frac {\partial f_t}{\partial net_{f,t}}=diag[f_t\bigodot(1-f_t)]\\\\<br>&amp;\frac {\partial net_{f,t}}{\partial h_{t-1}}=W_{fh}\\\\<br>&amp;\frac {\partial i_t}{\partial net_{i,j}}=diag[i_t\bigodot(1-i_t)]\\\\<br>&amp;\frac {\partial net_{i,t}}{\partial h_{t-1}}=W_{ih}\\\\<br>&amp;\frac {\partial \hat{c_t}}{\partial net_{\hat{c},t}}=diag[1-\hat{c_t}^2]\\\\<br>&amp;\frac {\partial net_{\hat{c},t}}{\partial h_{t-1}}=W_{ch}<br>\end{aligned}<br>$$<br>将上述偏导数带入公式7，可以得到：$$<br>\delta_{t-1} = \delta_{o,t}^T\frac {\partial net_{o,t}}{\partial h_{t-1}}+\delta_{f,t}^T\frac {\partial net_{f,t}}{\partial h_{t-1}}+\delta_{i,t}^T\frac {\partial net_{i,t}}{\partial h_{t-1}}+\delta_{\hat{c},t}^T\frac {\partial net_{\hat{c},t}}{\partial h_{t-1}}=\delta_{o,t}^TW_{oh}+\delta_{f,t}^TW_{fh}+\delta_{i,t}^TW_{i,h}+\delta_{\hat{c},t}^TW_{ch}    (公式8)$$</p><p>根据$\delta_{o,t}、\delta_{f,t}、\delta_{i,t}、\delta_{\hat{c},t}$的定义，可知：<br>$$<br>\begin{aligned}<br>&amp;\delta_{o,t}^T = \delta_t^T\bigodot tanh(c_t)\bigodot o_t\bigodot(1-o_t)(公式9)\\\\<br>&amp;\delta_{f,t}^T =\delta_t^T \bigodot o_t\bigodot(1-tanh(c_t)^2)\bigodot c_{t-1}\bigodot f_t \bigodot (1-f_t)(公式10)\\\\<br>&amp;\delta_{i,t}^T = \delta_t^T \bigodot o_t \bigodot(1-tanh(c_t)^2)\bigodot \hat{c_t}\bigodot i_t \bigodot (1-i_t)(公式11)\\\\<br>&amp;\delta_{\hat{c},t}^T=\delta_t^T\bigodot o_t\bigodot(1-tanh(c_t)^2)\bigodot i_t\bigodot (1-\hat{c}^2)(公式12)<br>\end{aligned}<br>$$<br>公式8到公式12就是将误差沿时间反向传播的一个时刻的公式。有了它，我们可以写出将误差向前传递到任意时刻k的公式：$$<br>\delta_k^T = \prod_{j=k}^t-1\delta_{o,j}^TW_{oh}+\delta_{f,j}^TW_{fh}+\delta_{i,j}^TW_{ih}+\delta_{\hat{c},j}^TW_{ch}<br>$$</p><h3 id="将误差传递到上一层"><a href="#将误差传递到上一层" class="headerlink" title="将误差传递到上一层"></a>将误差传递到上一层</h3><p>假设当前层为第l层，定义第l-1层的误差项是误差函数l-1层加权输入的导数，即:$$<br>\delta_t^{l-1}=\frac {\partial E}{\partial net_t^{l-1}}<br>$$<br>LSTM的输入$x_t$由下面的公式计算：<br>$$<br>x_t^l = f^{l-1}(net_t^{l-1})<br>$$<br>上式中，$f^{l-1}$表示第l-1层的激活函数。<br>因为$net_{f,t}^l、net_{i,t}^l、net_{\hat{c},t}^l、net_{o,t}^l$都是$x_t$的函数，$x_t$又是$net_t^{l-1}$的函数，因此，要求出E对$net_t^{l-1}$的导数，就需要使用全导数公式：<br>$$\begin{aligned}<br>\frac {\partial E}{\partial net_t^{l-1}}<br>&amp;=\frac {\partial E}{\partial net_{f,t}^l}\frac {\partial net_{f,t}^l}{\partial x_t^l}\frac {\partial x_t^l}{\partial net_t^{l-1}}+\frac {\partial E}{\partial net_{i,t}^l}\frac{\partial net_{i,t}^l}{\partial x_t^l}\frac {\partial x_t^l}{\partial net_{i,t}^{l-1}}+\frac {\partial E}{\partial net_{\hat{c},t}^l}\frac{\partial net_{\hat{c},t}^l}{\partial x_t^l}\frac {\partial x_t^l}{\partial net_t^{l-1}}+\frac{\partial E}{\partial net_{o,t}^l} \frac{\partial net_{o,t}^l}{\partial x_t^l}\frac{\partial x_t^l}{\partial net_{o,t}^{l-1}}\\\\<br>&amp;=\delta_{f,t}^TW_{fx}\bigodot f^{\prime}(net_t^{l-1})+\delta_{i,t}^TW_{ix}\bigodot f^{\prime}(net_t^{l-1})+\delta_{\hat{c},t}^TW_{cx}\bigodot f^{\prime}(net_t^{l-1})+\delta_{o,t}^TW_{ox}\bigodot f^{\prime}(net_t^{l-1})\\\\&amp;=<br>(\delta_{f,t}^TW_{fx}+\delta_{i,t}^TW_{ix}+\delta_{\hat{c},t}^TW_{cx}+\delta_{o,t}^TW_{ox})\bigodot f^{\prime}(net_l^{l-1})<br>\end{aligned}(公式14)$$</p><p>公式14就是将误差传递到上一层的公式。</p><h3 id="权重梯度计算"><a href="#权重梯度计算" class="headerlink" title="权重梯度计算"></a>权重梯度计算</h3><p>对度$W_{fh}、W_{ih}、W_{ch}、W_{oh}$的权重梯度，我们知道我们知道它的梯度是各个时刻梯度之和，我们首先求出它们在t时刻的梯度，然后再求出他们最终的梯度。我们已经求得误差项$\delta_{o,t}、\delta_{f,t}、\delta_{i,t}、\delta_{\hat{c},t}$很容易求出t时刻$W_{fh}、W_{ih}、W_{ch}、W_{oh}$的梯度。$$<br>\begin{aligned}<br>   &amp;\frac{\partial E}{\partial W_{oh,t}}=\frac{\partial E}{\partial net_{o,t}}\frac{\partial net_{o,t}}{\partial W_{oh,t}}=\delta_{o,t}h_{t-1}^T\\\\<br>   &amp;\frac{\partial E}{\partial W_{fh,t}}=\frac{\partial E}{\partial net_{f,t}}\frac{\partial net_{f,t}}{\partial W_{fh,t}}=\delta_{f,t}h_{t-1}^T\\\\<br>   &amp;\frac{\partial E}{\partial W_{ih,t}}=\frac{\partial E}{\partial net_{i,t}}\frac{\partial net_{i,t}}{\partial W_{ih,t}}=\delta_{i,t}h_{t-1}^T\\\\<br>   &amp;\frac{\partial E}{\partial W_{ch,t}}=\frac{\partial E}{\partial net_{\hat{c},t}}\frac{\partial net_{\hat{c},t}}{\partial W_{ch,t}}=\delta_{\hat{c},t}h_{t-1}^T<br>\end{aligned}<br>$$<br>将各个时刻的梯度加在一起，就能得到最终的梯度：$$<br>\begin{aligned}<br>&amp;\frac{\partial E}{\partial W_{oh}}=\sum_{j=1}^t\delta_{o,j}h_{j-1}^T\\\\<br>&amp;\frac{\partial E}{\partial W_{fh}}=\sum_{j=1}^t\delta_{f,j}h_{j-1}^T\\\\<br>&amp;\frac{\partial E}{\partial W_{ih}}=\sum_{j=1}^t\delta_{i,j}h_{j-1}^T\\\\<br>&amp;\frac{\partial E}{\partial W_{ch}}=\sum_{j=1}^t\delta_{\hat{c},j}h_{j-1}^T\\\\<br>\end{aligned}<br>$$<br>对于偏置项$b_f、b_i、b_c、b_o$的梯度，也是将各个时刻的梯度加在一起，下面是各个时刻的偏置梯度:<br>$$\begin{aligned}<br>&amp;\frac{\partial E}{\partial b_{o,t}}=\frac {\partial E}{\partial net_{o,t}}\frac{\partial net_{o,t}}{\partial b_{o,t}}=\delta_{o,t}\\\\<br>&amp;\frac{\partial E}{\partial b_{f,t}}=\frac {\partial E}{\partial net_{f,t}}\frac{\partial net_{f,t}}{\partial b_{f,t}}=\delta_{f,t}\\\\<br>&amp;\frac{\partial E}{\partial b_{i,t}}=\frac {\partial E}{\partial net_{i,t}}\frac{\partial net_{i,t}}{\partial b_{i,t}}=\delta_{i,t}\\\\<br>&amp;\frac{\partial E}{\partial b_{c,t}}=\frac {\partial E}{\partial net_{\hat{c},t}}\frac{\partial net_{\hat{c},t}}{\partial b_{c,t}}=\delta_{\hat{c},t}<br>\end{aligned}<br>$$<br>下面是最终的偏置项的梯度，即将各个时刻的偏置项梯度加在一起：<br>$$<br>\begin{aligned}<br>&amp;\frac {\partial E}{\partial b_o}=\sum_{j=1}^t\delta_{o,j}\\\\<br>&amp;\frac {\partial E}{\partial b_i}=\sum_{j=1}^t\delta_{i,j}\\\\<br>&amp;\frac {\partial E}{\partial b_f}=\sum_{j=1}^t\delta_{f,j}\\\\<br>&amp;\frac {\partial E}{\partial b_c}=\sum_{j=1}^t\delta_{\hat{c},j}<br>\end{aligned}<br>$$<br>对于$W_{fx}、W_{ix}、W_{cx}、W_{ox}$的权重梯度，只需要根据相应的误差项直接计算即可:<br>$$\begin{aligned}<br>&amp;\frac {\partial E}{\partial W_{ox}}=\frac{\partial E}{\partial net_{o,t}}\frac{\partial net_{o,t}}{\partial W_{ox}}=\delta_{o,t}x_t^T\\\\<br>&amp;\frac {\partial E}{\partial W_{fx}}=\frac{\partial E}{\partial net_{f,t}}\frac{\partial net_{f,t}}{\partial W_{fx}}=\delta_{f,t}x_t^T\\\\<br>&amp;\frac {\partial E}{\partial W_{ix}}=\frac{\partial E}{\partial net_{i,t}}\frac{\partial net_{i,t}}{\partial W_{ix}}=\delta_{i,t}x_t^T\\\\<br>&amp;\frac {\partial E}{\partial W_{cx}}=\frac{\partial E}{\partial net_{\hat{c},t}}\frac{\partial net_{\hat{c},t}}{\partial W_{cx}}=\delta_{\hat{c},t}x_t^T\\\\<br>\end{aligned}<br>$$</p><p>&lt;–end–&gt;</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文转自：&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/581764，对其进行一定的整理。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zybuluo.com/hanbingtao/note/581764，对其进行一定的整理。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;LSTM前向计算&quot;&gt;&lt;a href=&quot;#LSTM前向计算&quot; class=&quot;headerlink&quot; title=&quot;LSTM前向计算&quot;&gt;&lt;/a&gt;LSTM前向计算&lt;/h2&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="LSTM" scheme="https://ilewseu.github.io/tags/LSTM/"/>
    
      <category term="GRU" scheme="https://ilewseu.github.io/tags/GRU/"/>
    
  </entry>
  
  <entry>
    <title>理解LSTM</title>
    <link href="https://ilewseu.github.io/2017/12/31/%E7%90%86%E8%A7%A3LSTM/"/>
    <id>https://ilewseu.github.io/2017/12/31/理解LSTM/</id>
    <published>2017-12-31T09:35:20.000Z</published>
    <updated>2018-01-01T05:49:12.227Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文是<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/的译文，这篇文章对LSTM的原理讲解的非常清楚，故存下来。" target="_blank" rel="external">http://colah.github.io/posts/2015-08-Understanding-LSTMs/的译文，这篇文章对LSTM的原理讲解的非常清楚，故存下来。</a></p></blockquote><h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p>人类并非每一秒都从头开始思考问题。当你阅读这篇文章时，你是基于之前的单词来理解，每个单词。你并不会把所有的内容都抛弃掉，然后从头开始理解。你的思考具有持久性。<br><a id="more"></a><br>传统的神经网络并不能做到这一点，这似乎是其一个主要的缺点。例如，想象你要把一部电影里面的时间点正在发生的事情进行分类。传统神经网络并不知道怎样才能把关于之前事件的推理运用到之后的事件中去。RNN神经网络解决了这个问题。它们是一种具有循环的网络，具有保持信息的能力。如下图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/Bmh8aD2DdA.jpg?imageslim" alt="mark"><br></div><br>如上图所示，神经网络的模块A输入为$x_i$，输出为$h_t$。模块A的循环结构使得信息从网络的上一步传到了下一步。这个循环使得RNN看起来有点神秘。然而，如果你仔细想想就会发现它与普通的神经网络并没有太大不同。RNN可以被认为是相同网络的多重复制结构，每一个网络把消息传给其继承者。如果我们把循环体展开就是这样，如下图所示：<br><div align="center"><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/iK11DGDA83.jpg?imageslim" alt="mark"><br></div><p>这种链接属性表明，RNN与序列之间有着紧密的连续。这也是运用这类数据最自然的结构。当然它们已经得到了应用。过去几年中，RNNs已经被成功用于各式各样的问题中：语音识别、语言建模、翻译、图像标注..等等。RNNs取得的各种瞩目成果可以参考Andrej Karpathy的博客：The Unreasonable Effectiveness of Recurrent Neural Networks。确实效果让人非常吃惊。</p><p>取得这项成功的一个要素是LSTMs，这是一种非常特殊的周期神经网络，对于许多任务，比标准版要有效得多。几乎所有基于RNN的好成果都使用了它们。本文将着重介绍LSTMs。</p><h2 id="长期依赖问题-The-Problem-of-Long-Term-Dependencies"><a href="#长期依赖问题-The-Problem-of-Long-Term-Dependencies" class="headerlink" title="长期依赖问题(The Problem of Long-Term Dependencies)"></a>长期依赖问题(The Problem of Long-Term Dependencies)</h2><p>RNNs的一个想法是，它们可能会能够将之前的信息连接到现在的任务之中。例如，用视频前一帧的信息可以用于理解当前帧的信息。如果RNNs能够做到这些，那么将会非常有用。但是它们可以吗？ 这要看情况。</p><p>有时候，我们处理当前任务仅需要查看当前信息。例如，设想用一个语言模型基于当前单词尝试着去预测下一个单词。如果我们尝试着预测”the clouds are in the”的最后一个单词，我们并不需要任何额外的信息，很显然下一个但是sky。这样的话，如果目标预测的点与其他相关信息的点之间的间隔较小，RNNs可以学习利用过去的信息。</p><p>但是，也有时候我们需要更多的上下文信息。设想预测这句话的最后一个单词：”I grew up in France… I speak fluent <strong>French</strong>“。最近的信息表明下一个单词似乎是一种语言的名字，但是如果我们希望缩小确定语言类型的范围，我们需要更早之前作为France 的上下文。而且需要预测的点与其相关点之间的间隔非常有可能变得很大，如图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/3La0B0C0iK.jpg?imageslim" alt="mark"><br></div><br>不幸的是，随着间隔增长，RNNs变得难以学习连接之间的关系，如下图所示：<br><br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/d9IAjAB7IL.jpg?imageslim" alt="mark"><br></div><p>理论上来说，RNNs绝对能够处理这种『长期依赖』。人们可以小心选取参数来解决这种类型的小模型。悲剧的是，事实上，RNNs似乎并不能学习出来这些参数。这个问题已经在<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="external">Hochreiter (1991) German</a>与<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="external">Bengio, et al. (1994)</a>,中被深入讨论，他们发现了为何RNNs不起作用的一些基本原因。幸运的是，LSTMs可以解决这个问题!</p><div></div><h2 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h2><p>长短时间记忆网络(Long Short Term Memory networks, LSTMs)，是一种特殊的RNN，它能够学习长时间依赖。它们由<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">Hochreiter &amp; Schmidhuber (1997)</a>，后来由很多让人加以改进和推广。它们在大量的问题上都取得了巨大的成功，现在已经被广泛应用。</p><p>LSTMs是专门设计用来避免长期依赖问题的。记忆长期信息是LSTMs的默认行为，而不是它们努力学习的东西！所有RNN都具有链式的重复模块神经网络。在标准的RNNs中，这种重复模块具有非常简单的结构，比如是一个tanh层，如下图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/BF8kcb785A.jpg?imageslim" alt="mark"><br></div><br>LSTMs同样具有链式结构，但是其重复模块却有着不同的结构。不同于单独的神经网络层，它具有4个以特殊方式相互影响的神经网络层，如图所示：<br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/6IlDEAkI3L.jpg?imageslim" alt="mark"><br></div><br>不要担心接下来涉及到的细节。我们将会一步步讲解LSTM的示意图。下面是我们将要用到的符号，如图所示：<br><div><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/GC9cecCGhg.jpg?imageslim" alt="mark"><br></div><p>在上图中，每一条线代表一个完整的向量，从一个节点的输出到另一个节点的输入。粉红色圆形代表了逐点操作。例如，向量求和；黄色方框代表学习出的神经网络层；聚拢的线代表了串联，而分开的线代表了内容复制去了不同的地方。</p><h3 id="LSTMs背后的核心思想"><a href="#LSTMs背后的核心思想" class="headerlink" title="LSTMs背后的核心思想"></a>LSTMs背后的核心思想</h3><p>LSTMs的关键在于细胞状态，在图中以水平线表示。<br>细胞状态就像一个传送带。它顺着整个链条从头到尾运行，中间只有少许线性的交互。信息很容易顺着它流动而保持不变。如图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/92f48mcjDG.jpg?imageslim" alt="mark"><br></div><p>LSTM通过称之为门(gates)的结构来对细胞状态增加或删除信息。门是选择性的让信息通过的方式。它们的输出有一个sigmoid层和逐点乘积操作，如图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/Lek8DbbiFC.jpg?imageslim" alt="mark"><br></div><br>Sigmoid 层的输出在0到1之间，定义了各成分被放行通过的程度。0值意味着『不让任何东西过去』；1值意味着『让所有东西通过』。LSTM具有3种门，用于保护和控制细胞状态。<br><div> </div><h3 id="逐步讲解LSTM"><a href="#逐步讲解LSTM" class="headerlink" title="逐步讲解LSTM"></a>逐步讲解LSTM</h3><p>LSTM的第一步是决定我们要从细胞中抛弃何种信息。这个决定是由叫做『遗忘门』的sigmoid层决定的。它以$h_{t-1}$和$x_i$为输入，在$C_{t-1}$细胞输出一个介于0和1之间的数。其中，1代表『完全保留』，0代表『完全遗忘』。</p><p>让我们回到之前那个语言预测模型的例子，这个模型尝试着根据之前的单词学习预测下一个单词。在这个问题中，细胞状态可能包括了现在的主语的性别，因此能够使用正确的代词。当我们见到一个新的主语时，我们希望它能够忘记之前主语的性别。如下图所示：</p><div align="center"><img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/cG7AbK6EEC.jpg?imageslim" alt="mark"></div><br>下一步是决定细胞要存储何种信息。它有2个组成部分，首先一各叫做『输入门层』的sigmoid层决定我们将要更新哪些值。其次，一个tanh层创建一个新的候选向量$\hat{C}_t$，它可以加在状态之中。在下一步我们将结合两者来生成状态的更新。在语言模型的例子中，我们希望把新主语的性别加入到状态之中，从而取代我们打算遗忘的旧主语的性别，如下图所示：<br><div><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/Hce780KF2L.jpg?imageslim" alt="mark"><br></div><br>现在我们可以将旧细胞状态$C_{t-1}$更新为$C_t$了。之前的步骤已经决定了该怎么做，我们现在实际操作一下。我们把旧状态乘以$f_t$，用以遗忘之前我们决定忘记的信息。然后我们加上$i_t*\hat{C}_t$。这是新的候选值，根据我们决定更新状态的程度来作为缩放系数。<br><br>在语言模型中，这里就是我们真正丢弃关于旧主语性信息以及添加新信息的地方，如下图所示：<br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/BC8GaK4Fjc.jpg?imageslim" alt="mark"><br></div><br>最终，我们可以决定输出哪些内容。输出取决于我们的细胞状态，但是以一个过滤后的版本。首先，我们使用sigmoid层来决定我们要输出细胞状态的哪些部分。然后，把用tanh处理细胞状态（将状态值映射到-1至1之间）。最后，将其与sigmoid门的输出值相乘，从而我们能够输出我们决定输出的值。如下图所示：<br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/f2b7k0070c.jpg?imageslim" alt="mark"><br></div><br>对于语言模型，在预测下一个单词的例子中，当它输入一个主语，它可能会希望输出相关的动词。例如，当主语时单数或复数时，它可能会以相应形式的输出。<br><div></div><h3 id="各种LSTM的变化形式"><a href="#各种LSTM的变化形式" class="headerlink" title="各种LSTM的变化形式"></a>各种LSTM的变化形式</h3><p>目前，我所描述的都是普通的LSTM，然而并非所有的LSTM都是一样的。事实上，似乎每一篇使用LSTMs的文章都有细微的差别。这些差别很小，但是值得一提。</p><p>其中一个流行的LSTM变化形式是由<a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="external">Gers &amp; Schmidhuber (2000)</a>提出的，增加了『窥视孔连接（peephole connections）』。如下图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/8e4JBf654L.jpg?imageslim" alt="mark"><br></div><br>在上图中，所有的门都加上了窥视孔，但是许多论文中只在其中一些装了窥视孔。<br>另一个变种是使用了配对遗忘与输入门。与之前分别决定遗忘与添加信息不同，我们同时决定两者。只有当我们需要输入一些内容的时候我们才需要忘记。只有当早前信息被忘记之后我们才会输入。如图所示：<br><div><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/H16C53f5IC.jpg?imageslim" alt="mark"><br></div><p>LSTM 一个更加不错的变种是 Gated Recurrent Unit（GRU），是由<a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">Cho, et al. (2014)</a>提出的。这个模型将输入门与和遗忘门结合成了一个单独的『更新门』。而且同时还合并了细胞状态和隐含状态，同时也做了一下其他的修改。因此这个模型比标准LSTM模型要简单，并且越来越收到欢迎。如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/F2C472DeBH.jpg?imageslim" alt="mark"><br></div><br>这些仅仅只是LSTM的少数几个著名变种。还有很多其他的种类，例如由<a href="https://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="external">Yao, et al. (2015)</a> 提出的Depth Gated RNNs 。以及处理长期依赖问题的完全不同的手段，如<a href="https://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="external">Koutnik, et al. (2014)</a>提出的Clockwork RNNs。</p><p>那种变种是最好的？这些不同重要吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">Greff, et al. (2015)</a>将各种著名的变种做了比较，发现其实基本上是差不多的。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="external">Jozefowicz, et al. (2015)</a>测试了超过一万种RNN结构，发现了一些在某些任务上表现良好的模型。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>最开始我提到的杰出成就都是使用的是RNNs。本质上所有的这些成果都是使用了LSTMs。在大多数任务中，确实它们的表现非常优秀。</p><p>以公式的形式写下来，LSTMs看起来非常令人胆怯。然而本文的逐步讲解使得LSTM变得平易近人了。</p><p>LSTMs 是我们使用RNNs的重要一步。我们很自然地想到：还有下一个重要的一大步吗？研究者的普遍观点是：『有！下一大步就是「注意力」。』其基本思想就是让RNN的每一步从更大范围的信息中选取。例如，假设你为图片打标签，它可能会为它输出的每一个词语选取图片的一部分作为输入。事实上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="external">Xu, et al. (2015)</a>就是这么做的——如果你想探索『注意力』的话，这是个有趣的引子！已经有大量使用『注意力』得到的良好成果，而且似乎更多的陈果也将要出现……</p><p>『注意力』并非是RNN研究中唯一一个激动人心的方向。例如，<a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="external">Kalchbrenner, et al. (2015)</a>做出的Grid LSTMs 似乎很有前途。在生成模型中使用RNNs－例如<a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="external">Gregor, et al. (2015)</a>，<a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="external">Chung, et al. (2015)</a>以及<a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="external">Bayer &amp; Osendorfer (2015)</a>－似乎也很有趣。过去几年是RNN激动人心的阶段，未来几年将会更加如此！</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/的译文，这篇文章对LSTM的原理讲解的非常清楚，故存下来。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://colah.github.io/posts/2015-08-Understanding-LSTMs/的译文，这篇文章对LSTM的原理讲解的非常清楚，故存下来。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Recurrent-Neural-Networks&quot;&gt;&lt;a href=&quot;#Recurrent-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Recurrent Neural Networks&quot;&gt;&lt;/a&gt;Recurrent Neural Networks&lt;/h2&gt;&lt;p&gt;人类并非每一秒都从头开始思考问题。当你阅读这篇文章时，你是基于之前的单词来理解，每个单词。你并不会把所有的内容都抛弃掉，然后从头开始理解。你的思考具有持久性。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="LSTM" scheme="https://ilewseu.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络RNN 梯度推导(BPTT)</title>
    <link href="https://ilewseu.github.io/2017/12/30/RNN%E7%AE%80%E5%8D%95%E6%8E%A8%E5%AF%BC/"/>
    <id>https://ilewseu.github.io/2017/12/30/RNN简单推导/</id>
    <published>2017-12-30T03:28:20.000Z</published>
    <updated>2018-02-09T15:19:21.535Z</updated>
    
    <content type="html"><![CDATA[<h2 id="循环神经网络简介"><a href="#循环神经网络简介" class="headerlink" title="循环神经网络简介"></a>循环神经网络简介</h2><p>循环神经网络(Recurrent Neural Network,RNN)，是一种sequence model，它的思想就是使用序列信息。<strong>在前馈、卷积神经网络中，认为输入（和输出）彼此之间是互相独立的。但是对很多任务而言，这种处理方式很不合理。同时，在前馈、卷积神经网络中，输入和输出的维数都是固定的，不能任意改变，且无法处理变长的序列数据</strong>。<a id="more"></a>循环神经网络，它对于序列中的每个元素都执行相同的任务，输出依赖于之前的计算。另一种思考循环神经网络的方法是，它们有一个记忆，记忆可以捕获迄今为止已经计算过的信息。理论上循环神经网络可以利用任意长度的序列信息。但是，在实际应用中，由于梯度传播的原因，它们仅能利用有限步长。循环神经网络的网络结构图如下：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171230/HEIa45LFfD.jpg?imageslim" alt="mark"><br></div><p>在上图的网络中，网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。$h_t$的值不仅依赖于$x_t$，还取决于$s_{t-1}$。循环神经网络的计算方法如下：$$<br>o_t = g(V_{h_t})                      (公式1)\\\\<br>s_t = f(Ux_t+Ws_{t-1})    (公式2)<br>$$</p><p>其中，公式1是<strong>输出层</strong>的计算公式，输出层可以是一个全连接层，<strong>V</strong>是输出层的权重矩阵，<strong>g</strong>是相应的激活函数。公式2是<strong>隐藏层</strong>的计算公式，它是循环层。<strong>U</strong>是输入x的权重矩阵，<strong>W</strong>是上一层的输出值$s_{t-1}$作为这一次的输入的权重矩阵，<strong>f</strong>是激活函数。</p><p>可以看出，循环层和全连接层的区别就是多了一个权重矩阵W。如果把公式2反复带到公式1，我们可以得到：<br>$$<br>\begin{aligned}<br>o_t &amp;= g(Vs_t)\\\\<br>&amp;=Vf(Vs_t)\\\\<br>&amp;=Vf(Ux_t+Ws_{t-1})\\\\<br>&amp;=Vf(Ux_t+Wf(Ux_{t-1}+Ws_{t-2}))\\\\<br>&amp;=Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Ws_{t-3})))\\\\<br>&amp;=Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Wf(Ux_{t-3}+…))))<br>\end {aligned}<br>$$</p><p>从上面可以看出，循环神经网络的输出值$o_t$，是受前面的历次输入值$x_t、x_{t-1}、x_{t-2}、…$影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。<br><strong>循环神经网络的隐藏层的输出可以用于预测词汇/标签等符号的分布，隐藏层状态保留了到目前为止的历史信息。</strong></p><h2 id="循环神经网络的训练算法BPTT介绍"><a href="#循环神经网络的训练算法BPTT介绍" class="headerlink" title="循环神经网络的训练算法BPTT介绍"></a>循环神经网络的训练算法BPTT介绍</h2><p>循环神网络的训练算法是Backpropagation Through Time,BPTT算法，其基本原理和反向传播算法是一样的，只不过反向传播算法是按照层进行反向传播，BPTT是按照时间t进行反向传播。对于下图所示的循环神经网络：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171230/A0ih479fhh.jpg?imageslim" alt="mark"><br></div><br>这个图和上面的图所示的架构没有区别，只不过是把隐藏层的状态用$h_t$表示，t时刻的输出用$y_t$表示。$$<br>         h_t = f(Uh_{t-1}+Wx_t+b)\\\\y_t=softmax(Vh_t)<br>$$<br>假设循环神经网络在每个时刻t都有一个监督信息，损失为$J_t$，则整个序列的损失为$J=\sum_{t=1}^T J_t$。$$<br>J_t = -y_tlogy_t\\\\<br>    J = -\sum_{t=1}^T y_tlogy_t<br>$$</p><p><strong>1、J关于V的梯度计算</strong></p><p>$$<br>\frac {\partial J}{\partial V} = \frac {\partial}{\partial V} \sum_{t=1}^T J_t=\sum_{t=1}^T \frac {\partial J_t}{\partial V}<br>$$<br>令$$y_t = softmax(z_t)\\\ z_t = Vh_t$$，则$\frac {\partial J_t}{\partial V}$的计算公式如下：</p><p>$$<br>\frac {\partial J_t}{\partial V} = \frac {\partial J_t}{\partial y_t}\frac {\partial y_t}{\partial V}=\frac {\partial J_t}{\partial y_t} \frac {\partial y_t}{\partial z_t} \frac {\partial z_t}{\partial V}<br>$$</p><p><strong>2、损失J关于U的梯度计算</strong></p><p>$$<br>\frac {\partial J}{\partial U} = \frac {\partial}{\partial U} \sum_{t=1}^T \frac {\partial J_t}{\partial U}=\sum_{t=1}^T \frac {\partial h_t}{\partial U} \frac {\partial J_t}{\partial h_t}<br>$$<br>其中，$h_t$是关于U和$h_{t-1}$的函数，而$h_{t-1}$又是关于U和$h_{t-2}$的函数。</p><p>用链式法则可以得到：<br>$$\frac {\partial J}{\partial U} = \sum_{t=1}^T\sum_{k=1}^t \frac {\partial h_k}{\partial U} \frac {\partial h_t}{\partial h_k} \frac {\partial y_t}{\partial h_t}\frac {\partial J_t}{\partial y_t}\\\\<br>h_t = f(Uh_{t-1} + Wx_t+b)$$<br>其中，<br>$$<br>\frac {\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac {\partial h_i}{\partial h_{i-1}} = \prod_{i=k+1}^t U^Tdiag[f^{\prime}(h_{i-1})]<br>$$</p><p>则J对U的梯度为：<br>$$<br>\frac {\partial J}{\partial U} = \sum_{t=1}^T \sum_{k=1}^t \frac {\partial h_k}{\partial U}(\prod_{i=k+1}^t U^T diag[f^{\prime}(h_{i-1})])\frac {\partial y_t}{\partial h_t}\frac {\partial J_t}{\partial y_t}<br>$$</p><p><strong>3、损失J关于W的梯度计算</strong></p><p>$$\frac {\partial J}{\partial W} = \frac {\partial}{\partial W}\sum_{t=1}^T J_t=\sum_{t=1}^T \frac {\partial J_t}{\partial W}=\sum_{t=1}^T \frac {\partial h_t}{\partial W} \frac {\partial J_t}{\partial h_t}$$</p><p>用链式法则可以得到：<br>$$\frac {\partial J}{\partial W}=\sum_{t=1}^T \sum_{k=1}^t \frac{\partial h_k}{\partial W}\frac{\partial h_t}{\partial h_k}\frac {\partial y_t}{\partial h_t} \frac {\partial J_t}{\partial y_t}\\\\<br>h_t = f(Uh_{t-1} + Wx_t+b)$$</p><p>其中，<br>$$<br>\frac {\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac {\partial h_i}{\partial h_{i-1}} = \prod_{i=k+1}^t U^Tdiag[f^{\prime}(h_{i-1})]<br>$$<br>则，对W的梯度为：<br>$$<br>\frac {\partial J}{\partial W} = \sum_{t=1}^T\sum_{k=1}^t \frac {\partial h_k}{\partial W}(\prod_{i=k+1}^t U^Tdiag[f^{\prime}(h_{i-1})])\frac {\partial y_t}{\partial h_t}\frac {\partial J_t}{\partial y_t}<br>$$</p><p>如果定义$\gamma=||U^T diag(f^{\prime}(h_{i-1})||$，则在上面公式中的括号里面$\gamma^{t-k}$。如果$\gamma &gt; 1$，则当$t-k \rightarrow \infty$时，$\gamma^{t-k} \rightarrow \infty$，会造成系统的不稳定，也就是所谓的梯度爆炸问题；相反，如果$\gamma &lt; 1$，$t-k \rightarrow \infty$，$\gamma^{t-k} \rightarrow 0$，会出现和深度前馈神经网络类似的梯度消失的问题。</p><p>因此，虽然简单循环网络可从理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度爆炸或消失的存在，实际上只能学习到短期的依赖关系，这就是所谓的长期依赖问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;循环神经网络简介&quot;&gt;&lt;a href=&quot;#循环神经网络简介&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络简介&quot;&gt;&lt;/a&gt;循环神经网络简介&lt;/h2&gt;&lt;p&gt;循环神经网络(Recurrent Neural Network,RNN)，是一种sequence model，它的思想就是使用序列信息。&lt;strong&gt;在前馈、卷积神经网络中，认为输入（和输出）彼此之间是互相独立的。但是对很多任务而言，这种处理方式很不合理。同时，在前馈、卷积神经网络中，输入和输出的维数都是固定的，不能任意改变，且无法处理变长的序列数据&lt;/strong&gt;。
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="CNN" scheme="https://ilewseu.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络学习笔记</title>
    <link href="https://ilewseu.github.io/2017/12/23/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://ilewseu.github.io/2017/12/23/卷积神经网络学习笔记/</id>
    <published>2017-12-23T11:48:20.000Z</published>
    <updated>2018-01-01T05:50:41.359Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文是对近期学习的卷积神经网络相关知识的简单记录和梳理。</p></blockquote><h2 id="卷积神经网络简介"><a href="#卷积神经网络简介" class="headerlink" title="卷积神经网络简介"></a>卷积神经网络简介</h2><p>卷积神经网络(Convolution Neural Network，CNN或ConvNet)是一种前馈神经网络。卷积神经网络是受生物学上<strong>感受野</strong>(Receptive Field)的机制提出来的。一个神经元的感受野是指特定区域，只有这个区域内的刺激才能够激活该神经元。<br><a id="more"></a></p><blockquote><p>感受野，主要是指听觉、视觉等神经系统中一些神经元的特性，即神经元只接受其所支配的刺激区域内的信号。在视觉神经系统中，视觉皮层中的神经细胞的输出依赖于视网膜上的光感受器。视网膜上的光感受器受刺激兴奋时，将神经冲动信号传到视觉皮层，但不是所有的视觉皮层中的神经元都会接受这些信号。一个神经元的感受野指视网膜上的特定区域，只有这个区域内的刺激才能够激活该神经元。</p></blockquote><p>卷积神经网络最早是主要处理图像信息。如果用全连接前馈神经网络来处理图像时，会存在以下两个问题：</p><ul><li><strong>参数太多</strong>：如果图像的输入大小为100x100x3，在使用全连接前馈神经网络中，第一个隐藏层的每个神经元到输入层都有100x100x3=30,000个相互独立的连接，每个连接都对应一个权重参数。随着隐藏层神经元数量增多，参数的规模也会增加。这会导致整个神经网络的训练效率会非常低下，也会很容易出现过拟合。</li><li><strong>局部不变性特征</strong>： 自然图像中的物体都具有局部特征不变性，比如在尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈神经网络很难提取这些局部不变性，一般需要进行数据增强来提高性能。</li></ul><p>目前的卷积神经网络一般是由卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络，使用反向传播算法进行训练。卷积神经网络有三个结构上的特性：<strong>局部连接、权重共享以及子采样</strong>。这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数更少。</p><h2 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h2><h3 id="卷积介绍"><a href="#卷积介绍" class="headerlink" title="卷积介绍"></a>卷积介绍</h3><p>卷积(convolution)，是数学分析中一种重要的运算。在信号处理或图像中，经常使用一维卷积或二维卷积。<br><strong>一维卷积</strong>，一维卷积常用在信号处理中，用于计算信号的延迟累积。假设一个信号发生器每个时刻t产生一个信号$x_t$,其信息的衰减率为$f_k$，即在k-1个时间步长后，信息变为原来的$f_k$倍。假设$f_1=1,f_2=1/2,f_3=1/4$，那么在时刻t收到的信号为$y_t$为当前时刻产生的信息和以前时刻延迟信息的叠加，$$<br>y_t = 1\times x_t + 1/2 \times x_{t-1} + 1/4 \times x_{t-2} \\\\<br>=f_1 \times x_t + f_2 \times x_{t-1} + f_3 \times x_{t-2}<br>$$<br>我们把$f_1,f_2…$称为滤波器(filter)或卷积核(convolution kernel)。假设滤波器的长度为m，它和一个信号序列$x_1,x_2…$的卷积为：$$<br>y_t = \sum_{k-1}^m f_k \cdot x_{t-k+1}<br>$$<br>信号序列<strong>x</strong>和滤波器<strong>w</strong>的卷积定义为：$$<br>y = w \bigotimes x<br>$$<br>一般情况下，滤波器的长度m远小于信号序列的长度n。当$f_k=1/m$时，卷积相当于移动平均。下图是一个一维卷积的例子：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/8b34KGFaEd.jpg?imageslim" alt="mark"><br></div><p><strong>二维卷积</strong>，卷积也经常用于图像处理中。因为图像是一个二维结构，需要将一维卷积进行扩展。给定一个图像$X \in R^{M \times N}$和滤波器$W \in R^{M \times N }$，一般m&lt;&lt;M，n&lt;&lt;N，其卷积为：$$<br>y_{ij} = \sum_{u=1}^m \sum_{v=1}^n w_{uv}\cdot x_{i-u+1,j-v+1}    (公式1)<br>$$</p><h3 id="卷积的类型"><a href="#卷积的类型" class="headerlink" title="卷积的类型"></a>卷积的类型</h3><p>根据在输入信号两端的补0的情况可以将卷积分为：<strong>窄卷积、宽卷积和等长卷积</strong>。</p><p><strong>一维卷积</strong></p><ul><li><strong>窄卷积</strong>，在信号两端不补0，输出信号长度为n-m+1；</li><li><strong>宽卷积</strong>，信号两端各补m-1个0，输出信号长度为n+m-1；</li><li><strong>等长卷积</strong>，信号两端各补(m-1)/2个0， 输出信号长度为n;</li></ul><p><strong>二维卷积</strong></p><ul><li><strong>窄卷积</strong>，信号四周不补0，输出信号长度为M-m+1*N-n+1;</li><li><strong>宽卷积</strong>，信号四周补0，输出长度为M+m-1*N+n-1;</li><li><strong>等长卷积</strong>，信号四周补0，输出长度为M*N;</li></ul><h3 id="卷积神经网络中的卷积"><a href="#卷积神经网络中的卷积" class="headerlink" title="卷积神经网络中的卷积"></a>卷积神经网络中的卷积</h3><p>在机器学和图像处理领域，卷积主要的功能是在一个图像上滑动一个卷积核，通过卷积核操作得到一组新的特征。在计算卷积的过程中，需要进行卷积的翻转。在具体的实现上，一般会以互相关操作来代替卷积，从而会减少一些不必要的操作或开销。<strong>互相关</strong>(cross-correlation)是一个衡量两个序列相关性的函数，通常是用滑动窗口的点积计算来实现。给定一个图像$x\in R^{M \times N}$和卷积核$W\in R^{m\times n}$，它们的互相关为：$$<br>y_{ij}=\sum_{u=1}^m \sum_{v=1}^n w_{uv} \cdot x_{i+u-1,j+v-1}<br>$$<br>和公式1相比，互相关和卷积的主要区别在于卷积核仅仅是否进行翻转。因此，互相关也可以称为不翻转卷积。</p><blockquote><p>翻转，就是从两个维度(从上到下、从左到右)颠倒次序，即旋转180度。</p></blockquote><p>在神经网络中使用卷积是为了进行特征抽取，卷积核是否进行翻转和其特征的抽取的能力无关。特别是当卷积核是可学习的参数时，卷积核互相关是等价的。因此，为了实现上的方便，卷积神经网络中的卷积实际上用互相关操作来代替卷积，可以将互相关表示为$$<br>Y=W \bigotimes X<br>$$<br>其中，$Y\in R^{M-m+1,N-n+1}$为输出矩阵。</p><p>互相关运算，即常用的卷积神经网络中的卷积操作示例如下图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/E8hkhejggf.gif" alt="mark"><br></div><h3 id="卷积运算的性质"><a href="#卷积运算的性质" class="headerlink" title="卷积运算的性质"></a>卷积运算的性质</h3><p>卷积具有很多很好的性质，下面就介绍一下二维卷积的数学性质，同样适用于一维卷积。</p><p><strong>交换性</strong></p><p>如果不限制两个卷积信号的长度，卷积是具有交换性的，即$x \bigotimes y=y\bigotimes x$。</p><p>当输入信息和卷积核有固定长度时，它们的宽卷积依然具有交换性。</p><p>对于两维的图像$x\in R^{M \times N}$和卷积核$W\in R^{m\times n}$，对图像X的两个维度进行零填充，两端各补m-1和n-1个零，得到全填充的图像$\hat{X} \in R^{(M+2m-2) \times (N+2n-2)}$</p><p>图像X和卷积核W的宽卷积定义为：$$<br>\hat{Y}=W\hat{\bigotimes}X<br>$$<br>其中，$\hat{\bigotimes}$ 为宽卷积操作。<br>宽卷积具有交换性，即：$$<br>W\hat{\bigotimes}X=X\hat{\bigotimes}W<br>$$</p><p><strong>导数</strong></p><p>假设$Y=W\bigotimes X$，其中$x\in R^{M \times N}$，$W\in R^{m\times n}$，函数$f(Y)\in R$为一个标量函数，则：$$<br>\frac {\partial f(Y)}{\partial w_{uv}}=\sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}\frac{\partial y_{ij}}{\partial w_{uv}}\frac {\partial f(Y)}{\partial y_{ij}}\\\\<br>=\sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}x_{i+u-1,j+v-1}\frac {\partial f(Y)}{\partial y_{ij}}\\\\=\sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}\frac {\partial f(Y)}{\partial y_{ij}}x_{u+i-1,v+j-1}<br>$$<br>可以看到，f(Y)关于W的偏导数为X和$\frac {\partial f(Y)}{\partial Y}$的卷积$$<br>\frac {\partial f(Y)}{\partial (W)} = \frac {\partial f(Y)}{\partial Y} \bigotimes X     (公式2)<br>$$<br>同理可以得到：$$<br>\frac {\partial f(Y)}{\partial x_{st}} = \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1} \frac {\partial y_{ij}}{\partial x_{st}} \frac {\partial f(Y)}{\partial {y_{ij}}}\\\ = \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1} w_{s-i+1,t-j+1}\frac {\partial f(Y)}{\partial y_{ij}}     (公式3)<br>$$<br>其中，当(s-i+1)<1时，或(s-i+1)>m，或(t-j+1)<1，或(t-j+1)>n时，$w_{s-i+1,t-j+1}$，相当于对W进行了p=(M-m,N-n)的零填充。</1，或(t-j+1)></1时，或(s-i+1)></p><p>可以看到，f(Y)关于X的偏导数为W和$\frac {\partial f(Y)}{\partial Y}$的宽卷积。公式3中的卷积是真正的卷积，而不是互相关，为了一致性，我们用互相关的“卷积”，即：$$<br>\frac {\partial f(Y)}{\partial X} = rot180(\frac {\partial f(Y)}{\partial Y}) \hat {\bigotimes}W \\\\=rot180(W) \hat {\bigotimes} \frac {\partial f(Y)}{\partial Y}<br>$$</p><h2 id="卷积神经网络结构"><a href="#卷积神经网络结构" class="headerlink" title="卷积神经网络结构"></a>卷积神经网络结构</h2><p>首先，看一下典型的卷积神经网络的结构：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/0a5d261ald.png?imageslim" alt="mark"><br><br>    图片引自：<a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">https://www.zybuluo.com/hanbingtao/note/485480</a><br></div><br>如上图所示，一个卷积神经网络由若干<strong>卷积层、Pooling层、全连接层</strong>组成。通过设置不同的卷积层、Pooling层以及全连接层，可以构建不同的卷积神经网络结构，它常用的架构模式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Input -&gt; [[卷积层]*N-&gt;Pooling ?] * M -&gt; [全连接层]*K</div></pre></td></tr></table></figure><br><br>也就是N个卷积层叠加，然后(可选)叠加一个Pooling层，重复这个结构M次，最后叠加K个全连接层。下面介绍每个层的作用。<br><div></div><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>全连接前馈神经网络中，第l-1层的$n^{(l-1)}$个神经元和第l层的$n^{(l)}$个神经元的连接采用的是全连接的方式，则权重矩阵有$n^{(l)}\times n^{(l-1)}$个参数，当l-1层和l层的神经元过多时，权重矩阵的参数非常多，训练的效率会非常低。</p><p>如果采用卷积来代替全连接，第l层的净输入$z^{(l)}$为第l-1层激活值$a^{(l-1)}$和滤波器$w^{(l)}$的卷积，即：$$<br>z^{(l)} = w^{(l)} \bigotimes a^{(l-1)} + b^{(l)}      (公式4)<br>$$<br>其中，滤波器$w^{(l)}$为权重向量，$b^{(l)}\in R^{(n^{l-1})}$为偏置。</p><p>根据卷积的定义，卷积层具有如下两个性质：</p><ul><li><p><strong>局部连接</strong> 在卷积层中的每一个神经元都和下一层某个局部窗口内的神经元相连，构成一个局部连接网络。如下图所示，卷积层和下一层之间的连接数大大减少，由原来的$n^{(l)}\times n^{(l-1)}$个连接变为$n^{(l)} \times m$个连接，m为滤波器的大小。</p><div align="center"><br>  <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/0mjIc9fDE5.jpg?imageslim" alt="mark"><br></div></li><li><p><strong>权重共享</strong>  从公式4可以看出，作为参数的滤波器$w^{(l)}$对于第l层的所有的神经元都是相同的。从上图也可以看出，所有同颜色连接上的权重是相同的。</p></li></ul><p>卷积层的主要作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。上面介绍的卷积层的神经元和全连接都是一维结构。对于常见的图像为二维结构，因此为了更充分的利用图像的局部信息，通常将神经元组织为三维结构，其大小为$宽度M \times 高度N \times 深度D$，有D个$M\times N$的特征映射构成。</p><p><strong>特征映射（Feature Map)</strong> 为了增强卷积层的表示能力，我们可以使用K个不同的滤波器来得到K组不同的输出。每组输出都共享一个滤波器。如果我们把滤波器看成是一个特征提取器，每一组输出都可以看成是输入图像经过一个特征抽取后得到的特征。因此，在卷积神经网络中每一组输出也叫作一组<strong>特征映射</strong>。</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/CBbj606Kdd.jpg?imageslim" alt="mark"><br><br>    图片引自： <a href="https://nndl.github.io/" target="_blank" rel="external">https://nndl.github.io/</a> 卷积神经网络<br></div><p>在输入层，特征映射就是图像本身，如果图像是灰度图像，就是一个特征映射，深度为D=1；如果是彩色图像，分别有RGB三个颜色通道的特征映射，输入深度D=3。</p><p>假设一个卷积层的结构如下：</p><ul><li>输入特征映射组：$X \in R^{M \times N \times D}$为三维张量(tensor)，每个切片为矩阵$X^d \in R^{M \times N}$为一个输入特征映射，1&lt;=d&lt;=D；</li><li>输出特征映射组：$Y \in R^{M^{\prime}\times N^{\prime} \times P^{\prime}}$为三维张量，其中每个切片矩阵$Y^p \in R^{M^{\prime}\times N^{\prime}}，$1&lt;=p&lt;=P；</li><li>卷积核：$W \in R^{m\times n \times D \times P}$为四维张量，其中每个切片矩阵$W^{p,d}\in R^{m \times n}$为一个两维卷积核，1&lt;=d&lt;=D，1&lt;=p&lt;=P;</li></ul><p>为了计算输出特征映射$Y^p$，用卷积核$W^{p,1},W^{p,2},…,W^{p,D}$分别对输入特征$X^1,X^2,…,X^D$进行卷积，然后将卷积结果相加，并加上一个标量偏置b得到卷积层的净输入$Z^P$，再经过非线性激活函数得到最终的输出特征映射$Y^p$。$$<br>Z^p = W^p \bigotimes X + b^p = \sum_{d=i}^D W^{p,d} \bigotimes X^d + b^p\\\\<br>Y^p = f(Z^p)<br>$$</p><p>其中，$W^p \in R^{m \times n \times D}$为三维卷积核，f(.)为非线性激活函数，一般用ReLU函数。整个计算的过程如下图所示。如果希望卷积层输出P个特征映射，可以将上述计算过程重复P次，得到P个输出特征映射，$Y^1,Y^2,…,Y^P$。</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/AE2h0aHFgl.jpg?imageslim" alt="mark"><br><br>    图片引自： <a href="https://nndl.github.io/" target="_blank" rel="external">https://nndl.github.io/</a> 卷积神经网络<br></div><p>在输入为$X \in R^{M \times N \times D}$，输出为$Y \in R^{M^{\prime}\times N^{\prime} \times P^{\prime}}$的卷积层中，每一个输入特征映射都需要D个滤波器以及一个偏置。假设每个滤波器的大小为$m \times n$，那么共需要$P \times D \times (m \times n)+P$个参数。</p><h3 id="Pooling层"><a href="#Pooling层" class="headerlink" title="Pooling层"></a>Pooling层</h3><p>汇聚层(Pooling Layer)，也叫子采样层(subsampling layer)，作用就是进行特征选择，降低特征数量，从而减少参数的数量。卷积层虽然可以减少网络中连接的数量，但特征映射组中的神经元个数没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合。为了解决这个问题，可以在卷积层之后加一个汇聚层，从而降低特征的维数，避免过拟合。</p><p>对于卷积层得到的一个特征映射$X^{(l)}$，我们可以将$X^{(l)}$划分为很多区域$R_K,k=1,2…,K$。区域$R_k$可以重叠，也可以不重叠，则采样层的输出有：$$<br>X^{(l+1)} = f(W^{(l+1)}\cdot down(R_k)+b^{(l+1)})<br>$$<br>其中，$w^{(l+1)}$和$b^{(l+1)}$分别是可训练的权重和偏置参数。于是，可以简化成如下公式：$$<br>X^{(l+1)} = f(W^{(l+1)}\cdot down(X^{(l)})+b^{(l+1)})<br>$$<br>其中，$down(X^{(l)})$是指子采样后的特征映射。</p><p>常见的采样方式如下：</p><ul><li><p>最大值采样（Maximum Pooling） </p><p>  $pool_{max}(R_k) = max_{i\in R_k}a_i$</p></li><li><p>最小值采样（Minimum Pooling）<br>  $pool_{min}(R_k) = min_{i\in R_k}a_i$</p></li><li><p>平均值采样（Average pooling）<br>  $pool_{ave}(R_k) = \frac {1}{|R_k|}\sum_{i\in R_k}^{|R_k|} a_i$</p></li><li><p>TopK采样</p><p>  $pool_k(R_k) = topk_{i \in R_k}a_i$</p></li></ul><p>下图为最大值采样的一个示例：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/c6AaDDg9d1.jpg?imageslim" alt="mark"><br><br><br></div><p>典型的汇聚层是将每个特征映射划分为2X2的大小的不重叠的区域，然后使用最大汇聚的方式进行下采样。汇聚层也可以看做是一个特殊的卷积层，卷积核的大小为$m\times m$，步长为$s \times s$，卷积核为max函数或者mean函数。过大的采样区域会急剧减少神经元的数量，造成过多的信息损失。</p><h2 id="卷积神经网络参数学习"><a href="#卷积神经网络参数学习" class="headerlink" title="卷积神经网络参数学习"></a>卷积神经网络参数学习</h2><p>在卷积神经网络中，参数为卷积核中的权重以及偏置。和前馈神经网络类似，卷积神经网络也可以通过误差反向传播算法来进行参数学习。</p><p>在全连接前馈神经网络中，梯度主要通过每一层的误差项$\delta$进行反向传播，并进一步计算每层参数的梯度。在卷积神经网络中，主要有两种不同功能的神经层：<strong>卷积层和汇聚层</strong>。 </p><p>对第l层为卷积层，第l-1层的输入特征为$X^{(l-1)} \in R^{M \times N \times D}$，通过卷积计算得到第l层的特征净输入为$Z^{(l)}\in R^{M^{\prime} \times N^{\prime} \times P}$，第l层的第p个特征净输入为：$$<br>Z^{(l,p)} = \sum_i^D W^{(l,p,d)} \bigotimes X^{(l-1,d)} + b^{(l,p)}<br>$$<br>其中，$W^{(l,p,d)}$和$b^{(l,p)}$为卷积核及偏置。第l层中共有$P \times D$个卷积核和P个偏置，可以分别使用链式法则来计算器梯度。$$<br>\frac {\partial L(Y,\hat{Y})}{\partial W^{(l,p,d)}} = \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l,p)}} \bigotimes X^{(l-1, d)}\\\\=\delta^{l,p} \bigotimes X^{(l-1,d)}<br>$$<br>其中，$\delta^{(l,p)}$为损失函数关于第l层的第p个特征映射净输入$Z^{(p,l)}$的偏导数。</p><p>同理可以得到，损失函数关于第l层第p个偏置$b^{(l,p)}$的梯度为：<br>$$<br>\frac {\partial L(Y,\hat {Y})}{\partial b^{(l,p)}} = \sum_{i,j}[\delta^{(l,p)}]_{i,j}.<br>$$<br>因此，卷积网络的每层参数的梯度也依赖于其所在层的误差项$\delta^{(l,p)}$。卷积层和汇聚层中，误差项的计算有所不同，因此，需要分别计算误差项。</p><p><strong>汇聚层</strong></p><p>当第l+1层为汇聚层时，因为汇聚层是下采样操作，l+1层的每个神经元的误差项$\delta$对应于第l层的相应特征的一个区域。l层的第p个特征映射中的每个神经元都有一条边和l+1层的第p个特征映射中的一个神经元相连。根据链式法则，第l层的一个特征映射的误差项$\delta^{(l,p)}$，只需要将l+1层对应的特征映射误差项$\delta^{(l+1, p)}$进行上采样，再和l层的特征映射的激活值偏导数逐元素相乘就得到了$\delta^{(l,p)}$。 </p><p>第l层的第p个特征映射的误差项$\delta^{(l,p)}$的具体推导如下：$$<br>\delta^{(l,p)} = \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l,p)}}\\\\<br>=\frac {\partial X^{(l,p)}}{\partial Z^{(l,p)}} \cdot \frac {\partial Z^{(l+1,p)}}{\partial X^{(l,p)}} \cdot frac {\partial L(Y,\hat {Y})}{\partial Z^{(l+1, p)}}\\\\=f_l^\prime(Z^{(l,p)}) \bigodot up(\delta^{(l+1, k)})<br>$$<br>其中，$f_l^\prime(Z^{(l,p)})$为第l层使用的激活函数导数，up为上采样函数，与汇聚层中使用的下采样操作刚好相反。如果下采样是最大汇聚，误差项$\delta^{(l+1, k)}$中的每个值将会直接传递到上一层对应区域中最大值所对应的神经元，该区域中其他神经元的误差项都设为0,。如果采用平均采样，误差项$\delta^{(l+1, k)}$中的每个值都会被平均分配到上一层对应的区域中的所有神经元上。</p><p><strong>卷积层</strong></p><p>当第l+1层为卷积层时，假设特征映射净输入为$Z^{(l+1)} \in R^{M^\prime \times N^\prime \times K}$，其中第k个特征映射的净输入为：$$<br>Z^{(l+1,k)} = \sum_{p=1}^P W^{(l+1,k,p)} \bigotimes X^{(l,p)} + b^{(l+1, k)}<br>$$</p><p>其中，$W^{(l+1,k,p)}$和$b^{(l+1, k)}$为第l+1层的卷积核及偏置。第l+1层中共有$K\times P$个卷积核和K个偏置。第l层的第p个特征映射为误差项$\delta^{(l,p)}$的具体推导如下：<br>$$<br>\delta^{(l,p)} = \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l,p)}}<br>\\\\<br>=\frac {\partial X^{(l,p)}}{\partial Z^{l,p}}\cdot \frac {\partial L(Y,\hat{Y})}{\partial X^{(l,p)}}\\\\=f_l^\prime(Z^{(l)}) \bigodot \sum_{k=1}^K (rot180(W^{(l+1,k,p)}) \hat{\bigotimes} \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l+1, k)}})\\\\=f_l^\prime(Z^{(l)}) \bigodot \sum_{k=1}^K(rot180(W^{(l+1,k,p)}) \hat{\bigotimes} \delta^{(l=1,k)})<br>$$</p><p>其中，$\hat{\bigotimes}$为宽卷积。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://nndl.github.io/" target="_blank" rel="external">https://nndl.github.io/</a> 卷积神经网络</li><li><a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">https://www.zybuluo.com/hanbingtao/note/485480</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是对近期学习的卷积神经网络相关知识的简单记录和梳理。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;卷积神经网络简介&quot;&gt;&lt;a href=&quot;#卷积神经网络简介&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络简介&quot;&gt;&lt;/a&gt;卷积神经网络简介&lt;/h2&gt;&lt;p&gt;卷积神经网络(Convolution Neural Network，CNN或ConvNet)是一种前馈神经网络。卷积神经网络是受生物学上&lt;strong&gt;感受野&lt;/strong&gt;(Receptive Field)的机制提出来的。一个神经元的感受野是指特定区域，只有这个区域内的刺激才能够激活该神经元。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="CNN" scheme="https://ilewseu.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>前馈神经网络反向传播推导</title>
    <link href="https://ilewseu.github.io/2017/12/17/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC/"/>
    <id>https://ilewseu.github.io/2017/12/17/前馈神经网络反向传播推导/</id>
    <published>2017-12-17T12:08:08.000Z</published>
    <updated>2017-12-24T09:03:31.194Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前馈神经网络前向传播"><a href="#前馈神经网络前向传播" class="headerlink" title="前馈神经网络前向传播"></a>前馈神经网络前向传播</h2><a id="more"></a><p>一个三层的前馈神经网络如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/Jb3maf249i.png?imageslim" alt="mark"><br></div><br>对于第二层的输出$a_1^2,a_2^2,a_3^2$,有：<br>$$<br>a_1^2 = \sigma(z_1^2)=\sigma(w_{11}^2x_1+w_{12}^2x_2+w_{13}^2x_3+b_1^2)\\\\<br>a_2^2 = \sigma(z_2^2)=\sigma(w_{21}^2x_1+w_{22}^2x_2+w_{23}^2x_3+b_3^2)\\\\<br>a_3^2 = \sigma(z_3^2)=\sigma(w_{31}^2x_1+w_{32}^2x_2+w_{33}^2x_3+b_3^2)<br>$$<br>对于第三层的输出$a_1^3$，有：<br>$$a_1^3 = \sigma(z_1^3)=\sigma(w_{11}^3x_1+w_{12}^3x_2+w_{13}^3x_3+b_1^3)$$</p><p>用下面的符号来描述一个前馈神经网络：</p><ul><li>L:表示神经网络的层数；</li><li>$n^l$：表示第l层神经元的个数；</li><li>$f_l(.)$:表示第l层神经元的激活函数；</li><li>$W^{(l)}\in R^{n^l\times n^{l-1}}$:表示第l-1层到第l层的权重矩阵；</li><li>$b^{(l)\in R^{n^l}}$：表示第l-1层到第l层的偏置；</li><li>$z^{(l)\in R^{n^l}}$:表示第l层神经元的净输入；</li><li>$a^{(l)\in R^{n^l}}$:表示第l层神经元的输出（激活值）；</li><li>$W_{ij}^{(l)}$:表示第l-1层第j个输入到第l层第i个神经元的权重；</li></ul><p>前馈神经网络通过下面的公式进行信息传播：<br>$$<br>z^{(l)} = W^{(l)}\cdot a^{(l-1)} + b^{(l)} \\\\<br>a^{(l)} = f_l(z^{(l)})<br>$$<br>上面的公式可以合并为：$$<br>z^{(l)} = W^{(l)}\cdot f_{l-1}(z^{(l-1)}) + b^{(l)}<br>$$</p><p>这样，前馈神经网络可以通过逐层的信息传递，得到网络最后的输出$a^{(L)}$。整个网络可以看作一个复合函数$\phi(x;W,b)$,将输入x作为第1层的输入$a^{(0)}$，将第L层的输出作为$a^{(L)}$作为输出。<br>$$x=a^{(0)}\rightarrow z^{(l)} \rightarrow a^{(1)}\rightarrow z^{(2)}\rightarrow … \rightarrow a^{(L-1)} \rightarrow z^{(L)} \rightarrow a^{(L-1)}=\phi(x;W,b)$$</p><h2 id="反向传播推导"><a href="#反向传播推导" class="headerlink" title="反向传播推导"></a>反向传播推导</h2><p>假设损失函数是$L(y,\hat{y})$，对第l层中的参数$W^{(l)}$和$b^{(l)}$计算偏导数。因为$<br>\frac {\partial L(y,\hat{y})}{\partial W^{(l)}}$的计算涉及到矩阵的微分，十分繁琐，可以先计算偏导数$\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}}$。根据链式法则：$$<br>\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}} =\left(\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}}\right)^T\frac {\partial L(y,\hat{y})}{\partial z^{(l)}}   （1）<br>\\\\<br>\frac {\partial L(y,\hat{y})}{\partial b^{(l)}} =\left(\frac {\partial z^{(l)}}{\partial b^{(l)}}\right)^T\frac {\partial L(y,\hat{y})}{\partial z^{(l)}}     (2)<br>$$<br>公式(1)和(2)都是为目标函数关于第l层神经元$z^{(l)}$的偏导数，称为<strong>误差项</strong>，因此可以共用。我们只需要计算三个偏导数，分别为$\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}},\frac {\partial L(y,\hat{y})}{\partial b^{(l)}}和\frac {\partial L(y,\hat{y})}{\partial z^{(l)}}$</p><p><strong>1、计算偏导数$\frac {\partial z^{(l)} }{\partial W_{ij}^{(l)}}$</strong></p><p>因为$z^{(l)}和W_{ij}^{(l)}$的函数关系为$z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)}$，因此，偏导数：<br>$$<br>\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}}=\frac {\partial (W^{(l)}a^{(l-1)}+b^{(l)})}{\partial W_{ij}^{(l)}}= a_j^{(l-1)}<br>$$<br>其中，$W_{i:}^{(l)}$为权重矩阵$W^{(l)}$的第i行</p><p><strong>2、计算偏导数</strong>$\frac {\partial z^{l}}{\partial b^{(l)}}$</p><p>因为$z^{(l)}$和$b^{(l)}$的函数关系为$z^{(l)} = W^{(l)}a^{(l-1)}+b^{(l)}$，因此偏导数：<br>$$<br>\frac {\partial z^{l}}{\partial b^{(l)}}=I_{n^l}<br>$$<br>为$n^l\times n^l$的单位矩阵。</p><p><strong>3、计算偏导数</strong>$\frac {\partial L(y,\hat{y})}{\partial z{(l)}}$</p><p>用$\delta^{(l)}$来定义第l层的神经元误差项：<br>$$<br>\delta^{(l)} = \frac {\partial L(y,\hat{y})}{\partial z^{(l)}} \in R^{n^l}<br>$$<br>误差项$\delta^{l}$表示第l层的神经元对最终的误差的影响，也反映了最终的输出对第l层的神经元对最终误差的敏感程度。</p><p>根据$z^{(l+1)}=W^{(l+1)}a^{(l)} + b^{(l+1)}$，有：<br>$$<br>\frac {\partial z^{(l+1)}}{\partial a^{(l)}} = (W^{(l+1)})^T<br>$$<br>根据$a^{(l)}=f(z^{(l)})$，其中，$f_l(.)$为按位计算的函数，因此有：<br>$$<br>\frac {\partial a^{(l)}}{\partial z^{(l)}} = \frac {\partial f_l(z^{(l)})}{\partial z^{l}} = diag(f_l^{\prime}(z^{(l)}))<br>$$<br>因此，根据链式法则，第l层的误差项为：<br>$$<br>\delta^{(l)} = \frac {\partial L(y, \hat{y})}{\partial z^{(l)}}\\\\<br>=\frac {\partial a^{(l)}}{\partial z^{(l)}}\cdot\frac{\partial z^{(l+1)}}{\partial a^{(l)}}\cdot \frac {\partial L(y, \hat{y})}{\partial z^{(l+1)}}\\\\=diag(f_l^{\prime}(z^{(l)}))\bigodot ((W^{(l+1)})^T\delta^{(l+1)})  (3)<br>$$<br>其中，$\bigodot$是向量的点积运算，表示每个元素相乘。</p><p>从公式3可以看出，第l层的误差项可以通过第l+1层的误差项计算，这就是反向传播。<strong>反向传播算法的含义是：第l层的一个神经元的误差项是所有与该神经元相连的第l+1层神经元误差项的权重和，然后再乘上该神经元激活函数的梯度。</strong></p><p>在计算出三个偏导数之后，可以得到最终的偏导数：<br>$$<br>\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}} = \delta_i^{(l)}a_j^{(l-1)}<br>$$<br>进一步，$L(y,\hat{y})$关于第l层的权重$W^{(l)}$的梯度为：<br>$$<br>\frac {\partial L(y,\hat{y})}{\partial W^{(l)}} = \delta^{(l)}a^{(l-1)}<br>$$<br>同理可得，$L(y,\hat{y})$关于第l层偏置$b^(l)$的梯度为：<br>$$<br>\frac {\partial L(y,\hat{y})}{\partial b^{(l)}} = \delta^{(l)}<br>$$<br>基于随机梯度下降的反向传播算法如下：</p><p><div><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/Dg5jm8Hjg0.jpg?imageslim" alt="mark"><br></div><br>图片引自：<a href="https://github.com/nndl/nndl.github.io，chap-前馈神经网络.pdf。" target="_blank" rel="external">https://github.com/nndl/nndl.github.io，chap-前馈神经网络.pdf。</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前馈神经网络前向传播&quot;&gt;&lt;a href=&quot;#前馈神经网络前向传播&quot; class=&quot;headerlink&quot; title=&quot;前馈神经网络前向传播&quot;&gt;&lt;/a&gt;前馈神经网络前向传播&lt;/h2&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>CNN学习的相关资料</title>
    <link href="https://ilewseu.github.io/2017/12/17/CNN%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E5%8D%9A%E5%AE%A2/"/>
    <id>https://ilewseu.github.io/2017/12/17/CNN学习的相关博客/</id>
    <published>2017-12-17T11:48:20.000Z</published>
    <updated>2017-12-17T06:27:08.258Z</updated>
    
    <content type="html"><![CDATA[<h2 id="对卷积的理解"><a href="#对卷积的理解" class="headerlink" title="对卷积的理解"></a>对卷积的理解</h2><ul><li><a href="http://mengqi92.github.io/2015/10/06/convolution/" target="_blank" rel="external">http://mengqi92.github.io/2015/10/06/convolution/</a></li><li><a href="http://www.cnblogs.com/freeblues/p/5738987.html" target="_blank" rel="external">http://www.cnblogs.com/freeblues/p/5738987.html</a></li><li><a href="http://blog.csdn.net/bitcarmanlee/article/details/54729807" target="_blank" rel="external">http://blog.csdn.net/bitcarmanlee/article/details/54729807</a><a id="more"></a></li></ul><h2 id="卷积神经网络的原理及推导"><a href="#卷积神经网络的原理及推导" class="headerlink" title="卷积神经网络的原理及推导"></a>卷积神经网络的原理及推导</h2><ul><li>反向传播的推导：<a href="https://zhuanlan.zhihu.com/p/22473137" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/22473137</a></li><li>pinard:<a href="http://www.cnblogs.com/pinard/p/6483207.html" target="_blank" rel="external">http://www.cnblogs.com/pinard/p/6483207.html</a></li><li>zybuluo:<a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">https://www.zybuluo.com/hanbingtao/note/485480</a></li><li>一文读懂卷积神经网络CNN：<a href="http://www.sohu.com/a/126742834_473283" target="_blank" rel="external">http://www.sohu.com/a/126742834_473283</a></li><li>CS231N翻译：<a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/21930884</a></li><li>charlote:<a href="http://www.cnblogs.com/charlotte77/p/7759802.html" target="_blank" rel="external">http://www.cnblogs.com/charlotte77/p/7759802.html</a></li><li>卷积神经网络全面解析:<a href="http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi" target="_blank" rel="external">http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi</a></li><li>tornadomeet:<a href="http://www.cnblogs.com/tornadomeet/p/3468450.html" target="_blank" rel="external">http://www.cnblogs.com/tornadomeet/p/3468450.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;对卷积的理解&quot;&gt;&lt;a href=&quot;#对卷积的理解&quot; class=&quot;headerlink&quot; title=&quot;对卷积的理解&quot;&gt;&lt;/a&gt;对卷积的理解&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://mengqi92.github.io/2015/10/06/convolution/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://mengqi92.github.io/2015/10/06/convolution/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/freeblues/p/5738987.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cnblogs.com/freeblues/p/5738987.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/bitcarmanlee/article/details/54729807&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/bitcarmanlee/article/details/54729807&lt;/a&gt;
    
    </summary>
    
      <category term="记录" scheme="https://ilewseu.github.io/categories/%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="CNN" scheme="https://ilewseu.github.io/tags/CNN/"/>
    
      <category term="资料" scheme="https://ilewseu.github.io/tags/%E8%B5%84%E6%96%99/"/>
    
  </entry>
  
</feed>
