<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>记录思考</title>
  
  <subtitle>ML、DL、NLP</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ilewseu.github.io/"/>
  <updated>2018-06-18T14:14:45.282Z</updated>
  <id>https://ilewseu.github.io/</id>
  
  <author>
    <name>luerwei</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>自然语言处理基础-中文分词</title>
    <link href="https://ilewseu.github.io/2018/06/16/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"/>
    <id>https://ilewseu.github.io/2018/06/16/中文分词/</id>
    <published>2018-06-16T08:48:20.000Z</published>
    <updated>2018-06-18T14:14:45.282Z</updated>
    
    <content type="html"><![CDATA[<h2 id="中文分词问题介绍"><a href="#中文分词问题介绍" class="headerlink" title="中文分词问题介绍"></a>中文分词问题介绍</h2><p>词是自然语言中能够独立运用的最小单位，是信息处理的基本单位。自然语言处理的对象是一个个的句子，拿到句子之后一般要对句子进行分词。分词就是利用计算机识别出文本中词的过程。大部分的印欧语言，词与词之间有空格之类的显示标志指示词的边界。因此，利用很容易切分出句子中的词。而与大部分的印欧语言不同，中文语句中词与词之间没有空格标志指示，所以，需要专门的方法去实现中文分词。分词是文本挖掘的基础，通常用于自然语言处理、搜索引擎、推荐等领域。<br><a id="more"></a><br><strong>中文分词的难点主要有</strong>：</p><ul><li><strong>歧义无处不在</strong></li><li><strong>新词层出不穷</strong></li><li><strong>需求多种多样</strong></li></ul><p><strong>(1) 歧义无处不在</strong></p><p>分词的歧义主要包括如下几个方面：</p><ul><li><p><strong>交集型歧义</strong>，例如：</p><p>  研究/生命/的/起源</p><p>  研究生/命/的/起源</p></li><li><p><strong>组合型歧义</strong>，例如：</p><p>  门/把/手/弄/坏/了</p><p>  门/把手/弄/坏/了</p></li><li><p><strong>真歧义</strong>，例如：</p><p>  乒乓球/拍/卖/完了</p><p>  乒乓球/拍卖/完了</p></li></ul><p><strong>(2) 新词层出不穷</strong></p><ul><li>人名、地名、机构名</li><li>网名</li><li>公司名、产品名</li></ul><p><strong>(3) 需求多种多样</strong></p><ul><li>切分速度:搜索引擎vs单集版语音合成</li><li>结果呈现：<ul><li>切分粒度的要求不同；</li><li>分词重点要求不同；</li><li>唯一结果vs多结果；</li><li>新词敏感程度不同；</li></ul></li><li>处理对象：书面文本(规范/非规范)vs口语文本</li><li>硬件平台：嵌入式vs单机版vs服务器版</li></ul><p>本文只介绍分词相关的基本算法原理及思想，其他的暂时不介绍。</p><h2 id="中文分词算法"><a href="#中文分词算法" class="headerlink" title="中文分词算法"></a>中文分词算法</h2><p>目前，中文分词技术已经非常成熟，学者们研究出了很多的分词方法，这些方法大致可以分为三类。第一类是基于字符串匹配的，即扫描字符串，如果发现字符串的子串和词典中的词相同，就算匹配，比如机械分词方法。这类分词方法通常会加入一些启发式规则，例如，正向最大匹配、反向最大匹配、长词优先等。第二类时基于统计的分词方法，它们基于人工标注的词性和统计特征，对中文进行建模，即根据观测到的数据（标注好的语料）对模型参数进行训练，在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词结果作为最终结果。常见的序列标注模型有HMM和CRF。这类分词算法能够很好的处理歧义和未登录词问题，效果要比前一类效果好，但是需要大量的人工标注数据，且分词速度较慢。第三类是理解分词，通过让计算机模拟人对句子的理解，达到识别词的效果。由于汉语语义的复杂性，难以将各种语言信息组织成机器能够识别的形式，目前这种分词系统还处于试验阶段。</p><h3 id="基于字符串匹配的方法"><a href="#基于字符串匹配的方法" class="headerlink" title="基于字符串匹配的方法"></a>基于字符串匹配的方法</h3><p>这一类方法也叫基于词表的分词方法，基本思路是首先存在一份字典，对于要分词的文本从左至右或从右至左扫描一遍，遇到字典里有最长的词就标识出来，遇到不认识的字串就分割成单字词。常见的方法有：</p><ul><li>正向最大匹配法(forward maximum matching method,FMM)</li><li>逆向最大匹配法(backward maximum matching method, BMM)</li><li>N-最短路径方法</li></ul><p><strong>正向最大匹配法</strong>指从左到右对一个字符串进行匹配，所匹配的词越长越好，比如“中国科学院计算机研究所”，按照词典中最长匹配的切分原则切分的结果是：“中国科学院/计算研究所”，而不是”中国/科学院/计算/研究所”。但是，正向匹配会存在一些bad case，常见的例子如：”他从东经过我家”，使用正向最大匹配会得到错误的结果：”他/从/东经/过/我/家”</p><p><strong>逆向最大匹配法</strong>是从右向左倒着匹配，如果能够匹配到更长的词，则优先选择，上面的”他从东经过我家”逆向最大匹配能够得到正确的结果，”他/从/东/经过/我/家”。但是，逆向最大匹配同样也存在bad case：“他们昨日本应该回来”，逆向匹配会得到错误的结果”他们/昨/日本/应该/回来”</p><p>针对正向逆向匹配的问题，将双向切分的结果进行比较，选择切分词语数量最少的结果。但是最少切分结果同样有bad case，比如：”他将来上海”，正确的切分结果是”他/将/来/上海”，有4个词，而最少切分结果“他/将来/中国”只有3个词。</p><p>基于字符串匹配的方法的优点如下：</p><ul><li>程序简单易行，开发周期短；</li><li>没有任何复杂计算，分词速度快；</li></ul><p><strong>不足</strong>：</p><ul><li>不能处理歧义；</li><li>不能识别新词；</li><li>分词准确率不高，不能满足实际的需要；</li></ul><p><strong>N-最短路径方法</strong>的基本思想是：设待分字串$S=c_1c_2…c_n$，其中$c_i(i=1,2,…,n)$为单个字，n为串的长度，$n \geq 1$。建立一个结点数为n+1的切分有向无环图G，各结点编号依次为$V_0,V_1,V_2,…,V_n$。如下图所示：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180616/dlcc2jcgAd.jpg?imageslim" alt="mark"></p><p>其算法如下：</p><p>（1） 相邻结点$v_{k-1},v_k$之间建立有向边$<v_{k-1},v_k>$，边对应的词默认为$c_k(k=1,2,..,n)$</v_{k-1},v_k></p><p>（2） 如果$w=c_ic_{i+1}…c_j(0 &lt; i <j \leq="" n)$是一个词，则结点$v_{i-1},v_j$之间建立有向边，$<v_{i-1},v_j="">$，边对应的词为w。</j></p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180616/6JLiabm6eH.jpg?imageslim" alt="mark"></p><p>（3） 重复上述步骤(2)，直到没有新的路径（词序列）产生。</p><p>（4） 从产生的所有路径中，选择路径最短的（词数最少的）作为最终的分词结果。</p><p>例如：输入的字串：他只会诊断一般的疾病。</p><p>可能的输出： </p><ul><li>他/只会/诊断/一般/的/疾病。</li><li>他/只/会诊/断/一般/的/疾病。</li></ul><p>最终的分词结果为：他/只会/诊断/一般/的/疾病。</p><p>N-最短路径方法，采用的原则（切分出来的次数最少）符合汉语自身的规律；同时，需要的语言资源（词表）也不多。但是，它对许多歧义字段难以区分，最短路径有多条时，选择最终的输出结果缺乏应有的标准。当字符串长度较大和选取的最短路径数增大时，长度相同的路径数急剧增加，选择最终正确的结果困难越来越大。</p><h3 id="基于N-gram语言模型的分词方法"><a href="#基于N-gram语言模型的分词方法" class="headerlink" title="基于N-gram语言模型的分词方法"></a>基于N-gram语言模型的分词方法</h3><p>基于N-gram语言模型的方法是一个典型的生成式模型，早期很多统计分词均是以它为基本模型，然后配合其他未登录词识别模块进行扩展。其基本思想是：首先根据词典对句子进行简单匹配，找出所有可能的词典词，然后将它们和所有单个字作为结点，构造n元切分词图，图中的结点表示可能的此候选，边表示路径，边上的n元概率表示代价，最后利用相关搜索算法从中找到代价最小的路径作为最后的分词结果。</p><p>假设随机变量S为一个汉字序列，W是S上所有可能切分路径，对于分词，实际上就是求解使条件概率$P(W|S)$最大的切分路径$\hat W$，即:<br>$$\hat W=arg max_W P(W|s)$$<br>根据贝叶斯公式：<br>$$\hat W =arg max_W \frac {P(W)P(S|W)}{P(S)} $$<br>由于P(S)为归一化因子，P(S|W)恒为1，因此只需要求解P(W)。P(W)使用N-gram语言模型建模，定义如下(以Bi-gram为例)：<br>$$P(W) = P(w_1w_2…w_T)=P(w_1)P(w_2|w_1)…P(w_T|w_{T-1})$$<br>这样，各种切分路径的好坏程度(条件概率P(W|S))可以求解。简单的，可以根据DAG枚举全路径，暴力求解最优路径；也可以使用动态规划的方法的求解，jieba分词中不带HMM新词发现的分词，就是DAG+Uni-gram语言模型+后向动态规划的方式进行求解的。</p><p>基于N-gram语言模型的分词方法的优点如下：</p><ul><li>减少了很多手工标注知识库（语义词典、规则等）的工作；</li><li>在训练语料规模足够大和覆盖领域足够多的情况下，可以获得较高的切分正确率；</li></ul><p>缺点：</p><ul><li>训练语料的规模和覆盖领域不好把握；</li><li>计算量较大；</li><li>很难进行分词发现；</li></ul><h3 id="基于序列标注的分词方法"><a href="#基于序列标注的分词方法" class="headerlink" title="基于序列标注的分词方法"></a>基于序列标注的分词方法</h3><p>针对基于词典的机械分词所面对的问题，尤其是未登录词识别问题，使用基于统计模型的分词方式能够取得更好的结果。基于统计模型的分词方法，简单来说就是一个序列标注问题。在一句文本中，可以将每个字按照它们在词中的位置进行标注，常用的标记有如下四种：</p><ul><li>B：表示词开始的字；</li><li>M：表示词中间的字；</li><li>E：表示词结尾的字；</li><li>S：表示单字成词</li></ul><p>分词的过程就是序列标注的过程，将一句文本输入，然后得到相应的标记序列，然后根据标记序列进行分词。举例来说“他只会诊断一般的疾病”，经过模型后得到的理想标注序列是：“SBEBEBESBE”，最终还原分词结果是“他/只会/诊断/一般/的/疾病”。<br>在NLP领域中，解决序列标注问题的常见模型主要有HMM模型和CRF模型。</p><h4 id="基于HMM模型分词"><a href="#基于HMM模型分词" class="headerlink" title="基于HMM模型分词"></a>基于HMM模型分词</h4><p>一般而言，一个HMM模型可以用一个5元组表示$\mu=(Q,V,A,B,\pi)$，其中：</p><ul><li>Q：所有可能的状态集合；</li><li>V：所有可能的观测集合；</li><li>A：状态转移矩阵；</li><li>B：发射概率；</li><li>$\pi$：初始概率分布；</li></ul><p>HMM模型的三个基本问题：</p><ul><li><strong>概率计算问题</strong>：给定一个观察序列$O=O_1,O_2,…,O_t$和模型$\mu=(A,B,\pi)$，计算观察序列O的概率；</li><li><strong>学习问题</strong>：给定一个观察序列$O=O_1,O_2,…,O_t$，估计模型$\mu=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\mu)$最大，即用极大似然估计方法估计参数；</li><li><strong>预测问题</strong>：也称为解码问题。已知模型$\mu=(A,B,\pi)$和观测序列$O=O_1,O_2,…,O_t$，求出对于给定观测序列条件概率$P(I|O)$最大的状态序列$I={i_1,i_2,…,i_t}$，即给定观察序列，求最有可能对应的状态序列。</li></ul><p>HMM模型概率计算问题可以通过前向、后向的动态规划算法来求解；学习问题可以通过EM算法求解；预测问题可以通过viterbi算法求解。通过足够的语料数据，可以方便快速地学习HMM模型。</p><p>利用HMM模型求解分词的基本思路是根据观测值（词序列）找到真正的隐藏状态值序列。在分词中，一句文本的每个字符可以看做是一个观测值，而这个字符的标记可以看做是隐藏状态。使用HMM分词，通过对标注语料进行统计，可以得到模型的：初始概率分布、转移概率矩阵、发射概率矩阵、观察值集合、状态值集合。在概率矩阵中，起始概率矩阵表示序列的第一个状态值的概率，在中文分词中，理论上M和E的概率为0,。转移概率矩阵表示状态间的概率，比如B-&gt;M的概率，E-&gt;S的概率等等。而发射概率是一个条件概率，表示当前这个状态下，出现某个字的概率，比如P(人|B)表示在状态为B的情况下”人”字的概率。</p><p>HMM模型的参数从标注数据学习好之后，HMM问题最终转化成求解隐藏状态序列最大值的问题，求解这个问题使用的是Viterbi算法。具体可以参照<a href="http://www.52nlp.cn/itenyh%E7%89%88-%E7%94%A8hmm%E5%81%9A%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E4%B8%80%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%87%86%E5%A4%87" target="_blank" rel="external">Itenyh版-用HMM做中文分词</a>。</p><h4 id="基于CRF模型分词"><a href="#基于CRF模型分词" class="headerlink" title="基于CRF模型分词"></a>基于CRF模型分词</h4><p>CRF也是求解序列标注问题的一种比较常用的模型，和HMM模型不同，CRF是一种判别式模型，CRF模型通过定义条件概率$P(Y|X)$来描述模型。基于CRF的分词方法与传统的分类模型求解很相似，即给定feature（字级别的各种信息）输出label。具体的CRF的基本原理就不具体介绍了。分词所使用的是Linear-CRF，它由一组特征函数组成，包括权重$\lambda$和特征函数$f$，特征函数$f$的输入是整个句子S、当前位置i、前一个字的标记$y_{i-1}$和当前字的标记$y_i$。对于分词这个任务，同样是使用BMES来标记一句话，X表示观测到句子，通过利用CRF模型的解码来，来求出最大的$P(Y|X)$，Y即是BMES组成的序列，然后用Y序列得到实际的分词结果。与HMM模型相比，CRF存在以下优点：</p><ul><li>CRF可以使用输入文本的全局特征，而HMM只能看到输入文本在当前位置的局部特征；</li><li>CRF是判别式模型，直接对序列标注建模，HMM则引入了不必要的先验信息。</li></ul><p><strong>采用HMM模型的分词方法，属于生成式分词，其优点如下</strong>：</p><ul><li>在训练语料规模足够大和覆盖领域足够多的情况下，可以获得较高的切分正确率；</li></ul><p><strong>不足：</strong></p><ul><li>需要很大的训练语料；</li><li>新词识别能力弱；</li><li>解码速度相对较慢；</li></ul><p><strong>基于CRF的分词方法，属于判别式的分词，其优点如下</strong>：</p><ul><li>解码速度快；</li><li>分词精度高；</li><li>新词识别能力强；</li><li>所需学习样本少；</li></ul><p><strong>不足：</strong></p><ul><li>训练速度慢；</li><li>需要高配置的机器训练；</li></ul><h3 id="基于深度学习端到端的分词方法"><a href="#基于深度学习端到端的分词方法" class="headerlink" title="基于深度学习端到端的分词方法"></a>基于深度学习端到端的分词方法</h3><p>基于深度神经网络的序列标注算法在词性标注、命名实体识别问题上取得了不错的结果。词性标注、命名实体识别都属于序列标注问题，这些端到端的方法可以迁移到分词问题上，免去CRF的特征模板配置问题。但与所有深度学习的方法一样，它需要较大的训练语料才能体现优势。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180616/1g3aBAd38j.jpg?imageslim" alt="mark"></p><p>BiLSTM-CRF的网络结构如上图所示，输入层是一个embedding层，经过双向LSTM网络编码，输出层是一个CRF层。下图是BiLSTM-CRF各层的物理含义，可以看见经过双向LSTM网络输出的实际上是当前位置对于各词性的得分，CRF层的意义是对词性得分加上前一位置的词性概率转移的约束，其好处是引入一些语法规则的先验信息。利用此网络结构可以完成序列标注问题，同样分词任务也可以尝试这个思路。</p><p><strong>总结一下：</strong></p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180617/Lf8f5AimlK.jpg?imageslim" alt="mark"></p><h2 id="中文分词工具"><a href="#中文分词工具" class="headerlink" title="中文分词工具"></a>中文分词工具</h2><p>目前，存在多种中文分词工具供使用。比较具有代表性的有jieba分词、SnowNLP、THULAC、NLPIR和LTP，这些分词工具均提供Python库，可以很方便的使用。</p><h3 id="jieba分词"><a href="#jieba分词" class="headerlink" title="jieba分词"></a>jieba分词</h3><p>结巴分词是我比较常用的分词工具，Github库为：<a href="https://github.com/fxsjy/jieba" target="_blank" rel="external">https://github.com/fxsjy/jieba</a>，分词效果较好。它支持三种分词模式：</p><ul><li><strong>精确模式</strong>：试图将句子最精确地切开，适合文本分析；</li><li><strong>全模式</strong>：将句子中所有的可能成词的词语都扫描出来，速度非常快，但是不能解决歧义问题；</li><li><strong>搜索引擎模式</strong>：在精确模式的基础之上，对长词再次切分，提高召回率，适用于搜索引擎分词；</li></ul><p>另外，jieba分词还支持繁体分词，支持用户自定义词典。其使用的算法是基于统计的分词方法，主要有如下几种：</p><ul><li>基于前缀词典实现高效的词图扫描，生成句子中所有可能成词情况构成有向无环图(DAG);</li><li>采用了动态规划查找最大概率路径，找出基于词频的最大切分组合；</li><li>对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法；</li></ul><p>其使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># encoding=utf-8</span></div><div class="line"><span class="keyword">import</span> jieba</div><div class="line"></div><div class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>, cut_all=<span class="keyword">True</span>)</div><div class="line">print(<span class="string">"Full Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 全模式</span></div><div class="line"></div><div class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>, cut_all=<span class="keyword">False</span>)</div><div class="line">print(<span class="string">"Default Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 精确模式</span></div><div class="line"></div><div class="line">seg_list = jieba.cut(<span class="string">"他来到了网易杭研大厦"</span>)  <span class="comment"># 默认是精确模式</span></div><div class="line">print(<span class="string">", "</span>.join(seg_list))</div><div class="line"></div><div class="line">seg_list = jieba.cut_for_search(<span class="string">"小明硕士毕业于中国科学院计算所，后在日本京都大学深造"</span>)  <span class="comment"># 搜索引擎模式</span></div><div class="line">print(<span class="string">", "</span>.join(seg_list))</div></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学</div><div class="line"></div><div class="line">【精确模式】: 我/ 来到/ 北京/ 清华大学</div><div class="line"></div><div class="line">【新词识别】：他, 来到, 了, 网易, 杭研, 大厦    (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)</div><div class="line"></div><div class="line">【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造</div></pre></td></tr></table></figure><p>除了基本的分词功能，jieba分词还提供了关键词抽取、词性标注等功能，具体的使用参照github上的文档。<a href="https://blog.csdn.net/rav009/article/details/12196623" target="_blank" rel="external">Python中文分词模块结巴分词算法过程的理解和分析</a>对于jieba分词的源码进行了解析，有兴趣的可以看一下。</p><h3 id="SnowNLP"><a href="#SnowNLP" class="headerlink" title="SnowNLP"></a>SnowNLP</h3><p>SnowNLP:Simplified Chinese Text Processing，可以方便的处理中文文本内容，是受到TextBlob的启发而写的，由于大部分的自然语言处理库基本上是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现，并且自带了一些训练好的词典。GitHub地址为：<a href="https://github.com/isnowfy/snownlp" target="_blank" rel="external">https://github.com/isnowfy/snownlp</a>。SnowNLP的分词是基于Character-Based Generative Model 来实现的，论文地址：<a href="http://aclweb.org/anthology//Y/Y09/Y09-2047.pdf" target="_blank" rel="external">http://aclweb.org/anthology//Y/Y09/Y09-2047.pdf</a></p><h3 id="THULAC"><a href="#THULAC" class="headerlink" title="THULAC"></a>THULAC</h3><p>THULAC(THU Lexical Analyzer for Chinese)是由清华大学自然语言与社会人文计算实验室推出的一套中文分词工具包。Github的地址为：<a href="https://github.com/thunlp/THULAC-Python" target="_blank" rel="external">https://github.com/thunlp/THULAC-Python</a>，具有中文分词和词性标注功能。THULAC具有如下几个特点：</p><ul><li><strong>能力强</strong>:利用集成的目前世界上规模最大的人工分词和词性标注中文语料（约含5800万字）训练而成，模型标注能力强大；</li><li><strong>准确率高</strong>:该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当；</li><li><strong>速度较快</strong>:同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s；</li></ul><h3 id="NLPIR"><a href="#NLPIR" class="headerlink" title="NLPIR"></a>NLPIR</h3><p>NLPIR 分词系统，前身为2000年发布的 ICTCLAS 词法分析系统，Github地址：<a href="https://github.com/NLPIR-team/NLPIR" target="_blank" rel="external">https://github.com/NLPIR-team/NLPIR</a>，是由北京理工大学张华平博士研发的中文分词系统，经过十余年的不断完善，拥有丰富的功能和强大的性能。NLPIR是一整套对原始文本集进行处理和加工的软件，提供了中间件处理效果的可视化展示，也可以作为小规模数据的处理加工工具。主要功能包括：中文分词，词性标注，命名实体识别，用户词典、新词发现与关键词提取等功能。另外对于分词功能，它有 Python 实现的版本，Github地址为：<a href="https://github.com/tsroten/pynlpir" target="_blank" rel="external">https://github.com/tsroten/pynlpir</a>。</p><h3 id="LTP"><a href="#LTP" class="headerlink" title="LTP"></a>LTP</h3><p>语言技术平台（Language Technology Platform，LTP）是哈工大社会计算与信息检索研究中心历时十年开发的一整套中文语言处理系统。LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于动态链接库（Dynamic Link Library, DLL）的应用程序接口、可视化工具，并且能够以网络服务（Web Service）的形式进行使用。<br>LTP 有 Python 版本，GitHub地址：<a href="https://github.com/HIT-SCIR/pyltp" target="_blank" rel="external">https://github.com/HIT-SCIR/pyltp</a>，另外运行的时候需要下载模型，模型还比较大，下载地址：<a href="">http://ltp.ai/download.html</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前，分词算法已经非常成熟，并且存在很多的分词工具。虽然，不用我们自己去实现各种分词算法，但是还是有必要去了解各种分词算法的原理的。从最初的基于字符串匹配的分词方法到把分词任务转化为序列标注问题，以及将深度学习用于分词等，这些基本的研究方法可以为自然语言处理的其他任务的解决提供了思路，值的我们去学习和思考。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li>中文分词一席谈；</li><li><a href="https://zhuanlan.zhihu.com/p/33261835" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/33261835</a></li><li><a href="http://www.cnblogs.com/sxron/articles/6391926.html" target="_blank" rel="external">http://www.cnblogs.com/sxron/articles/6391926.html</a></li><li>统计自然语言处理-宗庆成</li><li>Lample G, Ballesteros M, Subramanian S, et al. Neural Architectures for Named Entity Recognition[J]. 2016:260-270.</li><li><a href="http://www.52nlp.cn/itenyh%E7%89%88-%E7%94%A8hmm%E5%81%9A%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E4%B8%80%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%87%86%E5%A4%87" target="_blank" rel="external">Itenyh版-用HMM做中文分词</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;中文分词问题介绍&quot;&gt;&lt;a href=&quot;#中文分词问题介绍&quot; class=&quot;headerlink&quot; title=&quot;中文分词问题介绍&quot;&gt;&lt;/a&gt;中文分词问题介绍&lt;/h2&gt;&lt;p&gt;词是自然语言中能够独立运用的最小单位，是信息处理的基本单位。自然语言处理的对象是一个个的句子，拿到句子之后一般要对句子进行分词。分词就是利用计算机识别出文本中词的过程。大部分的印欧语言，词与词之间有空格之类的显示标志指示词的边界。因此，利用很容易切分出句子中的词。而与大部分的印欧语言不同，中文语句中词与词之间没有空格标志指示，所以，需要专门的方法去实现中文分词。分词是文本挖掘的基础，通常用于自然语言处理、搜索引擎、推荐等领域。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理基础-条件随机场(Conditional Random Fields,CRFs)学习笔记</title>
    <link href="https://ilewseu.github.io/2018/05/21/CRF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://ilewseu.github.io/2018/05/21/CRF学习笔记/</id>
    <published>2018-05-21T12:57:20.000Z</published>
    <updated>2018-06-18T15:05:17.120Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇笔记来自李航老师的《统计学习方法》一书</p></blockquote><p>条件随机场最早由Lafferty等人2001年提出，其模型思想的主要来源是最大熵模型，模型的三个基本问题的解决用到了HMMs模型中的方法如forward-backward和Viterbi。我们可以把条件随机场看成是一个无向图模型或马尔科夫随机场，它是一种用来标记和切分序列化数据的统计模型。<a id="more"></a>该模型在给定需要标记观察序列的条件下，计算整个标记序列的联合概率，而不是在给定当前状态条件下，定义一个状态的分布。标记序列的分布条件属性，可以让CRFs很好的拟合现实数据，而在这些数据中，标记序列的条件概率依赖于观察序列中非独立的、相互作用的特征，并通过赋予特征不同权值来表示特征的重要程度。</p><h2 id="概率无向图模型"><a href="#概率无向图模型" class="headerlink" title="概率无向图模型"></a>概率无向图模型</h2><h3 id="概率图模型-Graphical-Models"><a href="#概率图模型-Graphical-Models" class="headerlink" title="概率图模型(Graphical Models)"></a>概率图模型(Graphical Models)</h3><p>概率图模型是一类用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的集合。图中的节点表示随机变量，缺少边表示条件独立假设。根据图中边有无方向，分为有向图和无向图。<strong>概率无向图</strong>，又称为马尔科夫随机场，是一个可以由无向图表示的联合概率分布。</p><p>图是由节点以及连接节点的边组成的集合。节点和边分别记作为v和e，节点和边的集合分别记作V和E，图记作G=(V,E)。无向图是指边没有方向的图。概率图模型是由图表示的概率分布。设有联合概率分布$P(Y),Y \in \cal Y$是一组随机变量。由无向图G=(V,E)表示概率分布P(Y)，即在图G中，结点$v\in V$表示一个随机变量$Y_v，Y=(Y_v)_{v\in V}$；边$e\in E$表示随机变量之间的概率依赖关系。</p><p>给定一个联合概率分布P(Y)和表示它的无向图G。首先定义无向图表示的随机变量之间存在的成对马尔科夫性(pairwise Markov property)、局部马尔科夫性(local Markov property)和全局马尔可夫性(global Markov property)。</p><p><strong>成对马尔科夫性</strong></p><p>设u和v是无向图G中任意两个没有边连接的结点，结点u和v分别对应随机变量$Y_u$和$Y_v$。其他所有结点为O,对应的随机变量组是$Y_O$，成对马尔科夫性是指给定随机变量组$Y_O$的条件下随机变量$Y_u$和$Y_v$是条件独立的，即：<br>$$<br>P(Y_u,Y_v|Y_O)=P(Y_u|Y_O)P(Y_v|Y_O)<br>$$</p><p><strong>局部马尔科夫性</strong></p><p>设$v\in V$是无向图G中任意一个结点，W是与v有边连接的所有结点，O是v，W以外的其他所有结点。v表示的随机变量是$Y_v$，W表示的随机变量是$Y_W$，O表示的随机变量组是$Y_O$。局部马尔科夫性是指在给定的随机变量组$Y_W$的条件下随机变量$Y_v$与随机变量组$Y_O$是独立的，即：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/Chi3EKLL2B.jpg?imageslim" alt="mark"><br></div><br>(注：图引自:<a href="http://www.cnblogs.com/Determined22/p/6915730.html" target="_blank" rel="external">http://www.cnblogs.com/Determined22/p/6915730.html</a>)<br><br>$$P(Y_v,Y_O|Y_W)=P(Y_v|Y_w)P(Y_O|Y_W)$$<br>在$P(Y_O|Y_W)&gt;0$时，等价地：$P(Y_v|Y_W)=P(Y_v|Y_W,Y_O)$<br><br><strong>全局马尔科夫性</strong><br><br>设结点集合A、B是在无向图G中被结点集合C分开的任意结点集合，全局马尔科夫性指：在给定$Y_C$的条件下，$Y_A$和$Y_B$条件独立，即:<br><div><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/22mb282if1.jpg?imageslim" alt="mark"></div><br>$$P(Y_A,Y_B|Y_C) = P(Y_A|Y_C)P(Y_B|Y_C)$$<br><br>上述成对的、局部的、全局的马尔科夫性定义是等价的。<br><div></div><h2 id="概率无向图模型定义"><a href="#概率无向图模型定义" class="headerlink" title="概率无向图模型定义"></a>概率无向图模型定义</h2><p>设有联合概率分布P(Y)，由无向图G(V,E)表示，在图G中，结点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率分布P(Y)满足成对、局部和全局马尔科夫性，就称此联合概率分布为概率无向图模型或马尔科夫随机场。<br>以上是<strong>概率无向图模型的定义</strong>，实际上，我们更关心如何求其联合概率分布。对于给定的概率无向图模型，我们希望将整体的联合概率写成若干子联合概率乘积的形式，也就是将联合概率进行因子分解，这样便于模型的学习与计算。事实上，概率无向图模型的最大特点就是易于因子分解，下面介绍这一知识点。</p><p>首先给出无向图中的团和最大团的定义：<br>无向图G中任何两个结点，均有边连接的结点子集称为团(clique)。若C是无向图G的一个团，并且不能再加进任何一个G的结点使其成为一个更大的团，则称此C为最大团(maximal clique)。<br>例如，下图表示由4个结点组成的无向图。图中的2个结点组成的团有5个：${Y_1,Y_2},{Y_2,Y_3},{Y_3,Y_4},{Y_4,Y_2},{Y_1,Y_3}$。有2个最大团：${Y_1,Y_2,Y_3},{Y_2,Y_3,Y_4}$。而${Y_1,Y_2,Y_3,Y_4}$不是一个团，因为$Y_1$和$Y_4$没有边连接</p><div align="center"><br>     <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/B76cDHL286.jpg?imageslim" alt="mark"><br></div><br>将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解(factorization)。给定概率无向图模型，设其无向图为G，C为G上的最大团，$Y_C$表示C对应的随机变量。那么概率无向图的联合概率分布$P(Y)$可以写作图中的所有最大团C上的函数$\Psi_C(Y_C)$的乘积形式，即:<br>$$<br>P(Y) = \frac {1}{Z} \prod_C \Psi_C(Y_C)<br>$$<br>其中，Z是规范化因子(normalization factror)，由公式：<br>$$<br>Z = \sum_Y \prod_C \Psi_C(Y_C)<br>$$<br>计算得出。规范化因子保证P(Y)构成一个概率分布。函数$\Psi_C(Y_C)$称为势函数(potential function)。这里要求势函数$\Psi_C(Y_C)$是严格正的，通常定义为指数函数：$$<br>\Psi_C(Y_C) = exp{-E(Y_C)}<br>$$<br>概率无向图模型的因子分解是由下述定理来保证的。<br><strong>(Hammersley-Clifford 定理)</strong>:概率无向图模型的联合概率分布P(Y)可以表示为如下形式：<br>$$<br>P(Y)=\frac {1}{Z}\prod_C \Psi_C(Y_C) \\<br>Z=\sum_Y \prod_C \Psi_C(Y_C)<br>$$<br>其中，C是无向图的最大团，$Y_C$是C的结点对应的随机变量，$\Psi_C(Y_C)$是C上定义的严格正函数，乘积是在无向图所有的最大团上进行的。<br><div></div><h2 id="条件随机场定义"><a href="#条件随机场定义" class="headerlink" title="条件随机场定义"></a>条件随机场定义</h2><p>条件随机场(conditional random field)是在给定随机变量X条件下，随机变量Y的马尔科夫随机场。这里主要介绍定义在线性链上的特殊的条件随机场，称为线性链条件随机场(linear chain conditional random field)。线性链条件随机场可以用于标注等问题。这时，在条件概率模型P(Y|X)中，Y是输出变量，表示标记序列，X是输入变量，表示需要标注的观测序列。也把标记序列称为状态序列。学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型$\hat P(Y|X)$;预测时，对于给定的输入序列x，求出条件概率$\hat P(y|x)$最大的输出序列$\hat y$。</p><p>首先定义一般的条件随机场，然后定义线性链条件随机场。</p><p>(<strong>条件随机场</strong>)，设X与Y是随机变量，P(Y|X)是在给定X的条件下Y的条件概率分布。若随机变量Y构成一个由无向图G=(V,E)表示的马尔科夫随机场，即：<br>$$P(Y_v|X, w\neq v)=P(Y_v|X,Y_w, w \sim v)$$<br>对任意结点v成立，则称条件概率分布P(Y|X)为条件随机场。式中$w \sim v$表示在图G=(V,E)中与结点v有结点v有边连接的所有结点w，$w \neq v$结点v以外的所有结点，$Y_v,Y_u与Y_w$为结点v,u与w对应的随机变量。</p><p>在定义中没有要求X和Y具有相同的结构。现实中，一般假设X和Y有相同的图结构。本书主要考虑无向图为下图所示的线性链的情况，即：<br>$$G=(V={1,2,…,n},E={(i,i+1)}), i=1,2,…, n-1$$<br>在此情况下，$X=(X_1,X_2,…,X_n)，Y=(Y_1,Y_2,…,Y_n)$，<strong>最大团是相邻两个结点的集合</strong>。</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/bLF9b1Bl8m.jpg?imageslim" alt="mark"><br></div><p><strong>线性链条件随机场有下面的定义</strong>：</p><p>设$X=(X_1,X_2,…,X_n)，Y=(Y_1,Y_2,…,Y_n)$均为线性链表示的随机变量序列，若下给定随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔科夫性：<br>$$<br>P(Y_i|X,Y_1,..,Y_{i-1},…,Y_n)= P(Y_i|X, Y_{i-1},Y_{i+1})\\<br>i=1,2,…,n(在i=1和n时只考虑单边)<br>$$<br>则称P(Y|X)为线性链条件随机场。在标注问题中，X表示输入观测序列，Y表示对应的输出标记序列或状态序列。</p><h3 id="参数化形式"><a href="#参数化形式" class="headerlink" title="参数化形式"></a>参数化形式</h3><p>根据Hammersley-Clifford定理,可以给出线性链条件随机场P(Y|X)的因子分解式，各因子是定义在相邻两个结点上的函数。<br><strong>定理(线性链条件随机场的参数化形式)</strong> 设P(Y|X)为线性链条件随机场，则在随机变量X取值为x的条件下，随机变量Y取值为y的条件概率具有如下形式：<br>$$<br>p(y|x)= \frac {1}{Z(x)} exp {\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_{l}s_l(y_i, x,i)}<br>$$<br>其中，<br>$$<br>Z(x)=\sum_y exp ({\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_{l}s_l(y_i, x,i)})<br>$$<br>式中，$t_k和s_l$是特征函数，$\lambda_k和\mu_l$是对应的权值。$Z(x)$是规范化因子，求和是在所有可能的输出序列上进行的。</p><p>上面的两个式子是线性链条件随机场模型的基本形式，表示给定输入序列x，对输出序列y预测的条件概率。$t_k$是定义在边上的特征函数，称为转移特征，依赖于当前和前一个位置，$s_l$是定义在结点上的特征函数，称为状态特征，依赖于当前位置。$t_k和s_l$都依赖于位置，是局部特征函数。通常，特征函数$t_k和s_l$取值为1或0;当满足特征条件时取值为1，否则为0。条件随机场完全由特征函数$t_k,s_l$和对应的权值$\lambda_k,\mu_l$确定。<br>线性链条件随机场也是对数线性模型(log linear model)。</p><h3 id="简化形式"><a href="#简化形式" class="headerlink" title="简化形式"></a>简化形式</h3><p>条件随机场还可以由简化形式表示。注意到条件随机场式中同一特征在各个位置都有定义，可以对同一个特征在各个位置求和，将局部特征函数转化为一个全局特征函数，这样就可以将条件随机场写成权值向量和特征向量的内积形式，即条件随机场的简化形式。</p><p>为简便起见，首先将转移特征和状态特征及其权值用统一的符号表示。设有$K_1$个转移特征，$K_2$个状态特征，$K=K_1+K_2$，记：<br>$$<br>f_k(y_{i-1},y_i, x,i) = \begin{cases}<br>t_k(y_{i-1},y_i, x, i)&amp; k=1,2,…,K_1\\<br>s_l(y_i, x, i)&amp; k=K_1+l; l=1,2,…,K_2<br>\end{cases}<br>$$<br>然后，对转移与状态特征在各个位置i求和，记作：<br>$$f_k(y,x) = \sum_{i=1}^n f_k(y_{i-1}, y_i, x,i), k=1,2,..,K$$<br>用$w_k$表示特征$f_k(y,x)$的权值，即：<br>$$<br>w_k = \begin{cases}<br>   \lambda_k, &amp; k=1,2,…,K_1\\<br>   \mu_l, &amp; k=K_1+l; l=1,2,…,K_2<br>\end{cases}<br>$$<br>于是，条件随机场可以表示为：<br>$$<br>P(y|x) = \frac {1}{Z(x)}exp \sum_{k=1}^K w_kf_k(y,x)\\<br>Z(x) = \sum_y exp \sum_{k=1}^K w_kf_k(y,x)<br>$$<br>若以w表示权值向量，即：<br>$$w = (w_1,w_2,…,w_K)^T$$<br>以F(y,x)表示全局特征向量，即：<br>$$F(y,x) = (f_1(y,x),f_2(y,x),…,f_k(y,x))^T$$<br>则条件随机场可以写成向量w与F(y,x)的内积形式：<br>$$<br>P_w(y|x) = \frac {exp(w\cdot F(y,x))}{Z_w(x)}<br>$$<br>其中，<br>$$Z_w(x) = \sum_y exp(w\cdot F(y,x))$$</p><h3 id="矩阵形式"><a href="#矩阵形式" class="headerlink" title="矩阵形式"></a>矩阵形式</h3><p>条件随机场还可以由矩阵表示。假设$P_w(y|x)$是由内积形式给出的线性链条件随机场，表示对给定观测序列x，相应的标记序列y的条件概率。引进特殊的起点和终点状态标记$y_0=start,y_{n+1}=stop$，这时$P_w(y|x)$可以通过矩阵的形式表示。</p><p>对观察序列x的每个位置i=1,2,…,n+1，定义一个m阶矩阵(m是标记$y_i$取值个数)<br>$$<br>M_i(x) = [M_i(y_{i-1},y_i|x)]\\<br>M_i(y_{i-1},y_i|x)=exp(W_i(y_{i-1},y_i|x))\\<br>W_i(y_{i-1},y_i|x) = \sum_{i=1}^K w_kf_k(y_{i-1},y_i,x,i)<br>$$</p><p>这样，给定观测序列x，标记序列y的非规范化概率可以通过n+1个矩阵的乘积$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$表示，于是，条件概率$P_w(y|x)$是:<br>$$<br>P_w(y|x)=\frac {1}{Z_w(x)}\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)<br>$$<br>其中，$Z_w(x)$为规范化因子，是n+1个矩阵的乘积(start,stop)元素：<br>$$<br>Z_w(x)= (M_1(x)M_2(x)…M_{n+1}(x))_{start,stop}<br>$$<br>注意，$y_0=start与y_{n+1}=stop$表示开始状态与终止状态，规范化因子$Z_w(x)$是以start为起点stop为终点通过状态的所有路径$y_1y_2,…,y_m$的非规范化概率$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$之和。</p><p>这个M矩阵和一阶HMM中的转移概率矩阵，因为链式CRF中只有相邻两个结点之间才有连接边。</p><h2 id="三个问题"><a href="#三个问题" class="headerlink" title="三个问题"></a>三个问题</h2><h3 id="概率计算问题"><a href="#概率计算问题" class="headerlink" title="概率计算问题"></a>概率计算问题</h3><p>条件随机场的概率计算问题是给定条件随机场P(Y|X)，输入序列x和输出序列y，计算条件概率$<br>P(Y_i=y_i|x),P(Y_{i-1}=y_{i-1},Y_i=y_i|x)$以及相应的数学期望问题。为了方便起见，像隐马尔科夫模型那样，引进前向-后向向量，递归地计算以上概率及期望值。像这样的算法称为前向-后向算法。</p><p><strong>前向-后向算法</strong><br>对每个指标$i=0,1,…,n+1$，定义前向向量$\alpha_i(x)$:<br>$$<br>\alpha_0(y|x)=\begin{cases}<br>   1, &amp; y=start \\<br>   0, &amp; 否则<br>\end{cases}<br>$$<br>递推公式为：<br>$$<br>\alpha_i^T(y_i|x)=\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x),i=1,2,…,n+1<br>$$<br>又可以表示为：<br>$$<br>\alpha_i^T(x) = \alpha_{i-1}^T(x)M_i(x)<br>$$<br>$\alpha_i(y_i|x)$表示在位置i的标记是$y_i$并且到位置i的前面部分标记序列的非规范化概率，$y_i$可取的值有m个，所以$\alpha_i(x)$是m维列向量。</p><p>同样，对于每个位置$i=0,1,…,n+1$，定义后向向量$\beta_i(x)$:<br>$$<br>\beta_{n+1}(y_{n+1}|x)=\begin{cases}<br>   1, &amp; y=stop \\<br>   0, &amp; 否则<br>\end{cases}\\<br>\beta_i(y_i|x)=M_{i+1}(y_i,y_{i+1}|x)\beta_{i+1}(y_{i+1}|x),i=1,2,…,n+1<br>$$<br>又可以表示为：<br>$$<br>\beta_i(y_i|x)=M_{i+1}(x)\beta_{i+1}(x)<br>$$<br>$\beta_i(y_i|x)$表示在位置i的标记$y_i$并且从i+1到n的后面标记序列的非规范化概率。</p><p>由前向-后向向量定义不难得到：<br>$$<br>Z(x) = \alpha_n^T(x)\cdot 1 = 1^T \cdot \beta_1(x)<br>$$<br>这里的1是元素均为1的m维列向量。</p><p><strong>概率计算</strong></p><p>按照前向-后向向量的定义，很容易计算标记序列在为位置i是标记$y_i$的条件概率和在位置$i-1$与i的标记$y_{i-1}和y_i$的条件概率。<br>$$<br>\begin{align}<br>&amp;P(Y_i=y_i|x) =\frac {\alpha_i^T(y_i|x)\beta_i(y_i|x)}{Z(x)}\\<br>&amp;P(Y_{i-1}=y_{i-1},Y_i=y_i|x)=\frac {\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i,|x)}{Z(x)}\\&amp;其中，Z(x)=\alpha_n^T(x)\cdot 1<br>\end{align}<br>$$</p><p><strong>期望计算</strong></p><p>利用前向-后向向量，可以计算特征函数关于联合分布P(X,Y)和条件分布P(Y|X)的数学期望。</p><p>特征函数$f_k$关于条件分布P(Y|X)的数学期望是：<br>$$<br>\begin{align}E_{P(Y|X)}[f_k]<br>&amp;= \sum_y P(y|x)f_k(y,x)\\<br>&amp;=\sum_{i=1}^{n+1} \sum_{y_{i-1},y_i}f_k(y_{i-1},y_i,x,i)\frac {\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i,|x)}{Z(x)}\\<br>&amp; k=1,2,…,K<br>\end{align}$$<br>其中，<br>$$<br>Z(x) = \alpha_n^T(x)\cdot 1<br>$$</p><p>假设经验分布$\widetilde P(X)$，特征函数$f_k$关于联合分布P(X,Y)的数学期望是:<br>$$<br>\begin{align}E_{P(X,Y)}[f_k]<br>&amp;=\sum_{x,y}P(x,y)\sum_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\\<br>&amp;=\sum_x \widetilde P(x)\sum_y P(y|x)\sum_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\\<br>&amp;=\sum_x \widetilde p(x)\sum_{i=1}^{n+1}\sum_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)\frac {\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}\\k=1,2,…,K<br>\end{align}<br>$$<br>其中，<br>$$<br>Z(x)=\alpha_n^T(x) \cdot 1<br>$$<br>对于转移特征$t_k(y_{i-1},y_i,x,i),k=1,2,…,K_1$，可以将式中的$f_k$替换成$t_k$；对于状态特征，可以将式中的$f_k$替换为$s_i$，表示为$s_l(y_i,x,u),k=K_1+l,l=1,2,…,K_2$。</p><p>有了上面的公式，对于给定的观测序列x与标记序列y，可以通过一次前向扫描计算$\alpha_i及Z(x)$，通过一次后向扫描计算$\beta_i$，从而计算所有的概率和特征的期望。</p><h3 id="学习方法"><a href="#学习方法" class="headerlink" title="学习方法"></a>学习方法</h3><p>条件随机场在时序数据上的对数线性模型，使用MLE或带正则的MLE来训练。类似于最大熵模型，可以采用改进的迭代尺度法(IIS)和拟牛顿法(如BFGS算法)来训练。</p><p>训练数据${(x^{j},y^{j})}_{j=1}^N$的对数似然函数为：<br>$$<br>\begin{align}<br>L(w) &amp;= L_{\widetilde P}(P_w)\\<br>&amp;=ln \prod_{j=1}^N P_w(Y=y^{(j)}|x^{(j)})\\<br>&amp;=\sum_{j=1}^N ln P_w(Y=y^{(j)}| x^{(j)})\\<br>&amp;=\sum_{j=1}^N ln \frac {exp \sum_{k=1}^K w_kf_k(y^{(j)},x^{(j)}) }{Z_w(x^{(j)})}\\<br>&amp;=\sum_{j=1}^N (\sum_{k=1}^K w_kf_k(y^{(j)},x^{(j)}) - ln Z_w(x^{(j)}) )<br>\end{align}<br>$$</p><p>或者可以写成这样：<br>$$<br>\begin{aligned}L(\textbf w)=L_{\widetilde P}(P_\textbf w)&amp;=\ln\prod_{x,y}P_{\textbf w}(Y=y|x)^{\widetilde P(x,y)}\\&amp;=\sum_{x,y}\widetilde P(x,y)\ln P_{\textbf w}(Y=y|x)\\&amp;=\sum_{x,y}\widetilde P(x,y)\ln \frac{\exp\sum_{k=1}^Kw_kf_k(y,x)}{Z_{\textbf w}(x)}\\&amp;=\sum_{x,y}\widetilde P(x,y)\sum_{k=1}^Kw_kf_k(y,x)-\sum_{x,y}\widetilde P(x,y)\ln Z_{\textbf w}(x)\\&amp;=\sum_{x,y}\widetilde P(x,y)\sum_{k=1}^Kw_kf_k(y,x)-\sum_{x}\widetilde P(x)\ln Z_{\textbf w}(x)\end{aligned}<br>$$</p><p>最后一个等号是因为$\sum_y P(Y=y|x)=1$。顺便求个导：<br>$$<br>\begin{aligned}\frac{\partial L(\textbf w)}{\partial w_i}&amp;=\sum_{x,y}\widetilde P(x,y)f_i(x,y)-\sum_{x,y}\widetilde P(x)P_{\textbf w}(Y=y|x)f_i(x,y)\\&amp;=\mathbb E_{\widetilde P(X,Y)}[f_i]-\sum_{x,y}\widetilde P(x)P_{\textbf w}(Y=y|x)f_i(x,y)\end{aligned}<br>$$<br>似然函数中的$ln Z_w(x)$项是一个指数函数的和的对数的形式。</p><h3 id="预测算法"><a href="#预测算法" class="headerlink" title="预测算法"></a>预测算法</h3><p>条件随机场的预测问题是给定条件随机场P(Y|X)和输入序列（观测序列）x,求条件概率最大的输出序列（标记序列）$y^*$，即对观测序列进行标注。条件随机场预测算法是采用维特比算法。</p><p>由$P_w(y|x)=\frac {exp(w\cdot F(y,x))}{Z_w(x)}$可得：</p><p>$$\begin{align}<br>y^{*}&amp;=arg max_yP_w(y|x)\\<br>&amp;=arg max_y \frac {exp(w \cdot F(y,x))}{Z_w(x)}\\<br>&amp;=arg max_y exp(w \cdot F(y,x))\\<br>&amp;=arg max_y(w \cdot F(y,x))<br>\end{align}$$</p><p>于是，条件随机场预测问题就成为求非规范化概率最大的最优路径问题：<br>$$<br>max_y(w \cdot F(y,x))<br>$$<br>这里，路径表示标记序列，其中，<br>$$\begin{align}<br>&amp;w = (w_1,w_2,…,w_K)^T\\<br>&amp;F(y,x) = (f_1(y,x),f_2(y,x),…,f_K(y,x))^T\\<br>&amp;f_k(y,x)=\sum_{i=1}^n f_k(y_{i-1},y_i, x,i), k=1,2,…,K<br>\end{align}$$</p><p>注意，这时只需计算非规范化概率，而不必计算概率，可以大大提高效率。为了求解路径，将$max_y(w \cdot F(y,x))$写成如下形式：<br>$$max_y \sum_{i=1}^n w\cdot F_i(y_{i-1,y_i,x})$$<br>其中，<br>$$<br>F_i(y_{i-1},y_i,x) = (f_1(y_{i-1},y_i,x,i), f_2(y_{i-1}, y_i, x,i), … , f_K(y_{i-1},y_i,x,i))^T<br>$$<br>是局部特征向量。<br>下面叙述维特比算法，首先求出位置1的各个标记j=1,2,…,m的非规范化概率：<br>$$<br>\delta_1(j) = w \cdot F_1(y_0=start, y_1=j,x), j=1,2,…,m<br>$$<br>一般地，由地推公式，求出到位置i的各个标记l=1,2,..,m的非规范化概率的最大值，同时记录非规范化概率最大值的路径：<br>$$<br>\delta_i(l) = max_{1\le j \le m} {\delta_{i-1}(j)+w \cdot F_i(y_{i-1}=j, y_i=l,x)},l=1,2,…,m\\<br>\Psi_i(l)  = arg max_{1 \le j \le m} { \delta_{i-1}(j) + w \cdot F_i(y_{i-1}=j, y_i=l,x)}, l =1,2,…,,m<br>$$<br>直到i=n时终止。这时求得非规范化概率的最大值为：<br>$$<br>max_y (w \cdot F(y,x)) = max_{1 \le j \le m}\delta_n(j)<br>$$<br>及最优的终点：<br>$$<br>y_n^<em> = \Psi_{i+1}(y_{i+1}^</em>),i = n-1,n-2,…,1<br>$$<br>求得最优路径$y^<em> = (y_1^</em>, y_2^<em>, …, y_n^</em>)^T$<br>维特比算法如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180527/9eFHIE0ha7.jpg?imageslim" alt="mark"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本篇笔记来自李航老师的《统计学习方法》一书&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;条件随机场最早由Lafferty等人2001年提出，其模型思想的主要来源是最大熵模型，模型的三个基本问题的解决用到了HMMs模型中的方法如forward-backward和Viterbi。我们可以把条件随机场看成是一个无向图模型或马尔科夫随机场，它是一种用来标记和切分序列化数据的统计模型。
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理基础-最大熵模型</title>
    <link href="https://ilewseu.github.io/2018/05/13/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"/>
    <id>https://ilewseu.github.io/2018/05/13/最大熵模型/</id>
    <published>2018-05-13T06:46:20.000Z</published>
    <updated>2018-06-18T14:32:50.143Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><h3 id="熵和条件熵"><a href="#熵和条件熵" class="headerlink" title="熵和条件熵"></a>熵和条件熵</h3><p>熵(entropy)原是一个热力学中的概念，后由香农引入到信息论中。在信息论和概率统计中，熵用来表示随机变量的不确定性的度量。其定义如下：<br><a id="more"></a><br>设$X \in {x_1,x_2,…,x_n}$为一个离散随机变量，其概率分布为$p(X=x_i)=p_i,i=1,2…,n$，则X的熵为：<br>$$<br>H(X)=-\sum_{i=1}^n p_ilogp_i<br>$$<br>其中，当$p_i=0$时，定义$0log0=0$。<br>上式中的对数log以2为底或者以e为底，分别对应bit或nat，熵的两种单位。由熵的公式可知H(X)仅依赖于X的分布，而与X的具体取值无关，因此，常将H(X)记为H(p)。<br><strong>熵的意义：</strong>可以认为熵是描述事物无序性的参数，熵越大则无序性不确定性越大。一个孤立系统的熵，自发性地趋于极大，随着熵的增加，有序状态逐步变为混沌状态，不可能自发的产生新的有序结构。当熵处于最小值，即能量集中程度最高、有效能量处于最大值时，那么整个系统也处于最有序的状态，相反为最无序状态。熵增原理预示着自然界越变越无序。s<br>由熵的计算公式可知熵的取值范围为：<br>$$<br>0 \leq H(X) \leq  log n<br>$$</p><ul><li>左边的等号在X为确定值的时候成立，即X没有变化的可能；</li><li>右边的等号在X为均匀分布的时候成立；</li></ul><p><strong>条件熵的定义</strong>：<br>设$X \in {x_1,x_2,…,x_n},Y \in {y_1,y_2,…,y_n)}$为离散的随机变量。在已知X的条件下，Y的条件熵（conditional entropy）可以定义为：<br>$$<br>H(Y|X)=\sum_i^n p(x_i)H(Y|X=x_i)=-\sum_{i=1}^n p(x_i) \sum_{j=1}^m p(y_j|x_i)log p(y_j|x_i)<br>$$<br>它表示已知X的情况下，Y的条件概率分布的熵对X的数学期望。</p><h2 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h2><p>最大熵的理论基础是熵增理论，即在无外力作用下，事物总是朝着最混乱的方向发展。同时，事物是约束和自由的统一体。事物总是在约束下争取最大的自由权，这其实也是自然界的根本原则。<strong>在已知条件下，熵最大的事物，最可能接近它的真实状态</strong>。<br>最大熵原理是在1957年由E.T.Jaynes提出的，其主要思想是，在只掌握关于未知分布的部分知识时，应该选取符合这些知识但熵值最大的概率分布。因为在这种情况下符合已知知识的概率分布可能不止一个。常在[3]中写道：“我们知道熵定义的实际上是一个随机变量的不确定性，熵最大的时候，说明随机变量最不确定，换句话说，也就是随机变量最随机，对其行为做准确的预测最困难。从这个意义上讲，那么最大熵原理的实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的唯一不偏不倚的选择，任何其他的选择都意味着我们增加了其他的约束和假设，这些约束和假设根据我们掌握的信息无法做出。”<br>吴军老师的《数学之美》第20章中提到了一个掷骰子的例子可以很好的解释最大熵原理。对于一个骰子，每面向上的概率是多少，可能我们会不加思索会说是$\frac {1}{6}$，但是，如果说骰子的其中四点被做过特殊处理，四点向上的概率为$\frac {1}{3}$，那么其他点向上的概率则变为$\frac {2}{15}$。虽然，这是一个很简单的问题，却隐含着最大熵的原理。首先，在骰子没做任何处理之前，我们认为骰子的各个面出现的概率是相同的，即符合均匀分布，此时熵最大。然后，当增加四点被做过特殊处理后，其他面的向上的概率变为$\frac {2}{15}$，在这里四点被做过特殊处理，即所谓的约束条件，满足约束条件之后，而对其它则不做任何假设其他面向上的概率是相同的，即为$\frac {2}{15}$，也是熵最大的。在这个例子中，不作任何假设就是使用“等概率”，这个时候概率分布最均匀，从而使得概率分布的熵最大，即最大熵原理。</p><h2 id="最大熵模型的定义"><a href="#最大熵模型的定义" class="headerlink" title="最大熵模型的定义"></a>最大熵模型的定义</h2><p>最大熵原理是统计学学习的一般原理，将它应用到分类得到最大熵模型。<br>假设分类模型是一个条件概率分布$P(Y|X)$，$X \in \cal X \subseteq R^*$表示输入，$Y \in \cal Y $表示输出，$\cal X和\cal Y$表示输入和输出的集合。这个模型表示的是对于给定的输入X，以条件概率$P(Y|X)$输出Y。<br>给定义一个训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，学习的目标是用最大熵原理选择最好的分类模型。首先，考虑模型应该满足的条件。给定训练数据集，可以确定联合概率分布P(X,)的经验分布和边缘分布P(X)经验分布，分别以$\hat P(X,Y)$和$\hat P(X)$表示，这里：<br>$$<br>\begin{align}<br>    &amp;\hat P(X=x,Y=y)=\frac {v(X=x,Y=y)}{N} \\<br>    &amp;\hat P(X=x)=\frac {v(X=x)}{N}<br>\end{align}<br>$$<br>其中，$v(X=x,Y=y)$表示训练数据中样本(x,y)出现的频数，$v(X=x)$表示训练数据中输入x出现的频数，N表示训练样本的容量。</p><p>用特征函数(feature function)$f(x,y)$描述输入x和输出y之间的某一事实，其定义为：<br>$$<br>f(x,y)=\begin{cases}<br>   1, &amp; x与y满足某一事实 \\<br>   0, &amp; 否则<br>\end{cases}\\<br>$$<br>它是一个二值函数，当x和y满足这个事实时取值为1，否则取值为0。<br>特征函数$f(x,y)$在训练数据集上关于经验分布$\hat P(X,Y)$的期望值，用$E_{\hat p}(f)$表示：<br>$$E_{\hat p}(f) = \sum_{x,y} \hat P(x, y)f(x,y)$$<br>特征函数$f(x,y)$关于模型上关于$p(x,y)$的数学期望为：<br>$$E_{p}(f) = \sum_{x,y}p(x,y)f(x,y)$$<br>$p(y|x)$与经验分布$\hat p(x)$的期望值用$E_p(f)$表示。需要注意的是$p(x,y)$是未知的，而且建模的目标是$p(y|x)$。因此，我们希望将排$p(x,y)$表示成$p(y|x)$的函数。利用贝叶斯定理有，$p(x,y)=p(x)p(y|x)$，但$p(x)$依然是未知的，此时，只好利用$\hat p(x)$来近似了。这样$E_p(f)$可以重新定义为：<br>$$E_p(f) = \sum_{x,y}\hat P(x)P(y|x)f(x,y)$$<br>对于概率分布$p(y|x)$，我们希望特征$f$的期望值应该和从训练数据中得到的特征期望值是一致的，因此，提出约束：<br>$$E_p(f) = E_{\hat p}(f)$$<br>即：<br>$$<br>\sum_{x,y}\hat P(x)P(y|x)f(x,y)=\sum_{x,y}p(x,y)f(x,y)<br>$$<br>假设从训练数据集中抽取了n个特征，相应地，便有n个特征函数$f_i(i=1,2,…,n)$以及n个约束条件：<br>$$<br>C_i:E_p(f_i)=E_{\hat p}(f_i),i=1,2,…,n.<br>$$<br><strong>最大熵模型定义</strong>：<br>利用最大熵原理选择一个最好的分类模型，即对于任意一个给定的输入$x \in \cal X$，可以以概率$p(y|x)$输出$y \in \cal Y$。前面已经讨论了特征函数和约束条件，要利用最大熵原理，还差一个熵的定义，注意，我们的目标是获取一个条件分布，因此，这里也采用相应的条件熵，如下：<br>$$H(p(y|x))= \sum_{x,y} \hat p(x)p(y|x)log p(y|x)$$<br>下文将$H(p(y|x))$简记为$H(p)$，为了后面计算方便，上式中的对数为自然对数，即以e为底。至此，可以给出最大熵模型的完整描述了。对于给定的训练数据T，特征函数$f_i(x,y),i=1,2,…,n$，最大熵模型就是求解：<br>$$<br>\begin{align}<br>&amp;max_{p\in C}   H(p)=(-\sum_{x,y} \hat p(x)p(y|x)log p(y|x)),\\<br>&amp;s.t.       \\<br>&amp;\sum_y p(y|x)=1<br>\end{align}<br>$$<br>或者:<br>$$<br>\begin{align}<br>&amp;min_{p\in C}   -H(p)=(-\sum_{x,y} \hat p(x)p(y|x)log p(y|x)),\\<br>&amp;s.t.       \sum_y p(y|x)=1<br>\end{align}<br>$$<br>则模型集合C中条件熵$H(p)$最大的模型称为最大熵模型。</p><h2 id="最大熵模型的学习"><a href="#最大熵模型的学习" class="headerlink" title="最大熵模型的学习"></a>最大熵模型的学习</h2><p>最大熵模型的学习过程就是最大熵模型的求解过程。最大熵模型的学习可以形式化为约束最优化问题，主要思路和步骤如下：</p><ol><li>利用拉格朗日乘子法将最大熵模型由一个带约束的最优化问题转化为一个与之等价的无约束的最优化问题，它是一个<strong>极小极大问题</strong>（min max)；</li><li>利用对偶问题等价性，转化为求解上一步得到的极小极大问题的对偶问题，它是一个<strong>极大极小问题</strong>（max min）</li></ol><p>在求解内层的极小问题时，可以导出最大熵模型的解具有指数形式，而在求解最外层的极大问题时，还将意外地发现其与最大似然估计的等价性。<br>对于给定的训练数据$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$以及特征函数$f_i(x,y),i=1,2,…n$，最大熵模型的学习等价于约束最优化问题：<br>$$<br>\begin{align}<br>&amp;max_{p\in C}   H(p)=-\sum_{x,y}\hat p(x)p(y|x)logp(y|x)\\<br>&amp;s.t.       E_p(f_i)=E_{\hat p}(f_i),i=1,2,…,n\\<br>&amp;            \sum_yp(y|x)=1<br>\end{align}<br>$$<br>按照最优化问题的习惯，将求<strong>最大值问题转换为求最小值的问题</strong>：<br>$$<br>\begin{align}<br>&amp;min_{p\in C}   -H(p)=\sum_{x,y}\hat p(x)p(y|x)logp(y|x)\\<br>&amp;s.t.       E_p(f_i)-E_{\hat p}(f_i)=0,i=1,2,…,n\\<br>&amp;            \sum_yp(y|x)=1<br>\end{align}<br>$$<br>求解约束最优化问题，所得出的解，就是最大熵模型学习的解，下面给出具体的推导。这里先将约束最优化的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题求解原始问题。<br>首先，引入拉格朗日乘子$\lambda_{0},\lambda_{1},…,\lambda_n$，记为$\lambda=(\lambda_{0},\lambda_{1},…,\lambda_n)^T$，定义拉格朗日函数为$L(p,\lambda)$，则：<br>$$\begin{align}<br>L(p,\lambda)&amp;=-H(p)+\lambda_0(1-\sum_yp(y|x))+\sum_{i=1}^n \lambda_{i}(E_{\hat p}(f_i)-E_p(f_i))\\<br>&amp;=\sum_{x,y}\hat p(x)p(y|x)log p(y|x)+\lambda_0(1-\sum_y p(y|x))+\sum_{i=1}^n(E_{\hat p}(f_i)-E_p(f_i))<br>\end{align}<br>$$<br>利用拉格朗日对偶性，可知最大熵模型等价于求解：<br>$$<br>min_{p\in C} max_{\lambda} L(p,\lambda)<br>$$<br>我们把上式称为<strong>原始问题</strong>（primary problem)极小极大问题；通过交换极大和极小的位置，可以得到原始问题的<strong>对偶问题</strong>（dual problem）为<br>$$max_{\lambda}min_{p\in C}L(p,\lambda)$$<br>由于$H(p)$是关于p的凸函数，因此，原始问题和对偶问题是等价的。这样可以通过求解对偶问题来求解原始问题。首先，对于对偶问题内层的极小问题$min_{p\in C}L(p,\lambda)$是关于参数$\lambda$的函数，将其记为$\psi(\lambda)$，即：<br>$$<br>\psi(\lambda)=min_{p\in C}L(p,\lambda)=L(p_{\lambda},\lambda)<br>$$<br>其中，<br>$$<br>p_{\lambda} = arg min_{p\in C}L(p,\lambda)<br>$$<br>首先，计算拉格朗日函数$L(p,\lambda)$对$p(y|x)$的偏导数。<br>$$\begin{align}<br>\frac {\partial L(p,\lambda)}{\partial p(y|x)}<br>&amp;=\sum_{x,y}\hat p(x)(logp(y|x)+1)-\sum_y \lambda_0 - \sum_{i=1}^n \lambda_i(\sum_{x,y}\hat p(x)f_i(x,y))\\<br>&amp;=\sum_{x,y}\hat p(x)(logp(y|x)+1)-\sum_x \hat p(x)\sum_y \lambda_0 - \sum_{x,y}\hat p(x)\sum_{i=1}^n \lambda_i f_i(x,y)\\<br>&amp;=\sum_{x,y} \hat p(x)(log p(y|x)+1) - \sum_{x,y}\hat p(x)\lambda_0 - \sum_{x,y}\hat p(x)\sum_{i=1}^n \lambda_i f_i(x,y)\\<br>&amp;=\sum_{x,y}\hat p(x)(log p(y|x)+1 - \lambda_0 - \sum_{i=1}^n \lambda_if_i(x,y))<br>\end{align}<br>$$<br>另偏导数等于0，在$\hat p(x)&gt;0$的情况下，进一步，令$\frac {\partial L(p, \lambda)}{\partial p(y|x)}=0$可得：<br>$$<br>log p(y|x)+1 - \lambda_0 - \sum_{i=1}^n \lambda_if_i(x,y)=0<br>$$<br>从而得到：<br>$$p(y|x)=e^{\lambda_0 - 1} \cdot e^{\sum_{i=1}^n \lambda_i f_i(x,y)}$$<br>将上式代入约束条件$\sum_y p(y|x)=1$，即：<br>$$<br>\sum_y p(y|x)=e^{\lambda_0 - 1} \cdot \sum_y e^{\sum_{i=1}^n \lambda_i f_i(x,y)=1}<br>$$<br>可得：<br>$$<br>e^{\lambda_0-1} = \frac {1}{\sum_y e^{\sum_{i=1}^n \lambda_i f_i(x,y)}}<br>$$<br>将上式代入到$p(y|x)=e^{\lambda_0 - 1} \cdot e^{\sum_{i=1}^n \lambda_i f_i(x,y)}$得到：<br>$$<br>p_{\lambda} = \frac {1}{Z_{\lambda}(x)}e^{\sum_{i=1}^n \lambda_if_i(x,y)}<br>$$<br>其中，<br>$$Z_{\lambda}(x)=\sum_{y} e^{\sum_{i=1}^n \lambda_if_i(x,y)}$$<br>称为规范化因子，注意，此时的$\lambda=(\lambda_1,\lambda_2,…,\lambda_n)^T$，不再包含$\lambda_0$了。</p><p>$p_{\lambda}$就是最大熵模型的解，它具有指数形式，其中$\lambda_1,\lambda_2,…,\lambda_n$为参数，分别对应$f_1,f_2,…,f_n$的权重，$\lambda_i$越大，表示特征$f_i$越重要。<br>之后求解对偶问题外部的极大化问题：<br>$$max_{\lambda} \psi (\lambda)$$<br>设其解为：<br>$$<br>\lambda^{*} = argmax_{\lambda} \psi (\lambda)<br>$$<br>则，最大熵模型的解为$p^* = p_{\lambda^*}$<br>为此，首先要给出$\psi(\lambda)$的表达式：<br>$$\begin{align}<br>   \psi(\lambda)<br>   &amp;=L(p_{\lambda},\lambda)\\<br>   &amp;=\sum_{x,y}\hat p(x)p_{\lambda}(y|x)logp_{\lambda}(y|x)+\sum_{i=1}^n \lambda_{i}(E_{\hat p}(f_i) - E_{p}(f_i))\\<br>   &amp;=\sum_{x,y}\hat p(x)p_{\lambda}(y|x)logp_{\lambda}(y|x)+\sum_{i=1}^n \lambda_{i}(\sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_{x,y}\hat p(x)p_{\lambda}(y|x)f_i(x,y))\\<br>   &amp;=\sum_{i=1}^n \lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) + \sum_{x,y} \hat p(x)p_{\lambda}(y|x)(log p_{\lambda}(y|x) - \sum_{i=1}^n \lambda_if_i(x,y))<br>\end{align}<br>$$<br>由于，<br>$$<br>log p_{\lambda}(y|x) = \sum_{i=1}^n \lambda_if_i(x,y) - log Z_{\lambda}(x)<br>$$<br>带入前面的$\psi (\lambda)$可得：<br>$$<br>\begin{align}<br>\psi(\lambda)&amp;=\sum_{i=1}^n\lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_{x,y}\hat p(x)p_{\lambda}(y|x)log Z_{\lambda}(x)\\<br>&amp;=\sum_{i=1}^n\lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_x \hat p(x)log Z_{\lambda}(x)\sum_y p_{\lambda}(y|x)\\<br>&amp;=\sum_{i=1}^n\lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_x \hat p(x)log Z_{\lambda}(x)<br>\end{align}<br>$$<br>有了$\psi(\lambda)$的具体表达式，就可以求解其最大值点$\lambda^*$了。<br>对$\psi (\lambda)$的最大化等价于最大熵模型的极大似然估计。已知训练数据的经验分布为$\hat p(x,y)$，条件概率分布$p(y|x)$的对数似然函数表示为：<br>$$<br>L_{\hat p}(p) = log \prod_{x,y}p(y|x)^{\hat p(x,y)}=\sum_{x,y} \hat p(x,y)logp(y|x)<br>$$<br>将$p_{\lambda}$带入似然函数，可得：<br>$$<br>\begin{align}<br>L_{\hat p}(p_{\lambda})&amp;=\sum_{x,y}\hat p(x,y)log p_{\lambda}(y|x)\\<br>&amp;=\sum_{x,y}\hat p(x,y)(\sum_{i=1}^n \lambda_if_i(x,y) - logZ_{\lambda}(x))\\<br>&amp;=\sum_{x,y}\hat p(x,y)\sum_{i=1}^n \lambda_if_i(x,y) - \sum_{x,y}logZ_{\lambda}(x)\\<br>&amp;=\sum_{i=1}^n \lambda_i(\sum_{x,y} \hat p(x,y)f_i(x,y)) - \sum_{x,y} \hat p(x,y)log Z_{\lambda}(x)\\<br>&amp;=\sum_{i=1}^n \lambda_i \sum_{x,y} \hat p(x,y)f_i(x,y) - \sum_x \hat p(x)log Z_{\lambda}(x)<br>\end{align}<br>$$<br>可以看到公式25和公式30是相等的，这说明最大化$\psi(\lambda)$和最大化似然估计是等价的。<br>最大熵模型最优化方法有多种，比如GIS算法、IIS算法以及梯度下降法等方法，这里就不具体介绍了，具体可参考《统计学习》方法。</p><h2 id="最大熵模型的优缺点"><a href="#最大熵模型的优缺点" class="headerlink" title="最大熵模型的优缺点"></a>最大熵模型的优缺点</h2><p>最大熵模型的优点如下：</p><ul><li>建模时，实验者只需要集中精力选择特征，而不需要花费精力考虑如何使用这些特征；</li><li>特征选择灵活，且不需要额外的独立假定或者内在约束；</li><li>模型应用在不同领域时的可移植性强；</li><li>可结合更丰富的信息；</li></ul><p>缺点：</p><ul><li>时空开销大；</li><li>数据稀疏问题严重；</li><li>对语料的依赖性较强；</li></ul><h2 id="特征引入算法"><a href="#特征引入算法" class="headerlink" title="特征引入算法"></a>特征引入算法</h2><p>在使用最大熵模型的时候，往往会构建很多特征，但并不是所有的特征都是有用的，所以需要作特征选择。尤其对于自然语言处理问题，这些特征的选取往往具有很大的主观性。同时，因为特征数量往往很多，必须引入一个客观标准自动计算以引入特征到模型中，同时针对特征进行参数估计。Della Pietra et al[6]对自然语言处理中随机域的特征选择进行了描述，在进行特征选取时，是由特征的信息增益作为衡量标准的。一个特征对所处理问题带来的信息越多，该特征越适合引入到模型中。一般我们首先形式化一个特征空间，所有可能的特征都为后补特征，然后从这个后补特征集内选取对模型最为有用的特征集合。特征引入的算法如下：</p><p>算法： 特征引入算法<br>输入：候选特征集合F，经验分布$\hat p(x,y)$<br>输出：模型选用的特征集合S，集合这些特征的模型$P_s$</p><ol><li>初始化：特征集合S为空，它所对应的模型$P_s$均匀分布，n=0;</li><li>对于候选特征集合F中的每一个特征$f\in F$，计算加入该特征后为模型带来的增益值$G_f$；</li><li>选择具有最大增益值$G(S,f)$的特征$f_n$;</li><li>把特征$f_n$加入到集合S中，$S=(f_1,f_2,…,f_n)$;</li><li>重新调整参数值，使用GIS算法计算模型$P_s$;</li><li>n=n+1，返回到第2步；</li></ol><p>在算法的第2步中，要计算每个特征的增益值，这里是根据Kullback-Leibler（简称KL）距离来计算的。衡量两个概率分布p和q的KL距离，公式如下：<br>$$<br>D(p||q)  = \sum_x p(x)ln \frac {p(x)}{q(x)}<br>$$<br>距离的大小与两种分布的相似程度成反比，距离越小表示两种分布越逼近。因此，在加入第n个特征前后，求的模型分布与样本分布之间的KL距离为：<br>$$<br>D(\hat p||p^{(n-1)}) = \sum_x \hat p(x) ln \frac {\hat p(x)}{p^{(n-1)}(x)}\\<br>D(\hat p||p^{(n)}) = \sum_x \hat p(x) ln \frac {\hat p(x)}{p^{(n)}(x)}<br>$$<br>这样，我们定义引入第n个特征$f^{(n)}$后的增益值为：<br>$$<br>f^{(n)} = D(\hat p||p^{(n-1)})  - D(\hat p||p^{(n)})<br>$$<br>因此，选择第n个特征为：<br>$$<br>f^{(n)} = arg max_{f \in F}G(p, f^{(n)})<br>$$</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://www.cnblogs.com/pinard/p/6093948.html" target="_blank" rel="external">https://www.cnblogs.com/pinard/p/6093948.html</a>；</li><li><a href="https://blog.csdn.net/itplus/article/details/26550597" target="_blank" rel="external">https://blog.csdn.net/itplus/article/details/26550597</a>；</li><li>自然语言处理的最大熵模型-常宝宝；</li><li>统计学习方法第6章；</li><li>自然语言处理技术中的最大熵模型方法-李素建、刘群等；</li><li>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty, Inducing features of random fields, IEEE Transactions on Pattern Analysis and Machine Intelligence 19:4, pp.380–393, April, 1997</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本知识&quot;&gt;&lt;a href=&quot;#基本知识&quot; class=&quot;headerlink&quot; title=&quot;基本知识&quot;&gt;&lt;/a&gt;基本知识&lt;/h2&gt;&lt;h3 id=&quot;熵和条件熵&quot;&gt;&lt;a href=&quot;#熵和条件熵&quot; class=&quot;headerlink&quot; title=&quot;熵和条件熵&quot;&gt;&lt;/a&gt;熵和条件熵&lt;/h3&gt;&lt;p&gt;熵(entropy)原是一个热力学中的概念，后由香农引入到信息论中。在信息论和概率统计中，熵用来表示随机变量的不确定性的度量。其定义如下：&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>统计语言模型</title>
    <link href="https://ilewseu.github.io/2018/05/07/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>https://ilewseu.github.io/2018/05/07/语言模型/</id>
    <published>2018-05-07T11:06:20.000Z</published>
    <updated>2018-05-12T15:29:18.900Z</updated>
    
    <content type="html"><![CDATA[<h2 id="语言模型概述"><a href="#语言模型概述" class="headerlink" title="语言模型概述"></a>语言模型概述</h2><p>语言模型(Language Model)，就是用来计算一个句子概率的模型。从统计的角度看，自然语言中的一个句子可以由任何词串构成。不过P(s)有大有小。比如：<br><a id="more"></a></p><ul><li>s1 = 我 刚 吃 过 晚饭</li><li>s2 = 刚 我 过 晚饭 吃</li></ul><p>可以看出P(s1)&gt;P(s2)。对于给定的句子而言，通常P(s)是未知的。对于一个服从某个概率分布P的语言L，根据给定的语言样本估计P的过程被称作语言建模。<br>根据语言样本估计出的概率分布P就称为语言L的语言模型。<br>$$\sum_{s\in L}P(s)=1$$<br>语言建模技术首先在语音识别研究中提出，后来陆续用到OCR、手写体识别、机器翻译、信息检索等领域。在语音识别中，如果识别结果有多个，则可以根据语言模型计算每个识别结果的可能性，然后挑选一个可能性较大的识别结果。语言模型也可以用于汉语歧义消解。<br>那么如何计算一个句子的概率呢？对于给定的句子:<br>$$S=w_1w_2,…,w_n$$<br>它的概率可以表示为：<br>$$P(S)=P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,w_2,…,w_{n-1})$$<br>由于上面的式子参数过多，因此需要近似的计算方法，常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机场(CRF)方法、神经网络方法等。本篇文章主要记录n-gram模型方法。</p><h2 id="n-gram模型"><a href="#n-gram模型" class="headerlink" title="n-gram模型"></a>n-gram模型</h2><p>对于给定的句子$S=w_1w_2…w_n,$,根据链式规则:<br>$$<br>P(S)=P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,w_2,…,w_{n-1})=\prod_{i=1}^np(w_i|w_1…w_{i-1})<br>$$<br>P(S)就是语言模型，即用来计算一个句子S概率的模型。<br>那么，如何计算$p(w_i|w_1,w_2,…,w_{i-1})$呢？最简单、直接的方法是计数后做除法，即最大似然估计(Maximum Likelihood Estimate，MLE)，如下：<br>$$<br>p(w_i|w_1,w_2,…,w_{i-1})=\frac {count(w_1,w_2,…,w_{i-1},w_i)}{count(w_1,w_2,…,w_{i-1})}<br>$$<br>其中，$count(w_1,w_2,…,w_{i-1},w_i)$表示次序列$(w_1,w_2,…,w_{i-1},w_i)$在预料库中出现的频率。</p><p>这里面临两个重要的问题：数据稀疏严重和参数空间过大，导致无法实用。实际中，我们一般较长使用N语法模型(n-gram)，它采用了马尔科夫假设，即认为语言中的每个词只与其前面长度为n-1的上下文有关。</p><ul><li><p>假设下一个词的出现不依赖前面的词，即为uni-gram，则有:$$ \begin {aligned}<br>  P(S)&amp;=P(w_1)P(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_1,w_2,…,w_{n-1})\\\\&amp;=p(w_1)p(w_2)…p(w_n)<br>\end{aligned}$$</p></li><li><p>假设下一个词的出现只依赖前面的一个词，即为bi-gram，则有：$$ \begin {aligned}<br>  P(S)&amp;=P(w_1)P(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_1,w_2,…,w_{n-1})\\\\&amp;=p(w_1)p(w_2|w_1)p(w_3|w_2)…p(w_n|w_{n-1})<br>\end{aligned}$$</p></li><li>假设下一个词的出现依赖它前面的两个词，即为tri-gram，则有：<br>$$<br>\begin {aligned} P(S)&amp;=P(w_1)P(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_1,w_2,…,w_{n-1})\\\\&amp;=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)…p(w_n|w_{n-2},w_{n-1})<br>\end {aligned}<br>$$</li></ul><p>实际上常常在对数空间里面计算概率，原因有两个：</p><ul><li>防止溢出；如果计算的句子很长，那么最后得到的结果将非常小，甚至会溢出，比如计算得到的概率是0.001，那么假设以10为底取对数的结果就是-3，这样就不会溢出；</li><li>对数空间里面加法可以代替乘法，因为log(p1p2) = logp1 +logp2，在计算机内部，显然加法比乘法执行更快；</li></ul><h3 id="n-gram中的n如何选择？"><a href="#n-gram中的n如何选择？" class="headerlink" title="n-gram中的n如何选择？"></a><strong>n-gram中的n如何选择？</strong></h3><ul><li><strong>n较大时</strong>：提供了更多的上下文语境信息，语境更具有区别性；但是，参数个数多、计算代价大、训练预料需要多，参数估计不可靠；</li><li><strong>n较小时</strong>：提供的上下文语境少，不具有区别性；但是，参数个数小、计算代价小、训练预料无须太多、参数估计可靠；</li></ul><p>理论上，n越大越好，经验上tri-gram用的最多，尽管如此，原则上，<strong>能用bi-gram解决，绝不使用tri-gram</strong>。</p><h3 id="建立n-gram语言模型"><a href="#建立n-gram语言模型" class="headerlink" title="建立n-gram语言模型"></a>建立n-gram语言模型</h3><p>构建n-gram语言模型，通过计算最大似然估计构造语言模型。一般的过程如下：</p><ul><li><strong>1、数据准备：</strong><ul><li>确定训练语料</li><li>对语料进行tokenization或切分</li><li>句子边界，增加特殊的词<bos>和<eos>开始和结束</eos></bos></li></ul></li><li><strong>2、参数估计：</strong><ul><li>利用训练语料，估计模型参数</li></ul></li></ul><p>令$c(w_1,…,w_n)$表示n-gram $w_1,…,w_n$在训练语料中出现的次数，则：<br>$$<br>    P_{MLE}(w_n|w_1,…,w_{n-1})=\frac {c(w_1,…,w_n)}{c(w_1,…,w_{n-1})}<br>$$</p><h3 id="语言模型效果评估"><a href="#语言模型效果评估" class="headerlink" title="语言模型效果评估"></a>语言模型效果评估</h3><p>目前主要有两种方法判断建立的语言模型的好坏：</p><ul><li>实用方法：通过查看该模型在实际应用（如拼写检查、机器翻译）中的表现来评价，优点是直观、实用，缺点是缺乏针对性、不够客观；</li><li>理论方法：困惑度(preplexity)，其基本思想是给测试集赋予较高概率值的语言模型较好；</li></ul><h2 id="平滑方法"><a href="#平滑方法" class="headerlink" title="平滑方法"></a>平滑方法</h2><p>最大似然估计给训练样本中未观察到的事件赋以0概率。如果某个n-gram在训练语料中没有出现，则该n-gram的概率必定是0。这就会使得在计算某个句子S的概率时，如果某个词没有在预料中出现，那么该句子计算出来的概率就会变为0，这是不合理的。<br>解决的办法是扩大训练语料的规模。但是无论怎样扩大训练语料，都不可能保证所有的词在训练语料中均出现。由于训练样本不足而导致估计的分布不可靠的问题，称为数据稀疏问题。在NLP领域中，稀疏问题永远存在，不太可能有一个足够大的语料，因为语言中的大部分词都属于低频词。</p><h3 id="Zipf定律"><a href="#Zipf定律" class="headerlink" title="Zipf定律"></a>Zipf定律</h3><p>Zipf定律描述了词频以及词在词频表中的位置之间的关系。针对某个语料库，如果某个词w的词频是f，并且该词在词频表中的序号为r(即w是所统计的语料中第r常用词)，则：<br>$$f*r=k(k是一个常数)$$<br>若$w_i$在词频表中的排名50，$w_j$在词频表中排名为150，则$w_i$的出现频率大约是$w_j$的频率的3倍。<br>Zipf定律告诉我们：</p><ul><li>语言中只有很少的常用词</li><li>语言中的大部分词都是低频词（不常用的词)</li></ul><p>Zipf的解释是Principle of Lease effort </p><ul><li>说话的人只想使用少量的常用词进行交流</li><li>听话的人只想使用没有歧义的词（量大低频）进行交流</li></ul><p>Zipf定律告诉我们：</p><ul><li>对于语言中的大多数词，它们在语料中出现是稀疏的</li><li>只有少量的词语料库可以提供它们规律的可靠样本</li></ul><h3 id="平滑技术"><a href="#平滑技术" class="headerlink" title="平滑技术"></a>平滑技术</h3><p>对于语言而言，由于数据稀疏的存在，MLE不是一种很好的参数估计方法。为了解决数据稀疏问题，人们为理论模型实用化而进行了众多的尝试，出现了一系列的平滑技术，<strong>它们的基本思想是降低已出现n-gram的条件概率分布，以使未出现的n-gram条件概率分布为非零，且经过平滑后保证概率和为1。</strong>目前，已经提出了很多数据平滑技术，如下：</p><ul><li>Add-one平滑</li><li>Add-delta平滑 </li><li>Good-Turing平滑</li><li>Interpolation平滑</li><li>回退模型-Katz平滑</li><li>…</li></ul><h4 id="Add-one平滑"><a href="#Add-one平滑" class="headerlink" title="Add-one平滑"></a>Add-one平滑</h4><p>加一平滑法，又称为拉普拉斯定律，其规定任何一个n-gram在训练预料中至少出现一次（即规定没有出现过的n-gram在训练预料中出现了1次）</p><p>$$P_{Add1}(w_1,w_2,…,w_n)=\frac {C(w_1,w_2,…,w_n)+1}{N+V}$$</p><ul><li>N:为训练预料中所所有的n-gram的数量(token);</li><li>V:所有的可能的不同的n-gram的数量(type);<br>下面两幅图分别演示了未平滑和平滑后的bi-gram示例：</li></ul><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180505/d6ljdKdckd.jpg?imageslim" alt="mark"></p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180505/EG0fA48HjC.jpg?imageslim" alt="mark"></p><p>(注：上图均来自《计算语言学-常宝宝》的课件)</p><p>训练语料中未出现的n-gram的概率不再为0，而是一个大于0的较小的概率值。但是，由于训练预料中未出现的n-gram数量太多，平滑后，所有未出现的n-gram占据了整个概率分布中的一个很大的比例。因此，在NLP中，Add-one给训练预料中没有出现过的n-gram分配太多的概率空间。同时，它认为所有未出现的n-gram概率相等，这是否合理？出现在训练语料中的哪些n-gram，都增加同样频度值，不一定合理。</p><h4 id="Add-delta平滑"><a href="#Add-delta平滑" class="headerlink" title="Add-delta平滑"></a>Add-delta平滑</h4><p>Add-delta平滑，不是加1，而是加一个比1小的整数$\lambda$:<br>$$<br>P_{AddD}(w_1,w_2,…,w_n)=\frac {C(w_1,w_2,…,w_n)+\lambda}{N+\lambda V}<br>$$<br>通常$\lambda =0.5$，此时又称为Jeffreys-Perks Law或ELE。它的效果要比Add-one好，但是仍然不理想。</p><h4 id="Good-Turing平滑"><a href="#Good-Turing平滑" class="headerlink" title="Good-Turing平滑"></a>Good-Turing平滑</h4><p>其基本思想是利用频率的类别信息对频率进行平滑。假设N是样本数据的大小，$n_r$是在样本中正好出现r次的事件的数目(在这里，事件为n-gram $w_1,w_2,…,w_n$)。即：出现1的$n_1$个，出现2次的$n_2$个,…。那么：<br>$$<br>N=\sum_{r=1}^{\infty} n_r r<br>$$<br>由于，$$N=\sum_{r=0}^{\infty}n_r r^*=\sum_{r=0}^{\infty}(r+1)n_{r+1}$$，所以，<br>$$<br>r^* = (r+1)\frac {n_{r+1}}{n_r}<br>$$</p><p>那么，Good-Turing估计在样本中出现r次的事件的概率为:<br>$$<br>P_r = \frac {r^*}{N}<br>$$<br>实际应用中，一般直接使用$n_{r+1}$代替$E(n_{r+1})$，$n_r$代替$E(n_r)$。这样，样本中所有事件的概率之和为：<br>$$<br>\sum_{r&gt;0} n_r * P_r = 1 - \frac {n_1}{N} &lt;1<br>$$<br>因此，有$\frac {n_1}{N}$的剩余的概率量就可以均分给所有未出现事件(r=0)。<br>Good-Turing估计适用于大词汇集产生的符合多项式分布的大量的观察数据。<br>在估计频度为r的n-gram的概率$p_r$时，如果数据集中没有频度为r+1的n-gram怎么办？此时，$N_{r+1}=0$导致$p_r=0$。解决的办法是对$N_r$进行平滑，设S(.)是平滑函数，S(r)是$N_r$的平滑值。<br>$$<br>r^* = (r+1)\frac {S(r+1)}{S(r)}<br>$$</p><h4 id="Interpolation平滑"><a href="#Interpolation平滑" class="headerlink" title="Interpolation平滑"></a>Interpolation平滑</h4><p>不管是Add-one，还是Good Turing平滑技术，对于未出现的n-gram都一视同仁，难免存在不合理性。所以介绍一种线性差值的的平滑技术，其基本思想是将高阶模型和低阶模型作线性组合，利用低阶n-gram模型对高阶n-gram模型进行线性差值。因为没有足够的数据对高阶n-gram模型进行概率估计时，低阶的n-gram模型通常可以提供有用的信息。因此，可以把不同阶的n-gram模型组合起来产生一个更好的模型。</p><p>把不同阶别的n-gram模型线性加权组合：<br>$$P(w_n|w_{n-1},w_{n-2})=\lambda_1P(w_n)+\lambda_2P(w_n|w_{n-1})+\lambda_3P(w_n|w_{n-1}w_{n-2})$$<br>其中，$0&lt;=\lambda_i&lt;=1,\sum_i \lambda_i=1$。$\lambda_i$可以根据实验凭经验设定，也也可以通过应用某些算法确定，例如EM算法。</p><h4 id="回退模型-Katz平滑"><a href="#回退模型-Katz平滑" class="headerlink" title="回退模型-Katz平滑"></a>回退模型-Katz平滑</h4><p>回退模型-Katz平滑，其基本思想是：当某一事件在样本中出现的概率大于K(通常K为0或1)，运用最大似然估计减值来估计其概率，否则使用低阶的，即(n-1)gram概率代替n-gram概率。而这种替代必须受归一化因子$\alpha$的作用。回退模型的一般形式如下：<br>$$<br>p_{smooth}(w_i|w_{i-n+1}^{i-1})=\begin{cases}<br>             \hat p(w_i|w_{i-n+1}^{i-1}), &amp;  if c(w_{i-n+1}^i)&gt;0 \\<br>             \alpha(w_{i-n+1}^{i-1})\cdot p_{smooth}(w_i|w_{i-n+2}^{i-1}), &amp; if c(w_{i-n+1}^{i-1})=0<br>            \end{cases}$$<br>参数$\alpha(w_{i-n+1}^{i-1})$是归一化因子，以保证$$\sum_{w_i}p_{smooth}(w_i|w_{i-n+1}^{i-1})=1$$<br>以bi-gram为例，令$r=c(w_{i-1}w_i)$，如果r&gt;0，则$p_{katz}(w_i|w_{i-1})=d_r\cdot p_{ML}(w_i|w_{i-1})$，$d_r$称为折扣率，给定$w_{i-1}$，从r&gt;0的bi-grams中折除的概率为：$$<br>S(w_{i-1}) = 1 - \sum_{w_i \in M(w_{i-1})} p_{katz}(w_i|w_{i-1}) \\其中，M(w_{i-1})={w_i|c(w_{i-1}w_i)&gt;0}<br>$$</p><p>对于给定的$w_{i-1}$，令：$$<br>Q(w_{i-1}) = {w_i|c(w_{i-1}w_i)=0}<br>$$<br><strong>如何把$S(w_{i-1})$分配给集合$Q(w_{i-1})$中的那些元素？</strong></p><p>对于$w_i \in Q(w_{i-1})$，如果$p_{ML}(w_i)$比较大，则应该分配更多的概率给它。所以，若r=0，则：$$<br>p_{katz}(w_i|w_{i-1})=\frac {p_{ML}(w_i)}{\sum_{w_j\in Q}p_{ML}(w_j)} \cdot S(w_{i-1})<br>$$<br>对于bi-gram模型，Katz平滑为：<br>$$p_{katz}(w_i|w_{i-1})=<br>             \begin{cases}<br>             d_r\cdot p_{ML}(w_i|w_{i-1}), &amp;  if r&gt;0 \\<br>             \alpha(w_{i-1}) \cdot p_{ML}(w_i), &amp; if r=0<br>             \end{cases}<br>\\<br>其中，\alpha(w_{i-1}) = \frac {1 - \sum_{w_j\in M} p_{katz}(w_j|w_{j-1}) }{\sum_{w_j\in Q} p_{ML}(w_j)}<br>$$</p><p><strong>如何计算$d_r$?</strong></p><ul><li>如果$r&gt;k$，不折扣，即$d_r=1$(Katz提出k=5)</li><li>如果$0&lt;r \leq k$，按照和Good-Turing估计同样的方式折扣，即按照$\frac {r^{*}}{r}$进行折扣。严格说，要求$d_r$满足，$1-d_r=u(1-\frac {r^{*}}{r})$</li><li>根据Good-Turing估计，未出现的n元组估计出现频次是$n_1$，$\sum_{r=1}^k n_r(1-d_r)r=n$</li><li>具体而言，若$0&lt;r \leq k$，有$$<br>d_r = \frac {\frac {r^*}{r} - \frac {(k+1)n_{k+1}}{n_1}} {1 - \frac {(k+1)n_{k+1}}{n!}}<br>$$</li></ul><p>big-gram的Katz平滑模型最终可描述为：<br>$$p_{katz}(w_i|w_{i-1})=\begin{cases}<br>             c(w_{i-1}w_i)/c(w_{i-1}), &amp;  if r&gt;k \\<br>             d_rc(w_{i-1}w_i)/c(w_{i-1}), &amp; if k \geq r &gt; 0 \\<br>             \alpha (w_{i-1})p_{katz}(w_i) &amp; r=0<br>             \end{cases}<br>$$<br>n-gram模型的Katz平滑可以此类推。<br>在回退模型和线性插值模型中，当高阶n-gram未出现时，使用低阶n-gram估算高阶n-gram的概率分布。在回退模型中，高阶n-gram一旦出现，就不再使用低阶n-gram进行估计。在线性插值模型中，无论高阶n-gram是否出现，低阶n-gram都会被用来估计高阶n-gram的概率分布。</p><h2 id="大规模n-gram的优化"><a href="#大规模n-gram的优化" class="headerlink" title="大规模n-gram的优化"></a>大规模n-gram的优化</h2><p>如果不想自己动手实现n-gram语言模型，推荐几款开源的语言模型项目：</p><ul><li>SRILM(<a href="http://www.speech.sri.com/projects/srilm/" target="_blank" rel="external">http://www.speech.sri.com/projects/srilm/</a>)</li><li>IRSTLM(<a href="http://hlt.fbk.eu/en/irstlm" target="_blank" rel="external">http://hlt.fbk.eu/en/irstlm</a>)</li><li>MITLM(<a href="http://code.google.com/p/mitlm/" target="_blank" rel="external">http://code.google.com/p/mitlm/</a>)</li><li>BerkeleyLM(<a href="http://code.google.com/p/berkeleylm/" target="_blank" rel="external">http://code.google.com/p/berkeleylm/</a>)</li></ul><p>在使用 n-gram 语言模型时，也有一些技巧在里面。例如，面对 Google N-gram 语料库，压缩文件大小为 27.9G，解压后 1T 左右，如此庞大的语料资源，使用前一般需要先剪枝（Pruning）处理，缩小规模，如仅使用出现频率大于 threshold 的 n-gram，过滤高阶的 n-gram（如仅使用 n&lt;=3 的资源），基于熵值剪枝，等等。</p><p>另外，在存储方面也需要做一些优化:</p><ul><li>采样Trie数的数据结构，可以优化时间复杂度为$O(log_{|V|}m)$|V|为字母的个数；</li><li>借助Bloom filter辅助查询，把String映射为int类型处理；</li><li>利用郝夫曼树对词进行编码，将词作为索引值而不是字符串进行存储，能将所有词编码成包含在2个字节内的索引值；</li><li>优化概率值存储，概率值原使用的数据类型是（float），用4-8bit来代替原来8Byte的存储内容；</li></ul><h2 id="N-gram模型的缺陷"><a href="#N-gram模型的缺陷" class="headerlink" title="N-gram模型的缺陷"></a>N-gram模型的缺陷</h2><ul><li>数据稀疏问题：利用平滑技术解决；</li><li>空间占用大；</li><li>长距离依赖问题；</li><li>多义性；</li><li>同义性；如 “鸡肉”和“狗肉”属于同一类词，p(肉|鸡)应当等于p(肉|狗)，而在训练集中学习到的概率可能相差悬殊；</li></ul><h2 id="语言模型应用"><a href="#语言模型应用" class="headerlink" title="语言模型应用"></a>语言模型应用</h2><h3 id="n-gram距离"><a href="#n-gram距离" class="headerlink" title="n-gram距离"></a>n-gram距离</h3><p>假设有一个字符串s，那么该字符串的n-gram就表示按长度n切分原词得到的词段，也就是s中长度为n的子串。假设有两个字符串，然后分别求它们的n-gram，那么就可以从它们的共有子串的数量这个角度定义两个字符串间的n-gram距离。但是仅仅是简单地对共有子串进行计数显然也存在不足，这种方案忽略了两个字符长度的差异，可能导致的问题。比如，字符串girl和girlfriend，二者拥有的公共子串数量显然与girl和其自身所拥有的公共子串数量相等，但是不能认为girl和girlfriend是两个等同的匹配。为解决该问题，有学者便提出以非重复的n-gram分词为基础来定义n-gram距离这一概念，可以用下面的公式来表述：</p><p>$$|G_N(s)|+|G_N(t)|-2*|G_N(s)\cap G_N(t)|$$</p><p>例如，字符串s=”ABC”，t=”AB”，分别在字符串首尾加上begin和end，采用二元语言模型，字符串s产生的bi-gram为：(begin,A),(A,B),(B,C),(C,end)；字符串t产生的bi-gram为：(begin,A),(A,B),(B,end)。<br>采用上面公式定义:4+3 - 2*2 = 3<br>显然，字符串之间的距离越小，它们就越接近。当两个字符串完全相等的时候，它们之间的距离就是0。</p><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>分词是NLP中一项比较基础且重要的任务。对于X=”我爱中国”这样一句话，有多种切分方案，对于$y_i$这种分词方案，如,$Y_0=(“我”，“爱”，“中国”),Y_1=(“我”，“爱中”，“国”)$，利用贝叶斯公式可以计算出每种切分的概率:$$<br>P(Y_i|X)=\frac {P(X|Y_i)P(Y_i)}{P(X)}\propto P(X|Y_i)P(Y_i),i=1,2,3,…<br>$$<br>无论在哪种$Y_i$下，最终都能生成句子X，因此$P(X|Y_i)=1$，所以$P(Y_i|S)\propto P(Y_i),i=1,2,3…$。所以，只需要最大化$P(Y_i)即可$。例如，根据bi-gram语言模型，$P(Y_0|X)\propto P(Y_0)=P(我)P(爱|我)P(中国|爱)$，$P(Y_1|X)\propto P(Y_1)=P(我)P(爱中|我)P(国|爱中)$，然后利用计算出的概率，选择最大的作为分词方案。</p><h3 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h3><p>词性标注（POS tagging）是一个典型的多分类问题，将每个词标注为名词、动词、形容词、副词、代词等。例如，在“我/爱/中国”句话中，“爱”有很多词性，比如，名词、动词。最简单的标注其语义的方案就是，看语料库中“爱”出现的次数，以及其词性，即：<br>$$<br>P(POS_i|爱)=\frac {c(“爱”作为POS_i )}{c(爱),i=1,2,…,k，k为词性总数}<br>$$<br>但是，这种简单的词性标注的方案依赖人工，且未考虑上下文。考虑到在一个句子中当前词的词性和前面一两个词关系比较大。因此，可以借用n-gram模型的思路进行求解。比如，在考虑“爱”的词性时，以前面一个词的词性作为参考，即“我”的词性，则，当前这个“爱”的词性概率分布为:<br>$$P(POS_i|我，爱)=P(POS_i|Pron.,爱)=\frac {前面被“副词”修饰的“爱”的POS_i}{c(前面被“副词”修饰的“爱”)},i=1,2,…,k,k为词性总数$$</p><p>计算这个概率需要对语料库进行统计。但是，前提是先判断好“我”的词性。因为，采用的是bi-gram模型，由于“我”已经是第一个词，在二元模型中主需要级简的方案判断即可。</p><h3 id="n-gram作为文本特征"><a href="#n-gram作为文本特征" class="headerlink" title="n-gram作为文本特征"></a>n-gram作为文本特征</h3><p>在处理文本的特征的时候，通常一个关键词作为一个特征。但是，在某些场景下可能词的表达能力不够，需要提取更多的特征，n-gram就是一个很好的特征。以bi-gram为例，在原始文本中，每个关键词可以作为一个特征，将每个关键词两两组合，得到一个bi-gram组合，在根据n-gram语言模型，计算各个bi-gram组合的概率，作为新的特征。</p><h3 id="英语介词短语消歧"><a href="#英语介词短语消歧" class="headerlink" title="英语介词短语消歧"></a>英语介词短语消歧</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>我们是这样理解语言的；</li><li>《计算语言学-常宝宝》的课件</li><li><a href="https://www.cnblogs.com/ljy2013/p/6425277.html" target="_blank" rel="external">https://www.cnblogs.com/ljy2013/p/6425277.html</a></li><li><a href="https://blog.csdn.net/TiffanyRabbit/article/details/72654180" target="_blank" rel="external">https://blog.csdn.net/TiffanyRabbit/article/details/72654180</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;语言模型概述&quot;&gt;&lt;a href=&quot;#语言模型概述&quot; class=&quot;headerlink&quot; title=&quot;语言模型概述&quot;&gt;&lt;/a&gt;语言模型概述&lt;/h2&gt;&lt;p&gt;语言模型(Language Model)，就是用来计算一个句子概率的模型。从统计的角度看，自然语言中的一个句子可以由任何词串构成。不过P(s)有大有小。比如：&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP基础-熵</title>
    <link href="https://ilewseu.github.io/2018/05/02/NLP%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E7%86%B5/"/>
    <id>https://ilewseu.github.io/2018/05/02/NLP中的各种熵/</id>
    <published>2018-05-02T12:52:20.000Z</published>
    <updated>2018-05-12T14:46:25.352Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一些基础"><a href="#一些基础" class="headerlink" title="一些基础"></a>一些基础</h2><h3 id="互斥"><a href="#互斥" class="headerlink" title="互斥"></a>互斥</h3><p>如果事件A和B不可能同时发生，即$AB=\Phi$，则称A与B是互斥的。<br><a id="more"></a></p><h3 id="对立"><a href="#对立" class="headerlink" title="对立"></a>对立</h3><p>如果A与B互斥，又在每次试验中不是出现A就是出现B，即$AB=\Phi$且$A+B=\Omega$，则称B是A的对立事件。</p><h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>在事件B发生的条件下，事件A发生的概率称为事件A在事件B已发生的条件下的条件概率，记作P(A|B)。当P(B)&gt;0时，规定:<br>$$<br>P(A|B)=\frac {P(AB)}{P(B)}<br>$$<br>当P(B)=0时，规定P(A|B)=0。由条件概率的定义，可以得到<strong>乘法公式</strong>：<br>$$\begin {aligned}<br>&amp;P(AB)=P(A)P(B|A)\\\\<br>&amp;P(A_1A_2…A_n)=P(A_1)P(A_2|A_1)P(A_3|A_2A_1)…P(A_n|A_{n-1}A_{n-2}…A_1)=\prod_i^n P(A_i|A_{i-1}A_{i-2}…A_1)<br>\end {aligned}$$<br>一般而言，条件概率P(A|B)与概率P(A)是不等的。但在某些情况下，它们是相等的。根据条件概率的定义和乘法公式有:$$P(AB)=P(A)P(B)$$这时，称事件A与B是相互独立的。</p><h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>根据乘法公式，可以的得到下面重要的公式，该公式称为贝叶斯公式：<br>$$P(A|B)=\frac {P(B|A)(A)}{P(B)}$$<br>一般地，事件$A_1,A_2,…,A_n$两两互斥，事件B满足$B\subset A_1+A_2+…+A_n$且$P(A_i)&gt;0(i=1,2,…,n),P(B)&gt;0$，贝叶斯公式可以推广为：$$<br>p(A_j|B)=\frac{P(A_j)P(B|A_j)}{P(A_1)P(B|A_1)+…+P(A_n)P(B|A_n)}=\frac {P(A_j)P(B|A_j)}{\sum_i^n P(A_i)P(B|A_i)}<br>$$<br>实用上称，$P(A_1),P(A_2),…,P(A_n)$的值称为先验概率，称$P(A_1|B),P(A_2|B),…,P(A_n|B)$的值称为后验概率，贝叶斯公式便是从先验概率计算后验概率的公式。</p><h2 id="各种熵-entropy"><a href="#各种熵-entropy" class="headerlink" title="各种熵(entropy)"></a>各种熵(entropy)</h2><p>在信息论中，如果发送一个消息所需要的编码的长度较大，则可以理解为消息所蕴涵的信息量较大，如果发送一个消息所需要的编码长度较小，则该消息所蕴涵的信息量较小，平均信息量即为发送一个消息的平均编码长度，可以用<strong>熵</strong>的概念来描述。</p><h3 id="自信息"><a href="#自信息" class="headerlink" title="自信息"></a>自信息</h3><p>自信息是熵的基础，自信息表示某一事件发生时所带来的信息量的多少。当事件发生的概率越大，则自信息越小。当一件事发生的概率非常小，并且实际上也发生了（观察结果），则此时的自信息较大。某一事件发生的概率非常大，并且实际上也发生了，则此时的自信息较小。如何度量它？现在要寻找一个函数，满足条件：事件发生的概率越大，则自信息越小；自信息不能是负值，最小是0；自信息应该满足可加性，并且两个对立事件的自信息应该等于两个事件单独的自信息。自信息的公式如下：<br>$$I(p_i)=-log(p_i)$$<br>其中，$p_i$表示随机变量的第i个事件发生的概率，自信息单位是bit,表征描述该信息需要多少位。可以看出，自信息的计算和随机变量本身数值没有关系，只和其概率有关。</p><h3 id="熵的定义"><a href="#熵的定义" class="headerlink" title="熵的定义"></a>熵的定义</h3><p>设X是取有限个值的随机变量，它的分布密度为$p(x)=P(X=x),且x\in X$，则X的熵的定义为：$$H(x)=-\sum _{x \in X}p(x)log_ap(x)$$<br>熵描述了随机变量的不确定性。一般也说，熵给出随机变量的一种度量。对于数底a可以是任何正数，对数底a决定了熵的单位，如果a=2，则熵的单位称为比特(bit)。</p><h3 id="熵的基本性质"><a href="#熵的基本性质" class="headerlink" title="熵的基本性质"></a>熵的基本性质</h3><ul><li>$H(x)&lt;=log|X|$，其中等号成立当且仅当$p(x)=\frac {1}{|x|}$，这里|X|表示集合X中的元素个数。该性质表明等概场具有的最大熵；</li><li>$H(X)&gt;=0$，其中等号成立的条件当且仅当对某个i,$p(x_i)=1$，其余的$p(x_k)=0 (k!=i)$。这表明确定场(无随机性)的熵最小；</li><li>熵越大，随机变量的不确定性就越大，分布越混乱，随机变量状态数越多；</li></ul><h3 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h3><p>设X,Y是两个离散随机变量，它们的联合分布密度为p(x,y)，则给定X时Y的条件熵定义为：<br>$$\begin{aligned}<br>H(Y|X) &amp;=-\sum_{x\in X}p(x)H(Y|X=x)\\\\<br>&amp;=\sum_{x\in X}p(x)[-\sum_{y \in Y}p(y|x)log p(y|x)]\\\\<br>&amp;=-\sum_{x \in X}\sum_{y \in Y} p(x,y)log p(y|x)<br>\end{aligned}<br>$$<br>联合熵和条件熵的关系可以用下面的公式来描述，该关系一般也称为链式规则：$$<br>H(X,Y)=H(X)+H(Y|X)$$<br>信息量的大小随着消息的长度增加而增加，为了便于比较，一般使用熵率的概率。熵率一般也称为字符熵(per-letter entropy)或词熵(per-word entropy)。</p><h3 id="熵率"><a href="#熵率" class="headerlink" title="熵率"></a>熵率</h3><p>对于长度为n的消息，熵率的定义为：<br>$$<br>H_{rate}=\frac{1}{n}H(x_{1n}) = -\frac {1}{n}\sum_{x_{1n}}p(x_{1n})log p(x_{1n})<br>$$<br>这里的$x_{1n}$表示随机变量序列$X_1,X-2…X_n,p(x_{1n})表示分布密度p(x_1,x_2,…,x_n)$。<br>可以把语言看做一系列语言单位构成的一个随机变量序列$L={X_1X_2…X_n}$，则语言L的熵可以定义这个随机变量序列的熵率:<br>$$<br>H_{rate}=\lim_{x \to +\infty}\frac{1}{n}H(H_1,H_2,…,H_n)<br>$$</p><h3 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h3><p>根据链式规则，有:$$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$可以推导出：$$<br>H(X)-H(X|Y)=H(Y)-H(Y|X)<br>$$<br>H(X)与H(X|Y)的差称为互信息，一般记作I(X;Y)。I(X;Y)描述了包含在X中的有关Y的信息量，或包含在Y中的有关X的信息量。下图很好的描述了互信息和熵之间的关系。<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180503/7Cceem39Dg.jpg?imageslim" alt="mark"><br>$$\begin{aligned}<br> I(X;Y)&amp;=H(X)-H(X|Y)\\\\&amp;=H(X)+H(Y)-H(X,Y)\\\\<br> &amp;=\sum_x p(x)log \frac {1}{p(x)}+\sum_x p(y)log \frac {1}{p(y)}+\sum_{x,y} p(x,y)log p(x, y)\\\\&amp;=\sum_{x,y}log \frac {p(x,y)}{p(x)p(y)}<br>\end{aligned}<br>$$<br><strong>互信息(mutual information),</strong>随机变量X,Y之间的互信息定义为：$$<br>I(X;Y)=\sum_{x,y}p(x,y)log \frac {p(x,y)}{p(x)p(y)}<br>$$<br><strong>互信息的性质</strong>：</p><ul><li>$I(X;Y)&gt;=0$，等号成立当且仅当X和Y相互独立。</li><li>$I(X;Y)=I(Y;X)$说明互信息是对称的。</li></ul><p>互信息相对于相对熵的区别就是，互信息满足对称性；<br>互信息的公式给出了两个随机变量之间的互信息。在计算语言学中，更为常用的是两个具体事件之间的互信息，一般称之为<strong>点式互信息</strong>。<br><strong>点式互信息(pointwise mutual information)</strong>，事件x,y之间的互信息定义为：$$<br>I(x,y) = log \frac {p(x,y)}{p(x)p(y)}<br>$$<br>一般而言，点间互信息为两个事件之间的相关程度提供一种度量，即：</p><ul><li>当$I(x,y)&gt;&gt;0$时，x和y是高度相关的；</li><li>当$I(x,y)=0$时，x和y是高度相互独立；</li><li>当$I(x,y)&lt;&lt;0$时，x和y呈互补分布；</li></ul><h3 id="交叉熵-cross-entropy"><a href="#交叉熵-cross-entropy" class="headerlink" title="交叉熵(cross entropy)"></a>交叉熵(cross entropy)</h3><p>交叉熵的概念是用来衡量估计模型与真实概率分布之间差异情况的，其定义为，设随机变量X的分布密度p(x)，在很多情况下p(x)是未知的，人们通常使用通过统计的手段得到X的近似分布q(x),则随机变量X的交叉熵定义为：<br>$$<br>H(p,q) = -\sum_{x \in X} p(x)log q(x)<br>$$<br>其中，p是真实样本的分布，q为预测样本分布。在信息论中，其计算数值表示：如果用错误的编码方式q去编码真实分布p的事件，需要多少bit数，是一种非常有用的衡量概率分布相似的数学工具。</p><h3 id="相对熵-relative-entropy"><a href="#相对熵-relative-entropy" class="headerlink" title="相对熵(relative entropy)"></a>相对熵(relative entropy)</h3><p>相对熵，设p(x),q(x)是随机变量X的两个不同的分布密度，则它们的相对熵定义为:$$<br>D(p||q)=\sum_{x \in X}p(x) log \frac {p(x)}{q(x)}=H(p,q)-H(p)<br>$$<br>相对熵较交叉熵有更多的优异性质，主要为：</p><ul><li>当p分布和q分布相等的时候，KL散度值为0；</li><li>可以证明是非负的；</li><li>KL散度是非对称的，通过公式可以看出，KL散度是衡量两个分布的不相似性，不相似性越大，则值越大，当完全相同时，取值为0；</li></ul><p>对比交叉熵和相对熵，可以发现仅仅差一个H(p)，如果从最优化的角度来看，p是真实分布，是固定值，最小化KL散度的情况下，H(p)可以省略，此时交叉熵等价于KL散度。</p><p><strong>在机器学习中，何时需要使用相对熵，何时使用交叉熵？</strong><br>在最优化问题中，最小化相对熵等价于最小化交叉熵；相对熵和交叉熵的定义其实可以从最大似然估计得到。最大化似然函数，等价于最小化负对数似然，等价于最小化交叉熵，等价于最小化KL散度。交叉熵大量应用在Sigmoid函数和SoftMax函数中，而相对熵大量应用在生成模型中，例如，GAN、EM、贝叶斯学习和变分推导中。从这可以看出：如果想通过算法对样本数据进行概率分布建模，那么通常都是使用相对熵，因为需要明确知道生成的分布和真实分布的差距，最好的KL散度值应该是0；而在判别模型中，仅仅只需要评估损失函数的下降值即可，交叉熵可以满足要求，其计算量比KL散度小。在《数学之美》一书中是这样描述它们的区别：<strong>交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除新系统的不确定性所需要付出的努力的大小；相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异。</strong></p><h3 id="困惑度-perplexity"><a href="#困惑度-perplexity" class="headerlink" title="困惑度(perplexity)"></a>困惑度(perplexity)</h3><p>对于语言$L=(x_i)~p(x)$，与其模型q的交叉熵定义为：$$<br>H(L,p)=-\lim_{x \to \infty} \frac {1}{n} \sum_{x_1^n}p(x_1^n)log q(x_1^n)<br>$$<br>其中，$x_1^n=x_1,…,x_n$为语言L的语句，$p(x_1^n)$为L中语句的概率，$q(x_1^n)$为模型q对$x_1^n$的概率估计。<br>我们可以假设这种语言是“理想”的，即n趋于无穷大时，其全部“单词”的概率和为1。就是说，根据信息论的定理：假定语言L是稳态(stationary) ergodic随机过程， L与其模型q的交叉熵计算公式就变为：<br>$$H(L,q)=-\lim_{x \to \infty} \frac {1}{n} log q(x_1^n)$$<br>由此，我们可以根据模型q和一个含义大量数据的L的样本来计算交叉熵。在设计模型q时，我们的目的是使交叉熵最小，从而使模型最接近真实的概率分布p(x)。</p><p>在设计语言模型时，通常用困惑度来代替交叉熵衡量语言模型的好坏。给定语言L的样本$l_1^n=l_1,,,,l_n$，L的困惑度为$PP_q$定义为：<br>$$<br>PP_q = 2^{H(L,q)}   \approx 2^{-\frac {1}{n}log q(l_1^n)} = [q(l_1^n)]^{- \frac {1}{n}}<br>$$<br>于是语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实语言的情况。从perplexity的计算式可以看出来，它是对于样本句子出现的概率，在句子长度上Normalize一下的结果。它越小，说明出现概率越大，所得模型就越好。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>机器学习各种熵：从入门到全面掌握：<a href="https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw##" target="_blank" rel="external">https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw##</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一些基础&quot;&gt;&lt;a href=&quot;#一些基础&quot; class=&quot;headerlink&quot; title=&quot;一些基础&quot;&gt;&lt;/a&gt;一些基础&lt;/h2&gt;&lt;h3 id=&quot;互斥&quot;&gt;&lt;a href=&quot;#互斥&quot; class=&quot;headerlink&quot; title=&quot;互斥&quot;&gt;&lt;/a&gt;互斥&lt;/h3&gt;&lt;p&gt;如果事件A和B不可能同时发生，即$AB=\Phi$，则称A与B是互斥的。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://ilewseu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的激活函数</title>
    <link href="https://ilewseu.github.io/2018/04/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>https://ilewseu.github.io/2018/04/21/神经网络中的激活函数/</id>
    <published>2018-04-21T06:16:20.000Z</published>
    <updated>2018-05-12T15:19:03.139Z</updated>
    
    <content type="html"><![CDATA[<h2 id="激活函数简介"><a href="#激活函数简介" class="headerlink" title="激活函数简介"></a>激活函数简介</h2><p>神经网络是目前比较流行深度学习的基础，神经网络模型模拟人脑的神经元。人脑神经元接收一定的信号，对接收的信号进行一定的处理，并将处理后的结果传递到其他的神经元，数以亿计的神经元组成了人体复杂的结构。在神经网络的数学模型中，神经元节点，将输入进行加权求和，加权求和后再经过一个函数进行变换，然后输出。这个函数就是激活函数，神经元节点的激活函数定义了对神经元输入的映射关系。<br><a id="more"></a></p><div align="center"><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/k641bmB33m.png?imageslim"><br></div><h3 id="激活函数的定义"><a href="#激活函数的定义" class="headerlink" title="激活函数的定义"></a>激活函数的定义</h3><p>在ICML2016的一篇论文：Noisy Activation Functions中给出的了激活函数的定义：激活函数是实数到实数的映射，且几乎处处可导。激活函数一般具有以下性质：</p><ul><li>非线性：弥补线性模型的不足；</li><li>几乎处处可导：反向传播时需要计算激活函数的偏导数，所以要求激活函数除个别点外，处处可导；</li><li>计算简单</li><li>单调性：当激活函数是单调的时候，单层网络能够保证是凸函数；</li><li>输出值范围有限：当激活函数的输出值有限的时候，基于梯度的优化方法会更加稳定；因为特定的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的learning rate。</li></ul><h3 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h3><ul><li>神经网络中的激活函数能够引入非线性因素，提高模型的表达能力；<br>网络中仅有线性模型的话，表达能力不够。比如一个多层的线性网络，其表达能力和单层的线性网络是相同的。网络中卷积层、池化层和全连接层都是线性的。所以，需要在网络中加入非线性的激活函数层。</li><li>一些激活函数能够起到特征组合的作用；<br>例如，对于Sigmoid函数$\sigma(x) = \frac {1}{1+e^(-x)}$，根据泰勒公式展开:$$<br>e^x = 1+ \frac {1}{1!}x + \frac {1}{2!}x^2 + \frac {1}{3!}x^3+O(x^3)<br>$$<br>对于输入特征为$x_1,x_2$，加权组合后如下：<br>$$<br>x = w_1x_1+w_2x_2<br>$$<br>将x带入到$e^x$泰勒展开的平方项，$$<br>x^2=(w_1x_1+w_2x_2)^2 = ((w_1x_1)^2+(w_2x_2)^2 + 2w_1x_1*w_2x_2)<br>$$<br>可以看出，平方项起到了特征两两组合的作用，更高阶的$x^3,x^4$等，则是更复杂的特征组合。</li></ul><h2 id="常见非线性激活函数"><a href="#常见非线性激活函数" class="headerlink" title="常见非线性激活函数"></a>常见非线性激活函数</h2><p>在介绍常见的激活函数之前，先介绍一下饱和(Saturated)的概念。</p><ul><li>左饱和：当函数h(x)满足，$\lim \limits_{x \to +\infty}h^{‘}(x)=0$;</li><li>右饱和：当函数h(x)满足，$\lim \limits_{x \to -\infty}h^{‘}(x)=0$;</li><li>饱和：当函数h(x)既满足左饱和又满足右饱和，称h(x)是饱和的;</li></ul><p>当激活函数是饱和的，对激活函数进行求导计算梯度时，计算出的梯度趋近于0，导致参数更新缓慢。</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>Sigmoid函数：</p><ul><li>定义：$\sigma(x) = \frac {1}{1+e^{-x}}$</li><li>值域：(0,1)</li><li>导数：$\sigma^{‘}(x) = \sigma(x)(1-\sigma(x))$<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/hB715EhmHJ.png?imageslim" alt="mark"></li></ul><p>从数学上看，非线性的Sigmoid函数对中央区域的信号增益较大，对两侧区域的信号增益较小，在信号的特征空间映射上，有很好的效果。从神经科学上来看，中央神经区酷似神经元的兴奋态，两侧区酷似神经元的抑制状态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。</p><p>Sigmoid的有以下优点：</p><ul><li>输出值域在(0,1)之间，可以被表示为概率；</li><li>输出范围有限，数据在传递的过程中不容易发散；</li><li>求导比较方便；</li></ul><p>Sigmoid的缺点如下：</p><ul><li>Sigmoid函数是饱和的，可能导致梯度消失(两个原因:(1)Sigmoid导数值较小；(2)Sigmoid是饱和的)，导致训练出现问题；</li><li>输出不以0为中心，可能导致收敛缓慢(待思考原因)；</li><li>指数计算，计算复杂度高；</li></ul><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>Tanh函数的表达式为：</p><p>$$tanh(x)=\frac {e^x-e^{-x}}{e^x+e^{-x}}=2\sigma(2x) - 1$$<br>它将输入值映射到[-1,1]区间内，其函数图像为：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/3jhB57gBEi.png?imageslim" alt="mark"></p><p>它的导数为$tanh^{‘}(x)=1-tanh^2(x)$<br>Tanh函数是Sigmoid函数的一种变体；与Sigmoid不同的是，Tanh是0均值的。因此，在实际应用中，Tanh会比Sigmoid更好，但Tanh函数现在也很少使用，其优缺点总结如下：</p><ul><li>相比Sigmoid函数，收敛速度更快；</li><li>相比Sigmoid函数，其输出是以0为中心的；</li><li>没有解决由于饱和性产生的梯度消失问题；</li></ul><h3 id="ReLU-Rectified-Linear-Units"><a href="#ReLU-Rectified-Linear-Units" class="headerlink" title="ReLU(Rectified Linear Units)"></a>ReLU(Rectified Linear Units)</h3><p>ReLU函数为现在使用比较广泛的激活函数，其表达式为：<br>$$<br>f(x)=max(0,x)=\begin{cases}<br>             x, &amp;  if x&gt;0 \\<br>             0  &amp; if x\leq0<br>             \end{cases}<br>$$<br>其函数图像如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/2HBL5Fa5ld.png?imageslim" alt="mark"></p><p>导数图像如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/dHg6L8J47d.png?imageslim" alt="mark"></p><p><strong>ReLU的优点如下:</strong></p><ul><li>相比Sigmoid和Tanh，ReLU在SGD中收敛速度要相对快一些；</li><li>Sigmoid和Tanh涉及到指数运算，计算复杂度高，ReLU只需要一个阈值就可以得到激活值，加快正向传播的计算速度；</li><li>有效的缓解了梯度消失的问题；</li><li>提供了神经网络的稀疏表达能力；</li></ul><p><strong>ReLU的缺点如下：</strong></p><ul><li>ReLU的输出不是以0为中心的；</li><li>训练时，网络很脆弱，很容易出现很多神经元值为0，从而再也训练不动；</li></ul><h3 id="ReLU的变体"><a href="#ReLU的变体" class="headerlink" title="ReLU的变体"></a>ReLU的变体</h3><p>为了解决上面的问题，出现了一些变体，这些变体的主要思路是将x&gt;0的部分保持不变，$x\leq0$的部分不直接设置为0，设置为$\alpha x$，如下三种变体:</p><ul><li>L-ReLU(Leaky ReLU):$\alpha$固定为比较小的值，比如：0.01，0.05；</li><li>P-ReLU(Parametric ReLU):$\alpha$作为参数，自适应地从数据中学习得到；</li><li>R-ReLU(Randomized ReLU):先随机生成一个$\alpha$，然后在训练过程中再进行修正；</li></ul><h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><p>ELU的函数形式如下：<br>$$<br>f(x)=\begin{cases}<br>             x, &amp;  if x&gt;0 \\<br>             \alpha(e^x-1)  &amp; if x\leq0<br>             \end{cases}<br>$$</p><p>其函数图像如下：<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/4aIakGg1G8.png?imageslim" alt="mark"></p><p>ELU也是为了解决ReLU存在的问题而提出的，它具有ReLU的基本所有优点，以及：</p><ul><li>不会有神经元死亡的问题；</li><li>输出的均值接近于0，zero-centered;</li><li>计算量稍大，理论上虽然好于ReLU，但在实际使用中，目前并没有好的证据证明ELU总是优于ReLU;</li></ul><h3 id="MaxOut"><a href="#MaxOut" class="headerlink" title="MaxOut"></a>MaxOut</h3><p>MaxOut函数定义如下：$$<br>y=f(x)=max_{j\in[1,k]}z_j \\<br>z_j = w_jx+b_j<br>$$<br>一个比较容易的介绍，如下图：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180507/GFGj5LcC7a.png?imageslim" ,width="600" height="400"><br></div><p>假设w是二维的，那么有:<br>$$<br>f(x)=max(w_1^Tx+b_1,w_2^Tx+b_2)<br>$$<br>可以看出，ReLU及其变体都是它的一个变形(当$w_1,b_1=0的时候，就是ReLU$)</p><h2 id="激活函数使用建议"><a href="#激活函数使用建议" class="headerlink" title="激活函数使用建议"></a>激活函数使用建议</h2><ul><li>如果想让结果在(0,1)之间，使用Sigmoid(如LSTM的各种Gates);</li><li>如果想神经网络训练的很深，不要使用S型的激活函数；</li><li>如果使用ReLU，要注意初始化和Learning Rates的设置；</li><li>如果使用ReLU，出现很多神经元死亡的问题，且无法解决，可以尝试使用L-ReLU、P-ReLU等ReLU的变体；</li><li>最好不要使用Sigmoid，可以尝试使用Tanh;</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;激活函数简介&quot;&gt;&lt;a href=&quot;#激活函数简介&quot; class=&quot;headerlink&quot; title=&quot;激活函数简介&quot;&gt;&lt;/a&gt;激活函数简介&lt;/h2&gt;&lt;p&gt;神经网络是目前比较流行深度学习的基础，神经网络模型模拟人脑的神经元。人脑神经元接收一定的信号，对接收的信号进行一定的处理，并将处理后的结果传递到其他的神经元，数以亿计的神经元组成了人体复杂的结构。在神经网络的数学模型中，神经元节点，将输入进行加权求和，加权求和后再经过一个函数进行变换，然后输出。这个函数就是激活函数，神经元节点的激活函数定义了对神经元输入的映射关系。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://ilewseu.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 变量管理</title>
    <link href="https://ilewseu.github.io/2018/03/11/Tensorflow%E5%8F%98%E9%87%8F%E7%AE%A1%E7%90%86/"/>
    <id>https://ilewseu.github.io/2018/03/11/Tensorflow变量管理/</id>
    <published>2018-03-11T02:47:20.000Z</published>
    <updated>2018-03-11T05:37:11.102Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>摘自：《TensorFlow实战Google深度学框架》一书，5.3节。</p></blockquote><p>Tensorflow提供了通过变量名称来创建或者获取一个变量的机制。通过这个机制，在不同的函数中可以直接通过变量的名字来使用变量，而不需要将变量通过参数的形式到处传递。TensorFlow中通过变量名获取变量的机制主要是通过tf.get_variable和tf.variable_scope函数实现的。下面将分别介绍如何使用这两个函数。<br><a id="more"></a><br>通过tf.Variable()函数可以创建一个变量。除了tf.Variable()函数，TensorFlow还提供了tf.get_variable函数来创建或者获取变量。当tf.get_variable用于创建变量时，它和tf.Variable()的功能是基本等价的。下面的代码给出通过这两个函数创建同一个变量的示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 下面这两个定义是等价的</span></div><div class="line">v = tf.get_variable(<span class="string">"v"</span>,shape=[<span class="number">1</span>],initializer=tf.constant_initializer(<span class="number">1.0</span>))</div><div class="line">v = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]),name=<span class="string">'v'</span>)</div></pre></td></tr></table></figure></p><p>从上面的代码可以看出，通过tf.Variable和tf.get_variable函数创建变量的过程基本上是一样的。tf.get_variable函数调用时提供的维度(shape)信息以及初始化方法(initializer)的参数和tf.Variable函数调用时提供的初始化过程中的参数也类似。TensorFlow中提供的initializer函数和随机数以及常量生成函数大部分是一一对应的。比如，在上面的样例程序中使用的常数初始化函数tf.constant_initializer和常数生成的函数tf.constant功能上是一致的。Tensorflow提供了7种不同的初始化函数，如下表所示：</p><table><thead><tr><th>初始化函数</th><th>功能</th><th>主要参数</th></tr></thead><tbody><tr><td>tf.constant_initializer</td><td>将变量初始化为给定常量</td><td>常量的取值</td></tr><tr><td>tf.random_normal_initializer</td><td>将变量初始化为满足正态分布的随机值</td><td>正态分布的均值和标准差</td></tr><tr><td>tf.truncated_normal_initializer</td><td>将变量初始化为满足正态分布的随机值，但如果随机出来的值<br>偏离平均值超过2个标准差，那么这个数将会被重新随机</td><td>正态分布的均值和标准差</td></tr><tr><td>tf.random_uniform_initializer</td><td>将变量初始化为满足均匀分布的随机值</td><td>最大值、最小值</td></tr><tr><td>tf.uniform_unit_scaling_initializer</td><td>将变量初始化为满足均匀分布但不影响输出数量级的随机值</td><td>factor(产生随机数时<br>乘以的系数)</td></tr><tr><td>tf.zeros_initializer</td><td>将被变量设置为0</td><td>变量维度</td></tr><tr><td>tf.ones_initializer</td><td>将变量设置为1</td><td>变量的维度</td></tr></tbody></table><p><strong>tf.get_variable函数与tf.Variable函数最大的区别在于指定变量名称的参数</strong>。对于tf.Variable函数， 变量名称是一个可选的参数，通过name=“v”的形式给出。但是对于tf.get_variable函数，变量名称是一个必填的参数。tf.get_variable会根据这个名字去创建或者获取变量。在上面的示例程序中，tf.get_variable首先会试图创建一个名字为v的参数，如果创建失败（比如已经有同名的参数），那么这个程序会报错。这是为了避免无意识的变量复用造成的错误。比如在定义神经网络参数时，第一层网络的权重已经叫weights了，如果创建第二层的神经网络时，如果参数名仍然叫weights，那么就会触发变量重用的错误。否则两层神经网络公用一个权重会出现一些比较难以发现的错误。<strong>如果需要通过tf.get_variable获取一个已经创建的变量，需要通过tf.variable_scope函数来生成一个上下文管理器，并明确指定在这个上下文管理器中，tf.get_variable将直接获取已经生成的变量。</strong>下面给出一段代码说明如何通过tf.variable_scope函数来控制tf.get_variable函数获取已经创建过的变量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 在名字为foo的命名空间内创建名字为v的变量</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</div><div class="line"></div><div class="line"><span class="comment"># 因为在命名空间foo中已经存在名为v的变量，所以下面的代码将会报错</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="comment"># 在生成上下文管理器时，将参数reuse设置为True。这样tf.get_vaiable函数将直接获取</span></div><div class="line"><span class="comment"># 已经声明的变量。</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>,reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line">    <span class="keyword">print</span> v==v1 <span class="comment">#输出为True，v和v1代表的是相同的变量</span></div><div class="line"></div><div class="line"><span class="comment"># 将参数reuse设置为True时，tf.variable_scope将只能获取已经创建过的变量。因为在命名</span></div><div class="line"><span class="comment"># 空间bar中还没有创建变量v，所以下面的代码将会报错</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>,reuse=<span class="keyword">True</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div></pre></td></tr></table></figure></p><p>上面的样例简单地说明了通过tf.variable_scope函数可以控制tf.get_variable函数的语义。<strong>当tf.variable_scope函数使用参数reuse=True生成上下文管理器时，这个上下文管理器内所有的tf.get_variable函数会直接获取已经创建的变量。如果变量没有被创建，则tf.get_variable将会报错；相反如果tf.variable_scope函数使用参数reuse=None或者reuse=False创建上下文管理器，tf.get_variable操作将创建新的变量。</strong>如果同名变量已经存在，则tf.get_variable函数将会报错。TensorFlow中tf.variable_scope函数是可以嵌套的。下面的程序说明了当tf.variable_scope函数嵌套时，reuse参数的取值时如何确定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"root"</span>):</div><div class="line">    <span class="comment"># 可以通过tf.get_variable_scope().reuse函数来获取当前上下文管理器中reuse参数的取值</span></div><div class="line">    <span class="keyword">print</span> tf.get_variable_scope().reuse   <span class="comment">#输出False，即最外层reuse是False</span></div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>,reuse=<span class="keyword">True</span>): <span class="comment"># 新建一个嵌套的上下文管理器，</span></div><div class="line">                                              <span class="comment"># 并指定reuse为True</span></div><div class="line">        <span class="keyword">print</span> tf.get_variable_scope().reuse   <span class="comment"># 输出为True</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):        <span class="comment"># 新建一个嵌套的上下文管理器</span></div><div class="line">                                              <span class="comment"># 但不指定reuse的取值，和外层的保持一致</span></div><div class="line">            <span class="keyword">print</span> tf.get_variable_scope().reuse <span class="comment"># 输出为True</span></div><div class="line">    <span class="keyword">print</span> tf.get_variable_scope().reuse      <span class="comment"># 输出False，退出reuse设置为True</span></div><div class="line">                                             <span class="comment"># 的上下文之后，reuse的值又回到了False</span></div></pre></td></tr></table></figure><p>tf.variable_scope函数生成的上下文管理器也会创建一个TensorFlow中的命名空间，在命名空间内创建的变量名称都会带上这个命名空间名作为前缀。所以，tf.variable_scope函数除了控制tf.get_variable执行的功能之外，这个函数也提供了一个管理变量命名空间的方式。下面的代码显示如何通过tf.variable_scope来管理变量的名称。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">v1 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line"><span class="keyword">print</span> v1.name    <span class="comment"># 输出v:0,"v"为变量名称，“：0”表示这个变量时生成变量这个运算的第一个结果</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v2 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line">    <span class="keyword">print</span> v2.name  <span class="comment"># 输出为foo/v:0。在tf.variable_scope中创建的变量，名称前面会</span></div><div class="line">                   <span class="comment"># 加入命名空间的名称，通过/来分隔命名空间的名称和变量的名称。</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">        v3 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</div><div class="line">        <span class="keyword">print</span> v3.name <span class="comment"># 输出为foo/bar/v:0。命名空间可以嵌套，同时变量的名称也会</span></div><div class="line">                      <span class="comment"># 加入所有命名空间的名称作为前缀。</span></div><div class="line">    v4 = tf.get_variable(<span class="string">"v1"</span>,[<span class="number">1</span>])</div><div class="line">    <span class="keyword">print</span> v4.name  <span class="comment"># 输出foo/v1:0。当命名空间退出之后，变量名称也就不会再被加入其前缀了。</span></div><div class="line"></div><div class="line"><span class="comment"># 创建一个名称为空的命名空间，并设置为reuse=True</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">""</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    v5 = tf.get_variable(<span class="string">"foo/bar/v"</span>,[<span class="number">1</span>]) <span class="comment"># 可以直接通过带命名空间名称的变量名来</span></div><div class="line">                                          <span class="comment"># 获取其他命名空间下的变量</span></div><div class="line">    <span class="keyword">print</span> v5 == v3 <span class="comment"># 输出为True</span></div><div class="line">    v6 = tf.get_variable(<span class="string">"foo/v1"</span>,[<span class="number">1</span>])</div><div class="line">    <span class="keyword">print</span> v6 == v4 <span class="comment"># 输出为True</span></div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;摘自：《TensorFlow实战Google深度学框架》一书，5.3节。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tensorflow提供了通过变量名称来创建或者获取一个变量的机制。通过这个机制，在不同的函数中可以直接通过变量的名字来使用变量，而不需要将变量通过参数的形式到处传递。TensorFlow中通过变量名获取变量的机制主要是通过tf.get_variable和tf.variable_scope函数实现的。下面将分别介绍如何使用这两个函数。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="TensorFlow" scheme="https://ilewseu.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Attention Model 注意力机制</title>
    <link href="https://ilewseu.github.io/2018/02/12/Attention%20Model%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>https://ilewseu.github.io/2018/02/12/Attention Model 注意力机制/</id>
    <published>2018-02-12T06:23:20.000Z</published>
    <updated>2018-02-13T15:12:11.778Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文是对:<a href="https://blog.heuritech.com/2016/01/20/attention-mechanism/" target="_blank" rel="external">https://blog.heuritech.com/2016/01/20/attention-mechanism/</a> 的翻译。这篇文章对Attention Model原理进行了一个比较清晰的阐述，所以记录一下。由于本人英语能力有限，翻译不周的地方，还请见谅。<br><a id="more"></a><br>在2015年，随着DeepLearning和AI的发展，神经网络中的注意力机制引起了许多研究者的兴趣。这篇博文的目的是从一个高的层次上对注意力机制进行解释，以及详细介绍attention的一些计算步骤。如果你要更多关于attention的公式或例子，文后的参考文献提供了一下，特别注意Cho et al[3]这篇文章。不幸的是，这些模型往往你自己实现不了，仅仅有一些开源的实现。</p></blockquote><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>神经科学和计算神经科学中的neural processes已经广泛研究了注意力机制[1,2]。视觉注意力机制是一个特别值得研究的方向：许多动物专注于视觉输入的特定部分，去计算适当的反映。这个原理对神经计算有很大的影响，因为我们需要选择最相关的信息，而不是使用所有可用的信息，所有可用信息中有很大一部分与计算神经元反映无关。一个类似于视觉专注于输入的特定部分，也就是注意力机制已经用于深度学习、语音识别、翻译、推理以及视觉识别。</p><h2 id="Attention-for-Image-Captioning"><a href="#Attention-for-Image-Captioning" class="headerlink" title="Attention for Image Captioning"></a>Attention for Image Captioning</h2><p>我们通过介绍一个例子，去解释注意力机制。这个任务是我们想实现给图片加标题：对于给定的图片，根据图片中的内容给图片配上标题(说明/描述)。一个典型的image captioning系统会对图片进行编码，使用预训练的卷积神经网络产生一个隐状态h。然后，可以使用RNN对这个隐状态h进行解码，生成标题。这种方法已经被不少团队采用，包括[11]，如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/27GEF0IlE8.png?imageslim" alt="mark"></p><p>这种方法的问题是：当模型尝试去产生标题的下一个词时，这个词通常是描述图片的一部分。使用整张图片的表示h去调节每个词的生成，不能有效地为图像的不同部分产生不同的单词。这正是注意力机制有用的地方。<br>使用注意力机制，图片首先被划分成n个部分，然后我们使用CNN计算图像每个部分的表示$h_1,…,h_n$，也就是对n个部分的图像进行编码。当使用RNN产生一个新的词时，注意力机制使得系统只注意图片中相关的几个部分，所以解码仅仅使用了图片的特定的几个部分。如下图所示，我们可以看到标题的每个词都是用图像(白色部分)的一部分产生的。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/cIDeG9Jgl9.png?imageslim" alt="mark"></p><p>更多的例子如下图所示，我们可看到图片相关的部分产生标题中划线的单词。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/IiaGamlfj5.png?imageslim" alt="mark"></p><p>现在我们要解释注意力机制是怎么工作的，文献[3]详细介绍了基于注意力机制的Encoder-Decoder Network的实现。</p><h2 id="What-is-an-attention-model"><a href="#What-is-an-attention-model" class="headerlink" title="What is an attention model"></a>What is an attention model</h2><p>在一般情况下，什么是注意力机制？</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/K49GLi96C2.png?imageslim" alt="mark"></p><p>一个attention model通常包含n个参数$y_1,..,y_n$(在前面的例子中，$y_i$可以是$h_i$) ,和一个上下文c。它返回一个z向量，这个向量可以看成是对$y_i$的总结，关注与上下文c相关联的信息。更正式地说是，它返回的$y_i$的加权算术平均值，并且权重是根据$y_i$与给定上下文c的相关性来选择的。</p><p>在上面的例子中，上下文是刚开始产生的句子，$y_i$是图像的每个部分的表示($h_i$)，输出是对图像进行一定的过滤（例如：忽略图像中的某些部分）后的表示，通过过滤将当前生成的单词的重点放在感兴趣的部分。<br>注意力机制有一个有趣的特征：计算出的权重是可访问的并且可以被绘制出来。这正是我们之前展示的图片，如果权重越高，则对应部分的像素越白。</p><p>但是，这个黑盒子做了什么？下图能够清晰的表示Attention Model的原理：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/mmcbj2eK5C.png?imageslim" alt="mark"></p><p>可能这个网络图看起来比较复杂，我们一步一步来解释这个图。<br>首先，我们能够看出 一个输入c，是上下文，$y_i$是我们正在研究的“数据的一部分”。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/591aa4eId3.png?imageslim" alt="mark"></p><p>然后，网络计算$m_1,…,m_n$通过一个tanh层。这意味着我们计算$y_i$和c的一个”聚合”。重要的一点是，每个$m_i$的计算都是在不考虑其他$y_j,j \neq i$的情况下计算出来的。它们是独立计算出来的。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/fejdk9cA73.png?imageslim" alt="mark"></p><p>$$m_i = tanh(W_{cm}c+W_{ym}y_i)$$</p><p>然后，我们计算每个weight使用softmax。softmax，就像他的名字一样，它的行为和argmax比较像，但是稍有不同。$argmax(x_1,…,x_n)=(0,..,0,1,0,..,0)$,在输出中只有一个1，告诉我们那个是最大值。但是，softmax的定义为：$softmax(x_1,…,x_n)=(\frac {e^{x_i}}{\sum_j e^{x_j}})_i$。如果其中有一个$x_i$比其他的都大，$softmax(x_1,…,x_n)将会非常接近argmax(x_1,…,x_n)$。<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/g3JgH62d65.png?imageslim" alt="mark"></p><p>$$s_i 正比于 exp(w_m, m_i)\\\ \sum_i s_i=1$$<br>这里的$s_i$是通过softmax进行归一化后的值。因此，softmax可以被认为是“相关性”最大值的变量，根据上下文。<br>输出$z$是所有$y_i$的算术平均，每个权重值表示$y_i,..,y_n$和上下文c的相关性。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/43eLlm4gDb.png?imageslim" alt="mark"></p><p>$$z = \sum_i s_iy_i$$</p><h2 id="An-other-computation-of-“relevance”"><a href="#An-other-computation-of-“relevance”" class="headerlink" title="An other computation of “relevance”"></a>An other computation of “relevance”</h2><p>上面介绍的attention model是可以进行修改的。首先，tanh层可以被其他网络或函数代替。重要的是这个网络或函数可以综合c和$y_i$。比如可以只使用点乘操作计算c和$y_i$的内积，如下图所示：</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/8b2LBfc266.png?imageslim" alt="mark"></p><p>这个版本的比较容易理解。上面介绍的Attention是softly-choosing与上下文最相关的变量（$y_i$）。据我们所知，这两种系统似乎都能产生类似的结果。<br>另外一个比较重要的改进是hard attention。</p><h2 id="Soft-Attention-and-Hard-Attention"><a href="#Soft-Attention-and-Hard-Attention" class="headerlink" title="Soft Attention and Hard Attention"></a>Soft Attention and Hard Attention</h2><p>上面我们描述的机制称为Soft Attention，因为它是一个完全可微的确定性机制，可以插入到现有的系统中，梯度通过注意力机制进行传播，同时它们通过网络的其余部分进行传播。<br>Hard Attention是一个随机过程：系统不使用所有隐藏状态作为解码的输入，而是以概率$s_i$对隐藏状态$y_i$进行采样。为了进行梯度传播，使用蒙特卡洛方法进行抽样估计梯度。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/5c1A061BL6.png?imageslim" alt="mark"></p><p>这两个系统都有自己的优缺点，但是研究的趋势是集中于Soft Attention，因为梯度可以直接计算，并不是通过随机过程来估计的。</p><h2 id="Return-to-the-image-captioning"><a href="#Return-to-the-image-captioning" class="headerlink" title="Return to the image captioning"></a>Return to the image captioning</h2><p>现在，我们来理解一下image captioning 系统是怎样工作的。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/E6AGi93bJ9.png?imageslim" alt="mark"></p><p>从上面的图我们可以看到image captioning的典型模型，但是添加了一个新的关于attention model的层。当我们想要预测标题的下一个单词时，发生了什么？如果我们要预测第i个词，LSTM的隐藏状态是$h_i$。我们选择图像相关的部分通过把$h_i$作为上下文。然后，attention model的输出是$z_i$，这是被过滤的图像的表示，只有图像的相关部分被保留，用作LSTM的输入。然后，LSTM预测一个下一个词，并返回一个隐藏状态$h_{i+1}$。</p><h2 id="Learning-to-Align-in-Machine-Translation"><a href="#Learning-to-Align-in-Machine-Translation" class="headerlink" title="Learning to Align in Machine Translation"></a>Learning to Align in Machine Translation</h2><p>Bahdanau, et al[5]中的工作提出了一个神经翻译模型将句子从一种语言翻译成另一种语言，并引入注意力机制。在解释注意力机制之前，vanillan神经网络翻译模型使用了编码-解码(Encoder-Decoder)框架。编码器使用循环神经网络(RNN,通常GRU或LSTM)将用英语表示的句子进行编码，并产生隐藏状态h。这个隐藏状态h用于解码器RNN产生正确的法语的句子。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/JL6JJK2g25.png?imageslim" alt="mark"></p><p>编码器不产生与整个句子对应的单个隐藏状态，而是产生与一个词对应的隐藏状态$h_j$。每当解码器RNN产生一个单词时，取决于每个隐藏状态作为输入的贡献，通常是一个单独的参数（参见下图）。这个贡献参数使用Softmax进行计算：这意味着attention weights $a_j$在$\sum a_j=1$的约束下进行计算并且所有的隐藏状态$h_j$给解码器贡献的权重为$a_j$。<br>在我们的例子中，注意力机制是完全可微的，不需要额外的监督，它只是添加在现有的编码-解码框架的顶部。<br>这个过程可以看做是对齐，因为网络通常在每次生成输出词时都会学习集中于单个输入词。这就意味着大多数的注意力权重是0(黑)，而一个单一的被激活(白色)。下面的图像显示了翻译过程中的注意权重，它揭示了对齐方式，并使解释网络所学的内容成为可能（这通常是RNNs的问题！）。</p><p><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/fbb3Gee98c.png?imageslim" alt="mark"></p><h2 id="Attention-without-Recurrent-Neural-Networks"><a href="#Attention-without-Recurrent-Neural-Networks" class="headerlink" title="Attention without Recurrent Neural Networks"></a>Attention without Recurrent Neural Networks</h2><p>到现在为止，我们仅介绍了注意力机制在编码-解码框架下的工作。但是，当输入的顺序无关紧要时，可以考虑独立的隐藏状态$h_j$。这个在Raffel et Al[10]中进行了介绍，这里attention model是一个前向全连接的网络。同样的应用是Mermory Networks[6]（参见下一节）。</p><h2 id="From-Attention-to-Memory-Addressing"><a href="#From-Attention-to-Memory-Addressing" class="headerlink" title="From Attention to Memory Addressing"></a>From Attention to Memory Addressing</h2><p>NIPS 2015会议上提出了一个非常有趣的工作叫做 RAM for Reasoning、Attention and Memory。它的工作包含Attention，但是也包括Memory Networks[6],Neural Turing Machines[7]或 Differentiable Stack RNNS[8]以及其他的工作。这些模型都有共同之处，它们使用一种外部存储器的形式，这种存储器可以被读取（最终写入）。<br>比较和解释这些模型是超出了这个本文的范围, 但注意机制和记忆之间的联系是有趣的。例如，在Memory Networks中，我们认为外部存储器-一组事实或句子$x_i$和一个输入q。网络学习对记忆的寻址，这意味着选择哪个事实$x_i$去关注来产生答案。这对应了一个attention model在外部存储器上。In Memory Networks, the only difference is that the soft selection of the facts (blue Embedding A in the image below) is decorrelated from the weighted sum of the embeddings of the facts (pink embedding C in the image).（PS：实在读不懂了）。在Neural Turing Machine中，使用了一个Soft Attention机制。这些模型将会是下个博文讨论的对象。<br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180212/cHmIh83088.png?imageslim" alt="mark"></p><h2 id="Final-Word"><a href="#Final-Word" class="headerlink" title="Final Word"></a>Final Word</h2><p>注意力机制和其他完全可微寻址记忆系统是目前许多研究人员广泛研究而的热点。尽管它们仍然年轻, 在现实世界系统中没有实现, 但它们表明,它们可以处理在编码-解码框架下以前遗留的许多问题，它们可以被用来击败最先进的系统。<br>在Heuritech，几个月前，我们对注意力机制产生了兴趣，组织了一个小组，去实现带注意力机制的编码-解码器。虽然我们还没有在生产中使用注意机制,但我们设想它在高级文本理解中有一个重要的作用, 在某些推理是必要的, 以类似的方式，Hermann et al[9]中的工作和此类似。<br>在另一个单独的博客帖子中,我将详细阐述我们在研讨会上所学到的内容以及在RAM研讨会上提出的最新进展。<br>Léonard Blier et Charles Ollion</p><h2 id="Acknowledgments"><a href="#Acknowledgments" class="headerlink" title="Acknowledgments"></a>Acknowledgments</h2><p>在此感谢Mickael Eickenberg 和 Olivier Grisel的有益的讨论。</p><h2 id="Bibliography"><a href="#Bibliography" class="headerlink" title="Bibliography"></a>Bibliography</h2><p>[1] Itti, Laurent, Christof Koch, and Ernst Niebur. « A model of saliency-based visual attention for rapid scene analysis. » IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 11 (1998): 1254-1259.</p><p>[2] Desimone, Robert, and John Duncan. « Neural mechanisms of selective visual attention. » Annual review of neuroscience 18.1 (1995): 193-222.</p><p>[3] Cho, Kyunghyun, Aaron Courville, and Yoshua Bengio. « Describing Multimedia Content using Attention-based Encoder–Decoder Networks. » arXiv preprint arXiv:1507.01053 (2015)</p><p>[4] Xu, Kelvin, et al. « Show, attend and tell: Neural image caption generation with visual attention. » arXiv preprint arXiv:1502.03044 (2015).</p><p>[5] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. « Neural machine translation by jointly learning to align and translate. » arXiv preprint arXiv:1409.0473 (2014).</p><p>[6] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. « End-to-end memory networks. » Advances in Neural Information Processing Systems. (2015).</p><p>[7] Graves, Alex, Greg Wayne, and Ivo Danihelka. « Neural Turing Machines. » arXiv preprint arXiv:1410.5401 (2014).</p><p>[8] Joulin, Armand, and Tomas Mikolov. « Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets. » arXiv preprint arXiv:1503.01007 (2015).</p><p>[9] Hermann, Karl Moritz, et al. « Teaching machines to read and comprehend. » Advances in Neural Information Processing Systems. 2015.</p><p>[10] Raffel, Colin, and Daniel PW Ellis. « Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems. » arXiv preprint arXiv:1512.08756 (2015).</p><p>[11] Vinyals, Oriol, et al. « Show and tell: A neural image caption generator. » arXiv preprint arXiv:1411.4555 (2014).</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是对:&lt;a href=&quot;https://blog.heuritech.com/2016/01/20/attention-mechanism/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://blog.heuritech.com/2016/01/20/attention-mechanism/&lt;/a&gt; 的翻译。这篇文章对Attention Model原理进行了一个比较清晰的阐述，所以记录一下。由于本人英语能力有限，翻译不周的地方，还请见谅。&lt;br&gt;
    
    </summary>
    
      <category term="翻译文章" scheme="https://ilewseu.github.io/categories/%E7%BF%BB%E8%AF%91%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="Attention Model" scheme="https://ilewseu.github.io/tags/Attention-Model/"/>
    
  </entry>
  
  <entry>
    <title>GRU神经网络</title>
    <link href="https://ilewseu.github.io/2018/01/20/GRU%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://ilewseu.github.io/2018/01/20/GRU神经网络/</id>
    <published>2018-01-20T05:58:20.000Z</published>
    <updated>2018-01-20T15:38:11.786Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GRU神经网络"><a href="#GRU神经网络" class="headerlink" title="GRU神经网络"></a>GRU神经网络</h2><p>GRU(Gated Recurrent Unit)是LSTM的一种变体，它对LSTM做了很多简化，同时却保持着和LSTM几乎相同的效果。因此，GRU最近变得非常流行。下图是GRU的网络架构图。<br><a id="more"></a><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/180113/Ja3I15iABg.jpg?imageslim" alt="mark"><br>GRU对LSTM做了两个大得改动：</p><ul><li>将<strong>输入门、遗忘门和输出门</strong>改变为两个门：<strong>更新门（Update Gate)$z_t$</strong>和<strong>重置门(Reset Gate)$r_t$</strong>。</li><li><strong>将单元状态与输出合并为一个状态：h</strong>。</li></ul><p>根据上图的架构图可以得出GRU的前向计算公式：<br>$$\begin{aligned}<br>&amp;r_t = \sigma(W_r \cdot [h_{t-1},x_t])\\\\<br>&amp;z_t =\sigma(W_z \cdot [h_{t-1},x_t])\\\\<br>&amp;\hat {h_t}=tanh(W_{\hat {h}} \cdot [r_t \bigodot h_{t-1},x_t])\\\\<br>&amp;h_t =(1-z_t)\bigodot h_{t-1}+z_t \bigodot \hat {h_t}\\\\<br>&amp;y_t=\sigma(W_o \cdot h_t)<br>\end{aligned}$$</p><h2 id="GRU的反向传播梯度计算"><a href="#GRU的反向传播梯度计算" class="headerlink" title="GRU的反向传播梯度计算"></a>GRU的反向传播梯度计算</h2><p>GRU的参数更新方式同样是基于沿时间反向传播的算法（BPTT）,为了为了更清晰的推导GRU反向传播梯度计算，对上文的GRU前向计算公式进行一定的改写，实质上还是一样的，只不过是将参数分开写而已。具体如下：假设，对于t时刻，GRU的输出为$\hat {y_t}$，输入为$x_t$，前一时刻的状态为$s_{t-1}$，则可以得出如下的前向计算的公式：<br>$$\begin{aligned}<br>    &amp;z_t = \sigma(U_zx_t+W_zs_{t-1}+b_z)\\\\<br>    &amp;r_t = \sigma(U_r x_t+W_rs_{t-1}+b_r)\\\\<br>    &amp;h_t = tanh(U_hx_t+W_h(s_{t-1}\bigodot  r_t)+b_h)\\\\<br>    &amp;s_t = (1-z_t)\bigodot  h_t + z_t \bigodot  s_{t-1}\\\\<br>    &amp;\hat {y_t}=softmax(Vs_t+b_V)<br>\end{aligned}<br>$$<br>其中，$\bigodot$表示向量的点乘；$z_t$表示更新门；$r_t$表示重置门；$\hat {y_t}$表示t时刻的输出。<br>如果采用交叉熵损失函数，那么在t时刻的损失为$L_t$:$$<br>L_t =sumOfAllElements(-y_t\bigodot log(\hat {y_t}))<br>$$<br>为了训练GRU，需要把所有时刻的损失加在一起，并最小化损失$L=\sum_{t=1}^T L_t$:<br>$$argmin_{\Theta}L$$<br>其中，$\Theta={U_z,U_r,U_c,W_z,W_r,W_c,b_z,b_r,b_c,V,b_V}$。</p><p>这是一个非凸优化问题，通常采用随机梯度下降法去解决问题。因此，需要计算$\partial L/ \partial U_z,\partial L/ \partial U_r,\partial L/ \partial U_h,\partial L/ \partial W_z,\partial L/ \partial W_r,\partial L/ \partial W_h,\partial L/ \partial b_z,\partial L/ \partial b_r,\partial L/ \partial b_h,\partial L/ \partial V,\partial L/ \partial b_v$。计算上面的梯度，最好的方式是利用链式法则从输出到输入一步一步去计算，为了更好得看清输入、中间值以及输出之间的关系，画了一张GRU的计算图，如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180120/bHHG1j84eF.jpg?imageslim" alt="mark"><br></div><br>根据计算图，利用链式法则计算梯度，需要从上至下沿着边进行计算。如果节点X有多条出边和目标节点T相连，如果要计算$\partial T / \partial X$，需要分别计算每条边对X的梯度，并将梯度进行相加。</p><p>以计算$\frac {\partial L}{\partial U_z}$为例，其他的计算方式和其相似。因为$L=\sum_{t=1}^T L_t$，所以，$\frac {\partial L}{\partial U_z}=\sum_{t=1}^T \frac {\partial L_t}{\partial U_z}$，因此，可以先计算$\frac {\partial L_t}{\partial U_z}$，然后将不同时刻结果加起来就可以。</p><p>根据链式法则:$$\frac {\partial L_t}{\partial U_z} = \frac {\partial L_t}{\partial s_t} \frac{\partial s_t}{\partial U_z}    (公式1)<br>$$<br>公式1右边的第一个式子的计算如下：$$\frac {\partial L_t}{\partial s_t}=V(\hat {y_t}-y_t)    (公式2)<br>$$<br>对于$\frac {\partial z}{\partial U_z}$，一些人可能会直接进行如下的求导计算<br>$$<br>\frac {\overline{\partial s_t}}{\partial U_z}=((s_{t-1}-h_t)\bigodot z_t \bigodot (1-z_t))x_t^T     (公式3)<br>$$<br>在$s_t$的计算公式里有$1-z$和$z\bigodot s_{t-1}$两个公式都会影响到$\frac {\partial s_t}{\partial U_z}$。正确的方法是分别计算每条边的偏导数，并将它们相加，因此，需要引入$\frac {\partial s_t}{\partial s_{t-1}}$。但是，公式3只计算了部分的梯度，因此用$\frac {\overline{\partial s_t}}{\partial U_z}$表示。</p><p>因为$s_{t-1}$同样依赖于$U_z$，因此我们不能把$s_{t-1}$作为一个常量处理。$s_{t-1}$同样会受到$s_i,i=1,…,t-2$的影响，因此，需要将公式1进行扩展，如下：<br>$$\begin{aligned}<br>    \frac {\partial L_t}{\partial U_z} = &amp;\frac {\partial L_t}{\partial s_t} \frac{\partial s_t}{\partial U_z}\\\\<br>    =&amp;\frac {\partial L_t}{\partial s_t}\sum_{i=1}^t(\frac{\partial s_t}{\partial s_i}\frac{\overline {\partial s_i}}{\partial U_z})\\\\<br>    =&amp;\frac {\partial L_t}{\partial s_t}\sum_{i=1}^t ((\prod_{j=i}^{t-1} \frac {\partial s_{j+1}}{\partial s_j})\frac{\overline {\partial s_i}}{\partial U_z})<br>\end{aligned}     (公式4)<br>$$<br>其中，$\frac{\overline {\partial s_i}}{\partial U_z}$是$s_i$对$U_z$的梯度，其计算公式如公式3所示。<br>$\frac {\partial s_t}{\partial s_{t-1}}$的计算和$\frac {\partial s_t}{\partial z}$的计算相似。因为从$s_{t-1}$到$s_t$有四条边，直接或间接相连，通过$z_t,r_t和h_t$，因此，需要计算这四条边上的梯度，然后进行相加，计算公式如下：$$\begin{aligned}<br>\frac {\partial s_t}{\partial s_{t-1}}=&amp;\frac {\partial s_t}{\partial h_t} \frac {\partial h_t}{\partial s_{t-1}}+\frac{\partial s_t}{\partial z_t}\frac{\partial z_t}{\partial s_{t-1}} + \frac {\overline {\partial s_t}}{\partial s_{t-1}}\\\\<br>=&amp;\frac {\partial s_t}{\partial h_t}(\frac {\partial h_t}{\partial r_t}\frac {\partial r_t}{\partial s_{t-1}}+\frac {\overline {\partial h_t}}{\partial s_{t-1}}) + \frac {\partial s_t}{\partial z_t}\frac{\partial z_t}{\partial s_{t-1}}+\frac {\overline {\partial s_t}}{\partial s_{t-1}}<br>\end{aligned}    (公式5)$$<br>其中，$\frac {\overline {\partial s_t}}{\partial s_{t-1}}$是对$s_t$关于$s_{t-1}$的导数，并将$h_t,z_t$看做常量。同样，$\frac {\overline {\partial h_t}}{\partial s_{t-1}}$是$h_t$关于$s_{t-1}$的导数，将$r_t$看做常量。最终，可以得到:<br>$$ \frac {\partial s_t}{\partial s_{t-1}}=(1-z_t)(W_r^T((W_h^T(1-h\bigodot h))\bigodot s_{t-1}\bigodot r \bigodot (1-r))+((W_h^T(1-h \bigodot h))\bigodot r_t))+\\\\W_z^T((s_{t-1}-h_t)\bigodot z_t \bigodot (1-z_t))+z     (公式6)$$<br>到此为止，$\frac {\partial L}{\partial U_z}$的计算已经完成，而其余的参数的计算和它的计算方式类似，沿着计算图一步一步计算，这里就不一一计算了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>A Tutorial On Backward Propagation Through Time (BPTT) In The Gated Recurrent Unit (GRU) RNN</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;GRU神经网络&quot;&gt;&lt;a href=&quot;#GRU神经网络&quot; class=&quot;headerlink&quot; title=&quot;GRU神经网络&quot;&gt;&lt;/a&gt;GRU神经网络&lt;/h2&gt;&lt;p&gt;GRU(Gated Recurrent Unit)是LSTM的一种变体，它对LSTM做了很多简化，同时却保持着和LSTM几乎相同的效果。因此，GRU最近变得非常流行。下图是GRU的网络架构图。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="GRU" scheme="https://ilewseu.github.io/tags/GRU/"/>
    
  </entry>
  
  <entry>
    <title>LSTM参数更新推导</title>
    <link href="https://ilewseu.github.io/2018/01/06/LSTM%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E6%8E%A8%E5%AF%BC/"/>
    <id>https://ilewseu.github.io/2018/01/06/LSTM参数更新推导/</id>
    <published>2018-01-06T07:45:20.000Z</published>
    <updated>2018-01-20T15:22:34.868Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转自：<a href="https://zybuluo.com/hanbingtao/note/581764，对其进行一定的整理。" target="_blank" rel="external">https://zybuluo.com/hanbingtao/note/581764，对其进行一定的整理。</a></p></blockquote><h2 id="LSTM前向计算"><a href="#LSTM前向计算" class="headerlink" title="LSTM前向计算"></a>LSTM前向计算</h2><a id="more"></a><p>在<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>一文中，介绍了LSTM的基本原理。LSTM网络使用了门(gete)的概念，门实际上就是一层全连接，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，b是偏置项，那么门可以表示为：$$<br>g(x) =\sigma(Wx+b)<br>$$<br>门的使用，就是用门的输出向量按元素乘以要控制的向量。因为门的输出是0到1之间的实数向量。所以，当门输出为0时，任何向量与之相乘都会得到0向量，这就相当于不能通过；当输出为1时，任何向量与之相乘都不会有任何改变，相当都通过。因为$\sigma$函数的值域是(0,1)，所以门的状态都是半开半闭的。</p><p>典型的LSTM的网络架构图如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180106/49m8miGdIC.jpg?imageslim" alt="mark"><br></div><br>相比RNN网络，LSTM新增加状态C，称为单元状态(cell state)，如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/180106/adGab6A7GF.png?imageslim" alt="mark"></div></p><pre><code>图引自：https://zybuluo.com/hanbingtao/note/581764</code></pre><p><br>从图中可以看出，LSTM的输入有三个：<strong>当前时刻网络的输入$x_t$、上一时刻LSTM的输出值$h_{t-1}$以及上一时刻的单元状态$c_{t-1}$</strong>。LSTM的输出有两个：<strong>当前时刻LSTM输出值$h_t$和当前时刻的单元状态$c_t$</strong>。在这里x、h、c都是向量。</p><p>LSTM中引入了三个门：<strong>遗忘门(forget gate)、输入门(input gate)和输出门(output gate)</strong>。</p><ul><li><strong>遗忘门</strong>：它决定了上一时刻的单元状态$c_{t-1}$有多少保留到当前时刻$c_t$;</li><li><strong>输入门</strong>：它决定了当前时刻网络的输入$x_t$有多少保留到单元状态$c_t$。</li><li><strong>输出门</strong>：来控制单元状态$c_t$有多少输出到LSTM的当前输出值$h_t$。</li></ul><p><strong>遗忘门计算</strong><br>$$f_t = \sigma(W_f\cdot[h_{t-1},x_t]+b_f)        (公式1)$$<br>其中，$W_f$是遗忘门的权重矩阵，$[h_{t-1},x_t]$是表示把两个向量连接成一个更长的向量，$b_f$是遗忘门的偏置项，$\sigma$是sigmoid函数。事实上权重矩阵$W_f$是由两个矩阵拼接而成的，一个是$W_{fh}$，它对应着输入项$h_{t-1}$，一个是$W_{fx}$。$W_f$可以写为：$$<br>[W_f]\begin{bmatrix}h_{t-1}\\\\x_t\end{bmatrix}=\begin{bmatrix}<br>    W_{fh}&amp;W_{fx}<br>\end{bmatrix}\begin{bmatrix}h_{t-1}\\\\x_t\end{bmatrix}=W_{fh}h_{t-1}+W_{fx}x_t<br>$$</p><p><strong>输出门计算</strong><br>$$i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)    (公式2)$$</p><p><strong>单元状态$\hat{c_t}$</strong><br>$$\hat{c_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)    (公式3)$$<br>单元状态$c_t$,它是有上一时刻的单元状态$c_{t-1}$按元素乘以遗忘门$f_t$，再用当前输入的单元状态$\hat{c_t}$按元素乘以输入门$i_t$，再将两个积加和产生的：<br>$$c_t = f_t \bigodot c_{t-1}+i_t \bigodot \hat{c_t}    (公式4)$$<br>其中$\bigodot$表示按位相乘。这样就把LSTM关于当前的记忆$\hat{c_t}$和长期的记忆$c_{t-1}$组合在一起，形成了新的状态单元$c_t$。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，又可以避免当前无关紧要的内容进入记忆。下面看一下输出门，它控制了长期记忆对当前输出的影响：<br><strong>输出门</strong><br>$$o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)    (公式5)$$</p><p>LSTM最终的输出，是由输出门和单元状态共同确定的：<br>$$h_t = o_t \bigodot tanh(c_t)     (公式6)$$<br>从公式1到公式6就是LSTM的前向计算的全部公式。<br>LSTM前向传播的更新过程如下：</p><ol><li><strong>更新遗忘门的输出：</strong>$$f_t = \sigma(W_f\cdot[h_{t-1},x_t]+b_f)$$</li><li><strong>更新输入门的两部分输出:</strong>$$i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\\\\hat{c_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)$$</li><li><strong>更新细胞状态:</strong>$$c_t = f_t \bigodot c_{t-1}+i_t \bigodot \hat{c_t}$$</li><li><strong>更新输出门状态:</strong>$$o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\\\h_t = o_t \bigodot tanh(c_t)$$</li><li><strong>更新当前序列输出</strong>:$$\hat {y_t}=\sigma(Vh_t+b_y)$$</li></ol><h2 id="LSTM反向传播"><a href="#LSTM反向传播" class="headerlink" title="LSTM反向传播"></a>LSTM反向传播</h2><p>LSTM的训练算法，仍然是反向传播算法，主要有三个步骤：</p><ol><li>前向计算每个神经元的输出值，对于LSTM来说，即$f_t、i_t、c_t、o_t、h_t$ 5组向量。</li><li>反向计算每个神经元的误差项$\delta$。与循环神网络一样，LSTM误差项的反向传播也是包括两个方向：一个是沿时间的反向传播，即从当前时刻t开始，计算每个时刻的误差项；一个是将误差项向上一层传播。</li><li>根据相应的误差项，计算每个权重的梯度。</li></ol><p>LSTM需要学习的参数共有8组，分别是：遗忘门的权重矩阵$W_f$和偏置项$b_f$、输入门的权重矩阵$W_i$和偏置项$b_i$、输出门的权重矩阵$W_o$和偏置项$b_o$以及计算单元状态的权重矩阵$W_c$和偏置项$b_c$。因为权重矩阵的两部分在反向传播中使用不同的公式，因此，权重矩阵$W_f$、$W_i$、$W_c$、$W_o$都将被写为分开的两个矩阵:$W_{fh}$、$W_{fx}$、$W_{ih}$、$W_{ix}$、$W_{ch}$、$W_{cx}$、$W_{oh}$、$W_{ox}$。</p><p>在t时刻，LSTM的输出值为$h_t$，定义t时刻的误差项为$\delta_t$为:$$<br>\delta_t = \frac {\partial E}{\partial h_t}$$</p><p>因为LSTM有四个加权输入，分别为$f_t、i_t、c_t、o_t、h_t$，定义这四个加权输入，以及他们对应的误差项。$$<br>net_{f,t} = W_f[h_{t-1},x_t]+b_f=W_{fh}h_{t-1}+W_{fx}x_t+b_f\\\\<br>net_{i,t} = W_i[h_{t-1},x_t]+b_i=W_{ih}h_{t-1}+W_{ix}x_t+b_i\\\\<br>net_{\hat{c},t} = W_c[h_{t-1},x_t]+b_f=W_{ch}h_{t-1}+W_{cx}x_t+b_c\\\\<br>net_{o,t} = W_o[h_{t-1},x_t]+b_o=W_{oh}h_{t-1}+W_{ox}x_t+b_o\\\\<br>\delta_{f,t}=\frac {\partial E}{\partial net_{f,t}}\\\\<br>\delta_{i,t}=\frac {\partial E}{\partial net_{i,t}}\\\\<br>\delta_{\hat{c},t}=\frac {\partial E}{\partial net_{\hat{c},t}}\\\\<br>\delta_{o,t}=\frac {\partial E}{\partial net_{o,t}}<br>$$</p><h3 id="误差项沿时间反向传递"><a href="#误差项沿时间反向传递" class="headerlink" title="误差项沿时间反向传递"></a>误差项沿时间反向传递</h3><p>沿时间反向传递误差项，就是要计算出t-1时刻的误差项$\delta_{t-1}$。<br>$$<br>\delta_{t-1}^T=\frac {\partial E}{\partial h_{t-1}}=\frac {\partial E}{\partial h_{t}}\frac {\partial h_t}{\partial h_{t-1}}=\delta_t^T\frac  {\partial h_t}{\partial h_{t-1}}<br>$$<br>其中，$\frac {\partial h_t}{\partial h_{t-1}}$是一个Jacobian矩阵。如果隐藏层h的维度是N的话，那么它就是一个N*N的矩阵。为了求出它，先列出$h_t$的计算公式，即公式6和公式4：<br>$$<br>h_t=o_t\bigodot tanh(c_t)\\\\c_t=f_t \bigodot c_{t-1}+i_t \bigodot \hat {c_t}<br>$$<br>可以看出，$o_t、f_t、i_t、\hat{c_t}$都是$h_{t-1}$的函数，那么利用全导数公式可得：$$<br>\delta_t^T\frac {\partial h_t}{\partial h_{t-1}}=\delta_t^T\frac {\partial h_t}{\partial o_t}\frac {\partial o_t}{\partial net_{o,t}}\frac {\partial net_{o,t}}{\partial h_{t-1}}+\delta_t^T\frac {\partial h_t}{\partial c_t}\frac {\partial c_t}{\partial f_t}\frac{\partial f_t}{\partial net_{f,t}}\frac {\partial net_{f,t}}{\partial h_{t-1}}+\delta_t^T\frac{\partial h_t}{\partial c_t}\frac {\partial c_t}{\partial i_t}\frac {\partial i_t}{\partial net_{i,t}}\frac {\partial net_{i,j}}{\partial h_{t-1}}\\\\=\delta_{o,t}^T\frac {\partial net_{o,t}}{\partial h_{t-1}}+\delta_{f,t}^T\frac {\partial net_{f,t}}{\partial h_{t-1}}+\delta_{i,t}^T\frac {\partial net_{i,t}}{\partial h_{t-1}}+\delta_{\hat{c_t},t}^T\frac{\partial net_{\hat{c_t},t}}{\partial h_{t-1}}\\\(公式7)<br>$$<br>下面要把公式7中的每个偏导数都求出来，根据公式6，我们可以求出：$$<br>\frac {\partial h_t}{\partial o_t}= diag[tanh(c_t)]\\\\<br>\frac {\partial h_t}{\partial c_t}= diag[o_t\bigodot(1-tanh(c_t)^2)]<br>$$<br>根据公式4，可以求出：$$<br>\frac {\partial c_t}{\partial f_t} = diag[c_{t-1}]\\\\<br>\frac {\partial c_t}{\partial i_t} = diag[\hat{c_t}]\\\\<br>\frac {\partial c_t}{\partial \hat{c_t}} =diag[i_t]<br>$$<br>因为：$$\begin{aligned}<br>    &amp;o_t = \sigma(net_{o,t})\\\\<br>&amp;net_{o,t} = W_{oh}h_{t-1}+W_{ox}x_t+b_o\\\ <br>&amp;f_t = \sigma(net_{f,t})\\\ <br>&amp;net_{f,t} = W_{ft}h_{t-1}+W_{fx}x_t+b_f\\\ <br>&amp;i_t = \sigma(net_{i,t})\\\\<br>&amp;net_{i,t}=W_{ih}h_{t-1}+W_{ix}x_t+b_i\\\ <br>&amp;\hat{c_t}=tanh(net_{\hat{c},t})\\\\<br>&amp;net_{\hat{c},t}=W_{ch}h_{t-1}+W_{cx}x_t+b_c<br>\end{aligned}<br>$$<br>很容易得出:<br>$$\begin{aligned}<br>&amp;\frac {\partial o_t}{\partial net_{o,t}}= diag[o_t\bigodot(1-o_t)]\\\\<br>&amp;\frac {\partial_{o,t}}{\partial h_{t-1}}=W_{oh}\\\\<br>&amp;\frac {\partial f_t}{\partial net_{f,t}}=diag[f_t\bigodot(1-f_t)]\\\\<br>&amp;\frac {\partial net_{f,t}}{\partial h_{t-1}}=W_{fh}\\\\<br>&amp;\frac {\partial i_t}{\partial net_{i,j}}=diag[i_t\bigodot(1-i_t)]\\\\<br>&amp;\frac {\partial net_{i,t}}{\partial h_{t-1}}=W_{ih}\\\\<br>&amp;\frac {\partial \hat{c_t}}{\partial net_{\hat{c},t}}=diag[1-\hat{c_t}^2]\\\\<br>&amp;\frac {\partial net_{\hat{c},t}}{\partial h_{t-1}}=W_{ch}<br>\end{aligned}<br>$$<br>将上述偏导数带入公式7，可以得到：$$<br>\delta_{t-1} = \delta_{o,t}^T\frac {\partial net_{o,t}}{\partial h_{t-1}}+\delta_{f,t}^T\frac {\partial net_{f,t}}{\partial h_{t-1}}+\delta_{i,t}^T\frac {\partial net_{i,t}}{\partial h_{t-1}}+\delta_{\hat{c},t}^T\frac {\partial net_{\hat{c},t}}{\partial h_{t-1}}=\delta_{o,t}^TW_{oh}+\delta_{f,t}^TW_{fh}+\delta_{i,t}^TW_{i,h}+\delta_{\hat{c},t}^TW_{ch}    (公式8)$$</p><p>根据$\delta_{o,t}、\delta_{f,t}、\delta_{i,t}、\delta_{\hat{c},t}$的定义，可知：<br>$$<br>\begin{aligned}<br>&amp;\delta_{o,t}^T = \delta_t^T\bigodot tanh(c_t)\bigodot o_t\bigodot(1-o_t)(公式9)\\\\<br>&amp;\delta_{f,t}^T =\delta_t^T \bigodot o_t\bigodot(1-tanh(c_t)^2)\bigodot c_{t-1}\bigodot f_t \bigodot (1-f_t)(公式10)\\\\<br>&amp;\delta_{i,t}^T = \delta_t^T \bigodot o_t \bigodot(1-tanh(c_t)^2)\bigodot \hat{c_t}\bigodot i_t \bigodot (1-i_t)(公式11)\\\\<br>&amp;\delta_{\hat{c},t}^T=\delta_t^T\bigodot o_t\bigodot(1-tanh(c_t)^2)\bigodot i_t\bigodot (1-\hat{c}^2)(公式12)<br>\end{aligned}<br>$$<br>公式8到公式12就是将误差沿时间反向传播的一个时刻的公式。有了它，我们可以写出将误差向前传递到任意时刻k的公式：$$<br>\delta_k^T = \prod_{j=k}^t-1\delta_{o,j}^TW_{oh}+\delta_{f,j}^TW_{fh}+\delta_{i,j}^TW_{ih}+\delta_{\hat{c},j}^TW_{ch}<br>$$</p><h3 id="将误差传递到上一层"><a href="#将误差传递到上一层" class="headerlink" title="将误差传递到上一层"></a>将误差传递到上一层</h3><p>假设当前层为第l层，定义第l-1层的误差项是误差函数l-1层加权输入的导数，即:$$<br>\delta_t^{l-1}=\frac {\partial E}{\partial net_t^{l-1}}<br>$$<br>LSTM的输入$x_t$由下面的公式计算：<br>$$<br>x_t^l = f^{l-1}(net_t^{l-1})<br>$$<br>上式中，$f^{l-1}$表示第l-1层的激活函数。<br>因为$net_{f,t}^l、net_{i,t}^l、net_{\hat{c},t}^l、net_{o,t}^l$都是$x_t$的函数，$x_t$又是$net_t^{l-1}$的函数，因此，要求出E对$net_t^{l-1}$的导数，就需要使用全导数公式：<br>$$\begin{aligned}<br>\frac {\partial E}{\partial net_t^{l-1}}<br>&amp;=\frac {\partial E}{\partial net_{f,t}^l}\frac {\partial net_{f,t}^l}{\partial x_t^l}\frac {\partial x_t^l}{\partial net_t^{l-1}}+\frac {\partial E}{\partial net_{i,t}^l}\frac{\partial net_{i,t}^l}{\partial x_t^l}\frac {\partial x_t^l}{\partial net_{i,t}^{l-1}}+\frac {\partial E}{\partial net_{\hat{c},t}^l}\frac{\partial net_{\hat{c},t}^l}{\partial x_t^l}\frac {\partial x_t^l}{\partial net_t^{l-1}}+\frac{\partial E}{\partial net_{o,t}^l} \frac{\partial net_{o,t}^l}{\partial x_t^l}\frac{\partial x_t^l}{\partial net_{o,t}^{l-1}}\\\\<br>&amp;=\delta_{f,t}^TW_{fx}\bigodot f^{\prime}(net_t^{l-1})+\delta_{i,t}^TW_{ix}\bigodot f^{\prime}(net_t^{l-1})+\delta_{\hat{c},t}^TW_{cx}\bigodot f^{\prime}(net_t^{l-1})+\delta_{o,t}^TW_{ox}\bigodot f^{\prime}(net_t^{l-1})\\\\&amp;=<br>(\delta_{f,t}^TW_{fx}+\delta_{i,t}^TW_{ix}+\delta_{\hat{c},t}^TW_{cx}+\delta_{o,t}^TW_{ox})\bigodot f^{\prime}(net_l^{l-1})<br>\end{aligned}(公式14)$$</p><p>公式14就是将误差传递到上一层的公式。</p><h3 id="权重梯度计算"><a href="#权重梯度计算" class="headerlink" title="权重梯度计算"></a>权重梯度计算</h3><p>对度$W_{fh}、W_{ih}、W_{ch}、W_{oh}$的权重梯度，我们知道我们知道它的梯度是各个时刻梯度之和，我们首先求出它们在t时刻的梯度，然后再求出他们最终的梯度。我们已经求得误差项$\delta_{o,t}、\delta_{f,t}、\delta_{i,t}、\delta_{\hat{c},t}$很容易求出t时刻$W_{fh}、W_{ih}、W_{ch}、W_{oh}$的梯度。$$<br>\begin{aligned}<br>   &amp;\frac{\partial E}{\partial W_{oh,t}}=\frac{\partial E}{\partial net_{o,t}}\frac{\partial net_{o,t}}{\partial W_{oh,t}}=\delta_{o,t}h_{t-1}^T\\\\<br>   &amp;\frac{\partial E}{\partial W_{fh,t}}=\frac{\partial E}{\partial net_{f,t}}\frac{\partial net_{f,t}}{\partial W_{fh,t}}=\delta_{f,t}h_{t-1}^T\\\\<br>   &amp;\frac{\partial E}{\partial W_{ih,t}}=\frac{\partial E}{\partial net_{i,t}}\frac{\partial net_{i,t}}{\partial W_{ih,t}}=\delta_{i,t}h_{t-1}^T\\\\<br>   &amp;\frac{\partial E}{\partial W_{ch,t}}=\frac{\partial E}{\partial net_{\hat{c},t}}\frac{\partial net_{\hat{c},t}}{\partial W_{ch,t}}=\delta_{\hat{c},t}h_{t-1}^T<br>\end{aligned}<br>$$<br>将各个时刻的梯度加在一起，就能得到最终的梯度：$$<br>\begin{aligned}<br>&amp;\frac{\partial E}{\partial W_{oh}}=\sum_{j=1}^t\delta_{o,j}h_{j-1}^T\\\\<br>&amp;\frac{\partial E}{\partial W_{fh}}=\sum_{j=1}^t\delta_{f,j}h_{j-1}^T\\\\<br>&amp;\frac{\partial E}{\partial W_{ih}}=\sum_{j=1}^t\delta_{i,j}h_{j-1}^T\\\\<br>&amp;\frac{\partial E}{\partial W_{ch}}=\sum_{j=1}^t\delta_{\hat{c},j}h_{j-1}^T\\\\<br>\end{aligned}<br>$$<br>对于偏置项$b_f、b_i、b_c、b_o$的梯度，也是将各个时刻的梯度加在一起，下面是各个时刻的偏置梯度:<br>$$\begin{aligned}<br>&amp;\frac{\partial E}{\partial b_{o,t}}=\frac {\partial E}{\partial net_{o,t}}\frac{\partial net_{o,t}}{\partial b_{o,t}}=\delta_{o,t}\\\\<br>&amp;\frac{\partial E}{\partial b_{f,t}}=\frac {\partial E}{\partial net_{f,t}}\frac{\partial net_{f,t}}{\partial b_{f,t}}=\delta_{f,t}\\\\<br>&amp;\frac{\partial E}{\partial b_{i,t}}=\frac {\partial E}{\partial net_{i,t}}\frac{\partial net_{i,t}}{\partial b_{i,t}}=\delta_{i,t}\\\\<br>&amp;\frac{\partial E}{\partial b_{c,t}}=\frac {\partial E}{\partial net_{\hat{c},t}}\frac{\partial net_{\hat{c},t}}{\partial b_{c,t}}=\delta_{\hat{c},t}<br>\end{aligned}<br>$$<br>下面是最终的偏置项的梯度，即将各个时刻的偏置项梯度加在一起：<br>$$<br>\begin{aligned}<br>&amp;\frac {\partial E}{\partial b_o}=\sum_{j=1}^t\delta_{o,j}\\\\<br>&amp;\frac {\partial E}{\partial b_i}=\sum_{j=1}^t\delta_{i,j}\\\\<br>&amp;\frac {\partial E}{\partial b_f}=\sum_{j=1}^t\delta_{f,j}\\\\<br>&amp;\frac {\partial E}{\partial b_c}=\sum_{j=1}^t\delta_{\hat{c},j}<br>\end{aligned}<br>$$<br>对于$W_{fx}、W_{ix}、W_{cx}、W_{ox}$的权重梯度，只需要根据相应的误差项直接计算即可:<br>$$\begin{aligned}<br>&amp;\frac {\partial E}{\partial W_{ox}}=\frac{\partial E}{\partial net_{o,t}}\frac{\partial net_{o,t}}{\partial W_{ox}}=\delta_{o,t}x_t^T\\\\<br>&amp;\frac {\partial E}{\partial W_{fx}}=\frac{\partial E}{\partial net_{f,t}}\frac{\partial net_{f,t}}{\partial W_{fx}}=\delta_{f,t}x_t^T\\\\<br>&amp;\frac {\partial E}{\partial W_{ix}}=\frac{\partial E}{\partial net_{i,t}}\frac{\partial net_{i,t}}{\partial W_{ix}}=\delta_{i,t}x_t^T\\\\<br>&amp;\frac {\partial E}{\partial W_{cx}}=\frac{\partial E}{\partial net_{\hat{c},t}}\frac{\partial net_{\hat{c},t}}{\partial W_{cx}}=\delta_{\hat{c},t}x_t^T\\\\<br>\end{aligned}<br>$$</p><p>&lt;–end–&gt;</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文转自：&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/581764，对其进行一定的整理。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zybuluo.com/hanbingtao/note/581764，对其进行一定的整理。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;LSTM前向计算&quot;&gt;&lt;a href=&quot;#LSTM前向计算&quot; class=&quot;headerlink&quot; title=&quot;LSTM前向计算&quot;&gt;&lt;/a&gt;LSTM前向计算&lt;/h2&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="LSTM" scheme="https://ilewseu.github.io/tags/LSTM/"/>
    
      <category term="GRU" scheme="https://ilewseu.github.io/tags/GRU/"/>
    
  </entry>
  
  <entry>
    <title>理解LSTM</title>
    <link href="https://ilewseu.github.io/2017/12/31/%E7%90%86%E8%A7%A3LSTM/"/>
    <id>https://ilewseu.github.io/2017/12/31/理解LSTM/</id>
    <published>2017-12-31T09:35:20.000Z</published>
    <updated>2018-01-01T05:49:12.227Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文是<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/的译文，这篇文章对LSTM的原理讲解的非常清楚，故存下来。" target="_blank" rel="external">http://colah.github.io/posts/2015-08-Understanding-LSTMs/的译文，这篇文章对LSTM的原理讲解的非常清楚，故存下来。</a></p></blockquote><h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p>人类并非每一秒都从头开始思考问题。当你阅读这篇文章时，你是基于之前的单词来理解，每个单词。你并不会把所有的内容都抛弃掉，然后从头开始理解。你的思考具有持久性。<br><a id="more"></a><br>传统的神经网络并不能做到这一点，这似乎是其一个主要的缺点。例如，想象你要把一部电影里面的时间点正在发生的事情进行分类。传统神经网络并不知道怎样才能把关于之前事件的推理运用到之后的事件中去。RNN神经网络解决了这个问题。它们是一种具有循环的网络，具有保持信息的能力。如下图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/Bmh8aD2DdA.jpg?imageslim" alt="mark"><br></div><br>如上图所示，神经网络的模块A输入为$x_i$，输出为$h_t$。模块A的循环结构使得信息从网络的上一步传到了下一步。这个循环使得RNN看起来有点神秘。然而，如果你仔细想想就会发现它与普通的神经网络并没有太大不同。RNN可以被认为是相同网络的多重复制结构，每一个网络把消息传给其继承者。如果我们把循环体展开就是这样，如下图所示：<br><div align="center"><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/iK11DGDA83.jpg?imageslim" alt="mark"><br></div><p>这种链接属性表明，RNN与序列之间有着紧密的连续。这也是运用这类数据最自然的结构。当然它们已经得到了应用。过去几年中，RNNs已经被成功用于各式各样的问题中：语音识别、语言建模、翻译、图像标注..等等。RNNs取得的各种瞩目成果可以参考Andrej Karpathy的博客：The Unreasonable Effectiveness of Recurrent Neural Networks。确实效果让人非常吃惊。</p><p>取得这项成功的一个要素是LSTMs，这是一种非常特殊的周期神经网络，对于许多任务，比标准版要有效得多。几乎所有基于RNN的好成果都使用了它们。本文将着重介绍LSTMs。</p><h2 id="长期依赖问题-The-Problem-of-Long-Term-Dependencies"><a href="#长期依赖问题-The-Problem-of-Long-Term-Dependencies" class="headerlink" title="长期依赖问题(The Problem of Long-Term Dependencies)"></a>长期依赖问题(The Problem of Long-Term Dependencies)</h2><p>RNNs的一个想法是，它们可能会能够将之前的信息连接到现在的任务之中。例如，用视频前一帧的信息可以用于理解当前帧的信息。如果RNNs能够做到这些，那么将会非常有用。但是它们可以吗？ 这要看情况。</p><p>有时候，我们处理当前任务仅需要查看当前信息。例如，设想用一个语言模型基于当前单词尝试着去预测下一个单词。如果我们尝试着预测”the clouds are in the”的最后一个单词，我们并不需要任何额外的信息，很显然下一个但是sky。这样的话，如果目标预测的点与其他相关信息的点之间的间隔较小，RNNs可以学习利用过去的信息。</p><p>但是，也有时候我们需要更多的上下文信息。设想预测这句话的最后一个单词：”I grew up in France… I speak fluent <strong>French</strong>“。最近的信息表明下一个单词似乎是一种语言的名字，但是如果我们希望缩小确定语言类型的范围，我们需要更早之前作为France 的上下文。而且需要预测的点与其相关点之间的间隔非常有可能变得很大，如图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/3La0B0C0iK.jpg?imageslim" alt="mark"><br></div><br>不幸的是，随着间隔增长，RNNs变得难以学习连接之间的关系，如下图所示：<br><br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/d9IAjAB7IL.jpg?imageslim" alt="mark"><br></div><p>理论上来说，RNNs绝对能够处理这种『长期依赖』。人们可以小心选取参数来解决这种类型的小模型。悲剧的是，事实上，RNNs似乎并不能学习出来这些参数。这个问题已经在<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="external">Hochreiter (1991) German</a>与<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="external">Bengio, et al. (1994)</a>,中被深入讨论，他们发现了为何RNNs不起作用的一些基本原因。幸运的是，LSTMs可以解决这个问题!</p><div></div><h2 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h2><p>长短时间记忆网络(Long Short Term Memory networks, LSTMs)，是一种特殊的RNN，它能够学习长时间依赖。它们由<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">Hochreiter &amp; Schmidhuber (1997)</a>，后来由很多让人加以改进和推广。它们在大量的问题上都取得了巨大的成功，现在已经被广泛应用。</p><p>LSTMs是专门设计用来避免长期依赖问题的。记忆长期信息是LSTMs的默认行为，而不是它们努力学习的东西！所有RNN都具有链式的重复模块神经网络。在标准的RNNs中，这种重复模块具有非常简单的结构，比如是一个tanh层，如下图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/BF8kcb785A.jpg?imageslim" alt="mark"><br></div><br>LSTMs同样具有链式结构，但是其重复模块却有着不同的结构。不同于单独的神经网络层，它具有4个以特殊方式相互影响的神经网络层，如图所示：<br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/6IlDEAkI3L.jpg?imageslim" alt="mark"><br></div><br>不要担心接下来涉及到的细节。我们将会一步步讲解LSTM的示意图。下面是我们将要用到的符号，如图所示：<br><div><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/GC9cecCGhg.jpg?imageslim" alt="mark"><br></div><p>在上图中，每一条线代表一个完整的向量，从一个节点的输出到另一个节点的输入。粉红色圆形代表了逐点操作。例如，向量求和；黄色方框代表学习出的神经网络层；聚拢的线代表了串联，而分开的线代表了内容复制去了不同的地方。</p><h3 id="LSTMs背后的核心思想"><a href="#LSTMs背后的核心思想" class="headerlink" title="LSTMs背后的核心思想"></a>LSTMs背后的核心思想</h3><p>LSTMs的关键在于细胞状态，在图中以水平线表示。<br>细胞状态就像一个传送带。它顺着整个链条从头到尾运行，中间只有少许线性的交互。信息很容易顺着它流动而保持不变。如图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/92f48mcjDG.jpg?imageslim" alt="mark"><br></div><p>LSTM通过称之为门(gates)的结构来对细胞状态增加或删除信息。门是选择性的让信息通过的方式。它们的输出有一个sigmoid层和逐点乘积操作，如图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/Lek8DbbiFC.jpg?imageslim" alt="mark"><br></div><br>Sigmoid 层的输出在0到1之间，定义了各成分被放行通过的程度。0值意味着『不让任何东西过去』；1值意味着『让所有东西通过』。LSTM具有3种门，用于保护和控制细胞状态。<br><div> </div><h3 id="逐步讲解LSTM"><a href="#逐步讲解LSTM" class="headerlink" title="逐步讲解LSTM"></a>逐步讲解LSTM</h3><p>LSTM的第一步是决定我们要从细胞中抛弃何种信息。这个决定是由叫做『遗忘门』的sigmoid层决定的。它以$h_{t-1}$和$x_i$为输入，在$C_{t-1}$细胞输出一个介于0和1之间的数。其中，1代表『完全保留』，0代表『完全遗忘』。</p><p>让我们回到之前那个语言预测模型的例子，这个模型尝试着根据之前的单词学习预测下一个单词。在这个问题中，细胞状态可能包括了现在的主语的性别，因此能够使用正确的代词。当我们见到一个新的主语时，我们希望它能够忘记之前主语的性别。如下图所示：</p><div align="center"><img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/cG7AbK6EEC.jpg?imageslim" alt="mark"></div><br>下一步是决定细胞要存储何种信息。它有2个组成部分，首先一各叫做『输入门层』的sigmoid层决定我们将要更新哪些值。其次，一个tanh层创建一个新的候选向量$\hat{C}_t$，它可以加在状态之中。在下一步我们将结合两者来生成状态的更新。在语言模型的例子中，我们希望把新主语的性别加入到状态之中，从而取代我们打算遗忘的旧主语的性别，如下图所示：<br><div><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/Hce780KF2L.jpg?imageslim" alt="mark"><br></div><br>现在我们可以将旧细胞状态$C_{t-1}$更新为$C_t$了。之前的步骤已经决定了该怎么做，我们现在实际操作一下。我们把旧状态乘以$f_t$，用以遗忘之前我们决定忘记的信息。然后我们加上$i_t*\hat{C}_t$。这是新的候选值，根据我们决定更新状态的程度来作为缩放系数。<br><br>在语言模型中，这里就是我们真正丢弃关于旧主语性信息以及添加新信息的地方，如下图所示：<br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/BC8GaK4Fjc.jpg?imageslim" alt="mark"><br></div><br>最终，我们可以决定输出哪些内容。输出取决于我们的细胞状态，但是以一个过滤后的版本。首先，我们使用sigmoid层来决定我们要输出细胞状态的哪些部分。然后，把用tanh处理细胞状态（将状态值映射到-1至1之间）。最后，将其与sigmoid门的输出值相乘，从而我们能够输出我们决定输出的值。如下图所示：<br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/f2b7k0070c.jpg?imageslim" alt="mark"><br></div><br>对于语言模型，在预测下一个单词的例子中，当它输入一个主语，它可能会希望输出相关的动词。例如，当主语时单数或复数时，它可能会以相应形式的输出。<br><div></div><h3 id="各种LSTM的变化形式"><a href="#各种LSTM的变化形式" class="headerlink" title="各种LSTM的变化形式"></a>各种LSTM的变化形式</h3><p>目前，我所描述的都是普通的LSTM，然而并非所有的LSTM都是一样的。事实上，似乎每一篇使用LSTMs的文章都有细微的差别。这些差别很小，但是值得一提。</p><p>其中一个流行的LSTM变化形式是由<a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="external">Gers &amp; Schmidhuber (2000)</a>提出的，增加了『窥视孔连接（peephole connections）』。如下图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/8e4JBf654L.jpg?imageslim" alt="mark"><br></div><br>在上图中，所有的门都加上了窥视孔，但是许多论文中只在其中一些装了窥视孔。<br>另一个变种是使用了配对遗忘与输入门。与之前分别决定遗忘与添加信息不同，我们同时决定两者。只有当我们需要输入一些内容的时候我们才需要忘记。只有当早前信息被忘记之后我们才会输入。如图所示：<br><div><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/H16C53f5IC.jpg?imageslim" alt="mark"><br></div><p>LSTM 一个更加不错的变种是 Gated Recurrent Unit（GRU），是由<a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">Cho, et al. (2014)</a>提出的。这个模型将输入门与和遗忘门结合成了一个单独的『更新门』。而且同时还合并了细胞状态和隐含状态，同时也做了一下其他的修改。因此这个模型比标准LSTM模型要简单，并且越来越收到欢迎。如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171231/F2C472DeBH.jpg?imageslim" alt="mark"><br></div><br>这些仅仅只是LSTM的少数几个著名变种。还有很多其他的种类，例如由<a href="https://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="external">Yao, et al. (2015)</a> 提出的Depth Gated RNNs 。以及处理长期依赖问题的完全不同的手段，如<a href="https://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="external">Koutnik, et al. (2014)</a>提出的Clockwork RNNs。</p><p>那种变种是最好的？这些不同重要吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">Greff, et al. (2015)</a>将各种著名的变种做了比较，发现其实基本上是差不多的。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="external">Jozefowicz, et al. (2015)</a>测试了超过一万种RNN结构，发现了一些在某些任务上表现良好的模型。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>最开始我提到的杰出成就都是使用的是RNNs。本质上所有的这些成果都是使用了LSTMs。在大多数任务中，确实它们的表现非常优秀。</p><p>以公式的形式写下来，LSTMs看起来非常令人胆怯。然而本文的逐步讲解使得LSTM变得平易近人了。</p><p>LSTMs 是我们使用RNNs的重要一步。我们很自然地想到：还有下一个重要的一大步吗？研究者的普遍观点是：『有！下一大步就是「注意力」。』其基本思想就是让RNN的每一步从更大范围的信息中选取。例如，假设你为图片打标签，它可能会为它输出的每一个词语选取图片的一部分作为输入。事实上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="external">Xu, et al. (2015)</a>就是这么做的——如果你想探索『注意力』的话，这是个有趣的引子！已经有大量使用『注意力』得到的良好成果，而且似乎更多的陈果也将要出现……</p><p>『注意力』并非是RNN研究中唯一一个激动人心的方向。例如，<a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="external">Kalchbrenner, et al. (2015)</a>做出的Grid LSTMs 似乎很有前途。在生成模型中使用RNNs－例如<a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="external">Gregor, et al. (2015)</a>，<a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="external">Chung, et al. (2015)</a>以及<a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="external">Bayer &amp; Osendorfer (2015)</a>－似乎也很有趣。过去几年是RNN激动人心的阶段，未来几年将会更加如此！</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/的译文，这篇文章对LSTM的原理讲解的非常清楚，故存下来。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://colah.github.io/posts/2015-08-Understanding-LSTMs/的译文，这篇文章对LSTM的原理讲解的非常清楚，故存下来。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Recurrent-Neural-Networks&quot;&gt;&lt;a href=&quot;#Recurrent-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Recurrent Neural Networks&quot;&gt;&lt;/a&gt;Recurrent Neural Networks&lt;/h2&gt;&lt;p&gt;人类并非每一秒都从头开始思考问题。当你阅读这篇文章时，你是基于之前的单词来理解，每个单词。你并不会把所有的内容都抛弃掉，然后从头开始理解。你的思考具有持久性。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="LSTM" scheme="https://ilewseu.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络RNN 梯度推导(BPTT)</title>
    <link href="https://ilewseu.github.io/2017/12/30/RNN%E7%AE%80%E5%8D%95%E6%8E%A8%E5%AF%BC/"/>
    <id>https://ilewseu.github.io/2017/12/30/RNN简单推导/</id>
    <published>2017-12-30T03:28:20.000Z</published>
    <updated>2018-02-09T15:19:21.535Z</updated>
    
    <content type="html"><![CDATA[<h2 id="循环神经网络简介"><a href="#循环神经网络简介" class="headerlink" title="循环神经网络简介"></a>循环神经网络简介</h2><p>循环神经网络(Recurrent Neural Network,RNN)，是一种sequence model，它的思想就是使用序列信息。<strong>在前馈、卷积神经网络中，认为输入（和输出）彼此之间是互相独立的。但是对很多任务而言，这种处理方式很不合理。同时，在前馈、卷积神经网络中，输入和输出的维数都是固定的，不能任意改变，且无法处理变长的序列数据</strong>。<a id="more"></a>循环神经网络，它对于序列中的每个元素都执行相同的任务，输出依赖于之前的计算。另一种思考循环神经网络的方法是，它们有一个记忆，记忆可以捕获迄今为止已经计算过的信息。理论上循环神经网络可以利用任意长度的序列信息。但是，在实际应用中，由于梯度传播的原因，它们仅能利用有限步长。循环神经网络的网络结构图如下：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171230/HEIa45LFfD.jpg?imageslim" alt="mark"><br></div><p>在上图的网络中，网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。$h_t$的值不仅依赖于$x_t$，还取决于$s_{t-1}$。循环神经网络的计算方法如下：$$<br>o_t = g(V_{h_t})                      (公式1)\\\\<br>s_t = f(Ux_t+Ws_{t-1})    (公式2)<br>$$</p><p>其中，公式1是<strong>输出层</strong>的计算公式，输出层可以是一个全连接层，<strong>V</strong>是输出层的权重矩阵，<strong>g</strong>是相应的激活函数。公式2是<strong>隐藏层</strong>的计算公式，它是循环层。<strong>U</strong>是输入x的权重矩阵，<strong>W</strong>是上一层的输出值$s_{t-1}$作为这一次的输入的权重矩阵，<strong>f</strong>是激活函数。</p><p>可以看出，循环层和全连接层的区别就是多了一个权重矩阵W。如果把公式2反复带到公式1，我们可以得到：<br>$$<br>\begin{aligned}<br>o_t &amp;= g(Vs_t)\\\\<br>&amp;=Vf(Vs_t)\\\\<br>&amp;=Vf(Ux_t+Ws_{t-1})\\\\<br>&amp;=Vf(Ux_t+Wf(Ux_{t-1}+Ws_{t-2}))\\\\<br>&amp;=Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Ws_{t-3})))\\\\<br>&amp;=Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Wf(Ux_{t-3}+…))))<br>\end {aligned}<br>$$</p><p>从上面可以看出，循环神经网络的输出值$o_t$，是受前面的历次输入值$x_t、x_{t-1}、x_{t-2}、…$影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。<br><strong>循环神经网络的隐藏层的输出可以用于预测词汇/标签等符号的分布，隐藏层状态保留了到目前为止的历史信息。</strong></p><h2 id="循环神经网络的训练算法BPTT介绍"><a href="#循环神经网络的训练算法BPTT介绍" class="headerlink" title="循环神经网络的训练算法BPTT介绍"></a>循环神经网络的训练算法BPTT介绍</h2><p>循环神网络的训练算法是Backpropagation Through Time,BPTT算法，其基本原理和反向传播算法是一样的，只不过反向传播算法是按照层进行反向传播，BPTT是按照时间t进行反向传播。对于下图所示的循环神经网络：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171230/A0ih479fhh.jpg?imageslim" alt="mark"><br></div><br>这个图和上面的图所示的架构没有区别，只不过是把隐藏层的状态用$h_t$表示，t时刻的输出用$y_t$表示。$$<br>         h_t = f(Uh_{t-1}+Wx_t+b)\\\\y_t=softmax(Vh_t)<br>$$<br>假设循环神经网络在每个时刻t都有一个监督信息，损失为$J_t$，则整个序列的损失为$J=\sum_{t=1}^T J_t$。$$<br>J_t = -y_tlogy_t\\\\<br>    J = -\sum_{t=1}^T y_tlogy_t<br>$$</p><p><strong>1、J关于V的梯度计算</strong></p><p>$$<br>\frac {\partial J}{\partial V} = \frac {\partial}{\partial V} \sum_{t=1}^T J_t=\sum_{t=1}^T \frac {\partial J_t}{\partial V}<br>$$<br>令$$y_t = softmax(z_t)\\\ z_t = Vh_t$$，则$\frac {\partial J_t}{\partial V}$的计算公式如下：</p><p>$$<br>\frac {\partial J_t}{\partial V} = \frac {\partial J_t}{\partial y_t}\frac {\partial y_t}{\partial V}=\frac {\partial J_t}{\partial y_t} \frac {\partial y_t}{\partial z_t} \frac {\partial z_t}{\partial V}<br>$$</p><p><strong>2、损失J关于U的梯度计算</strong></p><p>$$<br>\frac {\partial J}{\partial U} = \frac {\partial}{\partial U} \sum_{t=1}^T \frac {\partial J_t}{\partial U}=\sum_{t=1}^T \frac {\partial h_t}{\partial U} \frac {\partial J_t}{\partial h_t}<br>$$<br>其中，$h_t$是关于U和$h_{t-1}$的函数，而$h_{t-1}$又是关于U和$h_{t-2}$的函数。</p><p>用链式法则可以得到：<br>$$\frac {\partial J}{\partial U} = \sum_{t=1}^T\sum_{k=1}^t \frac {\partial h_k}{\partial U} \frac {\partial h_t}{\partial h_k} \frac {\partial y_t}{\partial h_t}\frac {\partial J_t}{\partial y_t}\\\\<br>h_t = f(Uh_{t-1} + Wx_t+b)$$<br>其中，<br>$$<br>\frac {\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac {\partial h_i}{\partial h_{i-1}} = \prod_{i=k+1}^t U^Tdiag[f^{\prime}(h_{i-1})]<br>$$</p><p>则J对U的梯度为：<br>$$<br>\frac {\partial J}{\partial U} = \sum_{t=1}^T \sum_{k=1}^t \frac {\partial h_k}{\partial U}(\prod_{i=k+1}^t U^T diag[f^{\prime}(h_{i-1})])\frac {\partial y_t}{\partial h_t}\frac {\partial J_t}{\partial y_t}<br>$$</p><p><strong>3、损失J关于W的梯度计算</strong></p><p>$$\frac {\partial J}{\partial W} = \frac {\partial}{\partial W}\sum_{t=1}^T J_t=\sum_{t=1}^T \frac {\partial J_t}{\partial W}=\sum_{t=1}^T \frac {\partial h_t}{\partial W} \frac {\partial J_t}{\partial h_t}$$</p><p>用链式法则可以得到：<br>$$\frac {\partial J}{\partial W}=\sum_{t=1}^T \sum_{k=1}^t \frac{\partial h_k}{\partial W}\frac{\partial h_t}{\partial h_k}\frac {\partial y_t}{\partial h_t} \frac {\partial J_t}{\partial y_t}\\\\<br>h_t = f(Uh_{t-1} + Wx_t+b)$$</p><p>其中，<br>$$<br>\frac {\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac {\partial h_i}{\partial h_{i-1}} = \prod_{i=k+1}^t U^Tdiag[f^{\prime}(h_{i-1})]<br>$$<br>则，对W的梯度为：<br>$$<br>\frac {\partial J}{\partial W} = \sum_{t=1}^T\sum_{k=1}^t \frac {\partial h_k}{\partial W}(\prod_{i=k+1}^t U^Tdiag[f^{\prime}(h_{i-1})])\frac {\partial y_t}{\partial h_t}\frac {\partial J_t}{\partial y_t}<br>$$</p><p>如果定义$\gamma=||U^T diag(f^{\prime}(h_{i-1})||$，则在上面公式中的括号里面$\gamma^{t-k}$。如果$\gamma &gt; 1$，则当$t-k \rightarrow \infty$时，$\gamma^{t-k} \rightarrow \infty$，会造成系统的不稳定，也就是所谓的梯度爆炸问题；相反，如果$\gamma &lt; 1$，$t-k \rightarrow \infty$，$\gamma^{t-k} \rightarrow 0$，会出现和深度前馈神经网络类似的梯度消失的问题。</p><p>因此，虽然简单循环网络可从理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度爆炸或消失的存在，实际上只能学习到短期的依赖关系，这就是所谓的长期依赖问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;循环神经网络简介&quot;&gt;&lt;a href=&quot;#循环神经网络简介&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络简介&quot;&gt;&lt;/a&gt;循环神经网络简介&lt;/h2&gt;&lt;p&gt;循环神经网络(Recurrent Neural Network,RNN)，是一种sequence model，它的思想就是使用序列信息。&lt;strong&gt;在前馈、卷积神经网络中，认为输入（和输出）彼此之间是互相独立的。但是对很多任务而言，这种处理方式很不合理。同时，在前馈、卷积神经网络中，输入和输出的维数都是固定的，不能任意改变，且无法处理变长的序列数据&lt;/strong&gt;。
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="CNN" scheme="https://ilewseu.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络学习笔记</title>
    <link href="https://ilewseu.github.io/2017/12/23/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://ilewseu.github.io/2017/12/23/卷积神经网络学习笔记/</id>
    <published>2017-12-23T11:48:20.000Z</published>
    <updated>2018-01-01T05:50:41.359Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文是对近期学习的卷积神经网络相关知识的简单记录和梳理。</p></blockquote><h2 id="卷积神经网络简介"><a href="#卷积神经网络简介" class="headerlink" title="卷积神经网络简介"></a>卷积神经网络简介</h2><p>卷积神经网络(Convolution Neural Network，CNN或ConvNet)是一种前馈神经网络。卷积神经网络是受生物学上<strong>感受野</strong>(Receptive Field)的机制提出来的。一个神经元的感受野是指特定区域，只有这个区域内的刺激才能够激活该神经元。<br><a id="more"></a></p><blockquote><p>感受野，主要是指听觉、视觉等神经系统中一些神经元的特性，即神经元只接受其所支配的刺激区域内的信号。在视觉神经系统中，视觉皮层中的神经细胞的输出依赖于视网膜上的光感受器。视网膜上的光感受器受刺激兴奋时，将神经冲动信号传到视觉皮层，但不是所有的视觉皮层中的神经元都会接受这些信号。一个神经元的感受野指视网膜上的特定区域，只有这个区域内的刺激才能够激活该神经元。</p></blockquote><p>卷积神经网络最早是主要处理图像信息。如果用全连接前馈神经网络来处理图像时，会存在以下两个问题：</p><ul><li><strong>参数太多</strong>：如果图像的输入大小为100x100x3，在使用全连接前馈神经网络中，第一个隐藏层的每个神经元到输入层都有100x100x3=30,000个相互独立的连接，每个连接都对应一个权重参数。随着隐藏层神经元数量增多，参数的规模也会增加。这会导致整个神经网络的训练效率会非常低下，也会很容易出现过拟合。</li><li><strong>局部不变性特征</strong>： 自然图像中的物体都具有局部特征不变性，比如在尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈神经网络很难提取这些局部不变性，一般需要进行数据增强来提高性能。</li></ul><p>目前的卷积神经网络一般是由卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络，使用反向传播算法进行训练。卷积神经网络有三个结构上的特性：<strong>局部连接、权重共享以及子采样</strong>。这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数更少。</p><h2 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h2><h3 id="卷积介绍"><a href="#卷积介绍" class="headerlink" title="卷积介绍"></a>卷积介绍</h3><p>卷积(convolution)，是数学分析中一种重要的运算。在信号处理或图像中，经常使用一维卷积或二维卷积。<br><strong>一维卷积</strong>，一维卷积常用在信号处理中，用于计算信号的延迟累积。假设一个信号发生器每个时刻t产生一个信号$x_t$,其信息的衰减率为$f_k$，即在k-1个时间步长后，信息变为原来的$f_k$倍。假设$f_1=1,f_2=1/2,f_3=1/4$，那么在时刻t收到的信号为$y_t$为当前时刻产生的信息和以前时刻延迟信息的叠加，$$<br>y_t = 1\times x_t + 1/2 \times x_{t-1} + 1/4 \times x_{t-2} \\\\<br>=f_1 \times x_t + f_2 \times x_{t-1} + f_3 \times x_{t-2}<br>$$<br>我们把$f_1,f_2…$称为滤波器(filter)或卷积核(convolution kernel)。假设滤波器的长度为m，它和一个信号序列$x_1,x_2…$的卷积为：$$<br>y_t = \sum_{k-1}^m f_k \cdot x_{t-k+1}<br>$$<br>信号序列<strong>x</strong>和滤波器<strong>w</strong>的卷积定义为：$$<br>y = w \bigotimes x<br>$$<br>一般情况下，滤波器的长度m远小于信号序列的长度n。当$f_k=1/m$时，卷积相当于移动平均。下图是一个一维卷积的例子：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/8b34KGFaEd.jpg?imageslim" alt="mark"><br></div><p><strong>二维卷积</strong>，卷积也经常用于图像处理中。因为图像是一个二维结构，需要将一维卷积进行扩展。给定一个图像$X \in R^{M \times N}$和滤波器$W \in R^{M \times N }$，一般m&lt;&lt;M，n&lt;&lt;N，其卷积为：$$<br>y_{ij} = \sum_{u=1}^m \sum_{v=1}^n w_{uv}\cdot x_{i-u+1,j-v+1}    (公式1)<br>$$</p><h3 id="卷积的类型"><a href="#卷积的类型" class="headerlink" title="卷积的类型"></a>卷积的类型</h3><p>根据在输入信号两端的补0的情况可以将卷积分为：<strong>窄卷积、宽卷积和等长卷积</strong>。</p><p><strong>一维卷积</strong></p><ul><li><strong>窄卷积</strong>，在信号两端不补0，输出信号长度为n-m+1；</li><li><strong>宽卷积</strong>，信号两端各补m-1个0，输出信号长度为n+m-1；</li><li><strong>等长卷积</strong>，信号两端各补(m-1)/2个0， 输出信号长度为n;</li></ul><p><strong>二维卷积</strong></p><ul><li><strong>窄卷积</strong>，信号四周不补0，输出信号长度为M-m+1*N-n+1;</li><li><strong>宽卷积</strong>，信号四周补0，输出长度为M+m-1*N+n-1;</li><li><strong>等长卷积</strong>，信号四周补0，输出长度为M*N;</li></ul><h3 id="卷积神经网络中的卷积"><a href="#卷积神经网络中的卷积" class="headerlink" title="卷积神经网络中的卷积"></a>卷积神经网络中的卷积</h3><p>在机器学和图像处理领域，卷积主要的功能是在一个图像上滑动一个卷积核，通过卷积核操作得到一组新的特征。在计算卷积的过程中，需要进行卷积的翻转。在具体的实现上，一般会以互相关操作来代替卷积，从而会减少一些不必要的操作或开销。<strong>互相关</strong>(cross-correlation)是一个衡量两个序列相关性的函数，通常是用滑动窗口的点积计算来实现。给定一个图像$x\in R^{M \times N}$和卷积核$W\in R^{m\times n}$，它们的互相关为：$$<br>y_{ij}=\sum_{u=1}^m \sum_{v=1}^n w_{uv} \cdot x_{i+u-1,j+v-1}<br>$$<br>和公式1相比，互相关和卷积的主要区别在于卷积核仅仅是否进行翻转。因此，互相关也可以称为不翻转卷积。</p><blockquote><p>翻转，就是从两个维度(从上到下、从左到右)颠倒次序，即旋转180度。</p></blockquote><p>在神经网络中使用卷积是为了进行特征抽取，卷积核是否进行翻转和其特征的抽取的能力无关。特别是当卷积核是可学习的参数时，卷积核互相关是等价的。因此，为了实现上的方便，卷积神经网络中的卷积实际上用互相关操作来代替卷积，可以将互相关表示为$$<br>Y=W \bigotimes X<br>$$<br>其中，$Y\in R^{M-m+1,N-n+1}$为输出矩阵。</p><p>互相关运算，即常用的卷积神经网络中的卷积操作示例如下图所示：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/E8hkhejggf.gif" alt="mark"><br></div><h3 id="卷积运算的性质"><a href="#卷积运算的性质" class="headerlink" title="卷积运算的性质"></a>卷积运算的性质</h3><p>卷积具有很多很好的性质，下面就介绍一下二维卷积的数学性质，同样适用于一维卷积。</p><p><strong>交换性</strong></p><p>如果不限制两个卷积信号的长度，卷积是具有交换性的，即$x \bigotimes y=y\bigotimes x$。</p><p>当输入信息和卷积核有固定长度时，它们的宽卷积依然具有交换性。</p><p>对于两维的图像$x\in R^{M \times N}$和卷积核$W\in R^{m\times n}$，对图像X的两个维度进行零填充，两端各补m-1和n-1个零，得到全填充的图像$\hat{X} \in R^{(M+2m-2) \times (N+2n-2)}$</p><p>图像X和卷积核W的宽卷积定义为：$$<br>\hat{Y}=W\hat{\bigotimes}X<br>$$<br>其中，$\hat{\bigotimes}$ 为宽卷积操作。<br>宽卷积具有交换性，即：$$<br>W\hat{\bigotimes}X=X\hat{\bigotimes}W<br>$$</p><p><strong>导数</strong></p><p>假设$Y=W\bigotimes X$，其中$x\in R^{M \times N}$，$W\in R^{m\times n}$，函数$f(Y)\in R$为一个标量函数，则：$$<br>\frac {\partial f(Y)}{\partial w_{uv}}=\sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}\frac{\partial y_{ij}}{\partial w_{uv}}\frac {\partial f(Y)}{\partial y_{ij}}\\\\<br>=\sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}x_{i+u-1,j+v-1}\frac {\partial f(Y)}{\partial y_{ij}}\\\\=\sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}\frac {\partial f(Y)}{\partial y_{ij}}x_{u+i-1,v+j-1}<br>$$<br>可以看到，f(Y)关于W的偏导数为X和$\frac {\partial f(Y)}{\partial Y}$的卷积$$<br>\frac {\partial f(Y)}{\partial (W)} = \frac {\partial f(Y)}{\partial Y} \bigotimes X     (公式2)<br>$$<br>同理可以得到：$$<br>\frac {\partial f(Y)}{\partial x_{st}} = \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1} \frac {\partial y_{ij}}{\partial x_{st}} \frac {\partial f(Y)}{\partial {y_{ij}}}\\\ = \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1} w_{s-i+1,t-j+1}\frac {\partial f(Y)}{\partial y_{ij}}     (公式3)<br>$$<br>其中，当(s-i+1)<1时，或(s-i+1)>m，或(t-j+1)<1，或(t-j+1)>n时，$w_{s-i+1,t-j+1}$，相当于对W进行了p=(M-m,N-n)的零填充。</1，或(t-j+1)></1时，或(s-i+1)></p><p>可以看到，f(Y)关于X的偏导数为W和$\frac {\partial f(Y)}{\partial Y}$的宽卷积。公式3中的卷积是真正的卷积，而不是互相关，为了一致性，我们用互相关的“卷积”，即：$$<br>\frac {\partial f(Y)}{\partial X} = rot180(\frac {\partial f(Y)}{\partial Y}) \hat {\bigotimes}W \\\\=rot180(W) \hat {\bigotimes} \frac {\partial f(Y)}{\partial Y}<br>$$</p><h2 id="卷积神经网络结构"><a href="#卷积神经网络结构" class="headerlink" title="卷积神经网络结构"></a>卷积神经网络结构</h2><p>首先，看一下典型的卷积神经网络的结构：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/0a5d261ald.png?imageslim" alt="mark"><br><br>    图片引自：<a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">https://www.zybuluo.com/hanbingtao/note/485480</a><br></div><br>如上图所示，一个卷积神经网络由若干<strong>卷积层、Pooling层、全连接层</strong>组成。通过设置不同的卷积层、Pooling层以及全连接层，可以构建不同的卷积神经网络结构，它常用的架构模式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Input -&gt; [[卷积层]*N-&gt;Pooling ?] * M -&gt; [全连接层]*K</div></pre></td></tr></table></figure><br><br>也就是N个卷积层叠加，然后(可选)叠加一个Pooling层，重复这个结构M次，最后叠加K个全连接层。下面介绍每个层的作用。<br><div></div><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>全连接前馈神经网络中，第l-1层的$n^{(l-1)}$个神经元和第l层的$n^{(l)}$个神经元的连接采用的是全连接的方式，则权重矩阵有$n^{(l)}\times n^{(l-1)}$个参数，当l-1层和l层的神经元过多时，权重矩阵的参数非常多，训练的效率会非常低。</p><p>如果采用卷积来代替全连接，第l层的净输入$z^{(l)}$为第l-1层激活值$a^{(l-1)}$和滤波器$w^{(l)}$的卷积，即：$$<br>z^{(l)} = w^{(l)} \bigotimes a^{(l-1)} + b^{(l)}      (公式4)<br>$$<br>其中，滤波器$w^{(l)}$为权重向量，$b^{(l)}\in R^{(n^{l-1})}$为偏置。</p><p>根据卷积的定义，卷积层具有如下两个性质：</p><ul><li><p><strong>局部连接</strong> 在卷积层中的每一个神经元都和下一层某个局部窗口内的神经元相连，构成一个局部连接网络。如下图所示，卷积层和下一层之间的连接数大大减少，由原来的$n^{(l)}\times n^{(l-1)}$个连接变为$n^{(l)} \times m$个连接，m为滤波器的大小。</p><div align="center"><br>  <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/0mjIc9fDE5.jpg?imageslim" alt="mark"><br></div></li><li><p><strong>权重共享</strong>  从公式4可以看出，作为参数的滤波器$w^{(l)}$对于第l层的所有的神经元都是相同的。从上图也可以看出，所有同颜色连接上的权重是相同的。</p></li></ul><p>卷积层的主要作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。上面介绍的卷积层的神经元和全连接都是一维结构。对于常见的图像为二维结构，因此为了更充分的利用图像的局部信息，通常将神经元组织为三维结构，其大小为$宽度M \times 高度N \times 深度D$，有D个$M\times N$的特征映射构成。</p><p><strong>特征映射（Feature Map)</strong> 为了增强卷积层的表示能力，我们可以使用K个不同的滤波器来得到K组不同的输出。每组输出都共享一个滤波器。如果我们把滤波器看成是一个特征提取器，每一组输出都可以看成是输入图像经过一个特征抽取后得到的特征。因此，在卷积神经网络中每一组输出也叫作一组<strong>特征映射</strong>。</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/CBbj606Kdd.jpg?imageslim" alt="mark"><br><br>    图片引自： <a href="https://nndl.github.io/" target="_blank" rel="external">https://nndl.github.io/</a> 卷积神经网络<br></div><p>在输入层，特征映射就是图像本身，如果图像是灰度图像，就是一个特征映射，深度为D=1；如果是彩色图像，分别有RGB三个颜色通道的特征映射，输入深度D=3。</p><p>假设一个卷积层的结构如下：</p><ul><li>输入特征映射组：$X \in R^{M \times N \times D}$为三维张量(tensor)，每个切片为矩阵$X^d \in R^{M \times N}$为一个输入特征映射，1&lt;=d&lt;=D；</li><li>输出特征映射组：$Y \in R^{M^{\prime}\times N^{\prime} \times P^{\prime}}$为三维张量，其中每个切片矩阵$Y^p \in R^{M^{\prime}\times N^{\prime}}，$1&lt;=p&lt;=P；</li><li>卷积核：$W \in R^{m\times n \times D \times P}$为四维张量，其中每个切片矩阵$W^{p,d}\in R^{m \times n}$为一个两维卷积核，1&lt;=d&lt;=D，1&lt;=p&lt;=P;</li></ul><p>为了计算输出特征映射$Y^p$，用卷积核$W^{p,1},W^{p,2},…,W^{p,D}$分别对输入特征$X^1,X^2,…,X^D$进行卷积，然后将卷积结果相加，并加上一个标量偏置b得到卷积层的净输入$Z^P$，再经过非线性激活函数得到最终的输出特征映射$Y^p$。$$<br>Z^p = W^p \bigotimes X + b^p = \sum_{d=i}^D W^{p,d} \bigotimes X^d + b^p\\\\<br>Y^p = f(Z^p)<br>$$</p><p>其中，$W^p \in R^{m \times n \times D}$为三维卷积核，f(.)为非线性激活函数，一般用ReLU函数。整个计算的过程如下图所示。如果希望卷积层输出P个特征映射，可以将上述计算过程重复P次，得到P个输出特征映射，$Y^1,Y^2,…,Y^P$。</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/AE2h0aHFgl.jpg?imageslim" alt="mark"><br><br>    图片引自： <a href="https://nndl.github.io/" target="_blank" rel="external">https://nndl.github.io/</a> 卷积神经网络<br></div><p>在输入为$X \in R^{M \times N \times D}$，输出为$Y \in R^{M^{\prime}\times N^{\prime} \times P^{\prime}}$的卷积层中，每一个输入特征映射都需要D个滤波器以及一个偏置。假设每个滤波器的大小为$m \times n$，那么共需要$P \times D \times (m \times n)+P$个参数。</p><h3 id="Pooling层"><a href="#Pooling层" class="headerlink" title="Pooling层"></a>Pooling层</h3><p>汇聚层(Pooling Layer)，也叫子采样层(subsampling layer)，作用就是进行特征选择，降低特征数量，从而减少参数的数量。卷积层虽然可以减少网络中连接的数量，但特征映射组中的神经元个数没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合。为了解决这个问题，可以在卷积层之后加一个汇聚层，从而降低特征的维数，避免过拟合。</p><p>对于卷积层得到的一个特征映射$X^{(l)}$，我们可以将$X^{(l)}$划分为很多区域$R_K,k=1,2…,K$。区域$R_k$可以重叠，也可以不重叠，则采样层的输出有：$$<br>X^{(l+1)} = f(W^{(l+1)}\cdot down(R_k)+b^{(l+1)})<br>$$<br>其中，$w^{(l+1)}$和$b^{(l+1)}$分别是可训练的权重和偏置参数。于是，可以简化成如下公式：$$<br>X^{(l+1)} = f(W^{(l+1)}\cdot down(X^{(l)})+b^{(l+1)})<br>$$<br>其中，$down(X^{(l)})$是指子采样后的特征映射。</p><p>常见的采样方式如下：</p><ul><li><p>最大值采样（Maximum Pooling） </p><p>  $pool_{max}(R_k) = max_{i\in R_k}a_i$</p></li><li><p>最小值采样（Minimum Pooling）<br>  $pool_{min}(R_k) = min_{i\in R_k}a_i$</p></li><li><p>平均值采样（Average pooling）<br>  $pool_{ave}(R_k) = \frac {1}{|R_k|}\sum_{i\in R_k}^{|R_k|} a_i$</p></li><li><p>TopK采样</p><p>  $pool_k(R_k) = topk_{i \in R_k}a_i$</p></li></ul><p>下图为最大值采样的一个示例：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171223/c6AaDDg9d1.jpg?imageslim" alt="mark"><br><br><br></div><p>典型的汇聚层是将每个特征映射划分为2X2的大小的不重叠的区域，然后使用最大汇聚的方式进行下采样。汇聚层也可以看做是一个特殊的卷积层，卷积核的大小为$m\times m$，步长为$s \times s$，卷积核为max函数或者mean函数。过大的采样区域会急剧减少神经元的数量，造成过多的信息损失。</p><h2 id="卷积神经网络参数学习"><a href="#卷积神经网络参数学习" class="headerlink" title="卷积神经网络参数学习"></a>卷积神经网络参数学习</h2><p>在卷积神经网络中，参数为卷积核中的权重以及偏置。和前馈神经网络类似，卷积神经网络也可以通过误差反向传播算法来进行参数学习。</p><p>在全连接前馈神经网络中，梯度主要通过每一层的误差项$\delta$进行反向传播，并进一步计算每层参数的梯度。在卷积神经网络中，主要有两种不同功能的神经层：<strong>卷积层和汇聚层</strong>。 </p><p>对第l层为卷积层，第l-1层的输入特征为$X^{(l-1)} \in R^{M \times N \times D}$，通过卷积计算得到第l层的特征净输入为$Z^{(l)}\in R^{M^{\prime} \times N^{\prime} \times P}$，第l层的第p个特征净输入为：$$<br>Z^{(l,p)} = \sum_i^D W^{(l,p,d)} \bigotimes X^{(l-1,d)} + b^{(l,p)}<br>$$<br>其中，$W^{(l,p,d)}$和$b^{(l,p)}$为卷积核及偏置。第l层中共有$P \times D$个卷积核和P个偏置，可以分别使用链式法则来计算器梯度。$$<br>\frac {\partial L(Y,\hat{Y})}{\partial W^{(l,p,d)}} = \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l,p)}} \bigotimes X^{(l-1, d)}\\\\=\delta^{l,p} \bigotimes X^{(l-1,d)}<br>$$<br>其中，$\delta^{(l,p)}$为损失函数关于第l层的第p个特征映射净输入$Z^{(p,l)}$的偏导数。</p><p>同理可以得到，损失函数关于第l层第p个偏置$b^{(l,p)}$的梯度为：<br>$$<br>\frac {\partial L(Y,\hat {Y})}{\partial b^{(l,p)}} = \sum_{i,j}[\delta^{(l,p)}]_{i,j}.<br>$$<br>因此，卷积网络的每层参数的梯度也依赖于其所在层的误差项$\delta^{(l,p)}$。卷积层和汇聚层中，误差项的计算有所不同，因此，需要分别计算误差项。</p><p><strong>汇聚层</strong></p><p>当第l+1层为汇聚层时，因为汇聚层是下采样操作，l+1层的每个神经元的误差项$\delta$对应于第l层的相应特征的一个区域。l层的第p个特征映射中的每个神经元都有一条边和l+1层的第p个特征映射中的一个神经元相连。根据链式法则，第l层的一个特征映射的误差项$\delta^{(l,p)}$，只需要将l+1层对应的特征映射误差项$\delta^{(l+1, p)}$进行上采样，再和l层的特征映射的激活值偏导数逐元素相乘就得到了$\delta^{(l,p)}$。 </p><p>第l层的第p个特征映射的误差项$\delta^{(l,p)}$的具体推导如下：$$<br>\delta^{(l,p)} = \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l,p)}}\\\\<br>=\frac {\partial X^{(l,p)}}{\partial Z^{(l,p)}} \cdot \frac {\partial Z^{(l+1,p)}}{\partial X^{(l,p)}} \cdot frac {\partial L(Y,\hat {Y})}{\partial Z^{(l+1, p)}}\\\\=f_l^\prime(Z^{(l,p)}) \bigodot up(\delta^{(l+1, k)})<br>$$<br>其中，$f_l^\prime(Z^{(l,p)})$为第l层使用的激活函数导数，up为上采样函数，与汇聚层中使用的下采样操作刚好相反。如果下采样是最大汇聚，误差项$\delta^{(l+1, k)}$中的每个值将会直接传递到上一层对应区域中最大值所对应的神经元，该区域中其他神经元的误差项都设为0,。如果采用平均采样，误差项$\delta^{(l+1, k)}$中的每个值都会被平均分配到上一层对应的区域中的所有神经元上。</p><p><strong>卷积层</strong></p><p>当第l+1层为卷积层时，假设特征映射净输入为$Z^{(l+1)} \in R^{M^\prime \times N^\prime \times K}$，其中第k个特征映射的净输入为：$$<br>Z^{(l+1,k)} = \sum_{p=1}^P W^{(l+1,k,p)} \bigotimes X^{(l,p)} + b^{(l+1, k)}<br>$$</p><p>其中，$W^{(l+1,k,p)}$和$b^{(l+1, k)}$为第l+1层的卷积核及偏置。第l+1层中共有$K\times P$个卷积核和K个偏置。第l层的第p个特征映射为误差项$\delta^{(l,p)}$的具体推导如下：<br>$$<br>\delta^{(l,p)} = \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l,p)}}<br>\\\\<br>=\frac {\partial X^{(l,p)}}{\partial Z^{l,p}}\cdot \frac {\partial L(Y,\hat{Y})}{\partial X^{(l,p)}}\\\\=f_l^\prime(Z^{(l)}) \bigodot \sum_{k=1}^K (rot180(W^{(l+1,k,p)}) \hat{\bigotimes} \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l+1, k)}})\\\\=f_l^\prime(Z^{(l)}) \bigodot \sum_{k=1}^K(rot180(W^{(l+1,k,p)}) \hat{\bigotimes} \delta^{(l=1,k)})<br>$$</p><p>其中，$\hat{\bigotimes}$为宽卷积。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://nndl.github.io/" target="_blank" rel="external">https://nndl.github.io/</a> 卷积神经网络</li><li><a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">https://www.zybuluo.com/hanbingtao/note/485480</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是对近期学习的卷积神经网络相关知识的简单记录和梳理。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;卷积神经网络简介&quot;&gt;&lt;a href=&quot;#卷积神经网络简介&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络简介&quot;&gt;&lt;/a&gt;卷积神经网络简介&lt;/h2&gt;&lt;p&gt;卷积神经网络(Convolution Neural Network，CNN或ConvNet)是一种前馈神经网络。卷积神经网络是受生物学上&lt;strong&gt;感受野&lt;/strong&gt;(Receptive Field)的机制提出来的。一个神经元的感受野是指特定区域，只有这个区域内的刺激才能够激活该神经元。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://ilewseu.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="CNN" scheme="https://ilewseu.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>前馈神经网络反向传播推导</title>
    <link href="https://ilewseu.github.io/2017/12/17/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC/"/>
    <id>https://ilewseu.github.io/2017/12/17/前馈神经网络反向传播推导/</id>
    <published>2017-12-17T12:08:08.000Z</published>
    <updated>2017-12-24T09:03:31.194Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前馈神经网络前向传播"><a href="#前馈神经网络前向传播" class="headerlink" title="前馈神经网络前向传播"></a>前馈神经网络前向传播</h2><a id="more"></a><p>一个三层的前馈神经网络如下图所示：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/Jb3maf249i.png?imageslim" alt="mark"><br></div><br>对于第二层的输出$a_1^2,a_2^2,a_3^2$,有：<br>$$<br>a_1^2 = \sigma(z_1^2)=\sigma(w_{11}^2x_1+w_{12}^2x_2+w_{13}^2x_3+b_1^2)\\\\<br>a_2^2 = \sigma(z_2^2)=\sigma(w_{21}^2x_1+w_{22}^2x_2+w_{23}^2x_3+b_3^2)\\\\<br>a_3^2 = \sigma(z_3^2)=\sigma(w_{31}^2x_1+w_{32}^2x_2+w_{33}^2x_3+b_3^2)<br>$$<br>对于第三层的输出$a_1^3$，有：<br>$$a_1^3 = \sigma(z_1^3)=\sigma(w_{11}^3x_1+w_{12}^3x_2+w_{13}^3x_3+b_1^3)$$</p><p>用下面的符号来描述一个前馈神经网络：</p><ul><li>L:表示神经网络的层数；</li><li>$n^l$：表示第l层神经元的个数；</li><li>$f_l(.)$:表示第l层神经元的激活函数；</li><li>$W^{(l)}\in R^{n^l\times n^{l-1}}$:表示第l-1层到第l层的权重矩阵；</li><li>$b^{(l)\in R^{n^l}}$：表示第l-1层到第l层的偏置；</li><li>$z^{(l)\in R^{n^l}}$:表示第l层神经元的净输入；</li><li>$a^{(l)\in R^{n^l}}$:表示第l层神经元的输出（激活值）；</li><li>$W_{ij}^{(l)}$:表示第l-1层第j个输入到第l层第i个神经元的权重；</li></ul><p>前馈神经网络通过下面的公式进行信息传播：<br>$$<br>z^{(l)} = W^{(l)}\cdot a^{(l-1)} + b^{(l)} \\\\<br>a^{(l)} = f_l(z^{(l)})<br>$$<br>上面的公式可以合并为：$$<br>z^{(l)} = W^{(l)}\cdot f_{l-1}(z^{(l-1)}) + b^{(l)}<br>$$</p><p>这样，前馈神经网络可以通过逐层的信息传递，得到网络最后的输出$a^{(L)}$。整个网络可以看作一个复合函数$\phi(x;W,b)$,将输入x作为第1层的输入$a^{(0)}$，将第L层的输出作为$a^{(L)}$作为输出。<br>$$x=a^{(0)}\rightarrow z^{(l)} \rightarrow a^{(1)}\rightarrow z^{(2)}\rightarrow … \rightarrow a^{(L-1)} \rightarrow z^{(L)} \rightarrow a^{(L-1)}=\phi(x;W,b)$$</p><h2 id="反向传播推导"><a href="#反向传播推导" class="headerlink" title="反向传播推导"></a>反向传播推导</h2><p>假设损失函数是$L(y,\hat{y})$，对第l层中的参数$W^{(l)}$和$b^{(l)}$计算偏导数。因为$<br>\frac {\partial L(y,\hat{y})}{\partial W^{(l)}}$的计算涉及到矩阵的微分，十分繁琐，可以先计算偏导数$\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}}$。根据链式法则：$$<br>\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}} =\left(\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}}\right)^T\frac {\partial L(y,\hat{y})}{\partial z^{(l)}}   （1）<br>\\\\<br>\frac {\partial L(y,\hat{y})}{\partial b^{(l)}} =\left(\frac {\partial z^{(l)}}{\partial b^{(l)}}\right)^T\frac {\partial L(y,\hat{y})}{\partial z^{(l)}}     (2)<br>$$<br>公式(1)和(2)都是为目标函数关于第l层神经元$z^{(l)}$的偏导数，称为<strong>误差项</strong>，因此可以共用。我们只需要计算三个偏导数，分别为$\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}},\frac {\partial L(y,\hat{y})}{\partial b^{(l)}}和\frac {\partial L(y,\hat{y})}{\partial z^{(l)}}$</p><p><strong>1、计算偏导数$\frac {\partial z^{(l)} }{\partial W_{ij}^{(l)}}$</strong></p><p>因为$z^{(l)}和W_{ij}^{(l)}$的函数关系为$z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)}$，因此，偏导数：<br>$$<br>\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}}=\frac {\partial (W^{(l)}a^{(l-1)}+b^{(l)})}{\partial W_{ij}^{(l)}}= a_j^{(l-1)}<br>$$<br>其中，$W_{i:}^{(l)}$为权重矩阵$W^{(l)}$的第i行</p><p><strong>2、计算偏导数</strong>$\frac {\partial z^{l}}{\partial b^{(l)}}$</p><p>因为$z^{(l)}$和$b^{(l)}$的函数关系为$z^{(l)} = W^{(l)}a^{(l-1)}+b^{(l)}$，因此偏导数：<br>$$<br>\frac {\partial z^{l}}{\partial b^{(l)}}=I_{n^l}<br>$$<br>为$n^l\times n^l$的单位矩阵。</p><p><strong>3、计算偏导数</strong>$\frac {\partial L(y,\hat{y})}{\partial z{(l)}}$</p><p>用$\delta^{(l)}$来定义第l层的神经元误差项：<br>$$<br>\delta^{(l)} = \frac {\partial L(y,\hat{y})}{\partial z^{(l)}} \in R^{n^l}<br>$$<br>误差项$\delta^{l}$表示第l层的神经元对最终的误差的影响，也反映了最终的输出对第l层的神经元对最终误差的敏感程度。</p><p>根据$z^{(l+1)}=W^{(l+1)}a^{(l)} + b^{(l+1)}$，有：<br>$$<br>\frac {\partial z^{(l+1)}}{\partial a^{(l)}} = (W^{(l+1)})^T<br>$$<br>根据$a^{(l)}=f(z^{(l)})$，其中，$f_l(.)$为按位计算的函数，因此有：<br>$$<br>\frac {\partial a^{(l)}}{\partial z^{(l)}} = \frac {\partial f_l(z^{(l)})}{\partial z^{l}} = diag(f_l^{\prime}(z^{(l)}))<br>$$<br>因此，根据链式法则，第l层的误差项为：<br>$$<br>\delta^{(l)} = \frac {\partial L(y, \hat{y})}{\partial z^{(l)}}\\\\<br>=\frac {\partial a^{(l)}}{\partial z^{(l)}}\cdot\frac{\partial z^{(l+1)}}{\partial a^{(l)}}\cdot \frac {\partial L(y, \hat{y})}{\partial z^{(l+1)}}\\\\=diag(f_l^{\prime}(z^{(l)}))\bigodot ((W^{(l+1)})^T\delta^{(l+1)})  (3)<br>$$<br>其中，$\bigodot$是向量的点积运算，表示每个元素相乘。</p><p>从公式3可以看出，第l层的误差项可以通过第l+1层的误差项计算，这就是反向传播。<strong>反向传播算法的含义是：第l层的一个神经元的误差项是所有与该神经元相连的第l+1层神经元误差项的权重和，然后再乘上该神经元激活函数的梯度。</strong></p><p>在计算出三个偏导数之后，可以得到最终的偏导数：<br>$$<br>\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}} = \delta_i^{(l)}a_j^{(l-1)}<br>$$<br>进一步，$L(y,\hat{y})$关于第l层的权重$W^{(l)}$的梯度为：<br>$$<br>\frac {\partial L(y,\hat{y})}{\partial W^{(l)}} = \delta^{(l)}a^{(l-1)}<br>$$<br>同理可得，$L(y,\hat{y})$关于第l层偏置$b^(l)$的梯度为：<br>$$<br>\frac {\partial L(y,\hat{y})}{\partial b^{(l)}} = \delta^{(l)}<br>$$<br>基于随机梯度下降的反向传播算法如下：</p><p><div><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/Dg5jm8Hjg0.jpg?imageslim" alt="mark"><br></div><br>图片引自：<a href="https://github.com/nndl/nndl.github.io，chap-前馈神经网络.pdf。" target="_blank" rel="external">https://github.com/nndl/nndl.github.io，chap-前馈神经网络.pdf。</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前馈神经网络前向传播&quot;&gt;&lt;a href=&quot;#前馈神经网络前向传播&quot; class=&quot;headerlink&quot; title=&quot;前馈神经网络前向传播&quot;&gt;&lt;/a&gt;前馈神经网络前向传播&lt;/h2&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>CNN学习的相关资料</title>
    <link href="https://ilewseu.github.io/2017/12/17/CNN%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E5%8D%9A%E5%AE%A2/"/>
    <id>https://ilewseu.github.io/2017/12/17/CNN学习的相关博客/</id>
    <published>2017-12-17T11:48:20.000Z</published>
    <updated>2017-12-17T06:27:08.258Z</updated>
    
    <content type="html"><![CDATA[<h2 id="对卷积的理解"><a href="#对卷积的理解" class="headerlink" title="对卷积的理解"></a>对卷积的理解</h2><ul><li><a href="http://mengqi92.github.io/2015/10/06/convolution/" target="_blank" rel="external">http://mengqi92.github.io/2015/10/06/convolution/</a></li><li><a href="http://www.cnblogs.com/freeblues/p/5738987.html" target="_blank" rel="external">http://www.cnblogs.com/freeblues/p/5738987.html</a></li><li><a href="http://blog.csdn.net/bitcarmanlee/article/details/54729807" target="_blank" rel="external">http://blog.csdn.net/bitcarmanlee/article/details/54729807</a><a id="more"></a></li></ul><h2 id="卷积神经网络的原理及推导"><a href="#卷积神经网络的原理及推导" class="headerlink" title="卷积神经网络的原理及推导"></a>卷积神经网络的原理及推导</h2><ul><li>反向传播的推导：<a href="https://zhuanlan.zhihu.com/p/22473137" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/22473137</a></li><li>pinard:<a href="http://www.cnblogs.com/pinard/p/6483207.html" target="_blank" rel="external">http://www.cnblogs.com/pinard/p/6483207.html</a></li><li>zybuluo:<a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">https://www.zybuluo.com/hanbingtao/note/485480</a></li><li>一文读懂卷积神经网络CNN：<a href="http://www.sohu.com/a/126742834_473283" target="_blank" rel="external">http://www.sohu.com/a/126742834_473283</a></li><li>CS231N翻译：<a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/21930884</a></li><li>charlote:<a href="http://www.cnblogs.com/charlotte77/p/7759802.html" target="_blank" rel="external">http://www.cnblogs.com/charlotte77/p/7759802.html</a></li><li>卷积神经网络全面解析:<a href="http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi" target="_blank" rel="external">http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi</a></li><li>tornadomeet:<a href="http://www.cnblogs.com/tornadomeet/p/3468450.html" target="_blank" rel="external">http://www.cnblogs.com/tornadomeet/p/3468450.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;对卷积的理解&quot;&gt;&lt;a href=&quot;#对卷积的理解&quot; class=&quot;headerlink&quot; title=&quot;对卷积的理解&quot;&gt;&lt;/a&gt;对卷积的理解&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://mengqi92.github.io/2015/10/06/convolution/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://mengqi92.github.io/2015/10/06/convolution/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/freeblues/p/5738987.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cnblogs.com/freeblues/p/5738987.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/bitcarmanlee/article/details/54729807&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/bitcarmanlee/article/details/54729807&lt;/a&gt;
    
    </summary>
    
      <category term="记录" scheme="https://ilewseu.github.io/categories/%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="CNN" scheme="https://ilewseu.github.io/tags/CNN/"/>
    
      <category term="资料" scheme="https://ilewseu.github.io/tags/%E8%B5%84%E6%96%99/"/>
    
  </entry>
  
  <entry>
    <title>CS231n课程笔记(9)  ConvNet notes</title>
    <link href="https://ilewseu.github.io/2017/12/16/CS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC9%E8%AF%BE-ConvNet-notes/"/>
    <id>https://ilewseu.github.io/2017/12/16/CS231n课程笔记-第9课-ConvNet-notes/</id>
    <published>2017-12-16T14:17:23.000Z</published>
    <updated>2017-12-16T16:53:04.888Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转载自：<a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit，原文为：http://cs231n.github.io/convolutional-networks/，并进行一定的修改。" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit，原文为：http://cs231n.github.io/convolutional-networks/，并进行一定的修改。</a></p></blockquote><p><strong>目录</strong><br><a id="more"></a></p><ul><li>结构概述</li><li>用来构建卷积神经网络的各种层<ul><li>卷积层</li><li>汇聚层</li><li>归一化层</li><li>全连接层</li><li>将全连接层转化成卷积层</li></ul></li><li>卷积神经网络的结构<ul><li>层的排列规律</li><li>层的尺寸设置规律</li><li>案例学习（LeNet/AlexNet/ZFNet/GoogLeNet/VGGNet）</li><li>计算上的考量</li></ul></li><li>拓展资源</li></ul><h2 id="卷积神经网络-CNNs-ConvNets"><a href="#卷积神经网络-CNNs-ConvNets" class="headerlink" title="卷积神经网络(CNNs/ConvNets)"></a>卷积神经网络(CNNs/ConvNets)</h2><p>卷积神经网络和上一章讲的常规神经网络非常相似：它们都是由神经元组成，神经元中有局域学习能力的权重和偏差。每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出的是不同类别的评分。在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或者softmax）,并且在神经网络中我们的各种技巧和要点依旧适用于卷积神经网络。</p><p>那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。</p><h2 id="结构概述"><a href="#结构概述" class="headerlink" title="结构概述"></a>结构概述</h2><p>回顾：常规神经网络。在上一章中，神经网络的输入是一个向量，然后在一些列的隐层中对它做变换。每个隐层是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接。但是在一个隐藏层中，神经元相互独立不进行任何连接。最后的全连接层被称为“输出层”，在分类问题中，它输出的值被看做是不同类别的评分值。</p><p>常规神经网络对于大尺寸图像效果不尽人意。在CIFAR-10中，图像的尺寸是32x32x3（宽高均为32像素，3个颜色通道），因此，对应的的常规神经网络的第一个隐层中，每一个单独的全连接神经元就有32x32x3=3072个权重。这个数量看起来还可以接受，但是很显然这个全连接的结构不适用于更大尺寸的图像。举例说来，一个尺寸为200x200x3的图像，会让神经元包含200x200x3=120,000个权重值。而网络中肯定不止一个神经元，那么参数的量就会快速增加！显而易见，这种全连接方式效率低下，大量的参数也很快会导致网络过拟合。</p><p><strong>神经元的三维排列</strong>。卷积神经网络针对输入全部是图像的情况，将结构调整得更加合理，获得了不小的优势。与常规神经网络不同，卷积神经网络的各层中的神经元是3维排列的：<strong>宽度、高度和深度</strong>（这里的<strong>深度</strong>指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）。举个例子，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是32x32x3（宽度，高度和深度）。我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的。下面是例子：</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/aBcKm9G2Lf.jpg?imageslim" alt="mark"><br></div>左边是一个3层的神经网络。右边是一个卷积神经网络，图例中网络将它的神经元都排列成3个维度（宽、高和深度）。卷积神经网络的每一层都将3D的输入数据变化为神经元3D的激活数据并输出。在这个例子中，红色的输入层装的是图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是3（代表了红、绿、蓝3种颜色通道）。<br><div></div><h2 id="用来构建卷积神经网络的各种层"><a href="#用来构建卷积神经网络的各种层" class="headerlink" title="用来构建卷积神经网络的各种层"></a>用来构建卷积神经网络的各种层</h2><p>一个简单的卷积神经网是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。卷积神经网络主要由三种类型的层构成：卷积层、汇聚层和全连接层（全连接层和常规的神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。</p><p>网络结构的例子：这仅仅是一个概述，下面会有更详细的介绍。一个用于CIFAP-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下：</p><ul><li>输入[32*3]存有图像的原始像素值，本例中，图像宽高均为32，有3个颜色通道。</li><li>卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积。卷积层会计算所有的神经元的输出。如果我们使用12个滤波器（也叫核），得到的输出数据体的维度就是[32<em>32</em>12]。</li><li>ReLU层将会逐个元素地进行激活函数操作，比如使用以0为阈值的max(0,x)作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。</li><li>汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。</li><li>全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。</li></ul><p>由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值。其中有的层含有参数，有的没有。具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。</p><p><strong>小结</strong></p><ul><li>简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）。</li><li>卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）。</li><li>每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据。</li><li>有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。</li><li>有的层有额外的参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）。</li></ul><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层是构建卷积神经网络的核心层，它产生了网络中的大部分的计算量。</p><p><strong>概述和直观介绍</strong>：首先讨论的是，在没有大脑和生物意义上的神经元之类的比喻下，卷积层到底在计算什么。<strong>卷积层的参数是有一些可学习的滤波器集合构成的。</strong>每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。<strong>直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案。</strong></p><p>在每个卷积层上，我们会有一整个集合的滤波器（比如12个），每个都会生成一个不同的二维激活图。将这些激活映射在深度方向上层叠起来就生成了输出数据。</p><p><strong>以大脑做比喻</strong>：如果你喜欢用大脑和生物神经元来做比喻，那么输出的3D数据中的每个数据项可以被看做是神经元的一个输出，而该神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。现在开始讨论神经元的连接，它们在空间中的排列，以及它们参数共享的模式。</p><p><strong>局部连接</strong>：在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。</p><p>例1：假设输入的数据体尺寸为<a href="比如CIFAR-10的RGB图像">32x32x3</a>，如果滤波器是5x5，那么卷积层中的每个神经元有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。</p><p>例2：假设输入数据体的尺寸是[16x16x20]，感受野尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20=180个连接。再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）。</p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/F4hfh6IIBE.jpg?imageslim" alt="mark"><br></div><br><strong>左边</strong>：红色的是输入数据体（比如CIFAR-10中的图像），蓝色的部分是第一个卷积层中的神经元。卷积层中的每个神经元都只是与输入数据体的一个局部在空间上相连，但是与输入数据体的所有深度维度全部相连（所有颜色通道）。在深度方向上有多个神经元（本例中5个），它们都接受输入数据的同一块区域（感受野相同）。至于深度列的讨论在下文中有。<br><br><strong>右边</strong>：神经网络章节中介绍的神经元保持不变，它们还是计算权重和输入的内积，然后进行激活函数运算，只是它们的连接被限制在一个局部空间。<br><br><strong>空间排列</strong>：上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及他们的排列方式。3个超参数控制着输出数据体的尺寸：深度(depth)、步长(stride)和零填充(zero-padding)。下面是对它们的讨论：<br><br>1. 首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界、或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为<strong>深度列</strong>，也有人使用纤维(fibre)来称呼它们。<br>2. 其次，在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。<br>3. 在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的。这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。<br><br>输出数据体在空间上的尺寸可以通过输入数据体尺寸(W)，卷积层中神经元的感受野尺寸(F)、步长(S)和零填充的数量(P)的函数来计算（假设输入数组的空间形状是正方形，即高度和宽度相等）输出数据体的空间尺寸为(W-F+2P)/S+1。比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出。如果步长为2，输出就是3x3。下面是例子：<br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/6fKc6kceA4.jpg?imageslim" alt="mark"><br></div><br>空间排列的图示。在本例中只有一个空间维度（x轴），神经元的感受野尺寸F=3，输入尺寸W=5，零填充P=1。左边：神经元使用的步长S=1，所以输出尺寸是(5-3+2)/1+1=5。右边：神经元的步长S=2，则输出尺寸是(5-3+2)/2+1=3。注意当步长S=3时是无法使用的，因为它无法整齐地穿过数据体。从等式上来说，因为(5-3+2)=4是不能被3整除的。<br><br>本例中，神经元的权重是[1,0,-1]，显示在图的右上角，偏差值为0。这些权重是被所有黄色的神经元共享的（参数共享的内容看下文相关内容）。<br><br><strong>使用零值填充</strong>：在上面的左边的例子中，注意输入维度是5，输出维度也是5。之所以如此，是因为感受野是3并且使用了1的零填充。如果不使用零填充，则输出数据体的空间维度就只有3，因为这就是滤波器整齐滑过并覆盖原始数据需要的数目。一般来说，当步长S=1时，零填充的值是P=（F-1）/2，这样就能保证输入和输出数据体有相同的空间尺寸。这样做非常常见，在介绍卷积神经网络的结构的时候我们会详细讨论其原因。<br><br><strong>步长的限制</strong>：注意这些空间排列的超参数之间是相互限制的。举例说来，当输入尺寸W=10，不使用零填充则P=0，滤波器尺寸F=3，这样步长S=2就行不通，因为(W-F+2P)/S+1=(10-3+0)/2+1=4.5，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体。因此，这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，或者修改零填充值来让设置合理，或者修改输入数据体尺寸来让设置合理，或者其他什么措施。在后面的卷积神经网络结构小节中，读者可以看到合理地设置网络的尺寸让所有的维度都能正常工作，这件事可是相当让人头痛的。而使用零填充和遵守其他一些设计策略将会有效解决这个问题。<br><br>真实案例：Krizhevsky构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227x227x3]。在第一个卷积层，神经元使用的感受野尺寸F=11，步长S=4，不使用零填充P=0。因为(227-11)/4+1=55，卷积层的深度K=96，则卷积层的输出数据体尺寸为[55x55x96]。55x55x96个神经元中，每个都和输入数据体中一个尺寸为[11x11x3]的区域全连接。在深度列上的96个神经元都是与输入数据体中同一个[11x11x3]区域连接，但是权重不同。有一个有趣的细节，在原论文中，说的输入图像尺寸是224x224，这是肯定错误的，因为(224-11)/4+1的结果不是整数。这件事在卷积神经网络的历史上让很多人迷惑，而这个错误到底是怎么发生的没人知道。我的猜测是Alex忘记在论文中指出自己使用了尺寸为3的额外的零填充。<br><br><strong>参数共享</strong>=290,400个神经元，每个有11x11x3=364个参数和1个偏差。将这些合起来就是290400x364：在卷积层中使用参数共享是用来控制参数的数量。就用上面的例子，在第一个卷积层就有55x55x96=105,705,600个参数。单单第一层就有这么多参数，显然这个数目是非常大的。<br>作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量。换言之，就是将深度维度上一个单独的2维切片看做深度切片（depth slice），比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]。在每个深度切片上的神经元都使用同样的权重和偏差。在这样的参数共享下，例子中的第一个卷积层就只有96个不同的权重集了，一个权重集对应一个深度切片，共有96x11x11x3=34,848个不同的权重，或34,944个参数（+96个偏差）。在每个深度切片中的55x55个权重使用的都是同样的参数。在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。<br><br>注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的卷积（这就是“卷积层”名字由来）。这也是为什么总是将这些权重集合称为滤波器（filter）（或卷积核（kernel）），因为它们和输入进行了卷积。<br><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/3mG935H4Ie.jpg?imageslim" alt="mark"><br></div><br>Krizhevsky等学习到的滤波器例子。这96个滤波器尺寸都是[11x11x3]，在一个深度切片中，每个滤波器都被55x55个神经元共享。注意参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也同样是有用的，这是因为图像结构具有平移不变性。所以，在卷积层的输出数据体的55x55个不同位置中，就没有必要重新学习探测一个水平边界了。<br><br>注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为<strong>局部连接层（Locally-Connected Layer）</strong>。<br><br>Numpy的例子：为了让讨论更加的具体，我们用代码来展示上述思路。假设输入数据体是numpy数组X。那么：一个位于(x,y)的深度列，将会是X[x,y,:]；在深度为d处的切片，或激活图应该是X[:,:,d]。<br>卷积层例子：假设输入数据体的尺寸X.shape:(11,11,4)，不使用零填充（P=0），滤波器的尺寸是F=5，步长S=2。那么输出数据体的空间尺寸就是(11-5)/2+1=4，即输出数据体的宽度和高度都是4。那么在输出数据体中的激活映射（称其为V）看起来就是下面这样（在这个例子中，只有部分元素被计算）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0</div><div class="line">V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0</div><div class="line">V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0</div><div class="line">V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</div></pre></td></tr></table></figure><br><br>在numpy中，<em>操作是进行数组间的逐元素相乘。权重向量W0是该神经元的权重，b0是其偏差。在这里，W0被假设尺寸是W0.shape: (5,5,4)，因为滤波器的宽高是5，输入数据量的深度是4。注意在每一个点，计算点积的方式和之前的常规神经网络是一样的。同时，计算内积的时候使用的是同一个权重和偏差（因为参数共享），在宽度方向的数字每次上升2（因为步长为2）。要构建输出数据体中的第二张激活图，代码应该是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1</div><div class="line">V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1</div><div class="line">V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1</div><div class="line">V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</div><div class="line">V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1 （在y方向上）</div><div class="line">V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1 （或两个方向上同时）</div></pre></td></tr></table></figure><br><br>我们访问的是V的深度维度上的第二层（即index1），因为是在计算第二个激活图，所以这次试用的参数集就是W1了。在上面的例子中，为了简洁略去了卷积层对于输出数组V中其他部分的操作。还有，要记得这些卷积操作通常后面接的是ReLU层，对激活图中的每个元素做激活函数运算，这里没有显示。<br><br><strong>小结</strong><br><br>- 输入数据体的尺寸为：$W_1xH_1xD_1$<br>- 4个超参数:滤波器的数量K;滤波器的空间尺寸F;步长S;零填充数量P<br>- 输出数据的尺寸为：$W_2xH_2xD_2$，其中：<br>    - $W_2 = (W_1 - F + 2P)/S+1$;<br>    - $H_2 = (H_1 - F + 2P)/S+1$(宽度和高度的计算方法相同);<br>    - $D_2=K$<br>-  由于参数共享，每个滤波器包含$F\cdot F\cdot D_1$个权重，卷积层一共有$F\cdot F\cdot D_1\cdot K$个权重和K个偏置。<br>-  在输出数据体中，第d个深度的切片(空间尺寸是$W_2xH_2$)，用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长S），最后在加上第d个偏差。<br>对这些超参数，常见的设置是F=2，S=1，P=1。同时设置这些超参数也有一些约定俗成的惯例和经验，可以在下面的卷积神经网络结构章节中查看。<br><br><em>*卷积层演示</em></em>：下面是一个卷积层的运行演示。因为3D数据难以可视化，所以所有的数据（输入数据体是蓝色，权重数据体是红色，输出数据体是绿色）都采取将深度切片按照列的方式排列展现。输入数据体的尺寸是$W_1=5,H_1=5,D_1=3$，卷积层参数K=2,F=3,S=2,P=1。就是说，有2个滤波器，滤波器的尺寸是$3\cdot 3$，它们的步长是2.因此，输出数据体的空间尺寸是(5-3+2)/2+1=3。注意输入数据体使用了零填充P=1，所以输入数据体外边缘一圈都是0。下面的例子在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来。<br><div><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/jmKmGIi9Ja.jpg?imageslim" alt="mark"><br></div><p><strong>矩阵乘法实现</strong>：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法：</p><ol><li>输入图像的局部区域被im2col操作拉伸为列。比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3=363的列向量。重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)/4+1=55，所以得到im2col操作的输出矩阵X_col的尺寸是[363x3025]，其中每列是拉伸的感受野，共有55x55=3,025个。注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。</li><li>卷积层的权重也同样被拉伸成行。举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵W_row，尺寸为[96x363]。</li><li>现在卷积的结果和进行一个大矩阵乘np.dot(W_row, X_col)是等价的了，能得到每个滤波器和每个感受野间的点积。在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出。</li><li>结果最后必须被重新变为合理的输出尺寸[55x55x96]。</li></ol><p>这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在X_col中被复制了多次。但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的BLAS API）。还有，同样的im2col思路可以用在汇聚操作中。<br><strong>反向传播</strong>：卷积操作的反向传播（同时对于数据和权重）还是一个卷积（但是是和空间上翻转的滤波器）。使用一个1维的例子比较容易演示。</p><p><strong>1x1卷积</strong>：一些论文中使用了1x1的卷积，这个方法最早是在论文Network in Network中出现。人们刚开始看见这个1x1卷积的时候比较困惑，尤其是那些具有信号处理专业背景的人。因为信号是2维的，所以1x1卷积就没有意义。但是，在卷积神经网络中不是这样，因为这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）。</p><p><strong>扩张卷积：</strong>最近一个研究（Fisher Yu和Vladlen Koltun的论文）给卷积层引入了一个新的叫扩张（dilation）的超参数。到目前为止，我们只讨论了卷积层滤波器是连续的情况。但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张。举例，在某个维度上滤波器w的尺寸是3，那么计算输入x的方式是：w[0]<em>x[0] + w[1]</em>x[1] + w[2]<em>x[2]，此时扩张为0。如果扩张为1，那么计算为： w[0]</em>x[0] + w[1]<em>x[2] + w[2]</em>x[4]。换句话说，操作中存在1的间隙。在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为在很少的层数内更快地汇集输入图片的大尺度特征。比如，如果上下重叠2个3x3的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中5x5的区域（可以成这些神经元的有效感受野是5x5）。如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长。</p><h3 id="汇聚层"><a href="#汇聚层" class="headerlink" title="汇聚层"></a>汇聚层</h3><p>通常，在连续的卷积层之间会周期性地插入一个汇聚层。**它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。汇聚层的一些公式：</p><ul><li>输入数据体尺寸$W_1\cdot H_1\cdot D_1$</li><li>有两个超参数：<ul><li>空间大小F</li><li>步长S</li></ul></li><li>输出数据体尺寸$W_2\cdot H_2\cdot D_2$，其中<ul><li>$W_2=(W_1-F)/S+1$</li><li>$H_2=(H_1-F)/S+1$</li><li>$D_2=D_1$</li></ul></li><li>因为对输入进行的是固定函数计算，所以没有引入参数。</li><li>在汇聚层中很少使用零填充。</li></ul><p>在实践中，最大汇聚层通常只有两种形式：一种是F=3,S=2，也叫重叠汇聚（overlapping pooling），另一个更常用的是F=2,S=2。对更大感受野进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性。</p><p><strong>普通汇聚（General Pooling）</strong>：除了最大汇聚，汇聚单元还可以使用其他的函数，比如平均汇聚（average pooling）或L-2范式汇聚（L2-norm pooling）。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。</p><p><div><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171216/6Ca9idlJK8.jpg?imageslim" alt="mark"><br></div><br>汇聚层在输入数据体的每个深度切片上，独立地对其进行空间上的降采样。左边：本例中，输入数据体尺寸[224x224x64]被降采样到了[112x112x64]，采取的滤波器尺寸是2，步长为2，而深度不变。右边：最常用的降采样操作是取最大值，也就是最大汇聚，这里步长为2，每个取最大值操作是从4个数字中选取（即2x2的方块区域中）。</p><p><strong>反向传播</strong>：回顾一下反向传播的内容，其中max(x,y)函数的反向传播可以简单理解为将梯度只沿着最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大的索引记录下来，这样在反向传播的时候梯度的路由就很高效。</p><p><strong>不使用汇聚层</strong>：很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。</p><h3 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h3><p>在卷积神经网络中的结果，提出很多不同类型的归一化层，有时候是为了实现生物大脑中观测到的抑制机制。但是，这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极有限的。对于不同类型的归一化层，可以看看Alex Krizhevsky的关于cuda-convnet library API的讨论。</p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。</p><h3 id="把全连接层转化层卷积层"><a href="#把全连接层转化层卷积层" class="headerlink" title="把全连接层转化层卷积层"></a>把全连接层转化层卷积层</h3><p>全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所有它们的函数形式是一样的。因此，将此两者相互转化是可能的：</p><ul><li>对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块（这是因为有局部连接），其余部分都是零。而在其中大部分块中，元素都是相等的（因为参数共享）。</li><li>相反，任何全连接层都可以被转化为卷积层。比如，一个K=4096的全连接层，输入数据体的尺寸是$7\times 7\times 512$，这个全连接层可以被等效地看做一个F=7,P=0,S=1,K=4096的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成$1\times 1\times$ 4096，这个结果就和使用初始的那个全连接层一样了。</li></ul><p><strong>全连接层转化为卷积层：</strong>在这两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是$224\times224\times3$的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：</p><ul><li>针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。</li><li>针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。</li><li>对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]。</li></ul><p>实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（译者注：即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。</p><p>举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！</p><blockquote><p>面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。</p></blockquote><p>自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。</p><p>最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。</p><h2 id="卷积神经网络的结构"><a href="#卷积神经网络的结构" class="headerlink" title="卷积神经网络的结构"></a>卷积神经网络的结构</h2><p>卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。</p><h3 id="层的排列规律"><a href="#层的排列规律" class="headerlink" title="层的排列规律"></a>层的排列规律</h3><p>卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：<br><strong>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC</strong><br>其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &gt;=0,通常N&lt;=3,M&gt;=0,K&gt;=0,通常K&lt;3。例如，下面是一些常见的网络结构规律：</p><ul><li>INPUT -&gt; FC,实现一个线性分类器，此处N = M = K = 0。</li><li>INPUT -&gt; CONV -&gt; RELU -&gt; FC</li><li>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; F- C。此处在每个汇聚层之间有一个卷积层。</li><li>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]<em>3 -&gt; [FC -&gt; RELU]</em>2 -&gt; FC。此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。</li></ul><p><strong>几个小滤波器卷积层的组合比一个大滤波器卷积层好</strong>：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7，但是就有一些缺点。首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有C个通道，那么单独的7x7卷积层将会包含C\times (7\times 7\times C)=49C^2个参数，而3个3x3的卷积层的组合仅有3\times (C\times (3\times 3\times C))=27C^2个参数。直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。</p><p>最新进展：传统的将层按照线性进行排列的方法已经受到了挑战，挑战来自谷歌的Inception结构和微软亚洲研究院的残差网络（Residual Net）结构。这两个网络（下文案例学习小节中有细节）的特征更加复杂，连接结构也不同。</p><h3 id="层的尺寸设置规律"><a href="#层的尺寸设置规律" class="headerlink" title="层的尺寸设置规律"></a>层的尺寸设置规律</h3><p>到现在为止，我们都没有提及卷积神经网络中每层的超参数的使用。现在先介绍设置结构尺寸的一般性规则，然后根据这些规则进行讨论：<br><strong>输入层</strong><br>（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。</p><p><strong>卷积层</strong><br>应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长S=1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当F=3，那就使用P=1来保持输入尺寸。当F=5,P=2，一般对于任意F，当P=(F-1)/2的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上。</p><p><strong>汇聚层</strong><br>负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野（即F=2）的最大值汇聚，步长为2（S=2）。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。</p><p><strong>减少尺寸设置的问题</strong><br>上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。</p><p><strong>为什么在卷积层使用1的步长？</strong><br>在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。</p><p><strong>为何使用零填充？</strong><br>使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。</p><p><strong>因为内存限制所做的妥协</strong>：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。</p><h3 id="案例学习"><a href="#案例学习" class="headerlink" title="案例学习"></a>案例学习</h3><p>下面是卷积神经网络领域中比较有名的几种结构：</p><ul><li>LeNet： 第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的。当然，最著名还是被应用在识别数字和邮政编码等的LeNet结构。<br>AlexNet：AlexNet卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的ImageNet ILSVRC 竞赛中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。</li><li>ZF Net：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为 ZFNet（Zeiler &amp; Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。</li><li>GoogLeNet：ILSVRC 2014的胜利者是谷歌的Szeged等实现的卷积神经网络。它主要的贡献就是实现了一个奠基模块，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了。GooLeNet还有几种改进的版本，最新的一个是Inception-v4。</li><li>VGGNet：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为VGGNet。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的预训练模型是可以在网络上获得并在Caffe中使用的。VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。</li><li>ResNet：残差网络（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的跳跃链接，大量使用了批量归一化（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲（视频，PPT），以及一些使用Torch重现网络的实验。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，可以看论文Identity Mappings in Deep Residual Networks，2016年3月发表。</li></ul><h3 id="计算上的考量"><a href="#计算上的考量" class="headerlink" title="计算上的考量"></a>计算上的考量</h3><p>在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈。大部分现代GPU的内存是3/4/6GB，最好的GPU大约有12GB的内存。要注意三种内存占用来源：</p><ul><li><strong>来自中间数据体尺寸</strong>：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）。通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）。在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到。但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量。</li><li><strong>来自参数尺寸</strong>：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存。因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多。</li><li>卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等。</li></ul><p>一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位。把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量。如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了。</p><h2 id="拓展资源"><a href="#拓展资源" class="headerlink" title="拓展资源"></a>拓展资源</h2><p>和实践相关的拓展资源：</p><ul><li>Soumith benchmarks for CONV performance</li><li>ConvNetJS CIFAR-10 demo 可以让你在服务器上实时地调试卷积神经网络的结构，观察计算结果。</li><li>Caffe，一个流行的卷积神经网络库。</li><li>State of the art ResNets in Torch7</li></ul><p>———————- end —————————</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文转载自：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit，原文为：http://cs231n.github.io/convolutional-networks/，并进行一定的修改。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit，原文为：http://cs231n.github.io/convolutional-networks/，并进行一定的修改。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="课程笔记" scheme="https://ilewseu.github.io/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="cs231n" scheme="https://ilewseu.github.io/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>CS231n课程笔记(8)  Neural Nets Notes 3</title>
    <link href="https://ilewseu.github.io/2017/12/13/CS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC8%E8%AF%BE-Neural-Nets-Notes-3/"/>
    <id>https://ilewseu.github.io/2017/12/13/CS231n课程笔记-第8课-Neural-Nets-Notes-3/</id>
    <published>2017-12-13T14:17:23.000Z</published>
    <updated>2017-12-16T16:07:10.963Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转载自：<a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit，原文为：http://cs231n.github.io/neural-networks-3/，并进行一定的修改。" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit，原文为：http://cs231n.github.io/neural-networks-3/，并进行一定的修改。</a></p></blockquote><p><strong>目录</strong><br><a id="more"></a></p><ul><li>梯度检查</li><li>合理性（Sanity）检查</li><li>检查学习过程<ul><li>损失函数</li><li>训练集与验证集准确率</li><li>权重：更新比例</li><li>每层的激活数据与梯度分布</li><li>可视化</li></ul></li><li>参数更新<ul><li>一阶（随机梯度下降）方法，动量方法，Nestrov动量方法</li><li>学习率退火</li><li>二阶方法</li><li>逐参数适应学习率方法（Adagrad，RMSProp）</li></ul></li><li>参数调优</li><li>评价<ul><li>模型集成</li></ul></li><li>总结</li></ul><h2 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h2><p>理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较。然而从实际操作层面上来说，这个过程更加复杂且容易出错。下面是一些提示、技巧和需要仔细注意的事情。<br><strong>使用中心化公式</strong>，在使用有限差值近似来计算数值梯度的时候，常见的公式是：$$<br>\frac {df(x)}{dx}=\frac {f(x+h) - f(x)}{h}(bad, do not use)<br>$$<br>其中，h是一个很小的数字，在实践中，近似为1e-5。在实践中证明，使用中心化公式效果更好：$$<br>\frac {df(x)}{dx}=\frac {f(x+h) - f(x-h)}{2h}(use instead)<br>$$<br>该公式在检查梯度的每个维度的时候，会要求计算两次损失函数（所以计算资源的耗费也是两倍），但是梯度的近似值会准确很多。要理解这一点，对f(x+h)和f(x-h)使用泰勒展开，可以看到第一个公式的误差近似O(h)，第二个公式的误差近似$O(h^2)$（是个二阶近似）。</p><p><strong>使用相对误差来比较，</strong>比较数值梯度$f_n^’$和解析梯度$f_a^’$的细节有哪些？如何得知此两者不匹配？你可能会倾向于监测它们的差的绝对值$|f_a^’-f_n^’|$或者差的平方值，然后定义该值如果超过某个规定阈值，就判断梯度实现失败。然而该思路是有问题的。想想，假设这个差值是1e-4，如果两个梯度值在1.0左右，这个差值看起来就很合适，可以认为两个梯度是匹配的。然而如果梯度值是1e-5或者更低，那么1e-4就是非常大的差距，梯度实现肯定就是失败的了。因此，使用相对误差总是更合适一些：$$<br>\frac {|f_a^’-f_n^’|}{max(|f_a^’|,|f_n^’|)}<br>$$<br>上式考虑了差值占两个梯度绝对值的比例。注意通常相对误差公式只包含两个式子中的一个（任意一个均可），但是我更倾向取两个式子的最大值或者取两个式子的和。这样做是为了防止在其中一个式子为0时，公式分母为0（这种情况，在ReLU中是经常发生的）。然而，还必须注意两个式子都为零且通过梯度检查的情况。在实践中：</p><ul><li>相对误差&gt;1e-2:通常就意味着梯度可能出错;</li><li>1e-2&gt;相对误差&gt;1e-4:要对这个值感到不舒服才行;</li><li>1e-4&gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高;</li><li>1e-7或者更小：好结果，可以高兴一把了。</li></ul><p>要知道的是网络的深度越深，相对误差就越高。所以，如果你是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。相反，如果一个可微函数的相对误差值是1e-2，那么通常说明梯度实现不正确。</p><p><strong>使用双精度</strong>：一个常见的错误是使用单精度浮点数来进行梯度检查，这样会导致即使梯度实现正确，相对误差值也会很高。在我的经验而言，出现过使用单精度浮点数时相对误差为1e-2，换成双精度浮点数时，就降低为1e-8的情况。</p><p><strong>保持在浮点数的有效范围</strong>，建议通读《What Every Computer Scientist Should Konw About Floating-Point Artthmetic》一文，该文将阐明你可能犯的错误，促使你写下更加细心的代码。例如，在神经网络中，在一个批量的数据上对损失函数进行归一化是很常见的。但是，如果每个数据点的梯度很小，然后又用数据点的数量去除，就使得数值更小，这反过来会导致更多的数值问题。这就是我为什么总是会把原始的解析梯度和数值梯度数据打印出来，确保用来比较的数字的值不是过小（通常绝对值小于1e-10就绝对让人担心）。如果确实过小，可以使用一个常数暂时将损失函数的数值范围扩展到一个更“好”的范围，在这个范围中浮点数变得更加致密。比较理想的是1.0的数量级上，即当浮点数指数为0时。</p><p><strong>目标函数的不可导点（kinks)</strong>，在进行梯度检查时，一个导致不准确的原因是不可导点问题。不可导点是指目标函数不可导的部分，由ReLU（max(0,x)）等函数，或SVM损失，Maxout神经元等引入。考虑当x=-1e6的时，对ReLU函数进行梯度检查。因为x<0，所以解析梯度在该点的梯度为0。然而，在这里数值梯度会突然计算出一个非零的梯度值，因为f(x+h)可能越过了不可导点(例如：如果h>1e-6)，导致了一个非零的结果。你可能会认为这是一个极端的案例，但实际上这种情况很常见。例如，一个用CIFAR-10训练的SVM中，因为有50,000个样本，且根据目标函数每个样本产生9个式子，所以包含有450,000个max(0,x)式子。而一个用SVM进行分类的神经网络因为采用了ReLU，还会有更多的不可导点。</0，所以解析梯度在该点的梯度为0。然而，在这里数值梯度会突然计算出一个非零的梯度值，因为f(x+h)可能越过了不可导点(例如：如果h></p><p>注意，在计算损失的过程中是可以知道不可导点有没有被越过的。在具有max(x,y)形式的函数中持续跟踪所有“赢家”的身份，就可以实现这一点。其实就是看在前向传播时，到底x和y谁更大。如果在计算f(x+h)和f(x-h)的时候，至少有一个“赢家”的身份变了，那就说明不可导点被越过了，数值梯度会不准确。</p><p><strong>使用少量数据点</strong>，解决上面的不可导点问题的一个办法是使用更少的数据点。因为含有不可导点的损失函数(例如：因为使用了ReLU或者边缘损失等函数)的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。还有，如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。</p><p><strong>谨慎设置步长h</strong>，在实践中h并不是越小越好，因为当h特别小的时候，就可能会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将h调到1e-4或者1e-6，然后突然梯度检查就可能恢复正常。</p><p><strong>在操作的特性模式中梯度检查</strong>，有一点必须要认识到：梯度检查是在参数空间中的一个特定（往往还是随机的）的单独点进行的。即使是在该点上梯度检查成功了，也不能马上确保全局上梯度的实现都是正确的。还有，一个随机的初始化可能不是参数空间最优代表性的点，这可能导致进入某种病态的情况，即梯度看起来是正确实现了，实际上并没有。例如，SVM使用小数值权重初始化，就会把一些接近于0的得分分配给所有的数据点，而梯度将会在所有的数据点上展现出某种模式。一个不正确实现的梯度也许依然能够产生出这种模式，但是不能泛化到更具代表性的操作模式，比如在一些的得分比另一些得分更大的情况下就不行。因此为了安全起见，最好让网络学习（“预热”）一小段时间，等到损失函数开始下降的之后再进行梯度检查。在第一次迭代就进行梯度检查的危险就在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。</p><p><strong>不要让正则化吞没数据</strong>，通常损失函数是数据损失和正则化损失的和，需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分（正则化部分的梯度表达式通常简单很多）。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化的强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。</p><p><strong>记得关闭随机失活（Dropout）和数据扩张（augmentation）</strong>,在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。关闭这些操作不好的一点是无法对它们进行梯度检查（例如随机失活的反向传播实现可能有错误）。因此，一个更好的解决方案就是在计算f(x+h)和f(x-h)前强制增加一个特定的随机种子，在计算解析梯度时也同样如此。</p><p><strong>检查少量的维度</strong>，在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度，然后假设其他维度是正确的。注意：确认在所有不同的参数中都抽取一部分来梯度检查。在某些应用中，为了方便，人们将所有的参数放到一个巨大的参数向量中。在这种情况下，例如偏置就可能只占用整个向量中的很小一部分，所以不要随机的从向量中取维度，一定要把这种情况考虑到，确保所有的参数都收到了正确的梯度。</p><h2 id="学习之前：合理性检查的提示与技巧"><a href="#学习之前：合理性检查的提示与技巧" class="headerlink" title="学习之前：合理性检查的提示与技巧"></a>学习之前：合理性检查的提示与技巧</h2><p>在进行费时费力的最优化之前，最好进行一些合理性检查：</p><ul><li><strong>寻找特定情况的正确损失值</strong>，在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。</li><li>第二个合理性检查：提高正则化强度时导致损失值变大。</li><li><strong>对小数据子集过拟合</strong>， 最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。除非能通过这一个正常性检查，不然进行整个数据集训练是没有意义的。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。</li></ul><h2 id="检查学习过程"><a href="#检查学习过程" class="headerlink" title="检查学习过程"></a>检查学习过程</h2><p>在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。<br>在下面的图表中，x轴通常都是表示周期（epochs）单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>训练期间第一个要跟踪的数值就是损失值，它再前向传播时对每个独立的批数据进行计算。下图是展示的是损失值随着时间的变化，尤其是曲线形状会给出关于学习率设置的情况：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171208/8Ala2gIhjB.jpg?imageslim" alt="mark"><br></div><br>左图展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。右图显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。</p><p>损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。</p><p>有的研究者喜欢用对数域对损失函数值作图。因为学习过程一般都是采用指数型的形状，图表就会看起来更像是能够直观理解的直线，而不是呈曲棍球一样的曲线状。还有，如果多个交叉验证模型在一个图上同时输出图像，它们之间的差异就会比较明显。</p><h3 id="训练集与验证集准确率"><a href="#训练集与验证集准确率" class="headerlink" title="训练集与验证集准确率"></a>训练集与验证集准确率</h3><p>在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。这个图表能够展现知道模型过拟合的程度：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171208/JJaikb2iaI.jpg?imageslim" alt="mark"><br></div><br>在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。</p><h3 id="权重：更新比例"><a href="#权重：更新比例" class="headerlink" title="权重：更新比例"></a>权重：更新比例</h3><p>最后一个应该跟踪的量是权重中更新值的数量和全部值的数量之间的比例。注意：是更新的，而不是原始梯度（比如，在普通sgd中就是梯度乘以学习率）。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 假设参数向量为W，其梯度向量为dW</span></div><div class="line">param_scale = np.linalg.norm(W.ravel())</div><div class="line">update = -learning_rate*dW <span class="comment"># 简单SGD更新</span></div><div class="line">update_scale = np.linalg.norm(update.ravel())</div><div class="line">W += update <span class="comment"># 实际更新</span></div><div class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># 要得到1e-3左右</span></div></pre></td></tr></table></figure></p><p>相较于跟踪最大和最小值，有研究者更喜欢计算和跟踪梯度的范式及其更新。这些矩阵通常是相关的，也能得到近似的结果。</p><h3 id="每层的激活数据与梯度分布"><a href="#每层的激活数据与梯度分布" class="headerlink" title="每层的激活数据与梯度分布"></a>每层的激活数据与梯度分布</h3><p>一个不正确的初始化可能让学习过程变慢，甚至彻底停止。还好，这个问题可以比较简单地诊断出来。其中一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。</p><h3 id="第一层可视化"><a href="#第一层可视化" class="headerlink" title="第一层可视化"></a>第一层可视化</h3><p>最后，如果数据是图像像素数据，那么把第一层特征可视化会有帮助。</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171208/mKheECkaAi.jpg?imageslim" alt="mark"><br></div><br>将神经网络第一层的权重可视化的例子。左图中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。右图的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。</p><h2 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h2><p>一旦能使用反向传播计算解析梯度，梯度就能被用来进行参数更新。进行参数更新有好几种方法，接下来都会进行讨论。</p><p>深度网络的最优化是现在非常活跃的研究领域。本节将重点介绍一些公认有效的常用的技巧，这些技巧都是在实践之中会遇到的。我们将简要介绍这些技巧的直观概念，但不进行细节分析。对细节感兴趣的读者，我们提供一些拓展阅读。</p><h3 id="随机梯度下降及各种更新方法"><a href="#随机梯度下降及各种更新方法" class="headerlink" title="随机梯度下降及各种更新方法"></a>随机梯度下降及各种更新方法</h3><p><strong>普通更新</strong>，最简单的更新形式是沿着负梯度方向改变参数（因为梯度指向的是上升的方法，但是我们通常希望最小化损失函数）。假设有一个参数向量x及其梯度dx，那么最简单的更新的形式是：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 普通更新</span></div><div class="line">x += - learning_rate * dx</div></pre></td></tr></table></figure></p><p>其中，learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。</p><p><strong>动量（Momentum）更新</strong>是另外一个方法，这个方法在深度网络上几乎总能得到更好的收敛速度。该方法可以看成是从物理角度上对最优化问题的得到的启发。损失函数可以理解为是山的高度（因此高度的势能是U=mgh），用随机数字初始化参数等同于在某个位置给质点设定初始速度为0.这样最优化过程就可以看成是模拟参数向量（即质点）在地形上滚动的过程。</p><p>因为作用于质点的力与梯度的潜在能量($F=-\nabla U$)有关，质点所受的力就是损失函数的负梯度。还有，因为F=ma，所以在这个观点下负梯度与质点的加速度是成比例的。注意这个理解和上面的随机梯度下降SGD是不同的，在普通版本中，梯度直接影响位置。而在这个版本的更新中，物理观点建议梯度只是影响速度，然后速度再影响位置：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 动量更新</span></div><div class="line">v = mu * v - learning_rate * dx <span class="comment"># 与速度融合</span></div><div class="line">x += v <span class="comment"># 与位置融合</span></div></pre></td></tr></table></figure></p><p>在这里引入一个初始化为0的变量v和一个超参数mu。说的不恰当一点，这个变量mu，在最优化的过程中被看做动量（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设置为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火类似，动量随时间变化的设置有时能略微改善最优化效果，其中动量在学习过程的后阶段会上升。一个典型的设置是刚开始将动量设置为0.5,而在后面的多个周期（epoch）中慢慢提升到0.99。</p><blockquote><p>通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。</p></blockquote><p><strong>Nesterov动量</strong>与普通动量有些许不同，最近变得比较流行。在理论上对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。<br><strong>Nesterov动量的核心思路是</strong>，当参数向量位于某个位置x时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过mu<em> v稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置x+mu</em> v看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，计算x+mu* v的梯度而不是“旧”位置x的梯度就有意义了。</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171209/j819bJD0FH.jpg?imageslim" alt="mark"><br></div><br>Nesterov动量。既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个“向前看”的地方计算梯度。<br>也就是说，添加一些注释后，实现代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x_ahead = x + mu * v</div><div class="line"><span class="comment"># 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)</span></div><div class="line">v = mu * v - learning_rate * dx_ahead</div><div class="line">x += v</div></pre></td></tr></table></figure></p><p>然而在实践中，人们更喜欢和普通SGD或上面的动量方法一样简单的表达式。通过对x_ahead = x + mu * v使用变量变换进行改写是可以做到的，然后用x_ahead而不是x来表示上面的更新。也就是说，实际存储的参数向量总是向前一步的那个版本。x_ahead的公式（将其重新命名为x）就变成了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">v_prev = v <span class="comment"># 存储备份</span></div><div class="line">v = mu * v - learning_rate * dx <span class="comment"># 速度更新保持不变</span></div><div class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># 位置更新变了形式</span></div></pre></td></tr></table></figure></p><p>对于NAG（Nesterov’s Accelerated Momentum）的来源和数学公式推导，我们推荐以下的拓展阅读：</p><ul><li>Yoshua Bengio的Advances in optimizing Recurrent Networks，Section 3.5。</li><li><a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf在section" target="_blank" rel="external">http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf在section</a> 7.2对于这个主题有更详尽的阐述。</li></ul><h3 id="学习率退火"><a href="#学习率退火" class="headerlink" title="学习率退火"></a>学习率退火</h3><p>在训练深度网络的时候，让学习率随着时间退火通常是有帮助的。可以这样理解：如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。知道什么时候开始衰减学习率是有技巧的：慢慢减小它，可能在很长时间内只能是浪费计算资源地看着它混沌地跳动，实际进展很少。但如果快速地减少它，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有3种方式：</p><ul><li><strong>随步数衰减：</strong>每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体的问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。</li><li><strong>指数衰减：</strong>数学公式是$\alpha = \alpha_0e^{-kt}$，其中,$\alpha_0,k$是超参数，t是迭代次数（也可以使用周期作为单位）。</li><li><strong>1/t衰减：</strong>数学公式是$\alpha=\alpha_0/(1+kt)$,$\alpha_0,k$是超参数，t是迭代次数。<br>在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。</li></ul><h3 id="二阶方法"><a href="#二阶方法" class="headerlink" title="二阶方法"></a>二阶方法</h3><p>在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下$$<br>x \leftarrow x - [Hf(x)]^{-1}\nabla f(x)<br>$$<br>这里Hf(x)是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。$\nabla f(x)$是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。<br>然而，上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的拟-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是L-BFGS，该方法使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。</p><p>然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。</p><p><strong>实践</strong>，在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。</p><p>参考资料：</p><ul><li>Large Scale Distributed Deep Networks 一文来自谷歌大脑团队，比较了在大规模数据情况下L-BFGS和SGD算法的表现。</li><li>SFO算法想要把SGD和L-BFGS的优势结合起来。</li></ul><h3 id="逐参数适应学习率方法"><a href="#逐参数适应学习率方法" class="headerlink" title="逐参数适应学习率方法"></a>逐参数适应学习率方法</h3><p>前面讨论的所有方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作投入到发明能够适应性地对学习率调参的方法，甚至是逐个参数适应学习率调参。很多这些方法依然需要其他的超参数设置，但是其观点是这些方法对于更广范围的超参数比原始的学习率方法有更良好的表现。在本小节我们会介绍一些在实践中可能会遇到的常用适应算法：</p><p>Adagrad是一个由Duchi等提出的适应性学习率算法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 假设有梯度和参数向量x</span></div><div class="line">cache += dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure></p><p>注意，变量cache的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数的梯度的平方和。这个一会儿将用来归一化参数更新步长，归一化是逐元素进行的。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强。有趣的是平方根的操作非常重要，如果去掉，算法的表现将会糟糕很多。用于平滑的式子eps（一般设为1e-4到1e-8之间）是防止出现除以0的情况。Adagrad的一个缺点是，在深度学习中单调的学习率被证明通常过于激进且过早停止学习。</p><p><strong>RMSprop</strong>是一个非常高效，但没有公开发表的适应性学习率方法。有趣的是，每个使用这个方法的人在他们的论文中都引用自Geoff Hinton的Coursera课程的第六课的第29页PPT。这个方法用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cache =  decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure></p><p>在上面的代码中，decay_rate是一个超参数，常用的值是[0.9,0.99,0.999]。其中x+=和Adagrad中是一样的，但是cache变量是不同的。因此，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这同样效果不错。但是和Adagrad不同，其更新不会让学习率单调变小。</p><p><strong>Adam</strong>是最近才提出的一种更新方法，它看起来像是RMSProp的动量版。简化的代码是下面这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">m = beta1*m + (<span class="number">1</span>-beta1)*dx</div><div class="line">v = beta2*v + (<span class="number">1</span>-beta2)*(dx**<span class="number">2</span>)</div><div class="line">x += - learning_rate * m / (np.sqrt(v) + eps)</div></pre></td></tr></table></figure></p><p>注意这个更新方法看起来真的和RMSProp很像，除了使用的是平滑版的梯度m，而不是用的原始梯度向量dx。论文中推荐的参数值eps=1e-8, beta1=0.9, beta2=0.999。在实际操作中，我们推荐Adam作为默认的算法，一般而言跑起来比RMSProp要好一点。但是也可以试试SGD+Nesterov动量。完整的Adam更新算法也包含了一个偏置（bias）矫正机制，因为m,v两个矩阵初始为0，在没有完全热身之前存在偏差，需要采取一些补偿措施。建议读者可以阅读论文查看细节，或者课程的PPT。<br>拓展阅读：</p><ul><li>Unit Tests for Stochastic Optimization一文展示了对于随机最优化的测试。</li></ul><h2 id="参数调优"><a href="#参数调优" class="headerlink" title="参数调优"></a>参数调优</h2><p>我们已经看到，训练一个神经网络会遇到很多超参数设置，神经网络最常用的设置有：</p><ul><li>初始化学习率；</li><li>学习率衰减方式（例如一个衰减常量）</li><li>正则化强度（L2惩罚，随机失活强度）<br>但是也可以看到，还有很多相对不那么敏感的超参数。比如在逐参数适应学习方法中，对于动量及时间表的设置等。在本节中将介绍一些额外的调参要点和技巧：</li></ul><p><strong>实现</strong>：更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要，因为这会影响你设计代码的思路。一个具体的设计是用<strong>仆程序</strong>持续地随机设置参数然后进行最优化。在训练过程中，<strong>仆程序</strong>会对每个周期后验证集的准确率进行监控，然后向文件系统写下一个模型的记录点（记录点中有各种各样的训练统计数据，比如随着时间的损失值变化等），这个文件系统最好是可共享的。在文件名中最好包含验证集的算法表现，这样就能方便地查找和排序了。然后还有一个<strong>主程序</strong>，它可以启动或者结束计算集群中的<strong>仆程序</strong>，有时候也可能根据条件查看<strong>仆程序</strong>写下的记录点，输出它们的训练统计数据等。</p><p><strong>比起交叉验证最好使用一个验证集</strong>：在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。你可能会听到人们说他们“交叉验证”一个参数，但是大多数情况下，他们实际是使用的一个验证集。</p><p><strong>超参数范围</strong>，在对数尺度上进行超参数搜索，例如，一个典型的学习率应该看起来是这样：learning_rate = 10**uniform(-6, 1)。也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有承的效果。例如：当学习率是0.001的时候，如果对其固定地增加0.001，那么对于学习进程会有很大的影响。然而当学习率是10的时候，影响就微乎其微了。。这就是因为学习率乘以了计算出的梯度。因此，比起加上或者减少某些值，思考学习率的范围是乘以或者除以某些值更加自然。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：dropout=uniform(0,1)）。</p><p><strong>随机搜索优于网格搜索</strong>，Bergstra和Bengio在文章Random Search for Hyper-Parameter Optimization中说“随机选择比网格化的选择更加有效”，而且在实践中也更容易实现。</p><p><strong>对于边界上的最优值要小心：</strong>这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使用learning_rate = 10**uniform(-6,1)来进行搜索。一旦我们得到一个比较好的值，一定确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。</p><p><strong>从粗到细地分阶段搜索</strong>，在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练一个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。</p><p><strong>贝叶斯超参数最优化</strong>是一整个研究领域，主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。基于这些模型，发展出很多的库，比较有名的有： Spearmint, SMAC, 和Hyperopt。然而，在卷积神经网络的实际使用中，比起上面介绍的先认真挑选的一个范围，然后在该范围内随机搜索的方法，这个方法还是差一些。这里有<a href="http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html更详细的讨论。" target="_blank" rel="external">http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html更详细的讨论。</a></p><h2 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h2><h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法：</p><ul><li><strong>同一个模型，不同的初始化</strong>，使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。这种方法的风险在于多样性只来自于不同的初始化条件。</li><li><strong>在交叉验证中发现最好的模型</strong>，使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。</li><li><strong>一个模型设置多个记录点</strong>，如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。</li><li><strong>在训练的时候跑参数的平均值</strong>，和上面一点相关的，还有一个也能得到1-2个百分点的提升的小代价方法，这个方法就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样你就对前几次循环中的网络状态进行了平均。你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。</li></ul><p>模型集成的一个劣势就是在测试数据的时候会花费更多时间。最近Geoff Hinton在“Dark Knowledge”上的工作很有启发：其思路是通过将集成似然估计纳入到修改的目标函数中，从一个好的集成中抽出一个单独模型。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>训练一个神经网络需要：</p><ul><li>利用小批量的数据对实现进行梯度检查，还要注意各种错误。</li><li>进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。</li><li>在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数相对于总参数的比例（一般在1e-3左右）然后如果是对于卷积神经网络，可以将第一层的权重可视化。</li><li>推荐的两个更新方法是SGD+Nesterov动量方法，或者是Adam方法。</li><li>随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。</li><li>使用随机搜索（不要使用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。</li><li>进行模型集成来获得额外的性能提高。</li></ul><h2 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><ul><li>Leon Botton的《SGD要点和技巧》：<a href="https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F192769%2Ftricks-2012.pdf" target="_blank" rel="external">https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F192769%2Ftricks-2012.pdf</a></li><li>Yann LeCun的《Efficient BackProp》：<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="external">http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf</a></li><li>Yoshua Bengio的《Practical Recommendations for Gradient-Based Training of Deep Architectures》。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文转载自：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit，原文为：http://cs231n.github.io/neural-networks-3/，并进行一定的修改。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit，原文为：http://cs231n.github.io/neural-networks-3/，并进行一定的修改。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="课程笔记" scheme="https://ilewseu.github.io/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="cs231n" scheme="https://ilewseu.github.io/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>CS231n课程笔记(7)  Neural Nets Notes 2</title>
    <link href="https://ilewseu.github.io/2017/12/12/CS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC7%E8%AF%BE-Neural-Nets-Notes-2/"/>
    <id>https://ilewseu.github.io/2017/12/12/CS231n课程笔记-第7课-Neural-Nets-Notes-2/</id>
    <published>2017-12-12T14:17:23.000Z</published>
    <updated>2017-12-16T16:06:55.660Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转自：<a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-2/</a></p></blockquote><p><strong>目录</strong><br><a id="more"></a></p><ul><li>设置数据和模型<ul><li>数据预处理</li><li>权重初始化</li><li>批量归一化（Batch Normalization）</li><li>正则化（L1/L2/Maxnorm/Dropout）</li></ul></li><li>损失函数</li><li>小结</li></ul><h2 id="设置数据和模型"><a href="#设置数据和模型" class="headerlink" title="设置数据和模型"></a>设置数据和模型</h2><p>上一节中介绍了神经元的模型，它在计算内积后进行非线性激活函数计算，神经网络将这些神经元组织成各个层。这些做法共同定义了评分函数（score function）的新形式，该形式是从前面线性分类章节中的简单线性映射发展而来的。具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。本节将讨论更多的算法设计选项，比如数据预处理，权重初始化和损失函数。</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>关于数据预处理我们有3个常用的符号，数据矩阵X，假设其尺寸是[N x D]（N是数据样本的数量，D是数据的维度）。</p><p><strong>均值减法（Mean Subtraction）</strong>:是预处理最常用的形式。它对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码X-=np.mean(X,axis=0)实现。对于图像，更常用的是对所有像素都减去一个值，可以用X -= np.mean(X)实现，也可以在3个颜色通道上分别操作。</p><p><strong>归一化操作（Normalization）</strong>:是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)。第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171207/4171gcBfCK.jpg?imageslim" alt="mark"><br></div><br>一般数据预处理流程：左边：原始的2维输入数据。中间：在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。右边：每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。<br><strong>PCA和白化（Whitening）</strong>:是另一种数据预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 假设输入数据矩阵X的尺寸为[N x D]</span></div><div class="line">X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># 对数据进行零中心化(重要)</span></div><div class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># 得到数据的协方差矩阵</span></div></pre></td></tr></table></figure><p>数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的协方差。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和半正定的。我们可以对数据协方差矩阵进行SVD（奇异值分解）运算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">U,S,V = np.linalg.svd(cov)</div></pre></td></tr></table></figure></p><p>U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Xrot = np.dot(X,U) <span class="comment"># 对数据去相关性</span></div></pre></td></tr></table></figure></p><p>注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算Xrot的协方差矩阵，将会看到它是对角对称的。np.linalg.svd的一个良好性质是在它的返回值U中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有方差的维度。 这个操作也被称为主成分分析（ Principal Component Analysis 简称PCA）降维：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment"># Xrot_reduced 变成 [N x 100]</span></div></pre></td></tr></table></figure></p><p>经过上面的操作，将原始的数据集的大小由[N x D]降到了[N x 100]，留下了数据中包含最大方差的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。<br>最后一个在实践中会看见的变换是白化（whitening）。白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 对数据进行白化操作:</span></div><div class="line"><span class="comment"># 除以特征值 </span></div><div class="line">Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>)</div></pre></td></tr></table></figure></p><p>警告：夸大的噪声。注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171207/5iFjlf3GKE.jpg?imageslim" alt="mark"><br></div><br>PCA/白化。左边是二维的原始数据。中间：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。右边：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。</p><p><strong>实践操作</strong>：在这个笔记中提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。</p><p><strong>常见错误</strong>：进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练、验证、测试集，那么这个做法是错误的。<strong>应该怎么做呢？应该先分成训练、验证、测试集，只是从训练集中求图片平均值，然后各个集（训练、验证、测试集）中的图像再减去这个平均值。</strong></p><h2 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h2><p>我们已经看到如何构建一个神经网络的结构并对数据进行预处理，但是在开始训练网络之前，还需要初始化网络的参数。</p><p><strong>错误：全零初始化</strong>：让我们从应该避免的错误开始。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。</p><p><strong>小随机数初始化：</strong>因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是：W = 0.01 * np.random.randn(D,H)。其中，randn函数是基于零均值和标准差的一个高斯分布来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。</p><p><strong>警告：</strong>并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会出现非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。</p><p><strong>使用1/sqrt(n)校准方差</strong>，上面的做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为：w = np.random.randn(n) / sqrt(n)。其中n是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。<br>上述结论的推导过程如下：假设权重w和输入x之间的内积为$s=\sum^n_iw_ix_i$，这是还没有进行非线性激活函数运算之前的原始数值。我们可以检查s的方差：$$<br>Var(s)=Var(\sum_i^n w_ix_i)\\\\=\sum_i^nVar(w_ix_i)\\\\=\sum_i^n[E(w_i)]^2Var(x_i)+E[(x_i)]^2Var(w_i)+Var(xIi)Var(w_i)\\\\=\sum_i^nVar(x_i)Var(w_i)\\\\=(nVar(w))Var(x)<br>$$<br>在前两步，使用了方差的性质。在第三步，因为假设输入和权重的平均值都是0，所以$E[x_i]=E[w_i]=0$。注意这并不是一般化情况，比如在ReLU单元中均值就为正。在最后一步，我们假设所有的$w_i,x_i$都服从同样的分布。从这个推导过程我们可以看见，如果想要s有和输入x一样的方差，那么在初始化的时候必须保证每个权重w的方差是1/n。又因为对于一个随机变量X和标量a，有$Var(aX)=a^2Var(X)$，这就说明可以基于一个标准高斯分布，然后乘以$a=\sqrt{1/n}$，使其方差为1/n，于是得出：w = np.random.randn(n) / sqrt(n)。</p><p>Glorot等在论文Understanding the difficulty of training deep feedforward neural networks中作出了类似的分析。在论文中，作者推荐初始化公式为 $Var(w) = 2/(n_{in} + n_{out})$ ，其中$n_{in}, n_{out}$是在前一层和后一层中单元的个数。这是基于妥协和对反向传播中梯度的分析得出的结论。该主题下最新的一篇论文是：Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification，作者是He等人。文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是2.0/n。代码为w = np.random.randn(n) * sqrt(2.0/n)。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。</p><p><strong>稀疏初始化（Sparse initialization）</strong>:另一个处理非标定方差的方法就是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。</p><p><strong>偏置（biases）的初始化</strong>，通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。</p><p><strong>实践</strong>，当前的推荐是使用ReLU激活函数，并且使用w = np.random.randn(n) * sqrt(2.0/n)来进行权重初始化。</p><p><strong>批量归一化（Batch Normalization）</strong>:批量归一化是loffe和Szegedy最近才提出的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛：）,其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层与激活函数之间添加一个BatchNorm层。对于这个技巧本节不会展开讲，因为参考文献：<a href="https://arxiv.org/abs/1502.03167中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。搞定！" target="_blank" rel="external">https://arxiv.org/abs/1502.03167中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。搞定！</a></p><h2 id="正则化Regularization"><a href="#正则化Regularization" class="headerlink" title="正则化Regularization"></a>正则化Regularization</h2><p>有不少方法是通过控制神经网络的容量来防止其过拟合的：<br><strong>L2正则化</strong>可能是最常用的正则化方法了，可以通过惩罚目标函数中所有参数的平方将其实现。即对网络中的每个权重w，向目标函数中增加一个$\frac {1}{2}\lambda w^2$，其中$\lambda$是正则化强度。前面这个1/2很常见，是因为加上1/2后，该式子关于w梯度就是$\lambda w$而不是$2\lambda w$了，L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。在线性分类章节中讨论过，由于输入和权重之间的乘法操作，这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以w += -lambda * W向着0线性下降。</p><p>L1正则化是另一个相对常用的正则化方法。对于每个w我们都向目标函数增加一个\lambda|w|。L1和L2正则化也可以进行组合：$\lambda_1|w|+\lambda_2w^2$，这也被称作Elastic net regularizaton。L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。</p><p><strong>最大范式约束（Max norm constraints)</strong>,另一种形式的正则化是给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量$\overrightarrow{w}$必须满足$||\overrightarrow{w}||_2&lt;c$这一条件，一般c值为3或者4。有研究者发文称在使用这种正则化方法时效果更好。这种正则化还有一个良好的性质，即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的。</p><p><strong>随机失活（Dropout）</strong>是一个简单又极其有效的正则化方法。该方法由Srivastava在论文Dropout: A Simple Way to Prevent Neural Networks from Overfitting中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0。</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171207/L8ekkFIBfC.jpg?imageslim" alt="mark"><br></div><br>图片来自于论文：<a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf。展示其核心思路，在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相对独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为对数量巨大的子网们做了模型集成，以此来计算出一个平均的预测。" target="_blank" rel="external">http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf。展示其核心思路，在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相对独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为对数量巨大的子网们做了模型集成，以此来计算出一个平均的预测。</a></p><p>一个3层神经网络的普通版随机失活可以用下面代码实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="string">""" 普通版随机失活: 不推荐实现 (看下面笔记) """</span></div><div class="line"></div><div class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></div><div class="line">  <span class="string">""" X中是输入数据 """</span></div><div class="line">  </div><div class="line">  <span class="comment"># 3层neural network的前向传播</span></div><div class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</div><div class="line">  U1 = np.random.rand(*H1.shape) &lt; p <span class="comment"># 第一个随机失活遮罩</span></div><div class="line">  H1 *= U1 <span class="comment"># drop!</span></div><div class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</div><div class="line">  U2 = np.random.rand(*H2.shape) &lt; p <span class="comment"># 第二个随机失活遮罩</span></div><div class="line">  H2 *= U2 <span class="comment"># drop!</span></div><div class="line">  out = np.dot(W3, H2) + b3</div><div class="line">  </div><div class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></div><div class="line">  <span class="comment"># 进行参数更新... (略)</span></div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></div><div class="line">  <span class="comment"># 前向传播时模型集成</span></div><div class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) * p <span class="comment"># 注意：激活数据要乘以p</span></div><div class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) * p <span class="comment"># 注意：激活数据要乘以p</span></div><div class="line">  out = np.dot(W3, H2) + b3</div></pre></td></tr></table></figure></p><p>在上面的代码中，train_step函数在第一个隐层和第二个隐层上进行了两次随机失活。在输入层上面进行随机失活也是可以的，为此需要为输入数据X创建一个二值的遮罩。反向传播保持不变，但是肯定需要将遮罩U1和U2加入进去。</p><p>注意：在predict函数中不进行随机失活，但是对于两个隐层的输出都要乘以p，调整其数值范围。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以p=0.5为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。为了理解这点，先假设有一个神经元x的输出，那么进行随机失活的时候，该神经元的输出就是px+(1-p)0，这是有1-p的概率神经元的输出为0。在测试时神经元总是激活的，就必须调整$x\to px$来保持同样的预期输出。在测试时会在所有可能的二值遮罩（也就是数量庞大的所有子网络）中迭代并计算它们的协作预测，进行这种减弱的操作也可以认为是与之相关的。</p><p>上述操作不好的性质是必须在测试时对激活数据要按照p进行数值范围调整。既然测试性能如此关键，实际更倾向使用反向随机失活（inverted dropout），它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="string">""" </span></div><div class="line"><span class="string">反向随机失活: 推荐实现方式.</span></div><div class="line"><span class="string">在训练的时候drop和调整数值范围，测试时不做任何事.</span></div><div class="line"><span class="string">"""</span></div><div class="line"></div><div class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></div><div class="line">  <span class="comment"># 3层neural network的前向传播</span></div><div class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</div><div class="line">  U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 第一个随机失活遮罩. 注意/p!</span></div><div class="line">  H1 *= U1 <span class="comment"># drop!</span></div><div class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</div><div class="line">  U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># 第二个随机失活遮罩. 注意/p!</span></div><div class="line">  H2 *= U2 <span class="comment"># drop!</span></div><div class="line">  out = np.dot(W3, H2) + b3</div><div class="line"></div><div class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></div><div class="line">  <span class="comment"># 进行参数更新... (略)</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></div><div class="line">  <span class="comment"># 前向传播时模型集成</span></div><div class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># 不用数值范围调整了</span></div><div class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</div><div class="line">  out = np.dot(W3, H2) + b3</div></pre></td></tr></table></figure></p><p>在随机失活发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献：</p><ul><li>Dropout paper by Srivastava et al. 2014.</li><li>Dropout Training as Adaptive Regularization：“我们认为：在使用费希尔信息矩阵（fisher information matrix）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。”</li></ul><p><strong>前向传播中的噪音：</strong>在更一般化的分类上，随机失活属于网络在前向传播中有随机行为的方法。测试时，通过分析法（在使用随机失活的本例中就是乘以p）或数值法（例如通过抽样出很多子网络，随机选择不同子网络进行前向传播，最后对它们取平均）将噪音边缘化。在这个方向上的另一个研究是DropConnect，它在前向传播的时候，一系列权重被随机设置为0。提前说一下，卷积神经网络同样会吸取这类方法的优点，比如随机汇合（stochastic pooling），分级汇合（fractional pooling），数据增长（data augmentation）。我们在后面会详细介绍。</p><p><strong>偏置正则化：</strong>在线性分类器的章节中介绍过，对于偏置参数的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差。这可能是因为相较于权重参数，偏置参数实在太少，所以分类器需要它们来获得一个很好的数据损失，那么还是能够承受的。</p><p><strong>每层正则化：</strong>对于不同的层进行不同强度的正则化很少见（可能除了输出层以外），关于这个思路的相关文献也很少。</p><p><strong>实践</strong>：通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。p值一般默认设为0.5，也可能在验证集上调参。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>我们已经讨论过损失函数的正则化损失部分，它可以看做是对模型复杂程度的某种惩罚。损失函数的第二个部分时数据损失。它是一个有监督学习问题，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性。数据损失是对所有样本的数据损失求平均。也就是说，$L=\frac{1}{N}\sum_iL_i$中，N是训练集数据的样本数。让我们把神经网络中输出层的激活函数简写为f=f(x_i;W)，在实际中你可能需要解决以下几类问题：</p><p><strong>分类问题</strong>我们一直讨论的。在该问题中，假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签（是固定分类标签之一）。在这类问题中，一个最常见的损失函数就是SVM（是Weston Watkins 公式）：<br>$$L_i=\sum_{j\not=y_i}max(0,f_j-f_{y_i}+1)$$<br>之前简要提起过，有些学者的论文中指出平方折叶损失（即使用max(0,f_j-f_{y_i}+1)^2）算法的结果会更好。第二个常用的损失函数是Softmax分类器，它使用交叉熵损失：<br>$$\displaystyle L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})$$</p><p><strong>问题：类别数目巨大。</strong>当标签集非常庞大（例如字典中的所有英语单词，或者ImageNet中的22000种分类），就需要使用分层Softmax（Hierarchical Softmax）了（参考文献：<a href="https://arxiv.org/pdf/1310.4546.pdf）分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。" target="_blank" rel="external">https://arxiv.org/pdf/1310.4546.pdf）分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。</a></p><p><strong>属性（Attribute）分类。</strong>上面两个损失公式的前提，都是假设每个样本只有一个正确的标签y_i。但是如果y_i是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间并不相互排斥呢？比如在Instagram上的图片，就可以看成是被一个巨大的标签集合中的某个子集打上标签，一张图片上可能有多个标签。在这种情况下，一个明智的方法是为每个属性创建一个独立的二分类的分类器。例如，针对每个分类的二分类器会采用下面的公式：$$<br>L_i=\sum_j max(0,1-y_{ij}f_j)<br>$$<br>上式中，求和是对所有分类j，$y_{ij}$的值为1或者-1，具体根据第i个样本是否被第j个属性打标签而定，当该类别被正确预测并展示的时候，分值向量$f_j$为正，其余情况为负。可以发现，当一个正样本的得分小于+1，或者一个负样本的得分大于-1的时候，算法就会累计损失值。</p><p>另一种方法是对每种属性训练一个独立的逻辑回归分类器，二分类的逻辑回归只有两个分类（0，1），其中对于分类1的概率计算为：<br>$$P(y=1|x;w,b) = \frac {1}{1+e^{-(w^Tx+b)}}=\sigma(w^Tx+b)$$因为类别0和类别1的概率和为1，所以类别0的概率为：$\displaystyle P(y=0|x;w,b)=1-P(y=1|x;w,b)$。这样，如果$\sigma(w^Tx+b)&gt;0.5$或者$w^Tx+b&gt;0$，那么样本就要被分类成为正样本（y=1）。然后损失函数最大化这个对数似然函数，问题可以简化为：$$<br>L_i = \sum_j y_{ij}log(\sigma(f_j)) + (1-y_{ij})log(1-\sigma(f_j))$$式中，假设标签$y_{ij}$非0即1，$\sigma(.)$就是sigmoid函数。上面的公式看起来吓人，但是f的梯度实际上非常简单：$\displaystyle \frac{\partial L_i}{\partial f_j}=y_{ij}-\sigma(f_j)$（你可以自己求导来验证）。</p><p><strong>回归问题</strong>是预测实数的值的问题，比如房价预测，预测图片中某个东西的长度等等。对于这种问题，通常是计算预测值和真实值之间的损失。然后用L2或者L1范数度量差异。对于某个样本，L2范数计算如下：$$L_i=||f-y_i||_2^2<br>$$<br>之所以在目标函数中要进行平方，是因为梯度算起来更加简单。因为平方是一个单调运算，所以不用改变最优参数。L1范式则是要将每个维度上的绝对值加起来：$$<br>L_i = ||f - y_i||_1=\sum_j|f_j - (y_i)j|<br>$$<br>在上式中，如果有多个数量被预测了，就要对预测的所有维度的预测求和，即$\sum_j$。观察第i个样本的第j维，用$\delta_{ij}$表示预测值与真实值之间的差异。关于该维度的梯度（也就是$\partial L_i/\partial f_j）$能够轻松地通过被求导为L2范式的$\delta_{ij}$或$sign(\delta_{ij})$。这就是说，评分值的梯度要么与误差中的差值直接成比例，要么是固定的并从差值中继承sign。</p><p>注意：L2损失比起较为稳定的Softmax损失来，其最优化过程要困难很多。直观而言，它需要网络具备一个特别的性质，即对于每个输入（和增量）都要输出一个确切的正确值。而在Softmax中就不是这样，每个评分的准确值并不是那么重要：只有当它们量级适当的时候，才有意义。还有，L2损失鲁棒性不好，因为异常值可以导致很大的梯度。所以在面对一个回归问题时，先考虑将输出变成二值化是否真的不够用。例如，如果对一个产品的星级进行预测，使用5个独立的分类器来对1-5星进行打分的效果一般比使用一个回归损失要好很多。分类还有一个额外优点，就是能给出关于回归的输出的分布，而不是一个简单的毫无把握的输出值。如果确信分类不适用，那么使用L2损失吧，但是一定要谨慎：L2非常脆弱，在网络中使用随机失活（尤其是在L2损失层的上一层）不是好主意。</p><blockquote><p>当面对一个回归任务，首先考虑是不是必须这样。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。</p></blockquote><p><strong>结构化预测（structured prediction）</strong>,结构化损失是指标签可以是任意的结构，例如图表、树或者其他复杂物体的情况。通常这种情况还会假设结构空间非常巨大，不容易进行遍历。结构化SVM背后的基本思想就是在正确的结构y_i和得分最高的非正确结构之间画出一个边界。解决这类问题，并不是像解决一个简单无限制的最优化问题那样使用梯度下降就可以了，而是需要设计一些特殊的解决方案，这样可以有效利用对于结构空间的特殊简化假设。我们简要地提一下这个问题，但是详细内容就超出本课程范围。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化到[-1,1]范围之内。</li><li>使用标准差为\sqrt{2/n}的高斯分布来初始化权重，其中n是输入的神经元数。例如用numpy可以写作：w = np.random.randn(n) * sqrt(2.0/n)。</li><li>使用L2正则化和随机失活的倒置版本。</li><li>使用批量归一化。</li><li>讨论了在实践中可能要面对的不同任务，以及每个任务对应的常用损失函数。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文转自：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-2/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-2/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="课程笔记" scheme="https://ilewseu.github.io/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="cs231n" scheme="https://ilewseu.github.io/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>CS231n课程笔记(6)  Neural Nets Notes 1</title>
    <link href="https://ilewseu.github.io/2017/12/10/CS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC6%E8%AF%BE-Neural-Nets-Notes-1/"/>
    <id>https://ilewseu.github.io/2017/12/10/CS231n课程笔记-第6课-Neural-Nets-Notes-1/</id>
    <published>2017-12-10T14:17:23.000Z</published>
    <updated>2017-12-16T16:06:37.837Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转自：<a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-1/" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-1/</a></p></blockquote><p><strong>目录</strong></p><ul><li>简介<a id="more"></a></li><li>单个神经元建模<ul><li>生物动机和连接</li><li>作为线性分类器的单个神经元</li><li>常用的激活函数</li></ul></li><li>神经网络结构<ul><li>层组织</li><li>前向传播计算例子</li><li>表达能力</li><li>设置层的数量和尺寸</li></ul></li><li>小结</li></ul><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在线性分类一节中，在给出图像的情况下，使用$s=Wx$来计算不同视觉类别的评分，其中W是一个矩阵，x是一个输入列向量，它包含了图像的全部像素数据。在使用数据库CIFAR-10的案例中，x是一个[3072<em>1]的列向量，W是一个[10</em>3072]的矩阵，所有输出的评分是一个包含10个类别评分的向量。<br>神经网络算法则不同，它的计算公式是$s=W_2max(0,W_1x)$。其中$W_1$的含义是这样的：举个例子来说，它可以是一个[100*3072]的矩阵，其作用是将图像转化为一个100维的过渡向量。函数max(0,-)是非线性的，它会作用到每个元素。这个非线性函数有多种选择，后续将会学到。但这个形式是一个最常用的选择，它就是简单地设置阈值，将所有小于0的值变成0。最终，矩阵$W_2$的尺寸是[10x100]，因此将得到10个数字，这10个数字可以解释为是分类的评分。注意非线性函数在计算上是至关重要的，如果略去这一步，那么两个矩阵将会合二为一，对于分类的评分计算将重新变成关于输入的线性函数。这个非线性函数就是改变的关键点。参数$W_1$,$W_2$将通过随机梯度下降来学习到，他们的梯度在反向传播过程中，通过链式法则来求导计算得出。</p><p>一个三层的神经网络可以类比地看做$s=W_3max(0,W_2max(0,W_1x))$，其中$W_1$,$W_2$,$W_3$是需要进行学习的参数。中间隐层的尺寸是网络的超参数，后续将学习如何设置它们。现在让我们先从神经元或者网络的角度理解上述计算。</p><h2 id="单个神经元建模"><a href="#单个神经元建模" class="headerlink" title="单个神经元建模"></a>单个神经元建模</h2><p>神经网络算法领域最初是对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好的效果。然而，讨论将还是从对生物系统的一个高层次的简要描述开始，因为神经网络毕竟是从这里得到了启发。</p><h3 id="生物动机与连接"><a href="#生物动机与连接" class="headerlink" title="生物动机与连接"></a>生物动机与连接</h3><p>大脑的基本计算单位是神经元（neuron）。人类的神经系统中大约有860亿个神经元，它们被大约10^14-10^15个突触（synapses）连接起来。下面图表的左边展示了一个生物学的神经元，右边展示了一个常用的数学模型。每个神经元都从它的树突获得输入信号，然后沿着它唯一的轴突（axon）产生输出信号。轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。</p><p><div align="center"><br><img src="http://of6h3n8h0.bkt.clouddn.com/blog/171205/EK18LFI9dh.jpg?imageslim" alt="mark"><br></div><br>在神经元的计算模型中，沿着轴突传播信号（比如将$x_0$）将基于突触的突触强度（比如$w_0$）<br>，与其他神经元的树突进行乘法交互（比如$w_0x_0$）。其观点是，突触的强度（也就是权重w），是可学习的且可以控制一个神经元对于另一个神经元的影响强度（还可以控制影响方向：使其兴奋（正权重）或使其抑制（负权重））。在基本模型中，树突将信号传递到细胞体，信号在细胞体中相加。如果最终之和高于某个阈值，那么神经元将会激活，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号的频率在交流信息。基于这个速率编码的观点，将神经元的激活率建模为激活函数（activation function）f，它表达了轴突上激活信号的频率。由于历史原因，激活函数常常选择使用sigmoid函数$\sigma$，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。在本节后面部分会看到这些激活函数的各种细节。<br>一个神经元前向传播的实例代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neuron</span><span class="params">(object)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(inputs)</span>:</span></div><div class="line">    <span class="string">""" 假设输入和权重是1-D的numpy数组，偏差是一个数字 """</span></div><div class="line">    cell_body_sum = np.sum(inputs * self.weights) + self.bias</div><div class="line">    firing_rate = <span class="number">1.0</span> / (<span class="number">1.0</span> + math.exp(-cell_body_sum)) <span class="comment"># sigmoid激活函数</span></div><div class="line">    <span class="keyword">return</span> firing_rate</div></pre></td></tr></table></figure></p><p>换句话说，每个神经元都对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数（或称为激活函数）。本例中使用的是sigmoid函数$\sigma(x)=1/(1+e^{-x})$。在本节的末尾部分将介绍不同激活函数的细节。</p><h3 id="作为线性分类器的单个神经元"><a href="#作为线性分类器的单个神经元" class="headerlink" title="作为线性分类器的单个神经元"></a>作为线性分类器的单个神经元</h3><p>神经元模型的前向计算数学公式看起来可能比较眼熟。就像在线性分类器中看到的那样，神经元有能力”喜欢”（激活函数值接近1），或者不喜欢（激活函数值接近0）输入空间中的某些线性区域。因此，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。</p><p><strong>二分类Softmax分类器</strong>，举例来说，可以把$\sigma(\sum_i w_ib_i+b)$看做其中一个分类的概率$P(y_i=1|x_i;w)$，其他分类的概率为$P(y_i=0|x_i;w)=1-P(y_i=1|x_i;w)$，因为它们加起来必须为1。根据这种理解，可以得到交叉熵损失，这个在线性分一节中已经介绍。然后将它最优化为二分类的Softmax分类器（也就是逻辑回归）。因为sigmoid函数输出限定在0-1之间，所以分类器做出预测的基准是神经元的输出是否大于0.5。</p><p><strong>二分类SVM分类器</strong>，或者可以在神经元的输出外增加一个最大边界折叶损失（max-margin hinge loss）函数，将其训练成一个二分类的支持向量机。</p><blockquote><p>一个单独的神经元可以用来实现一个二分类器，比如二分类的Softmax或者SVM分类器。</p></blockquote><h2 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h2><p>每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171205/d62HBm98JC.jpg?imageslim" alt="mark"><br></div><br>左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。</p><p><strong>Sigmoid函数</strong><br>Sigmoid非线性函数的数学公式是$\sigma(x) = \frac {1}{1+e^{-x}}$，函数图像如上图的左边所示。在前面一节中已经提到，它输入实数值，并将其“挤压”到0到1范围内。更具体的说很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活(0)到在求和后的最大频率处的完全饱和的激活（1）。然而，现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点：</p><ul><li><strong>Sigmoid函数饱和使梯度消失</strong>：Sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。</li><li><strong>Sigmoid函数的输出不是零中心的：</strong>这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在$f=w^Tx+b$中每个元素都x&gt;0），那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式f而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。</li></ul><p><strong>Tanh函数</strong><br>Tanh函数图像如上图右边所示。它将实数值压缩到[-1,1]之间。和Sigmoid神经元一样，它也存在饱和的问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。注意tanh神经元是一个简单放大的sigmoid神经元，具体说来就是：$tanh(x)=2\sigma(2x)-1$。</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171206/FFaKj6dLAf.jpg?imageslim" alt="mark"><br></div><br>左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当x=0时函数值为0。当x&gt;0函数的斜率为1。右边是从Krizhevsky等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。</p><p><strong>ReLU</strong>:在近些年ReLU变得非常流行。它的函数公式是$f(x)=max(0,x)$。换句话说，这个激活函数就是一个关于0的阈值。使用Relu有以下一些优缺点：</p><ul><li>优点：相较于Sigmoid和Tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用。据称这是由它的线性，非饱和的公式导致的。</li><li>优点：Sigmoid和Tanh神经元含有指数运算等耗费计算资源的操作，而Relu可以简单地对一个矩阵进行阈值计算得到。</li><li>缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。</li></ul><p><strong>Leaky ReLU:</strong>Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x<0时，函数值为0。而leaky relu则是给出一个很小的负数梯度值，比如0.01。所以其函数公式为$f(x)="1(x<0)(\alpha" x)+1(x="">=0)(x)$其中$\alpha$是一个小的常量。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。</0时，函数值为0。而leaky></p><p><strong>Maxout</strong>:一些其他类型的单元被提了出来，它们对于权重和数据的内积结果不再使用$f(w^Tx+b)$函数形式。一个相关的流行选择是Maxout（最近由Goodfellow等发布）神经元。Maxout是对ReLU和leaky ReLU的一般化归纳，它的函数是：$max(w^T_1x+b_1,w^T_2x+b_2)$。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当$w_1,b_1=0$的时候）。这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。</p><p>以上就是一些常用的神经元及其激活函数。最后需要注意一点：在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有什么根本性问题来禁止这样做。</p><p><strong>一句话</strong>：“那么该用那种呢？”用ReLU非线性函数。注意设置好学习率，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。</p><h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><h3 id="灵活地组织层"><a href="#灵活地组织层" class="headerlink" title="灵活地组织层"></a>灵活地组织层</h3><p><strong>将神经网络算法以神经元的形式图形化</strong>。神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接。也就是说，一些神经元的输出是另一些神经元的输入。在网络中是不允许循环的，因为这样会导致前向传播的无限循环。通常神经网络模型中神经元是分层的，而不是像生物神经元一样聚合成大小不一的团状。对于普通神经网络，最普通的层的类型是全连接层（fully-connected layer）。全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接。下面是两个神经网络的图例，都使用的全连接层：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171206/7bhbD6c9F0.jpg?imageslim" alt="mark"><br></div><br>左边是一个2层神经网络，隐层由4个神经元（也可称为单元（unit））组成，输出层由2个神经元组成，输入层是3个神经元。右边是一个3层神经网络，两个含4个神经元的隐层。注意：层与层之间的神经元是全连接的，但是层内的神经元不连接。<br><strong>命名规则</strong>：当我们说N层神经网络的时候，我们没有把输入层算入。因此，单层的神经网络就是没有隐层的（输入直接映射到输出）。因此，有的研究者会说LR或者SVM只是单层神经网络的一个特例。研究者们也会使用人工神经网络或者多层感知器来指代神经网络。很多研究者并不喜欢神经网络算法和人类大脑之间的类比，它们更倾向于用单元(unit)而不是神经元作为术语。<br><strong>输出层</strong>：和神经网络中其他层不同，输出层的神经元一般是不会有激活函数的（或者也可以认为它们有一个线性相等的激活函数）。这是因为最后的输出层大多用于表示分类评分值，因此是任意值的实数，或者某种实数值的目标数（比如在回归中）。<br><strong>确定网络尺寸：</strong>用来度量神经网络的尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数，用上面图示的两个网络举例：</p><ul><li>第一个网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。</li><li>第二个网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。</li></ul><p>为了方便对比，现代卷积神经网络能包含约1亿个参数，可由10-20层构成（这就是深度学习）。然而，有效（effective）连接的个数因为参数共享的缘故大大增多。在后面的卷积神经网络内容中我们将学习更多。</p><h2 id="前向传播计算举例"><a href="#前向传播计算举例" class="headerlink" title="前向传播计算举例"></a>前向传播计算举例</h2><p>不断重复的矩阵乘法与激活函数交织。将神经网络组织成层状的一个主要原因，就是这个结构让神经网络算法使用矩阵向量操作变得简单和高效。用上面用上面那个3层神经网络举例，输入是[3x1]的向量。一个层所有连接的强度可以存在一个单独的矩阵中。比如第一个隐层的权重W1是[4x3]，所有单元的偏置储存在b1中，尺寸[4x1]这样，每个神经元的权重都在W1的一个行中，于是矩阵乘法np.dot(W1, x)就能计算该层中所有神经元的激活数据。类似的，W2将会是[4x4]矩阵，存储着第二个隐层的连接，W3是[1x4]的矩阵，用于输出层。完整的3层神经网络的前向传播就是简单的3次矩阵乘法，其中交织着激活函数的应用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 一个3层神经网络的前向传播:</span></div><div class="line">f = <span class="keyword">lambda</span> x: <span class="number">1.0</span>/(<span class="number">1.0</span> + np.exp(-x)) <span class="comment"># 激活函数(用的sigmoid)</span></div><div class="line">x = np.random.randn(<span class="number">3</span>, <span class="number">1</span>) <span class="comment"># 含3个数字的随机输入向量(3x1)</span></div><div class="line">h1 = f(np.dot(W1, x) + b1) <span class="comment"># 计算第一个隐层的激活数据(4x1)</span></div><div class="line">h2 = f(np.dot(W2, h1) + b2) <span class="comment"># 计算第二个隐层的激活数据(4x1)</span></div><div class="line">out = np.dot(W3, h2) + b3 <span class="comment"># 神经元输出(1x1)</span></div></pre></td></tr></table></figure></p><p>在上面的代码中，W1，W2，W3，b1，b2，b3都是网络中可以学习的参数。注意x并不是一个单独的列向量，而可以是一个批量的训练数据（其中每个输入样本将会是x中的一列），所有的样本将会被并行化的高效计算出来。注意神经网络最后一层通常是没有激活函数的（例如，在分类任务中它给出一个实数值的分类评分）。</p><blockquote><p>全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。</p></blockquote><h2 id="表达能力"><a href="#表达能力" class="headerlink" title="表达能力"></a>表达能力</h2><p>理解具有全连接层的神经网络的一个方式是：可以认为它们定义了一个由一系列函数组成的函数族，网络的权重就是每个函数的参数。如此产生的问题是：该函数族的表达能力如何？存在不能被神经网络表达的函数吗？</p><p>现在看来，拥有至少一个隐层的神经网络是一个通用的近似器。在研究（例如1989年的论文Approximation by Superpositions of Sigmoidal Function，或者Michael Nielsen的这个直观解释。）中已经证明，给出任意连续函数f(x)和任意$\epsilon &gt;0$，均存在一个至少含1个隐层的神经网络g(x)（并且网络中有合理选择的非线性激活函数，比如sigmoid，对于$\forall x$，使得$|f(x)-g(x)|&lt;\epsilon$。换句话说，<strong>神经网络可以近似任何连续函数。</strong></p><p><strong>既然一个隐层就能近似任何函数，那为什么还要构建更多层来将网络做得更深？</strong></p><p>答案是：虽然一个2层网络在数学理论上能完美地近似所有连续函数，但在实际操作中效果相对较差。在一个维度上，虽然以a,b,c为参数向量“指示块之和”函数$g(x)=\sum_ic_i1(a_i&lt;x&lt;b_i)$ 也是通用的近似器，但是谁也不会建议在机器学习中使用这个函数公式。神经网络在实践中非常好用，是因为它们表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，网络通过最优化算法（例如梯度下降）能比较容易地学习到这个函数。类似的，虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好。</p><p>另外，在实践中3层的神经网络会比2层的表现好，然而继续加深（做到4，5，6层）很少有太大帮助。卷积神经网络的情况却不同，在卷积神经网络中，对于一个良好的识别系统来说，深度是一个极端重要的因素（比如数十(以10为量级)个可学习的层）。对于该现象的一种解释观点是：因为图像拥有层次化结构（比如脸是由眼睛等组成，眼睛又是由边缘组成），所以多层处理对于这种数据就有直观意义。</p><h2 id="设置层的数量和尺寸"><a href="#设置层的数量和尺寸" class="headerlink" title="设置层的数量和尺寸"></a>设置层的数量和尺寸</h2><p>在面对一个具体问题的时候该确定网络结构呢？到底是不用隐层呢？还是一个隐层？两个隐层或更多？每个层的尺寸该多大？</p><p>首先，要知道当我们增加层的数量和尺寸时，网络容量上升了。即神经元们可以合作表达许多复杂的函数，所以表达函数的空间增加。例如，如果有一个在二维平面上的二分类问题，我们可以训练3个不同的神经网络，每个网络都只有一个隐藏层，但是每层的神经元数目不同：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171206/ik4JL8K3l1.jpg?imageslim" alt="mark"><br></div><br>数据是用不同颜色的圆点表示他们的不同类别，决策边界是由训练过的神经网络做出的。</p><p>在上图中，可以看见有更多神经元的神经网络可以表达更复杂的函数。然而，这既是优势也是不足，优势是可以分类更复杂的数据，不足是可能造成对训练数据的过拟合。过拟合是网络对数据中的噪音有很强的拟合能力，而没有重视数据间（假设）的潜在基本关系。举例来说，有20个神经元隐层的网络拟合了所有的训练数据，但是其代价是把决策边界变成了许多不相连的红绿区域。而有3个神经元的模型的表达能力只能用比较宽泛的方式去分类数据。它将数据看做是两个大块，并把个别在绿色区域内的红色点看做噪声。在实际中，这样可以在测试数据中获得更好的泛化（generalization）能力。</p><p>基于上面的讨论，看起来如果数据不是足够复杂，则似乎小一点的网络更好，因为可以防止过拟合。然而并非如此，防止神经网络的过拟合有很多方法（L2正则化，dropout和输入噪音等），后面会详细讨论。在实践中，使用这些方法来控制过拟合比减少网络神经元数目要好得多。</p><p>不要减少网络神经元的数目的主要原因<strong>在于小网络更难使用梯度下降等局部方法来进行训练。</strong>虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。因为神经网络是非凸的，就很难从数学上研究这些特性。即便如此，还是有一些文章尝试对这些目标函数进行理解，例如The Loss Surfaces of Multilayer Networks这篇论文。在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。<br>重申一下，<strong>正则化强度是控制神经网络过拟合的好方法</strong>。看下图结果：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171206/08eJ9cAAKD.jpg?imageslim" alt="mark"><br></div><br>不同正则化强度的效果：每个神经网络都有20个隐层神经元，但是随着正则化强度增加，它的决策边界变得更加平滑。</p><p><strong>需要记住的是：不应该因为害怕出现过拟合而使用小网络。相反，应该尽可能使用大网络，然后使用正则化技术来控制过拟合。</strong></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本节课主要介绍如下内容：</p><ul><li>介绍了生物神经元的粗略模型；</li><li>讨论了几种不同类型的激活函数，其中ReLU是最佳推荐；</li><li>介绍了神经网络，神经元通过全连接层连接，层间神经元两两相连，但是层内神经元不连接；</li><li>理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算；</li><li>理解了神经网络是一个通用函数近似器，但是该性质与其广泛使用无太大关系。之所以使用神经网络，是因为它们对于实际问题中的函数的公式能够某种程度上做出“正确”假设。</li><li>讨论了更大网络总是更好的这一事实。然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合。在后续章节中我们讲学习更多正则化的方法，尤其是dropout。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>使用Theano的deeplearning.net tutorial:<a href="http://www.deeplearning.net/tutorial/mlp.html" target="_blank" rel="external">http://www.deeplearning.net/tutorial/mlp.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文转自：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-1/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-1/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;简介
    
    </summary>
    
      <category term="课程笔记" scheme="https://ilewseu.github.io/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="cs231n" scheme="https://ilewseu.github.io/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>CS231n课程笔记(5)  Backprop Note</title>
    <link href="https://ilewseu.github.io/2017/12/09/CS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC5%E8%AF%BE-Backprop-Note/"/>
    <id>https://ilewseu.github.io/2017/12/09/CS231n课程笔记-第5课-Backprop-Note/</id>
    <published>2017-12-09T14:17:23.000Z</published>
    <updated>2017-12-16T16:06:23.168Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转自：<a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/optimization-2/" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/optimization-2/</a></p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>目的</strong>:本节帮助读者对<strong>反向传播</strong>形成直观而专业的理解。反向传播是利用链式法则递归计算梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。<br><a id="more"></a><br><strong>问题描述</strong>：核心问题是：给定函数f(x),其中x是输入数据向量，需要计算函数f关于x的梯度，也就是$\nabla f(x)$</p><p><strong>原因</strong>：之所以关注上述问题，是因为在神经网络中f对应的是损失函数L，输入x里面包含训练数据和神经网络权重。举个例子，损失函数可以是SVM的损失函数，输入则包含了训练数据$(x_i,y_i),i=1,…N$，权重W和偏差b。给定训练数据，权重是可以控制的变量。因此，即使使用反向传播计算输入数据$x_i$上的梯度，但在实践上为了进行参数更新，通常也只计算参数(W和b)的梯度。然而$x_i$<br> 的梯度有时仍然是有用的：比如将神经网络所做的事情可视化便于直观理解的时候，就能用上。</p><h2 id="梯度的简单表达、解释"><a href="#梯度的简单表达、解释" class="headerlink" title="梯度的简单表达、解释"></a>梯度的简单表达、解释</h2><p>首先，考虑一个简单的二元函数f(x,y)=xy。对两个输入变量分别求偏导数，能够很简单求出：$$<br>f(x,y)=xy \rightarrow \frac {df}{dx}=y \frac {df}{dy}=x$$<br><strong>解释：</strong>要牢记导数的意义：函数变量在某个点周围的极小区域内变化，而导数就是变量变化导致的函数在该方向上的变化率。$$<br>\frac {df(x)}{dx} = \lim_{h\rightarrow0} \frac {f(x+h)-f(x)}{h}$$<strong>对于上述公式，当h的值非常小时，函数可以被一条直线近似，而导数就是这条直线的斜率。</strong>换句话说，每个变量的导数指明了整个表达式对于该变量的值的敏感程度。例如，若x=4,y=-3，则f(x,y)=-12,x的导数$\frac {\partial f}{\partial x}=-3$，这就说明将变量x的值变大一点，整个表达式的值就会变小，而且变小的量是x变大的量的三倍。</p><p>如上所述，梯度$\nabla f$是偏导数的向量，所以有$\nabla f(x)=[\frac {\partial f}{\partial x},\frac {\partial f}{\partial y}] = [y,x]$。我们可以对加法操作进行求导：$$f(x,y)=x+y \rightarrow \frac {df}{dx}=1 \frac {df}{dy}=1$$这就是说，无论其值如何，x,y的导数均为1。这是有道理的，因为无论增加x,y中任一个的值，函数f的值都会增加，并且增加的变化率独立于x,y的具体值（情况和乘法操作不同）。取最大值操作也是常常使用的：$$<br>f(x,y)=max(x,y)\rightarrow \frac {df}{dx}=1(x&gt;=y) \frac {df}{dy}=1(y&gt;=x)$$上式是说，如果该变量比另一个变量大，那么梯度是1，反之为0。例如，若x=4,y=2，那么max是4，所以函数对于y就不敏感。也就是说，在y上增加h，函数还是输出为4，所以梯度是0：因为对于函数输出是没有效果的。当然，如果给y增加一个很大的量，比如大于2，那么函数f的值就变化了，但是导数并没有指明输入量有巨大变化情况对于函数的效果，他们只适用于输入量变化极小时的情况，因为定义已经指明：$lim_{h\to 0}$。</p><h2 id="使用链式法则计算复杂表达式的导数"><a href="#使用链式法则计算复杂表达式的导数" class="headerlink" title="使用链式法则计算复杂表达式的导数"></a>使用链式法则计算复杂表达式的导数</h2><p>现在考虑更复杂的包含多个函数的复合函数，比如$f(x,y,z)=(x+y)z$。虽然，这个表达式足够简单，可以直接进行微分，但是在此使用一种有助于直观理解反向传播的算法。将公式分为两部分：$q=x+y,f=qz$。在前面已经介绍过如何对这分开的两个公式进行计算导数：$$<br>\frac {\partial f}{\partial q} = z,\frac {\partial f}{\partial z}=q<br>$$因为，q=x+y，所以，$$<br>\frac {\partial q}{\partial x}=1,\frac {\partial q}{\partial y}=1<br>$$然而，并不需要关心中间量q的梯度，因为$\frac {\partial f}{\partial q}$没有用。相反，函数f关于x、y、z的梯度才是需要关注的。<strong>链式法则</strong>指出将这些梯度表达式链接起来的正确方式是相乘，比如$\frac {\partial f}{\partial x}=\frac {\partial f}{\partial q} \frac {\partial q}{\partial x}$在实际的操作中，只是简单地将两个梯度数值相乘。<br>最后得到变量的梯度[dfdx, dfdy, dfdz]，它们告诉我们函数f对于变量[x, y, z]的敏感程度。这是一个最简单的反向传播。一般会使用一个更简洁的表达符号，这样就不用写df了。这就是说，用dq来代替dfdq，且总是假设梯度是关于最终输出的。<br>这次计算可以被可视化为如下计算线路的图像：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171202/eicdBebej5.jpg?imageslim" alt="mark"><br></div><br>上图的真实值计算线路展示了计算的视觉化过程。前向传播从输入计算到输出（绿色），反向传播从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。</p><h2 id="反向传播的直观理解"><a href="#反向传播的直观理解" class="headerlink" title="反向传播的直观理解"></a>反向传播的直观理解</h2><p>反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输出并立即计算两个东西：</p><ol><li>这个门的输出值；</li><li>其输出值关于输入值的局部梯度。<br>门单元完成这两件事是完全独立的，它不需要知道计算路线中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元将最终获得整个网络的最终输出值在自己的输出值上的梯度。<strong>链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。</strong></li></ol><blockquote><p>这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等等。</p></blockquote><p>下面通过例子来对这一过程进行理解。加法门收到了输入[-2, 5]，计算输出是3。既然这个门是加法操作，那么对于两个输入的局部梯度都是+1。网络的其余部分计算出的最终值为-12。在反向传播时将递归地使用链式法则，算到加法门的时候，知道加法门的输出梯度是-4。如果网络想要输出值更高，那么可以认为它会想要加法门的输出更小一点，而且还有一个4的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让-4乘以x和y的局部梯度，x和y的局部梯度都是1，所以最终都是-4）。可以看到得到了想要的效果：如果x，y减小（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。</p><p>因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高。</p><h2 id="模块化：Sigmoid例子"><a href="#模块化：Sigmoid例子" class="headerlink" title="模块化：Sigmoid例子"></a>模块化：Sigmoid例子</h2><p>上面介绍的门是相对随意的。任何可微分的函数都可以看做门。可以将多个门组合成一个门，也可以根据需求将一个函数拆成多个门。现在看一个表达式：$$<br>f(w, x) = \frac {1}{1+e^{-(w_0x_0+w_1x_1+w_2}}<br>$$<br>在后面的课程中可以看到，这个表达式描述了一个含输入x和权重w的2维的神经元，该神经元使用了sigmoid激活函数。但是现在只是看做是一个简单的输入为x和w，输出为一个数字的函数。这个函数是由多个门组成的。除了上文介绍的加法门，乘法门，取最大值门，还有下面这4种：$$<br>f(x) = \frac {1}{x} \rightarrow \frac{df}{dx} = - \frac {1}{x^2}\\\\<br>f_c(x) = c+x \rightarrow \frac{df}{dx} = 1 \\\\<br>f(x) = e^x \rightarrow \frac{df}{dx} = e^x\\\\<br>f_a(x) = ax \rightarrow \frac{df}{dx} = a\\\\<br>$$其中，函数$f_c$使用对输入值进行了常量c的平移，$f_a$将输入值扩大了常量a倍。它们是加法和乘法的特例，但是这里将其看做一元门单元，因为确实需要计算常量c，a的梯度，整个计算的线路如下：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171205/bdDBIj68lK.jpg?imageslim" alt="mark"><br></div><br>在上面的例子中可以看见一个函数操作的长链条，链条上的门都对w和x的点积结果进行操作。该函数被称作为sigmoid函数，sigmoid函数关于其输入的求导是可以简化的：$$<br>\sigma(x) = \frac {1}{1+e^{-x}}\\\\<br>\frac {d\sigma(x)}{dx} = \frac {e^{-x}}{(1+e^{-x})^2}=(\frac {1+e^{-x}-1}{1+e^{-x}})(\frac {1}{1+e^{-x}}) = (1-\sigma(x))\sigma(x)<br>$$可以看到梯度计算简单了很多。举个例子，sigmoid表达式输入为1.0，则在前向传播中计算出输出为0.73。根据上面的公式，局部梯度为(1-0.73)*0.73~=0.2，和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。</p><h2 id="反向传播实践：分段计算"><a href="#反向传播实践：分段计算" class="headerlink" title="反向传播实践：分段计算"></a>反向传播实践：分段计算</h2><p>看另外一个例子，假设有如下函数：$$<br>f(x,y) = \frac {x+\sigma(y)}{\sigma(x)+(x+y)^2}<br>$$首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子，需要强调的是，如果对x或y进行微分运算，运算结束后会得到一个巨大而复杂的表达式。然而做如此复杂的运算实际上并无必要，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码模式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">x = <span class="number">3</span> <span class="comment"># 例子数值</span></div><div class="line">y = <span class="number">-4</span></div><div class="line"><span class="comment"># 前向传播</span></div><div class="line">sigy = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-y)) <span class="comment"># 分子中的sigmoi          #(1)</span></div><div class="line">num = x + sigy <span class="comment"># 分子                                    #(2)</span></div><div class="line">sigx = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-x)) <span class="comment"># 分母中的sigmoid         #(3)</span></div><div class="line">xpy = x + y                                              <span class="comment">#(4)</span></div><div class="line">xpysqr = xpy**<span class="number">2</span>                                          <span class="comment">#(5)</span></div><div class="line">den = sigx + xpysqr <span class="comment"># 分母                               #(6)</span></div><div class="line">invden = <span class="number">1.0</span> / den                                       <span class="comment">#(7)</span></div><div class="line">f = num * invden                                         <span class="comment">#(8)</span></div></pre></td></tr></table></figure></p><p>到了表达式最后，就完成了前向传播。注意在构建代码s时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。这样计算反向传播就简单了：我们对前向传播时产生每个变量(sigy, num, sigx, xpy, xpysqr, den, invden)进行回传。我们会有同样数量的变量，但是都以d开头，用来存储对应变量的梯度。注意在反向传播的每一小块中都将包含了表达式的局部梯度，然后根据使用链式法则乘以上游梯度。对于每行代码，我们将指明其对应的是前向传播的哪部分。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 回传 f = num * invden</span></div><div class="line">dnum = invden <span class="comment"># 分子的梯度                                         #(8)</span></div><div class="line">dinvden = num                                                     <span class="comment">#(8)</span></div><div class="line"><span class="comment"># 回传 invden = 1.0 / den </span></div><div class="line">dden = (<span class="number">-1.0</span> / (den**<span class="number">2</span>)) * dinvden                                <span class="comment">#(7)</span></div><div class="line"><span class="comment"># 回传 den = sigx + xpysqr</span></div><div class="line">dsigx = (<span class="number">1</span>) * dden                                                <span class="comment">#(6)</span></div><div class="line">dxpysqr = (<span class="number">1</span>) * dden                                              <span class="comment">#(6)</span></div><div class="line"><span class="comment"># 回传 xpysqr = xpy**2</span></div><div class="line">dxpy = (<span class="number">2</span> * xpy) * dxpysqr                                        <span class="comment">#(5)</span></div><div class="line"><span class="comment"># 回传 xpy = x + y</span></div><div class="line">dx = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></div><div class="line">dy = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></div><div class="line"><span class="comment"># 回传 sigx = 1.0 / (1 + math.exp(-x))</span></div><div class="line">dx += ((<span class="number">1</span> - sigx) * sigx) * dsigx <span class="comment"># Notice += !! See notes below  #(3)</span></div><div class="line"><span class="comment"># 回传 num = x + sigy</span></div><div class="line">dx += (<span class="number">1</span>) * dnum                                                  <span class="comment">#(2)</span></div><div class="line">dsigy = (<span class="number">1</span>) * dnum                                                <span class="comment">#(2)</span></div><div class="line"><span class="comment"># 回传 sigy = 1.0 / (1 + math.exp(-y))</span></div><div class="line">dy += ((<span class="number">1</span> - sigy) * sigy) * dsigy                                 <span class="comment">#(1)</span></div><div class="line"><span class="comment"># 完成!</span></div></pre></td></tr></table></figure></p><p>需要注意的一些事情：<br><strong>对前向传播变量进行缓存：</strong>计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际的操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播时也能用上。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。</p><p><strong>在不同分支的梯度要相加</strong>：如果变量x、y在前向传播的表达式中出现多次，那么进行反向传播时要非常小心使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。</p><h2 id="回传流中的模式"><a href="#回传流中的模式" class="headerlink" title="回传流中的模式"></a>回传流中的模式</h2><p>一个有趣的现象是在多数情况下，反向传播中的梯度可以被很直观的解释。例如，神经网络中最常用的加法、乘法和取最大值的这三个门单元，它们在反向传播过程中的行为都非常简单的解释，先看下面的这个例子：</p><p><div align="center"><br>    <img src="http://of6h3n8h0.bkt.clouddn.com/blog/171205/9j7IEJIBf6.jpg?imageslim" alt="mark"><br></div><br>一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。从此例可知：</p><p><strong>加法门单元</strong>：把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门就把梯度2.0不变且相等地路由给了两个输入。</p><p><strong>取最大值门单元</strong>：对梯度做路由，和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余是0。上例中，取最大值门将梯度2.0转给类z变量，因为z的值比w高，于是w的梯度保持为0。</p><p><strong>乘法门单元</strong>：相对不容易解释，它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.0*2.0 = -8.0。</p><p>非直观影响及其结果。注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，那么乘法门的操作将会不是那么直观：它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。在线性分类器中，权重和输入是进行点积$w^Tx_i$，这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本$x_i$乘以1000，那么权重的梯度将会增大1000倍，这样就必须降低学习率来弥补。这就是为什么数据预处理关系重大，它即使只是有微小变化，也会产生巨大影响。对于梯度在计算线路中是如何流动的有一个直观的理解，可以帮助读者调试网络。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li><p>对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。</p></li><li><p>讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文转自：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/optimization-2/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/optimization-2/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;:本节帮助读者对&lt;strong&gt;反向传播&lt;/strong&gt;形成直观而专业的理解。反向传播是利用链式法则递归计算梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。&lt;br&gt;
    
    </summary>
    
      <category term="课程笔记" scheme="https://ilewseu.github.io/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="DeepLearning" scheme="https://ilewseu.github.io/tags/DeepLearning/"/>
    
      <category term="cs231n" scheme="https://ilewseu.github.io/tags/cs231n/"/>
    
  </entry>
  
</feed>
