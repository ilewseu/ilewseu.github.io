<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[词向量]]></title>
    <url>%2F2018%2F07%2F08%2F%E8%AF%8D%E5%90%91%E9%87%8F%2F</url>
    <content type="text"><![CDATA[看了多篇关于词向量的论文及博文，特此整理一下，以加强理解。 简介在NLP的相关任务中，要将自然语言交给机器学习中的算法来处理，通常需要首先将语言数学化，将语言转化为机器能够识别的符号，然后再进行各种计算。词向量就是用来将语言中的词进行数学化的一种方式。顾名思义，词向量就是把一个词表示成一个向量。 一种最简单的词向量是One-hot Representation，它利用一个很长的向量表示一个词，向量的长度为词典D的大小$|V|$，向量的分量只有一个1，其他全是0,1的位置对应该词在词典中的索引。这种词向量表示有如下缺点： 存在语义鸿沟，不能很好地刻画词与词之间的相似性； 维数灾难、稀疏； 无法表示新词。 另一种就是Distributed Representation，它最早是Hinton在1986年提出的，可以克服one-hot Representation的缺点。其基本想法是：通过训练将某种语言中的每一个词映射为一个固定长度的向量，所有的向量构成一个词向量空间，可以在这个词向量空间上进行各种NLP任务。其词向量表示的核心是：利用上下文信息进行词表示；（具有相同（类似）上下文信息的词应该具有相同的词表示）。根据建模的不同，主要可以分为三类： 基于矩阵的分布表示； 基于聚类的分布表示； 基于神经网络的分布表示； 三种类型的分布表示使用了不同的技术手段获取词向量，它们的核心思想分为两步：（1）选择一种方式描述上下文；（2）选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系。下面就介绍几种常见的词向量: word2vecCBOW和Skip-gram介绍Word2Vec是Google于2013年提出的一种高效训练词向量的模型,核心的思想是上下文相似的两个词,它们的词向量也应该相似。比如，香蕉和苹果经常出现在相同的上下文中，因此这两词的表示向量应该比较相似。word2vec模型利用这个基本思想提出了两种基本的模型CBOW和Skip-gram，其架构如下： word2vec中比较重要的概念是词的上下文，其实就是一个词其周围的词，比如$w_t$的范围为1的上下文就是$w_{t-1}$和${w_{t+1}}$。可以看出，两个模型都包含三层：输入层、投影层和输出层。CBOW是在已知当前词$w_t$的上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$的前提下预测当前词$w_t$；Skip-gram是在已知当期词$w_t$的前提下，预测上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$。 基于神经网络的语言模型的目标函数通常取为如下对数似然函数: $$L=\sum_{w\in C}log p(w|Context(w))$$ 其中，$w$表示当前词，$C$表示语料库中的所有的词。其中关键是条件概率p(w|Context(w))的构建。word2vec中的CBOW模型，其优化的目标函数形如上式，即已知上下文是使当前词出现概率最大，而Skip-gram模型的优化目标则形如:$$L=\sum_{w\in C}log p(Context(w)|w)$$即已知当前词使上下文出现的概率最大。 为什么这样做就能够得到词向量？ 模型的目标函数可以理解为对于语料中的词及其上下文，最大化当前词和上下文同时出现的概率，而通过训练模型中的参数能够使优化的目标函数达到最优，即最优化后的模型的参数已经学习到语料中语义信息，能够对于给定的上下文给出最有可能的当前词，而对于相同的上下文，能够给出比较相似的当前词。这在一定程度上满足了上下文相同的词，其语义也是相似的假设。所以，可以把学习到的参数作为词的向量表示。（个人理解） 基于Softmax的梯度更新推导无论是CBOW和Skip-gram都可以看做是一个分类问题，即对于给定的输入(当前词或上下文)，预测可能的词（上下文或当前词）。CBOW和Skip-gram的架构均可以看成是一个只包含一个隐层的神经网络，只不过没有激活函数，其梯度更新推导可以按照BP神经网络的过程来进行，本文按照[3]中的方式，从上下文只有一个词的情况开始，然后扩展到CBOW和Skip-gram，分别介绍模型的参数更新方式。 One-word Context model 首先，对于输入只有一个词,且输出只有一个词，如下图所示： 其中： $V$：语料库中词汇个数； $N$：隐层神经元个数，同时也是词向量的维度； $W\in R^{V*N}$：输出层到隐层的权重矩阵，每一行代表一个词的词向量； $W^{‘}$ R^{N*V}$:隐层到输出层权重矩阵，其中每一列可以看作是额外的一种词向量； 模型表示用输入词预测输出的词，输入层的词$w_I$使用One-hot表示，即在上图输入层中的结点$x_1,x_2,…,x_V$只有$x_k$为1，其余为0，其中$k$可以是输入的词在词汇表中的索引下标。从输入层到隐藏层,输入的词$w_I$经过与矩阵$W$相乘，相当于取出$W$中的第k行，实际也就是输入词$w_I$的$N$维向量，使用$v_{w_I}$表示，以此作为隐藏层的输入。与神经网络不同的是，word2vec隐层并没有激活函数:$$h = W^T\cdot X=v_{w_I}^T$$然后，从隐层的$h$到输出层$Y$，$h$与矩阵$W^{‘}$相乘，得到一个$V*1$的向量$u$:$$u=W^{‘}\cdot h$$ 其中：$u$每个元素$u_j$就是$W^{‘}$的第$j$列用$v_{w_j}^{‘}$表示，与$h$做内积得到：$u_j=v_{w_j}^{‘}\cdot h$，表示词汇表中第$j$个词的得分。 因为是对于给定的词预测输出词，由于词汇表中的词是多个，因此使用softmax将u归一化到[0,1]之间，从而作为输出词的概率，即：$$p(w_j|w_I)=y_j = \frac {exp(u_j)}{\sum_{k\in V}exp(u_k)} = \frac {exp(v_{w_j}^{‘T}\cdot v_{w_I})}{\sum_{k\in V}exp(v_{w_k}^{‘T}\cdot v_{w_I})}$$其中$v_w$与$v_w^{‘}$都称为词$w$的词向量，一般使用前者作为词向量。 上面的模型可以看成是对于给定的一个输出词，给出一个输出词。因此，训练数据中一个训练样本可以是$(w_I,w_O)$的形式，$w_I$和$w_O$分别是One-Hot表示，因此模型的目标函数可以表示为如下形式：$$\begin{align}L &amp;= max P(w_O|w_I)=max_{y_{j*}}=max log y_{j^*}\\&amp;=max log(\frac {exp(u_{j^*})}{\sum exp(u_k)})\\&amp;=max u_{j^*} - log \sum_{k=1}^V exp(u_k)\end{align}$$ 将最大化目标函数，转换为最小化目标函数，因此，损失函数可以定义为:$$E = -u_{j^*}+log \sum_{k=1}^V exp(u_k)$$其中，$j^*$是真实单词在词汇表中的下标。 损失函数已经给定，剩下的就是结合反向传播算法，使用梯度下降法来更新参数。 隐层到输出层矩阵$W^{‘}$梯度更新计算： $$\begin{align}\frac {\partial E}{\partial w_{ij}^{‘}}&amp;=\frac {\partial E}{\partial u_j} \frac {\partial u_j}{\partial w_{ij}^{‘}}=(y_j - t_j)h_i\end{align}$$其中，$\frac {\partial E}{\partial u_j}=y_j - t_j$,$y_j$表示模型的输出，即归一化后的第$j$项概率值，$t_j$真实值的第$j$项$y_j - t_j$可以理解为输出值第j项和真实值的差值。 因此，隐层到输出层参数$W^{‘}$更新公式为：$$w_{ij}^{‘}=w_{ij}^{‘} - \eta(y_j - t_j)h_i=w_{ij}^{‘} - \eta e_j h_i\\e_j=y_j-t_j$$写成向量的形式为：$$v_{w_j}^{‘}=v_{w_j}^{‘} - \eta e_j h, j\in {1,2,…,V}$$ 输入层到隐层参数W的梯度更新计算: 首先，看一下隐层结点$h_i$的梯度计算：$$\frac {\partial E}{\partial h_i} = \sum_{j=1}^V \frac {\partial E}{\partial u_j}\frac {\partial u_j}{\partial h_i} = \sum_{j=1}^V e_j w_{ij}^{‘}=EH_i$$ 其中,$h_i$是隐层的第$i$个结点的输出。定义$EH$为一个$N$维向量，由$E_i,i=1,2,…,N$组成，则整个隐藏结点$h$关于损失函数的梯度为：$$\frac {\partial E}{\partial h}=EH^T$$从上面的介绍可以看出$h$就是$W$的一行，但是因为输入中只有一个1，因此每次只能更新一行，其余行的梯度为0，所以$v_{W_I}$的更新公式为：$$v_{W_I}^T =v_{W_I}^T - \eta EH^T $$ 到此为止，一个训练样本的反向传播训练过程就计算完了。相比传统的神经网络，在这里没有引入激活函数，在一定程度上能够加速训练，但是每次更新需要变量整个词表计算一遍概率，计算量还是很大的。 CBOW 上面介绍的是对于输入只有一个词，输出也只有一个词的模型，本部分将word2vec的第一种模型：CBOW，即给定上下文预测当前词，其模型如下所示： 和上面介绍的模型不同的是，输入不再是一个词，而是多个词，上图中一个C个词：$x_{1k},x_{2k},…,x_{Ck}$每个$x$都是One-Hot表示，这些词就是上下文$Context(x)$。这样，计算隐层的输入$h$时，不再是直接复制某一行，而是将输入的C个词对应$W$中的$C$行取出，然后取均值，作为隐层的输入$h$:$$h = \frac {1}{C} W^T(x_1+x_2+…+x_C)=\frac {1}{c}(v_{w_1}+v_{w_2}+…+v_{w_C})^T$$这样损失函数变成下面的写公式：$$E = -log p(w_O|w_{I,1},…,w_{I,C})=-u_{j^*} + log \sum_{k=1}^V exp(u_k)$$ 从模型结构可以看出，隐层到输出层与上面介绍的模型是一样的，因此采用反向传播训练，$W^{‘}$的参数更新公式是一样的，即：$$v_{w_j}^{‘}=v_{w_j}^{‘} - \eta e_j h, j\in {1,2,…,V}$$ 同样，隐层神经元的梯度也是一样的：$$\frac {\partial E}{\partial h}=EH^T$$ 从输入层到隐层的更新，因为输入变为$C$个词，所以，每个词对应的向量都应该更新，更新公式如下：$$v_{w_{I,c}} = v_{w_{I,c}} - \frac {1}{C} \cdot \eta \cdot EH^T, c = 1,2,…,C$$可以看出，CBOW的参数更新方式与一个词的基本一样。 Skip-gram Skip-gram是word2vec的第二种模型，是根据当前词预测上下文，这个模型与第一个介绍的模型比，Skip-gram的输出有多个词，而不是一个词，这样输出层就不是一个多项式分布了，而是$C$个多项式分布，模型架构图如下所示： 我们还是使用$v_{w_I}$表示输出词的向量，从输入层到隐层与One-word Context Model相同，隐层神经元的计算方式如下： $$h = W_{(k,.)}^T=v_{w_I}^T$$ 输出层不在是计算一个多项式分布，而是计算$C$个多项式分布$y_1,y_2,…,y_C$，因此前向计算的过程也需要分开计算。因此，计算第$c$个输出单词的预测的多项式分布中的第$j$项相比One-word Context Model多一个$c$参数:$$p(w_{c,j}=w_{O,c}|w_I)=y_{c,j} = \frac {exp(u_{c,j})}{\sum_{k=1}^V exp(u_{c,k})}$$需要注意的是这$C$个输出向量是相互独立的，可以看成是$C$个独立的One-word Context Model中的输出向量，相互之间没有影响。虽然，输出是相互独立的，但隐层到输出层的参数$W^{‘}$是共享的，所以：$$u_{c,j} = u_j = {v_{w_j}^{‘}}^T$$ 所以，从前向后，根据上述公式计算出$C$个输出向量之后，在每个$V$维向量中，选取概率最大的作为输出的单词，这样就根据输入单词$w_I$得到$C$个输出单词，也就达到根据单词预测上下文的目的了。 Skip-gram的损失函数和One-word Context Model的损失函数有所不同，其损失函数如下：$$\begin{align}E &amp;= -log p(w_1w_2,…,w_C|w_I)\\&amp;=-log \prod_{c=1}^C p(w_c|w_I)\\&amp;=-log \prod_{c=1}^C \frac {exp(u_{c,j})} {\sum_{k=1}^V exp(u_{c,k})}\\&amp;=- \sum_{c=1}^C u_{j_c^*} + C \cdot \sum_{k=1}^V exp(u_k)\end{align}$$其中，$j_c^*$的含义同One-word Context Model中的$u_{j^*}$一样，都表示训练样本真实输出单词在词汇表中的下标。 下面从后向前介绍参数更新方式，对第$c$个词对应的多项式分布的第$j$项的梯度为：$$\frac {\partial E}{\partial u_{c,j}} = y_{c,j} - t_{c,j}=e_{c,j}$$表示输出结点的预测误差，我们用一个$V$维的向量表示该输出误差$EI={EI_1,…,EI_V}$为所有上下文词预测输出的误差之和： $$EI_j = \sum_{c=1}^C e_{c,j}$$下一步，我们就可以计算隐层到输入层的参数$W^{‘}$的梯度：$$\frac {\partial E}{\partial w_{ij}^{‘}}=\sum_{c=1}^C \frac {\partial E}{\partial u_{c,j}} \frac {\partial u_{c,j}}{\partial w_{ij}^{‘}}=EI_j \cdot h_i$$因此，可以得到参数的更新方式：$$w_{ij}^{‘} = w_{ij}^{‘} - \eta \cdot EI_j \cdot h_i$$或者写成向量的形式：$$v_{w_j}^{‘} = v_{w_j}^{‘} - \eta \cdot EI_j \cdot h , for j=1,2,…,V $$ 接着计算隐层神经元的梯度：$$\frac {\partial E}{\partial h_i} = \sum_{c=1}^C\sum_{j=1}^V \frac {\partial E}{\partial u_{c,j}} \frac {\partial u_{c,j}}{\partial h_i}= \sum_{c=1}^C\sum_{j=1}^V e_{c,j}W_{ij}^{‘}=\sum_{j=1}^V EI_j w_{ij}^{‘} = W_i^{‘} \cdot EI$$同样，整理成向量的形式：$$\frac {\partial E}{\partial h} = W^{‘}\cdot EI$$ 因为输出只有一个词，所以，每次训练更新只更新$W$的一行：$$v_{w_I}^T = v_{w_I}^T - \eta W^{‘}\cdot EI$$至此，Skip-gram的参数更新方式也介绍完毕。 两种加速训练方法到目前为止，我们讨论的模型都是原始形式，没有使用任何的优化方法，训练效率还是很低的。对于所有讨论过的模型，首先需要学习两个词向量矩阵$W$和$W^{‘}$。对于每一个训练样本而言，CBOW更新$W$的$C$行，Skip-gram更新W其中的一行；但是对于$W^{‘}$，无论是CBOW还是Skip-gram，对于每个训练样本，需要对$W^{‘}$的所有元素进行更新，考虑到词汇表中的词汇一般是几十万甚至千万级别，这个计算量还是很大的。除了，参数更新计算量比较大，在输出层softmax函数计算输出层V个元素，计算量也是很大。针对此存在两种优化策略Hierarchical Softmax和Negative Sampling。二者的出发点一致，就是在每个训练样本中，不再完全计算或者更新$W^{‘}$这个矩阵。二者都不再显示使用$W^{‘}$这个矩阵。下面就分别介绍这两种加速训练方法： Hierarchical SoftmaxHierarchical Softmax是Bengio在2005年最早提出来专门为了加速计算神经语言模型中的Softmax的一种方式，使用一个哈夫曼树表示词汇表中的$V$个词，$V$个词分布在树的叶节点，可以证明非叶节点有$V-1$个。如下图所示： 由于在隐层和输出层的Softmax计算量较大，层次Softmax避免要计算所有词的Softmax，采用哈夫曼树来代替从隐层到输出Softmax层的映射，计算Softmax概率只要沿着哈夫曼树结构进行。首先，定义如下符号：（以下部分来自4） $p^w$:表示从根节点到w对应的叶子结点的路径； $l^w$:表示路径$p^w$上包含的结点个数； $p_1^w,p_2^w,…,p_{l^w}^w$:表示路径$p^w$上对应的结点； $d_2^w,d_3^w,…,d_{l^w}^w\in {0,1}$:表示路径上的Huffman编码，根节点不对应编码； $\theta_1^w,\theta_2^w,…,\theta_{l^w}^w \in R^m$:表示路径上非叶子结点对应的向量； 从根节点出发到某个叶子结点的路径上，每个分支都可视为进行了一次二分类。默认左边（编码为0）是父类，右边（编码为1）是正类。 分为正类的概率为：$\sigma(X_w^T\theta)=\frac {1}{1+e^{-X_w^T\theta}}$ 分为负类的概率为：$1-\sigma(X_w^T\theta)$ 其中，$\theta$为当前非叶子结点对应的词向量，$X_w^T$表示隐藏的输出； 所以，Hierarchical Softmax的思想就是： 对于词典中的任意词$w$，Huffman树中必存在一条从根节点到词$w$对应叶子节点$p^w$的路径。路径$p^w$上存在$l^w-1$个分支，将每个分支看做一次二分类，每次分类就产生一个概率，将这些概率连乘，即$p(w|Context(w))$。 对于CBOW 根据上下文$Context(w)$，预测当前词$w$,基于Hierarchical Softmax的思想，可以得出: $$p(w|Context(w))=\prod_{j=2}^{l^w}p(d_j^w|X_w,\theta_{j-1}^w) \\其中 \ p(d_j^w|X_w,\theta_{j-1}^w) =\begin{cases}\sigma(X_w^T\theta_{j-1}^w) &amp; d_j^w=0 \ 1-\sigma(X_w^T\theta_{j-1}^w) &amp; d_j^w=1\end{cases} \\= [\sigma(X_w^T\theta_{j-1}^w)]^{1-d_j^w} \cdot [1-\sigma(X_w^T\theta_{j-1}^w)]^{d_j^w}$$带入最大似然函数，得到要优化的目标函数：:$$\mathcal{L}=\sum_{w\in C}log\prod_{j=2}^{l^w}{[\sigma(X_w^T\theta_{j-1}^w)]^{1-d_j^w} \cdot [1-\sigma(X_w^T\theta_{j-1}^w)]^{d_j^w}} \\= \sum_{w\in C}\sum_{j=2}^{l^w}{(1-d_j^w) \cdot log[\sigma(X_w^T\theta_{j-1}^w)]+d_j^w \cdot log[1-\sigma(X_w^T\theta_{j-1}^w)] }$$上面的公式即是CBOW要优化的目标函数，可以采用梯度上升法求解，具体梯度计算就不再一一推导，请参看[4]中的详细推导。 对于Skip-gram 已知当前词是$w$，对其上下文$Context(w)$中的词进行预测。同样，基于Hierarchical Softmax的思想，可以得出$$\begin{align} &amp;p(Context(w)|w)=\prod_{u\in Context(w)}p(u|w) \ &amp;p(u|w)=\prod_{j=2}^{l^u}p(d_j^u|v(w),\theta_{j-1}^u)= [\sigma(v(w)^T\theta_{j-1}^u)]^{1-d_j^u}\cdot [1-\sigma(v(w)^T\theta_{j-1}^u)]^{d_j^u}\end{align}$$ 带入最大似然函数，得到要优化的目标函数：$$\mathcal{L}=\sum_{w\in C}log\prod_{u\in Context(w)}\prod_{j=2}^{l^u}{[\sigma(v(w)^T\theta_{j-1}^u)]^{1-d_j^u}\cdot [1-\sigma(v(w)^T\theta_{j-1}^u)]^{d_j^u}} \\= \sum_{w\in C}log\sum_{u\in Context(w)}\sum_{j=2}^{l^u}{(1-d_j^u)\cdot log[\sigma(v(w)^T\theta_{j-1}^u)] + d_j^u \cdot log[1-\sigma(v(w)^T\theta_{j-1}^u)]}$$ Negative SamplingNegative Sampling是另一种加速训练的方式，我们分别介绍CBOW和Skip-gram利用Negative Sampling后其要优化的目标函数是什么样子的。 对于CBOW 我们已知词$w$的上下文$Context(w)$,需要预测$w$。假设我们已经选好一个关于$w$的负样本子集$NEG(w)$，并且定义了对于词典中的任意词$w’$，都有：$$L^w(w’)=\begin{cases}1 &amp; w’=w \ 0 &amp; w’\neq w\end{cases}$$ 对于一个给定的正样本$(Context(w),w)$，我们希望最大化：$$g(w)=\prod_{u\in {w}\bigcup NEG(w)} p(u|Context(w)) \\其中\\p(u|Context(w))=\begin{cases}\sigma(X_w^T\theta^u) &amp; L^w(u)=1 \ 1-\sigma(X_w^T\theta^u) &amp; L^w(u)=0\end{cases} \\= [\sigma(X_w^T\theta^u)]^{L^w(u)} \cdot [1-\sigma(X_w^T\theta^u)]^{1-L^w(u)}$$ 所以， $$g(w)=\sigma(X_w^T\theta^w)\prod_{u\in NEG(w)} [1-\sigma(X_w^T\theta^u)]$$为什么要最大化$g(w)$? 因为$\sigma(X_w^T\theta^w)$表示的是上下文为$Context(w)$时，预测中心词$w$的概率，而$\sigma(X_w^T\theta^u)$表示的是上下文为$Context(w)$时，预测中心词为$u$的概率。因此，最大化$g(w)$即相当于增大正样本的概率，同时降低负样本的概率，而这就是我们所期望的。 所以，对于给定的语料库$C$来说，整体的优化目标即为最大化$G=\prod_{w\in C}g(w)$，所以目标函数为： $$\begin{align}\mathcal{L}&amp;=log G=log\prod_{w\in C}g(w)=\sum_{w \in C} log\,g(w) \\&amp;= \sum_{w \in C} log \prod_{u\in {w}\bigcup NEG(w)} {[\sigma(X_w^T\theta^u)]^{L^w(u)} \cdot [1-\sigma(X_w^T\theta^u)]^{1-L^w(u)}} \ &amp;= \sum_{w \in C} \sum_{u\in {w}\bigcup NEG(w)} {L^w(u) \cdot log[\sigma(X_w^T\theta^u)] + [1-L^w(u)] \cdot log[1-\sigma(X_w^T\theta^u)]}\end{align}$$ 对于Skip-gram 和上面介绍的CBOW模型所用的思想是一样的。因此，可以直接从目标函数出发。首先，对于一个给定的样本$(w,Context(w))$，我们希望最大化：$$g(w)=\prod_{\hat{w} \in Context(w)} \prod_{u \in {w}\bigcup NEG^{\hat {w}}(w)}其中\\p(u|\hat{w})=\begin{cases}\sigma(v(\hat {w})^T\theta^u) &amp; L^w(u)=1 \ 1-\sigma(v(\hat{w})^T\theta^u) &amp; L^w(u)=0\end{cases}$$或者，写成整体的表达式：$$p(u|\hat{w}) = [\sigma(v(\hat{w})^T \theta^u)]^{L^w(u)}\cdot[1-\sigma(v(\hat{w})^T\theta^u)]^{1-L^w(u)}$$这里，$NEG^{\hat {w}}(w)$表示处理词$\hat{w}$时生成的负样本子集。所以，对于一个给定的语料库$C$,函数：$$G=\prod_{w\in C} g(w)$$就可以作为整体优化的目标。同样，我们取$G$的对数，最终的目标函数是：$$\begin{align}\mathcal{L}&amp;=logG=log \prod_{w \in C}g(w)=\sum_{w\in C}log g(w)\\&amp;=\sum_{w \in C} log \prod_{\hat {w} \in Context(w)} \prod_{w\in {w} \bigcup NEG^{\hat{w}}(w)} { [\sigma(v(\hat{w})^T \theta^u)]^{L^w(u)}\cdot[1-\sigma(v(\hat{w})^T\theta^u)]^{1-L^w(u)} } \\&amp;=\sum_{w \in C} \sum_{\hat{w}\in Context(w)} \sum_{u\in {w}\bigcup NEG^{\hat{w}}(w)}{L^w(u)\cdot log[\sigma(v(\hat{w})^T \theta^u)]+[1-L^w(u)]\cdot log[1-\sigma(v(\hat{w})^T\theta^u)]}\end{align}$$同样，最大化目标函数也同样可以利用梯度上升法进行计算梯度，这里就不在一一推导了，具体请参考：[4]。 Glove上面介绍的word2vec是一种基于预测的词向量模型。除了基于预测的词向量模型，还存在基于统计的词向量模型，以基于SVD分解技术的LSA模型为代表，通过构建一个共现矩阵，然后对矩阵进行分解得到隐层的语义向量，充分利用了全局的统计信息。然而，这类模型得到的语义向量很难获取到词与词之间的线性关系。基于预测的词向量模型，比如Skip-gram模型，通过预测一个词出现在上下文里的概率得到词向量，这类模型的缺陷在于对统计信息的利用不充分，训练时间与语料大小息息相关，其得到的词向量能够很好捕捉到词与词之间的线性关系，因此在很多任务上的表现都要优于SVD模型。 Glove模型综合了两者的优点，即使用了语料库的全局统计特征，也使用了局部的上下文特征（即滑动窗口），模型的代价函数如下： $$J = \sum_{i,j}^N f(X_{i,j})(v_i^Tv_j+b_i+b_j - log(X_{i,j}))^2$$ 其中，$v_i,v_j$是单词i和j的词向量，$b_i,b_j$是两个标量，$f$是权重函数，$N$表示词汇表的大小(共现矩阵的维度为$N*N$)。 Glove模型首先基于语料库构建词的共现矩阵，然后基于共现矩阵学习词向量。设共现矩阵为$X$，其元素为$X_{i,j}$,表示在整个语料库中，词$i$和词$j$共同出现在一个窗口中的次数，比如对于语料库： glove model is very nice 语料只有一个句子，涉及到5个单词：glove、model、is、very、nice。如果采用窗口宽度为3，左右长度都为1的统计窗口，那么就有以下窗口内容： 窗口编号 中心词 窗口内容 0 glove glove model 1 model glove model is 2 is model is very 3 very is very nice 4 nice very nice 扫描语料，可以生成上面所示的句子片段，这就包含了上下文特征，然后统计所有窗口内容中不同词的共现次数，比如$X_{glove,model}=2$，可以统计出如下的共现矩阵。 glove model is very nice glove 0 2 1 0 0 model 2 0 2 1 0 is 0 2 0 2 1 very 0 0 2 0 2 nice 0 0 1 2 0 Glove模型就是基于这个词共现矩阵来计算词向量的。首先，$X$元素$X_{i,j}$是语料库中出现在词$i$上下文中的词$j$的次数。如下，引入一些变量：$$X_i = \sum_{j=1}^N X_{i,j}\\P_{i,k} = \frac {X_{i,k}}{X_i}$$$P_{i,k}$表示条件概率，表示单词k出现在单词i上下文中的概率。$$ratio_{i,j,k} = \frac {P_{i,k}}{P_{j,k}}$$表示两个条件概率的比值。Glove模型的作者发现$ratio_{i,j,k}$这个指标有如下规律： $ratio_{i,j,k}$的值 单词j,k相关 单词j,k不相关 单词i,k相关 趋近1 很大 单词i,k不相关 很小 趋近1 可以看出ratio的值能够反映词之间的相关性，而Glove模型就是利用这个Ratio的值进行建模。如果已经得到词向量，词i,词j和词k的词向量分别用$v_i,v_j,v_k$表示，通过某种函数计算$ratio_{i,j,k}$能够得到同样的规律，那么说明词向量与共现矩阵具有很好的一致性，也就说明得到的词向量中蕴涵了共现矩阵中所蕴含的信息。假设这个未知的函数是f，则：$$F(v_i,v_j,v_k)=ratio_{i,j,k}=\frac {P_{ik}}{P_{j,k}}$$即$F(v_i,v_j,v_k)$和$ratio_{i,j,k}$应该尽可能的接近即可。因此，可以用两者的差来作为代价函数：$$J = \sum_{i,j,k}^N (\frac {P_{i,k}}{P_{j,k}} - F(v_i,v_j,v_k))^2$$如果函数F的形式确定下来，就可以通过优化算法求解词向量了。那么Glove模型的作者是怎样将F确定下来的呢？具体形式如下： $\frac {P_{ik}}{P_{jk}}$考察了$i,j,k$三个两两之间的相似关系，不妨单独考察$i,j$两个词和他们词向量$v_i,v_j$，线性空间中的相似关系自然想到的是两个向量的差$(v_i - v_j)$，所以F的函数形式可以是：$F(w_i-w_j, w_k)=\frac {P_{ik}}{P_{jk}}$ $\frac {P_{ik}}{P_{jk}}$是一个标量，而F是作用在两个向量上，向量和标量之间的关系自然想到用内积的方式。所以，F函数的形式可以进一步确定为：$F((v_i-v_j)^Tv_k)=F(v_iv_k - v_jv_k)=\frac {P_{ik}}{P_{jk}}$ 可以看到F的形式为$F(v_iv_k - v_jv_k)=\frac {P_{ik}}{P_{jk}}$，左边是差的形式，右边是商的形式，模型通过将F取exp来将差和商关联起来：$exp(v_iv_k-v_jv_k)=\frac {exp(v_i^Tv_k)}{exp(v_j^Tv_k)} = \frac {P_{ik}}{P_{jk}}$ 现在只需要让分子分母分别相等上式即可成立，所以：$exp(v_iv_k)=P_{ik},exp(v_jv_k)=P_{jk}$ 所以，只需要整个语料库中考察$exp(v_i,v_j)=P_{ij}=\frac{X_{ij}}{X_i}$，即：$v_i^Tv_j=log(\frac {X_{ij}}{X_i})=log X_{ij}-logX_i$ 由于$i$和$j$都是随机选取的，即交换$i$和$j$的顺序$v_i^Tv_j$和$v_j^Tv_i$应该是相等的，但是等式右边将$i$和$k$交换顺序$logX_{ij} - logX_i \ne logX_{ji} - logX_j$。为了解决这个问题，作者在模型中引入两个偏置项$b_i,b_j$，从而将模型变成了：$log X_{ij}=v_i^Tv_j+b_i+b_j$,即添加了一个偏置项$b_j$,并将$log(X_i)$吸收到偏置项$b_i$中。 最后，代价函数变成了如下的形式：$J=\sum_{i,j}^N (v_i^Tv_j + b_i+b_j-log(X_{i,j}))^2$ 由于出现频率越高的词对权重应该越大，所有在代价函数中加入权重项，于是代价函数进一步完善：$$J = \sum_{i,j}^N f(X_{i,j})(v_i^Tv_j+b_i+b_j - log(X_{i,j}))^2$$ 模型认为权重函数f应该符合以下三个特点： $f(0)=0$,如果两个词没有共同出现过，权重为0； $f(x)$必须是非减函数，即两个词出现的次数越多，不能权重反而变小了； $f(x)$对于较大的值$x$不能取太大的值，类似于停用词。论文中给出的权重函数为： $$f(x)=\begin{cases} (\frac{x}{x_{max}})^{0.75}, &amp; if x=x_{max}\end{cases}\\$$ 以上就是Glove模型是怎样构造的过程，可以看出有很多技巧在里面。 FastTextFastText是Facebook于2016年开源的一个词向量计算和文本分类工具，典型的应用场景是“有监督的文本分类问题”。提供简单而高效的文本和特征学习的方法，性能比肩神深度学习而且速度更快。 FastText方法包含三部分：模型架构、层次SoftMax和N-gram特征。下面就分别简要介绍各个部分。 模型架构FastText模型的输入是一个词的序列（一段文本或者一句话），输出这个词序列属于不同类别的概率。序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。FastText在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。FastText模型架构和Word2Vec中的CBOW模型很相似，不同之处在于，FastText预测的是标签，而CBOW模型预测的是中间词。FastText模型的架构如下： 层次SoftMax将输入层中的词和词组构成的特征向量，再将特征向量通过线性变换映射到隐藏层，隐藏层通过求解最大似然函数，然后根据每个类别的权重和模型参数构建Huffman树，将Huffman树作为输出。对于有大量类别的数据集，FastText使用了一个分层分类器，层次Softmax建立在Huffman编码的基础上，利用类别不均衡对标签进行编码，缩小模型预测目标的数量。当类别数为K，word embedding大小为$d$时，计算复杂度可以从$O(Kd)$降到$O(dlog(K))$。 N-gram特征如果仅仅是以词和词组构成的向量作为特征向量，FastText隐藏层通过简单的求和取平均得到，会损失次序信息。为了弥补这个不足，FastText增加了N-gram特征。具体的做法是把N-gram当成一个词，也用embedding向量来表示，在计算隐层时，把N-gram的embedding向量也加进去求和取平均。例如，假设某篇文章只有3个词，$w_1,w_2,w_3$，N-gram的N取2，$w_1、w_2、w_3$以及$w_{12}、w_{23}$分别表示词$w_1、w_2$和bigram$w_1w_2、w_2w_3$的word embidding向量，那么文章的隐层可表示为：$$h = \frac {1}{5}(w_1+w_2+w_3+w_{12}+w_{23})$$在具体实现上，由于N-gram的量远比word大的多，完全存下所有的n-gram也不现实。FastText采样了Hash桶的方式，把所有的n-gram都Hash到buckets个桶中。Hash到同一个桶的所有n-gram共享一个embedding向量。如下图所示： (注：图片来自[8]) 图中每行代表一个word或N-gram的embeddings向量，其中前V行是word embeddings，后Buckets行是n-grams embeddings。每个N-gram经过Hash函数Hash到0~bucket-1的位置，得到对应的embedding向量。 FastText对比word2vecFastText=word2vec中的CBOW+层次softmax的灵活使用，主要体现在如下两个方面： 模型的输入层：word2vec是context window内的word，而FastText是整个sentence的内容，包括word以及n-gram的内容； 模型的输出层：wore2vec对应的是每个word，计算某个word的概率最大，而FastText的输出层对应的是分类的label。 层次Softmax的使用：word2vec的目的是得到词向量，该词向量最终是在输入层得到，输出层对应的层次Softmax也会生成一些列的词向量，但最终都被抛弃，不会使用；FastText则使用了层次Softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label(一个或多个)。 词向量开源工具word2vec、Glove和FastText都提供了开源的代码及工具包供我们使用，具体地址如下： word2vec C版本：https://github.com/svn2github/word2vec C++版本：https://github.com/jdeng/word2vec python版本可使用gensim包 GloVe C版本：http://github.com/stanfordnlp/glove python版本：https://github.com/maciejkula/glove-python FastText GitHub地址：https://github.com/facebookresearch/fastText 如何训练出比较好的词向量？在来博士的论文中，对不同的词向量在不同的任务上做了相关的实验并对比，给出了训练词向量的一些建议，这部分主要来自于该博士论文。 模型比较 对于评价语言学特性的任务，通过上下文预测目标词的模型，比上下文与目标词联合打分的C&amp;B模型效果更好。 对于实际的自然语言处理任务，各模型的差异不大，选用简单的模型即可； 简单模型在小语料上整体表现更好，复杂的模型需要更大的语料作支撑。 语料影响 同领域的语料，一般语料越大效果越好； 领域内的语料对相似领域任务的效果提升非常明显，但在领域不契合时甚至会有负面作用。 规模和领域的权衡 语料的领域纯度比语料的规模更重要。 迭代次数 根据词向量的损失函数选择迭代次数不合适； 条件允许的话，选择目标任务的验证集性能作为参考标准； 具体任务性能指标趋势一样，可以选简单任务的性能表现最好的点停止迭代（比如同义词检测任务）； 词向量的维度 对于分析词向量语言学特性的任务，维度越大效果越好； 对于提升自然语言处理任务而言，50维词向量通常足够好； 总结 选择一个合适的模型。复杂的模型相比简单的模型，在较大的语料中才有优势； 选择一个合适领域的语料，在此前提下，语料规模越大越好。使用大规模的语料进行训练，可以普遍提升词向量的性能，如果使用领域内的语料，对同领域的任务会显著的提升； 训练时，迭代优化的终止条件最好根据具体任务的验证集来判断，或者近似地选取其他任务作为指标，但是不应该选用训练词向量时的损失函数；（一般可以根据训练语料大小，一般选用10~25次）； 词向量的维度一般需要选择50维及以上，特别当衡量词向量的语言学特性时，词向量的维度越大，效果越好。 参考 word2vec原论文：Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean：Efficient estimation of word representations in vector space 来斯惟：基于神经网络的词和文档语义向量表示方法研究博士论文 Xin Rong:word2vec Parameter Learning Explained word2vec中的数学原理 word2vec原理 理解Glove 如何产生好的词向量 玩转Fasttext]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理基础-句法分析]]></title>
    <url>%2F2018%2F07%2F08%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文主要是对宗成庆老师《自然语言理解》讲义，第9章的一个学习笔记，同时，也参考了刘群老师《计算语言学》的句法分析讲义，所有的配图均来自这两个讲义。通过该讲义能够对句法分析是做什么的以及怎么做，有一个基本的认识。 句法分析介绍句法分析(syntactic parsing)的任务是识别句子的句法结构(syntactic structure)，是从单词串得到句法结构的过程。不同的语法形式，对应的句法分析算法也不尽相同。句法分析的类型可以分为两类： 短语结构分析(Phrase parsing) 完全句法分析（Full parsing)—&gt;针对整个句子 局部句法分析（Partial parsing)—&gt;针对句子中的一部分 依存句法分析 由于短语结构语法（特别是上下文无关语法）应用得最为广泛，因此以短语结构树为目标的句法分析器研究得最为彻底。很多其他形式语法对应的句法分析器都可以通过对短语结构语法的句法分析器进行简单的改造得到。 人们在正常交流中所使用的语言，放在特定的环境下看，一般是没有歧义的，否则人们将无法交流（某些特殊情况如幽默或双关语除外）。如果不考虑语言所处的环境和语言单位上下文，将发现语言的歧义现象无所不在。但是，一般来说，语言单位的歧义现象在引入更大的上下文范围或者语言环境时总是可以被消解的。句法分析的核心任务就是消解一个句子在句法结构上的歧义。例如，对于下面的句子,可以得到两种句法分析结果。 句法分析策略目前，比较成熟的句法分析主要是基于上下文无关的分析方法，本文也主要介绍基于上下文无关的句法分析方法。句法分析策略指的是句法分析的过程中按照何种方式进行分析，通常采用的策略有： 自顶向下分析法； 自底向上分析法； 左角分析法； 其他策略。 常见的上下文无关语法的句法分析算法有： 线图分析法（Chart parsing） CYK算法； Earley算法； Tomita算法； … 句法分析的过程也可以理解为句法树的构造分析过程。所谓自顶向下分析法也就是先构造句法树的根节点，再逐步向下扩展，直到叶结点；所谓自底向上分析法就是先构造句法树的叶节点，再逐步上上合并， 直到根节点。左角分析法是一种自顶向下和自底向上相结合的方法。 自顶向下的方法又称为基于预测的方法，也就是说，这种方法产生对后面将要出现的成分的预期，然后再通过逐步吃进待分析的字符串来验证预期。如果预期得到了证明，就说明待分析的字符串可以被分析为所预期的句法结构。如果某一个环节上出现了差错，那就要用另外的预期来替换（即回溯）。如果所有环节上所有可能的预期都被吃进的待分析字符串所“反驳”，那就说明待分析的字符串不可能是一个合法的句子，分析失败。 自底向上的方法也叫基于规约的方法。就是说，这种方法是先逐步吃进待分析字符串，把它们从局部到整体层层规约为可能的成份。如果整个待分析字符串被规约为开始字符号S，那么分析成功。如果在某个局部证明不可能有任何从这里把整个待分析字符串规约为句子的方案，那么就需要回溯。 例如，对于下面的一个句子及语法规则： 采用自顶向下分析出句法结构的步骤如下：自底向上分析法及左角分析法与自顶向下分析方法类似，具体就不一一在举例说明。 句法分析算法基于上下文无关的句法分析有多种，本文简要介绍线图分析算法、CYK算法和概率上下文无关算法PCFG。 线图(Chart)分析算法线图分析法同样有3种策略来构建句法分析树，分别是： 自底向上； 自顶向下； 从上到下和从下到上结合。 本文介绍基于自底向上的Chart分析算法。算法的具体情况如下： 给定一组CFG规则：$XP \rightarrow \alpha_1…\alpha_n(n \geq 1)$ 给定一个句子的词性序列：$S=W_1W_2…W_n$ 构造一个线图：一组结点和边的集合 建立一个二维表：记录每一条边的起始位置和终止位置。 执行如下操作：查看任意相邻几条边上的词性串是否与某条重写规则的右部相同，如果相同，则增加一条新的边跨越原来的相应的边，新增加边上的标记为这条重写规则的头（左部）。重复这个过程，直到没有新的边产生。 线图分析法中引入点规则，表示规则右部被规约（匹配）的程度。点规则的具体定义如下：线图分析法中用到如下数据结构： 线图(Chart):保存分析过程中已经建立的成份（包括终结符和非终结符）、位置（包括起点和终点）。通常以n*n的数组表示（n为句子包含的词数）； 代理表（代处理表）（Agenda):记录刚刚得到的一些重写规则所代表的成分，这些重写规则的右端符号串与输入的词性串（或短语标志串）中的一段完全匹配，通常以栈或线性队列表示； 活动边集（ActiveArc）:记录那些右端符号串与输入串的某一段相匹配，但还未完全匹配的重写规则，通常以数组或列表存储。 基于如上定义的点规则和数据结构，线图分析法的算法描述如下：从输入串的起始位置到最后位置，循环执行如下步骤： (1) 如果待处理表（Agenda）为空，则找到下一个位置上的词，将该词对应的（所有）词类X附以(i,j)作为元素放到待处理表中，即X(i,j)。其中，i,j分别是该词的起始位置和终止位置，$j&gt;i$,j-i为该词的长度。 (2) 从Agenda中取出一个元素X(i,j) (3) 对于每条规则$A \rightarrow X\gamma$，将$A \rightarrow X \cdot \gamma(i,j)$加入到活动边集ActiveArc中，然后调用扩展弧子程序 扩展弧子程序: (a) 将X插入图表(Chart)的(i，j)位置中。 (b) 对于活动边集(ActiveArc)中每个位置为$(k,i)(i \leq k &lt; i)$的点规则，如果该规则具有如下形式：$A \rightarrow \alpha \cdot X$，如果A=S,则把S(1,n+1)加入到Chart中，并给出一个完整的分析结果；否则，将A(k,j)加入到Agenda表中。 (c) 对于每个位置为(k,i)的点规则:$A \rightarrow \alpha \cdot X \beta$，则将$A \rightarrow \alpha X \cdot \beta(k,j)$加入到活动边集中。 整体的流程大致可以描述为：从Agenda弹出元素，如果Agenda为空，则找下一个位置上的词，形成X(i,j)放入到Agenda中，然后对X(i,j)匹配所有的规则，形成点规则，加入到ActiveArc中，并扩展弧。扩展弧操作即将X(i,j)加入到Chart中，然后，检查活动边集上位置为(k,i)的点规则，进行合并，将合并出的点规则加入到ActiveArc中，并将(k,j)加入到Agenda表中，然后从Agenda中弹出元素，匹配规则，加入到Archive，然后在扩展弧，这样不断循环。 线图分析法的优点是算法简单，容易实现，开发周期短；但是，其算法效率低，时间复杂度为$Kn^3$且需要高质量的规则，分析结果与规则质量密切相关；也难以区分歧义结构。 CYK算法CYK算法的全称是Coke-Younger-Kasami(CYK)算法，是一种自下而上的分析方法，其构造一个(n+1)*(n+1)的识别矩阵，n为输入句子长度。假设输入句子为$x=w_1w_2…w_n,w_i$为构成句子的单词，$n=|x|$。识别矩阵具有如下特点： 方阵对角线以下全部为0； 主对角线以上的元素由文法G的非终结符（句法成分）构成； 主对角线上的元素由输入句子的终结符构成（单词）构成； 下图为识别矩阵的示例图： 识别矩阵构造的步骤如下： (1) 首先构造主对角线，另$t_{0,0}=0$，然后，从$t_{1,1}$到$t_{n,n}$在主对角线的位置上依次放入输入句子x的单词$w_i$。 (2) 构造主对角线上紧靠主对角线的元素$t_{i,i+1}$，其中，$i=0,1,2,…,n-1$。对于输入句子$x=w_1w_2…w_n$，从$w_1$开始分析。如果在文法G的产生式集中有一条规则：$A \rightarrow w_1$，则$t_{0,1}=A$。以此类推，如果有$A \rightarrow w_{i+1}$则$t_{i,i+1}=A$。即，对于主对角线的每一个终结符$w_i$，所有可能推导出它的非终结符写在它的右边主对角线方的位置上。 (3) 按平行于主对角线的方向，一层一层地向上填写矩阵的各个元素$t_{i,j}$，其中，$i=0,1,…,n-d,j=d+i, d=2,3,…,n$。如果存在一个正整数k，$i+1 \leq k \leq j-1$，在文法G的规则集中有产生式$A \rightarrow BC$，并且，$B \in t_{i,k},C \in t_{k,j}$，那么，将A写到矩阵$t_{i,j}$位置上。 算法的示意图如下： 下面就用具体的实例，来看CYK算法是如何进行句法分析的，对于给定的如下文法： 分析句子”他喜欢读书”的句法结构。 (1) 首先，进行分词和词性标注： 他/P喜欢/V 读/v 书/N n=4 (2) 构造识别矩阵，并执行分析过程，具体如下： 首先，由规则$VP \rightarrow V V$，产生如下结果： 然后，继续执行分析过程，将没有匹配的直接复制到右边或者上面一个位置，如下图的P和N： 然后，由规则$S \rightarrow P VP$，由于S不在最右上角的位置，说明匹配失败，不能执行这条规则的匹配，如下图： 接着，可以执行规则$VP \rightarrow VP N$，如下图： 然后，将没有匹配的直接复制到右边或上面一个位置，结果如下图所示： 最后，匹配规则$S \rightarrow P VP$，完成句法分析过程： 最终，句法分析的结果如下： CYK算法要比Chart算法要容易理解一些，其优点是：简单易行，执行效率高；但是，必须对文法进行范式化处理，且无法区分歧义。 概率上下文无关文法上面介绍的句法分析算法都是基于规则的句法分析方法，其具有一定的缺陷： 很难穷尽所有的句法规则； 很难保证规则间的一致性； 通常只能处理有限的比较规范的句子； 对真实语料的处理能力不够； 分析效率通常不高； 概率上下文无关文法，PCFG即Probabilistic CFG，直观的意义就是基于概率的短语结构分析。乔姆斯基的短语结构文法表示为一个四元组：G=(X,V,S,R)。而PCFG中增加了一个概率信息，变为五元组PCFG=(X,V,S,R,P)，其中： X：是一个有限词汇的集合(词典)。它的元素被称为词汇或终结符； V：是一个有限标注的集合，叫做非终结符集合。它的元素称为变量或非终结符； $S \in V$：称为文法的开始符号； R：是一个有序偶对$(\alpha, \beta)$的集合，也就是产生的规则集； P：代表每个产生规则的统计概率。 如果把形如”-&gt;”看作一个运算符，PCFG可以写成如下形式： 形式： $A-&gt;\alpha, P$ 约束： $\sum_{\alpha} P(A-&gt;\alpha)=1$ 比如，对于如下给定的规则集： 对于给定句子 S: Astronomers saw stars with ears。采用基于规则的句法分析会分析出如下两种结果： 利用PCFG能够计算出$t_1$和$t_2$产生的概率。最后，选择概率最大的作为句法分析的结果。PCFG计算分析树的概率作出了三个基本假设： 位置不变性：子树的概率与其管辖的词在整个句子中所处的位置无关，即对于任意的k,$p(A_{k(k+C)} \rightarrow w)$一样。 上下文无关性： 子树的概率与子树管辖范围以外的词无关，即$p(A_{kl} \rightarrow w| 任何超出$k~l$范围的上下文)=p(A_{kl} \rightarrow w)$。 祖先无关性：子树的概率与推导出该子树的祖先结点无关，即$p(A_{kl}\rightarrow w|任何除A以外的祖先结点)=p(A_{kl}\rightarrow w)$。 下图为$t_1$基于三种基本假设下的计算过程： 依据三个基本假设，可以得到$t_1$的产生的概率： $$\begin{align}P(t_1)&amp;=S*NP*VP*V*NP*NP*PP*P*NP\\&amp;=1.0*0.1*0.7*1.0*0.4*0.18*1.0*1.0*0.18\\&amp;= 0.000 9072\\P(t_2)&amp;=S*NP*VP*VP*VP*V*NP*PP*P*NP\\&amp;=1.0*0.1*0.3*0.7*1.0*0.18*1.0*1.0*1.0*0.18\\&amp;=0.000 6804\end{align}$$ 因此，对于给定的句子S，两棵句法分析树的概率不等$P(t_1)&gt;P(t_2)$，因此，可以得出结论：分析结果$t_1$正确的可能性大于$t_2$。 根据上述的例子，可以很自然地想到关于PCFG算法的如下三个基本问题： (1) 给定句子$W=w_1w_2…w_n$和PCFG G，如何快速计算$p(W|G)$? (2) 给定句子$W=w_1w_2…w_n$和PCFG G，如何快速地选择最佳句法结构树？ (3) 给定句子$W=w_1w_2…w_n$和CFG G，如何调节G的参数，使得$p(w|G)$最大？ 内向算法与外向算法内向算法与外向算法是解决第一个问题——计算句子的句法树概率的方法。假设文法G(S)的规则只有两种形式：$$A \rightarrow w, w\in V_{T}\\A \rightarrow BC,B,C \in V_{N}$$可以通过规范式化处理，使CFG规则满足上述形式。这种假设的文法形式称为乔姆斯基范式（Chomsky Normal Form）。 内向算法的基本思想是从给定字符串的底层向上逐次推导出句子的全部概率。利用动态规划法计算非终结符A推导出的某个字串片段$w_1w_{i+1}…w_j$的概率$\alpha_{ij}(A)$。语句$W=w_1w_2…w_n$的概率即为文法G(S)中S推导出的子串概率$\alpha_{1n}(S)$。 定义： 内向变量$\alpha_{ij}(A)$是由非终结符A推导出的语句W中子字串$w_iw_{i+1}…w_j$的概率：$$\alpha_{ij}(A) = p(A \rightarrow w_iw_{i+1}…w_j)$$计算$\alpha_{ij}(A)$的递推公式：$$\begin{align}&amp;\alpha_{ii}(A) = p(A \rightarrow w_i)\\&amp;\alpha_{ij}(A) = \sum_{B,C\in V_N}\sum_{i \leq k \leq j}p(A \rightarrow BC)\alpha_{ik}(B)\alpha_{(k+1)j}(C)\end{align}$$ 当$i==j$时，字符串$w_iw_{i+1}…w_j$只有一个字$w_{ii}$，可以简单记作$w_i$，由A推导出$w_i$的概率就是产生式$A \rightarrow w_i$的概率$p(A \rightarrow w_i)$; 当$i \neq j$时，也就是说，字符串$w_iw_{i+1}…w_j$至少有两个词，根据约定，A要推导出该词串，必须首先运用产生式$A \rightarrow BC$，那么，可用B推导出前半部$w_i…w_k$，用C推导出后半部$w_{k+1}…w_j$。由这一推导过程产生$w_iw_{i+1}…w_j$的概率为：$p(A \rightarrow BC)\alpha_{ik}(B)\alpha_{(k+1)j}(C)$。考虑到B、C和k取值的任意性，应计算各个概率的总和。 内向算法的描述如下： 输入：文法G(S)，语句$W=w_1w_2…w_n$ 输出：$p(S\rightarrow w_1w_2…w_n)$ (1) 初始化：$\alpha_{ij}(A) = p(A \rightarrow w_i) A\in V_N, 1 \leq i \leq j \leq n$ (2) 归纳计算：$j=1…n,i=1…n-j$，重复下列计算：$$\alpha_{ij}(A) = \sum_{B,C\in V_N}\sum_{i \leq k \leq i+j}p(A \rightarrow BC)\alpha_{ik}(B)\alpha_{(k+1)j}(C)$$ (3) 终结：$p(S\rightarrow w_1w_2…w_n)=\alpha_{1n}(S)$ 外向算法的基本思想是从给定字符串的顶层向下逐次推导出句子的概率。 定义：外向变量$\beta_{ij}(A)$是由文法初始符号S推导出语句$W=w_1w_2…w_n$的过程中，到达扩展符号串$w_1…w_{i-1}Aw_{j+1}…w_n$的概率$A=w_i…w_j$:$$\beta_{ij}(A) = p(S\rightarrow w_1…w_{i-1}Aw_{j+1}…w_n)$$同样，$\beta_{ij}(A)$可由动态规划算法求得，其递推公式为：$$\begin{align} &amp;\beta_{1n}=\delta(A,S) (初始化) \\ &amp;\beta_{ij}(A)=\sum_{B,C}\sum_{k&gt;j} \beta_{ik}(B)p(B \rightarrow AC)\alpha_{(j+1)k}(C)+\sum_{B,C}\sum_{k_i}\beta_{kj}(B)p(B\rightarrow CA)\alpha_{k(i-1)}(C)\end{align}$$ 具体的解释如下： (1) 当$i=1,j=n$时，即$w_iw_{i+1}…w_j$是整个语句W时，根据乔姆斯基语法范式的约定，不可能有规则$S \rightarrow A$，因此，由S推导出W的过程中，如果$A \neq S$的话，A推导出W的概率为0，即$\beta_{1n}(A)=0$。如果$A=S,\beta_{1n}(A)$为由初始符S推导出W的概率，因此，$\beta_{1n}(A)=1$ (2) 当$i\neq 1$或者$j \neq n$时，如果在S推导出W的过程中出现字符串$w_1…w_{i-1}Aw_{j+1}…w_n$，则推导过程必定使用规则$B \rightarrow AC$或$B \rightarrow CA$。假定运用了规则$B \rightarrow AC$推导出$w_i…w_j w_{j+1}…w_k$，则该推导可以分解为以下三步： 由S推导出$w_1…w_{i-1}Bw_{k+1}…w_n$，其概率为$\beta_{ik}(B)$; 运用产生式$B \rightarrow AC$扩展非终结符B，其概率为$p(B \rightarrow AC)$; 由非终结符C推导出$w_{j+1}…w_k$，其概率为$\alpha_{(j+1)k}(C)$。 考虑到B、C和k的任意性，在计算$\beta_{ik}(B)$时，必须考虑所有可能的B、C和k，因此，计算概率时必须要考虑所有情况下的概率之和。同样方法，可以计算出运用产生式$B \rightarrow CA$推导出$w_i…w_jw_{j+1}…w_k$的概率。 其示意图如下： 外向算法描述如下： 输入：PCFG G=(S,N,T,P)，语句$W=w_1w_2…w_n$输出：$\beta_{ij},A,A\in N, 1\leq i \leq j \leq n$ (1) 初始化： $\beta_{1n}(A) = \delta(A,S),A \in N$;(2) 归纳： j从n-1到0，i从1到n-j，重复计算：$$\beta_{i(i+j)}(A) = \sum_{B,C}\sum_{i+j\leq k \leq n}p(B \rightarrow AC)\alpha_{(i+j+1)k}(C)\beta_{ik}(B)+\sum_{B,C}\sum_{1 \leq k \leq i}p(B \rightarrow CA)\alpha_{k(i-1)}(C)\beta_{k(i+j)}(B)$$(3) 终结： $p(S \rightarrow w_1w_2…w_n)=\beta_{1n}(S)$ Viterbi算法第二个问题是如何选择最佳的句法树，其解决方法是使用Viterbi算法，在短语结构文法中，Viterbi的定义如下： Viterbi变量的$\gamma_{ij}(A)$是由非终结符A推导出语句W中子字串$w_iw_{i+1}…w_j$的最大概率; 变量$\psi_{i,j}$用于记忆字串$W=w_1w_2…w_n$的Viterbi语法分析结果。 Viterbi算法描述如下： 输入：文法G(S)，语句$W=w_1w_2…w_n$输出：$\gamma_{1n}(S)$ (1) 初始化： $\gamma_{ii}A=p(A\rightarrow w_i),A\in V_N, 1\leq i \leq j \leq n$ (2) 归纳计算： j=1…n, i=1…n-j，重复下列计算：$$\begin{align}&amp;\gamma_{i(i+j)}(A) = max_{B,C\in V_N;i\leq k \leq i+j}p(A\rightarrow BC)\gamma_{ik}(B)\gamma_{(k+1)(i+j)}(C)\\&amp;\psi_{i(i+j)}(A) = max_{B,C\in V_N;i\leq k \leq i+j}p(A\rightarrow BC)\gamma_{ik}(B)\gamma_{(k+1)(i+j)}(C)\end{align}$$(3) 终结：$p(S\rightarrow w_1w_2…w_n)=\gamma_{1n}(S)$ 参数估计如果有大量已经标注语法结构的训练语料，则可以通过语料直接计算每个语法规则的使用次数。已知训练语料中的语法结构，记录每个语法规则的使用次数，使用最大似然估计来计算PCFG的参数。 一般来说，树库的标注需要投入较大的人力，时间和资金投入都比较大。而且，树库也不可能覆盖所有的词汇信息，在NLP中，使用有限的资源预估目标的概率分布，这种情况是常态。所谓参数估计，就是对未知现象出现概率的一种计算方法。PCFG中常用的方式是EM算法。 EM算法的基本思想是：初始时随机地给参数赋值，得到语法$G_0$，依据$G_0$和训练语料，得到语法规则使用次数的期望值，以期望次数用于最大似然估计，得到语法参数新的估计值，由此得到新的语法$G_1$，由$G_1$再次得到语法规则的使用次数的期望值，然后又可以重新估计语法参数。循环这个过程，语法参数将收敛与最大似然估计值。 PCFG中使用的方法称为内向、外向算法：给定CFG G和训练数据$W=w_1w_2…w_n$，语法规则$A\rightarrow BC$使用次数的期望值为：$$\begin{align}Count(A \rightarrow BC) &amp;=\sum_{1\leq i\leq k \leq j \leq n}P(A_{ij},B_{ik},C_{(k+1)j}|w_1…w_n,G)\\&amp;=\frac {1}{p(w_1…w_n|G)}\sum_{1\leq i\leq k \leq j \leq n} p(A_{ij},B_{ik},C_{(k+1)j},w_1,…w_n|G)\\&amp;=\frac {1}{p(w_1…w_n|G)}\sum_{1\leq i\leq k \leq j \leq n} \beta_{ij}(A)p(A\rightarrow BC)\alpha_{ik}(B)\alpha_{(k+1)j}(C)\end{align}$$ 上面的公式的解释：给定了语句$W=w_1…w_n$PCFG G中产生$A\rightarrow BC$在产生的W的过程中被使用次数的期望值为：在所有可能的情况下，即在条件$1\leq i\leq k \leq j \leq n$下，W的语法分析结构中$w_i…w_k$由B导出，$w_{k+1},…w_{j}$由C导出，$w_i…w_j$由A导出的概率总和。 类似的，语法规则$A\rightarrow a$的使用次数的期望值为：$$\begin{align} Count(A\rightarrow a) &amp;=\sum_{1 \leq i \leq n} p(A_{ii}|w_1…w_n,G)\\&amp;=\frac {1}{p(w_1…w_n|G)}\sum_{1 \leq i \leq n}p(A_{ii},w_1…w_n|G)\\&amp;= \frac {1}{p(w_1…w_n|G)}\sum_{1 \leq i \leq n}\beta_{ii}(A)p(A\rightarrow a)\delta(a,w_i)\end{align}$$G的参数可由如下公式重新估计：$$\hat p(A\rightarrow \mu)= \frac {C(A\rightarrow \mu)}{\sum_{\mu}C(A \rightarrow \mu)}$$其中，$\mu$要么为终结符号，要么为两个非终结符号串，即$A\rightarrow \mu$为乔姆斯基语法范式要求的两种形式。 内向、外向算法的步骤如下： (1) 初始化：随机地给出$P(A\rightarrow \mu)$的赋值，使得$\sum_{\mu}P(A\rightarrow \mu)=1$，由此得到语法$G_0$,令$i=0$。 (2) EM算法步骤如下： E步：根据$G_i$的公式，计算期望值$Count(A\rightarrow BC)$和$Count(A \rightarrow a)$； M步:用E-步所得的期望值，根据公式重新估计$p(A\rightarrow \mu)$，得到$G_(i+1)$ (3) 循环：i=i+1，重复EM步骤，直至$p(A\rightarrow \mu)$值收敛。 依存句法分析 这一部分暂时先挖个坑 参考 宗成庆《自然语言理解》讲义-第9章 刘群《计算语言学》讲义-第7章]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理基础-中文分词]]></title>
    <url>%2F2018%2F06%2F16%2F%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[中文分词问题介绍词是自然语言中能够独立运用的最小单位，是信息处理的基本单位。自然语言处理的对象是一个个的句子，拿到句子之后一般要对句子进行分词。分词就是利用计算机识别出文本中词的过程。大部分的印欧语言，词与词之间有空格之类的显示标志指示词的边界。因此，利用很容易切分出句子中的词。而与大部分的印欧语言不同，中文语句中词与词之间没有空格标志指示，所以，需要专门的方法去实现中文分词。分词是文本挖掘的基础，通常用于自然语言处理、搜索引擎、推荐等领域。中文分词的难点主要有： 歧义无处不在 新词层出不穷 需求多种多样 (1) 歧义无处不在 分词的歧义主要包括如下几个方面： 交集型歧义，例如： 研究/生命/的/起源 研究生/命/的/起源 组合型歧义，例如： 门/把/手/弄/坏/了 门/把手/弄/坏/了 真歧义，例如： 乒乓球/拍/卖/完了 乒乓球/拍卖/完了 (2) 新词层出不穷 人名、地名、机构名 网名 公司名、产品名 (3) 需求多种多样 切分速度:搜索引擎vs单集版语音合成 结果呈现： 切分粒度的要求不同； 分词重点要求不同； 唯一结果vs多结果； 新词敏感程度不同； 处理对象：书面文本(规范/非规范)vs口语文本 硬件平台：嵌入式vs单机版vs服务器版 本文只介绍分词相关的基本算法原理及思想，其他的暂时不介绍。 中文分词算法目前，中文分词技术已经非常成熟，学者们研究出了很多的分词方法，这些方法大致可以分为三类。第一类是基于字符串匹配的，即扫描字符串，如果发现字符串的子串和词典中的词相同，就算匹配，比如机械分词方法。这类分词方法通常会加入一些启发式规则，例如，正向最大匹配、反向最大匹配、长词优先等。第二类时基于统计的分词方法，它们基于人工标注的词性和统计特征，对中文进行建模，即根据观测到的数据（标注好的语料）对模型参数进行训练，在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词结果作为最终结果。常见的序列标注模型有HMM和CRF。这类分词算法能够很好的处理歧义和未登录词问题，效果要比前一类效果好，但是需要大量的人工标注数据，且分词速度较慢。第三类是理解分词，通过让计算机模拟人对句子的理解，达到识别词的效果。由于汉语语义的复杂性，难以将各种语言信息组织成机器能够识别的形式，目前这种分词系统还处于试验阶段。 基于字符串匹配的方法这一类方法也叫基于词表的分词方法，基本思路是首先存在一份字典，对于要分词的文本从左至右或从右至左扫描一遍，遇到字典里有最长的词就标识出来，遇到不认识的字串就分割成单字词。常见的方法有： 正向最大匹配法(forward maximum matching method,FMM) 逆向最大匹配法(backward maximum matching method, BMM) N-最短路径方法 正向最大匹配法指从左到右对一个字符串进行匹配，所匹配的词越长越好，比如“中国科学院计算机研究所”，按照词典中最长匹配的切分原则切分的结果是：“中国科学院/计算研究所”，而不是”中国/科学院/计算/研究所”。但是，正向匹配会存在一些bad case，常见的例子如：”他从东经过我家”，使用正向最大匹配会得到错误的结果：”他/从/东经/过/我/家” 逆向最大匹配法是从右向左倒着匹配，如果能够匹配到更长的词，则优先选择，上面的”他从东经过我家”逆向最大匹配能够得到正确的结果，”他/从/东/经过/我/家”。但是，逆向最大匹配同样也存在bad case：“他们昨日本应该回来”，逆向匹配会得到错误的结果”他们/昨/日本/应该/回来” 针对正向逆向匹配的问题，将双向切分的结果进行比较，选择切分词语数量最少的结果。但是最少切分结果同样有bad case，比如：”他将来上海”，正确的切分结果是”他/将/来/上海”，有4个词，而最少切分结果“他/将来/中国”只有3个词。 基于字符串匹配的方法的优点如下： 程序简单易行，开发周期短； 没有任何复杂计算，分词速度快； 不足： 不能处理歧义； 不能识别新词； 分词准确率不高，不能满足实际的需要； N-最短路径方法的基本思想是：设待分字串$S=c_1c_2…c_n$，其中$c_i(i=1,2,…,n)$为单个字，n为串的长度，$n \geq 1$。建立一个结点数为n+1的切分有向无环图G，各结点编号依次为$V_0,V_1,V_2,…,V_n$。如下图所示： 其算法如下： （1） 相邻结点$v_{k-1},v_k$之间建立有向边$$，边对应的词默认为$c_k(k=1,2,..,n)$ （2） 如果$w=c_ic_{i+1}…c_j(0 &lt; i]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理基础-条件随机场(Conditional Random Fields,CRFs)学习笔记]]></title>
    <url>%2F2018%2F05%2F21%2FCRF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[本篇笔记来自李航老师的《统计学习方法》一书 条件随机场最早由Lafferty等人2001年提出，其模型思想的主要来源是最大熵模型，模型的三个基本问题的解决用到了HMMs模型中的方法如forward-backward和Viterbi。我们可以把条件随机场看成是一个无向图模型或马尔科夫随机场，它是一种用来标记和切分序列化数据的统计模型。该模型在给定需要标记观察序列的条件下，计算整个标记序列的联合概率，而不是在给定当前状态条件下，定义一个状态的分布。标记序列的分布条件属性，可以让CRFs很好的拟合现实数据，而在这些数据中，标记序列的条件概率依赖于观察序列中非独立的、相互作用的特征，并通过赋予特征不同权值来表示特征的重要程度。 概率无向图模型概率图模型(Graphical Models)概率图模型是一类用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的集合。图中的节点表示随机变量，缺少边表示条件独立假设。根据图中边有无方向，分为有向图和无向图。概率无向图，又称为马尔科夫随机场，是一个可以由无向图表示的联合概率分布。 图是由节点以及连接节点的边组成的集合。节点和边分别记作为v和e，节点和边的集合分别记作V和E，图记作G=(V,E)。无向图是指边没有方向的图。概率图模型是由图表示的概率分布。设有联合概率分布$P(Y),Y \in \cal Y$是一组随机变量。由无向图G=(V,E)表示概率分布P(Y)，即在图G中，结点$v\in V$表示一个随机变量$Y_v，Y=(Y_v)_{v\in V}$；边$e\in E$表示随机变量之间的概率依赖关系。 给定一个联合概率分布P(Y)和表示它的无向图G。首先定义无向图表示的随机变量之间存在的成对马尔科夫性(pairwise Markov property)、局部马尔科夫性(local Markov property)和全局马尔可夫性(global Markov property)。 成对马尔科夫性 设u和v是无向图G中任意两个没有边连接的结点，结点u和v分别对应随机变量$Y_u$和$Y_v$。其他所有结点为O,对应的随机变量组是$Y_O$，成对马尔科夫性是指给定随机变量组$Y_O$的条件下随机变量$Y_u$和$Y_v$是条件独立的，即：$$P(Y_u,Y_v|Y_O)=P(Y_u|Y_O)P(Y_v|Y_O)$$ 局部马尔科夫性 设$v\in V$是无向图G中任意一个结点，W是与v有边连接的所有结点，O是v，W以外的其他所有结点。v表示的随机变量是$Y_v$，W表示的随机变量是$Y_W$，O表示的随机变量组是$Y_O$。局部马尔科夫性是指在给定的随机变量组$Y_W$的条件下随机变量$Y_v$与随机变量组$Y_O$是独立的，即： (注：图引自:http://www.cnblogs.com/Determined22/p/6915730.html)$$P(Y_v,Y_O|Y_W)=P(Y_v|Y_w)P(Y_O|Y_W)$$在$P(Y_O|Y_W)&gt;0$时，等价地：$P(Y_v|Y_W)=P(Y_v|Y_W,Y_O)$全局马尔科夫性设结点集合A、B是在无向图G中被结点集合C分开的任意结点集合，全局马尔科夫性指：在给定$Y_C$的条件下，$Y_A$和$Y_B$条件独立，即:$$P(Y_A,Y_B|Y_C) = P(Y_A|Y_C)P(Y_B|Y_C)$$上述成对的、局部的、全局的马尔科夫性定义是等价的。 概率无向图模型定义设有联合概率分布P(Y)，由无向图G(V,E)表示，在图G中，结点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率分布P(Y)满足成对、局部和全局马尔科夫性，就称此联合概率分布为概率无向图模型或马尔科夫随机场。以上是概率无向图模型的定义，实际上，我们更关心如何求其联合概率分布。对于给定的概率无向图模型，我们希望将整体的联合概率写成若干子联合概率乘积的形式，也就是将联合概率进行因子分解，这样便于模型的学习与计算。事实上，概率无向图模型的最大特点就是易于因子分解，下面介绍这一知识点。 首先给出无向图中的团和最大团的定义：无向图G中任何两个结点，均有边连接的结点子集称为团(clique)。若C是无向图G的一个团，并且不能再加进任何一个G的结点使其成为一个更大的团，则称此C为最大团(maximal clique)。例如，下图表示由4个结点组成的无向图。图中的2个结点组成的团有5个：${Y_1,Y_2},{Y_2,Y_3},{Y_3,Y_4},{Y_4,Y_2},{Y_1,Y_3}$。有2个最大团：${Y_1,Y_2,Y_3},{Y_2,Y_3,Y_4}$。而${Y_1,Y_2,Y_3,Y_4}$不是一个团，因为$Y_1$和$Y_4$没有边连接 将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解(factorization)。给定概率无向图模型，设其无向图为G，C为G上的最大团，$Y_C$表示C对应的随机变量。那么概率无向图的联合概率分布$P(Y)$可以写作图中的所有最大团C上的函数$\Psi_C(Y_C)$的乘积形式，即:$$P(Y) = \frac {1}{Z} \prod_C \Psi_C(Y_C)$$其中，Z是规范化因子(normalization factror)，由公式：$$Z = \sum_Y \prod_C \Psi_C(Y_C)$$计算得出。规范化因子保证P(Y)构成一个概率分布。函数$\Psi_C(Y_C)$称为势函数(potential function)。这里要求势函数$\Psi_C(Y_C)$是严格正的，通常定义为指数函数：$$\Psi_C(Y_C) = exp{-E(Y_C)}$$概率无向图模型的因子分解是由下述定理来保证的。(Hammersley-Clifford 定理):概率无向图模型的联合概率分布P(Y)可以表示为如下形式：$$P(Y)=\frac {1}{Z}\prod_C \Psi_C(Y_C) \\Z=\sum_Y \prod_C \Psi_C(Y_C)$$其中，C是无向图的最大团，$Y_C$是C的结点对应的随机变量，$\Psi_C(Y_C)$是C上定义的严格正函数，乘积是在无向图所有的最大团上进行的。 条件随机场定义条件随机场(conditional random field)是在给定随机变量X条件下，随机变量Y的马尔科夫随机场。这里主要介绍定义在线性链上的特殊的条件随机场，称为线性链条件随机场(linear chain conditional random field)。线性链条件随机场可以用于标注等问题。这时，在条件概率模型P(Y|X)中，Y是输出变量，表示标记序列，X是输入变量，表示需要标注的观测序列。也把标记序列称为状态序列。学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型$\hat P(Y|X)$;预测时，对于给定的输入序列x，求出条件概率$\hat P(y|x)$最大的输出序列$\hat y$。 首先定义一般的条件随机场，然后定义线性链条件随机场。 (条件随机场)，设X与Y是随机变量，P(Y|X)是在给定X的条件下Y的条件概率分布。若随机变量Y构成一个由无向图G=(V,E)表示的马尔科夫随机场，即：$$P(Y_v|X, w\neq v)=P(Y_v|X,Y_w, w \sim v)$$对任意结点v成立，则称条件概率分布P(Y|X)为条件随机场。式中$w \sim v$表示在图G=(V,E)中与结点v有结点v有边连接的所有结点w，$w \neq v$结点v以外的所有结点，$Y_v,Y_u与Y_w$为结点v,u与w对应的随机变量。 在定义中没有要求X和Y具有相同的结构。现实中，一般假设X和Y有相同的图结构。本书主要考虑无向图为下图所示的线性链的情况，即：$$G=(V={1,2,…,n},E={(i,i+1)}), i=1,2,…, n-1$$在此情况下，$X=(X_1,X_2,…,X_n)，Y=(Y_1,Y_2,…,Y_n)$，最大团是相邻两个结点的集合。 线性链条件随机场有下面的定义： 设$X=(X_1,X_2,…,X_n)，Y=(Y_1,Y_2,…,Y_n)$均为线性链表示的随机变量序列，若下给定随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔科夫性：$$P(Y_i|X,Y_1,..,Y_{i-1},…,Y_n)= P(Y_i|X, Y_{i-1},Y_{i+1})\\i=1,2,…,n(在i=1和n时只考虑单边)$$则称P(Y|X)为线性链条件随机场。在标注问题中，X表示输入观测序列，Y表示对应的输出标记序列或状态序列。 参数化形式根据Hammersley-Clifford定理,可以给出线性链条件随机场P(Y|X)的因子分解式，各因子是定义在相邻两个结点上的函数。定理(线性链条件随机场的参数化形式) 设P(Y|X)为线性链条件随机场，则在随机变量X取值为x的条件下，随机变量Y取值为y的条件概率具有如下形式：$$p(y|x)= \frac {1}{Z(x)} exp {\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_{l}s_l(y_i, x,i)}$$其中，$$Z(x)=\sum_y exp ({\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_{l}s_l(y_i, x,i)})$$式中，$t_k和s_l$是特征函数，$\lambda_k和\mu_l$是对应的权值。$Z(x)$是规范化因子，求和是在所有可能的输出序列上进行的。 上面的两个式子是线性链条件随机场模型的基本形式，表示给定输入序列x，对输出序列y预测的条件概率。$t_k$是定义在边上的特征函数，称为转移特征，依赖于当前和前一个位置，$s_l$是定义在结点上的特征函数，称为状态特征，依赖于当前位置。$t_k和s_l$都依赖于位置，是局部特征函数。通常，特征函数$t_k和s_l$取值为1或0;当满足特征条件时取值为1，否则为0。条件随机场完全由特征函数$t_k,s_l$和对应的权值$\lambda_k,\mu_l$确定。线性链条件随机场也是对数线性模型(log linear model)。 简化形式条件随机场还可以由简化形式表示。注意到条件随机场式中同一特征在各个位置都有定义，可以对同一个特征在各个位置求和，将局部特征函数转化为一个全局特征函数，这样就可以将条件随机场写成权值向量和特征向量的内积形式，即条件随机场的简化形式。 为简便起见，首先将转移特征和状态特征及其权值用统一的符号表示。设有$K_1$个转移特征，$K_2$个状态特征，$K=K_1+K_2$，记：$$f_k(y_{i-1},y_i, x,i) = \begin{cases}t_k(y_{i-1},y_i, x, i)&amp; k=1,2,…,K_1\\s_l(y_i, x, i)&amp; k=K_1+l; l=1,2,…,K_2\end{cases}$$然后，对转移与状态特征在各个位置i求和，记作：$$f_k(y,x) = \sum_{i=1}^n f_k(y_{i-1}, y_i, x,i), k=1,2,..,K$$用$w_k$表示特征$f_k(y,x)$的权值，即：$$w_k = \begin{cases} \lambda_k, &amp; k=1,2,…,K_1\\ \mu_l, &amp; k=K_1+l; l=1,2,…,K_2\end{cases}$$于是，条件随机场可以表示为：$$P(y|x) = \frac {1}{Z(x)}exp \sum_{k=1}^K w_kf_k(y,x)\\Z(x) = \sum_y exp \sum_{k=1}^K w_kf_k(y,x)$$若以w表示权值向量，即：$$w = (w_1,w_2,…,w_K)^T$$以F(y,x)表示全局特征向量，即：$$F(y,x) = (f_1(y,x),f_2(y,x),…,f_k(y,x))^T$$则条件随机场可以写成向量w与F(y,x)的内积形式：$$P_w(y|x) = \frac {exp(w\cdot F(y,x))}{Z_w(x)}$$其中，$$Z_w(x) = \sum_y exp(w\cdot F(y,x))$$ 矩阵形式条件随机场还可以由矩阵表示。假设$P_w(y|x)$是由内积形式给出的线性链条件随机场，表示对给定观测序列x，相应的标记序列y的条件概率。引进特殊的起点和终点状态标记$y_0=start,y_{n+1}=stop$，这时$P_w(y|x)$可以通过矩阵的形式表示。 对观察序列x的每个位置i=1,2,…,n+1，定义一个m阶矩阵(m是标记$y_i$取值个数)$$M_i(x) = [M_i(y_{i-1},y_i|x)]\\M_i(y_{i-1},y_i|x)=exp(W_i(y_{i-1},y_i|x))\\W_i(y_{i-1},y_i|x) = \sum_{i=1}^K w_kf_k(y_{i-1},y_i,x,i)$$ 这样，给定观测序列x，标记序列y的非规范化概率可以通过n+1个矩阵的乘积$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$表示，于是，条件概率$P_w(y|x)$是:$$P_w(y|x)=\frac {1}{Z_w(x)}\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$$其中，$Z_w(x)$为规范化因子，是n+1个矩阵的乘积(start,stop)元素：$$Z_w(x)= (M_1(x)M_2(x)…M_{n+1}(x))_{start,stop}$$注意，$y_0=start与y_{n+1}=stop$表示开始状态与终止状态，规范化因子$Z_w(x)$是以start为起点stop为终点通过状态的所有路径$y_1y_2,…,y_m$的非规范化概率$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$之和。 这个M矩阵和一阶HMM中的转移概率矩阵，因为链式CRF中只有相邻两个结点之间才有连接边。 三个问题概率计算问题条件随机场的概率计算问题是给定条件随机场P(Y|X)，输入序列x和输出序列y，计算条件概率$P(Y_i=y_i|x),P(Y_{i-1}=y_{i-1},Y_i=y_i|x)$以及相应的数学期望问题。为了方便起见，像隐马尔科夫模型那样，引进前向-后向向量，递归地计算以上概率及期望值。像这样的算法称为前向-后向算法。 前向-后向算法对每个指标$i=0,1,…,n+1$，定义前向向量$\alpha_i(x)$:$$\alpha_0(y|x)=\begin{cases} 1, &amp; y=start \\ 0, &amp; 否则\end{cases}$$递推公式为：$$\alpha_i^T(y_i|x)=\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x),i=1,2,…,n+1$$又可以表示为：$$\alpha_i^T(x) = \alpha_{i-1}^T(x)M_i(x)$$$\alpha_i(y_i|x)$表示在位置i的标记是$y_i$并且到位置i的前面部分标记序列的非规范化概率，$y_i$可取的值有m个，所以$\alpha_i(x)$是m维列向量。 同样，对于每个位置$i=0,1,…,n+1$，定义后向向量$\beta_i(x)$:$$\beta_{n+1}(y_{n+1}|x)=\begin{cases} 1, &amp; y=stop \\ 0, &amp; 否则\end{cases}\\\beta_i(y_i|x)=M_{i+1}(y_i,y_{i+1}|x)\beta_{i+1}(y_{i+1}|x),i=1,2,…,n+1$$又可以表示为：$$\beta_i(y_i|x)=M_{i+1}(x)\beta_{i+1}(x)$$$\beta_i(y_i|x)$表示在位置i的标记$y_i$并且从i+1到n的后面标记序列的非规范化概率。 由前向-后向向量定义不难得到：$$Z(x) = \alpha_n^T(x)\cdot 1 = 1^T \cdot \beta_1(x)$$这里的1是元素均为1的m维列向量。 概率计算 按照前向-后向向量的定义，很容易计算标记序列在为位置i是标记$y_i$的条件概率和在位置$i-1$与i的标记$y_{i-1}和y_i$的条件概率。$$\begin{align}&amp;P(Y_i=y_i|x) =\frac {\alpha_i^T(y_i|x)\beta_i(y_i|x)}{Z(x)}\\&amp;P(Y_{i-1}=y_{i-1},Y_i=y_i|x)=\frac {\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i,|x)}{Z(x)}\\&amp;其中，Z(x)=\alpha_n^T(x)\cdot 1\end{align}$$ 期望计算 利用前向-后向向量，可以计算特征函数关于联合分布P(X,Y)和条件分布P(Y|X)的数学期望。 特征函数$f_k$关于条件分布P(Y|X)的数学期望是：$$\begin{align}E_{P(Y|X)}[f_k]&amp;= \sum_y P(y|x)f_k(y,x)\\&amp;=\sum_{i=1}^{n+1} \sum_{y_{i-1},y_i}f_k(y_{i-1},y_i,x,i)\frac {\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i,|x)}{Z(x)}\\&amp; k=1,2,…,K\end{align}$$其中，$$Z(x) = \alpha_n^T(x)\cdot 1$$ 假设经验分布$\widetilde P(X)$，特征函数$f_k$关于联合分布P(X,Y)的数学期望是:$$\begin{align}E_{P(X,Y)}[f_k]&amp;=\sum_{x,y}P(x,y)\sum_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\\&amp;=\sum_x \widetilde P(x)\sum_y P(y|x)\sum_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\\&amp;=\sum_x \widetilde p(x)\sum_{i=1}^{n+1}\sum_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)\frac {\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}\\k=1,2,…,K\end{align}$$其中，$$Z(x)=\alpha_n^T(x) \cdot 1$$对于转移特征$t_k(y_{i-1},y_i,x,i),k=1,2,…,K_1$，可以将式中的$f_k$替换成$t_k$；对于状态特征，可以将式中的$f_k$替换为$s_i$，表示为$s_l(y_i,x,u),k=K_1+l,l=1,2,…,K_2$。 有了上面的公式，对于给定的观测序列x与标记序列y，可以通过一次前向扫描计算$\alpha_i及Z(x)$，通过一次后向扫描计算$\beta_i$，从而计算所有的概率和特征的期望。 学习方法条件随机场在时序数据上的对数线性模型，使用MLE或带正则的MLE来训练。类似于最大熵模型，可以采用改进的迭代尺度法(IIS)和拟牛顿法(如BFGS算法)来训练。 训练数据${(x^{j},y^{j})}_{j=1}^N$的对数似然函数为：$$\begin{align}L(w) &amp;= L_{\widetilde P}(P_w)\\&amp;=ln \prod_{j=1}^N P_w(Y=y^{(j)}|x^{(j)})\\&amp;=\sum_{j=1}^N ln P_w(Y=y^{(j)}| x^{(j)})\\&amp;=\sum_{j=1}^N ln \frac {exp \sum_{k=1}^K w_kf_k(y^{(j)},x^{(j)}) }{Z_w(x^{(j)})}\\&amp;=\sum_{j=1}^N (\sum_{k=1}^K w_kf_k(y^{(j)},x^{(j)}) - ln Z_w(x^{(j)}) )\end{align}$$ 或者可以写成这样：$$\begin{aligned}L(\textbf w)=L_{\widetilde P}(P_\textbf w)&amp;=\ln\prod_{x,y}P_{\textbf w}(Y=y|x)^{\widetilde P(x,y)}\\&amp;=\sum_{x,y}\widetilde P(x,y)\ln P_{\textbf w}(Y=y|x)\\&amp;=\sum_{x,y}\widetilde P(x,y)\ln \frac{\exp\sum_{k=1}^Kw_kf_k(y,x)}{Z_{\textbf w}(x)}\\&amp;=\sum_{x,y}\widetilde P(x,y)\sum_{k=1}^Kw_kf_k(y,x)-\sum_{x,y}\widetilde P(x,y)\ln Z_{\textbf w}(x)\\&amp;=\sum_{x,y}\widetilde P(x,y)\sum_{k=1}^Kw_kf_k(y,x)-\sum_{x}\widetilde P(x)\ln Z_{\textbf w}(x)\end{aligned}$$ 最后一个等号是因为$\sum_y P(Y=y|x)=1$。顺便求个导：$$\begin{aligned}\frac{\partial L(\textbf w)}{\partial w_i}&amp;=\sum_{x,y}\widetilde P(x,y)f_i(x,y)-\sum_{x,y}\widetilde P(x)P_{\textbf w}(Y=y|x)f_i(x,y)\\&amp;=\mathbb E_{\widetilde P(X,Y)}[f_i]-\sum_{x,y}\widetilde P(x)P_{\textbf w}(Y=y|x)f_i(x,y)\end{aligned}$$似然函数中的$ln Z_w(x)$项是一个指数函数的和的对数的形式。 预测算法条件随机场的预测问题是给定条件随机场P(Y|X)和输入序列（观测序列）x,求条件概率最大的输出序列（标记序列）$y^*$，即对观测序列进行标注。条件随机场预测算法是采用维特比算法。 由$P_w(y|x)=\frac {exp(w\cdot F(y,x))}{Z_w(x)}$可得： $$\begin{align}y^{*}&amp;=arg max_yP_w(y|x)\\&amp;=arg max_y \frac {exp(w \cdot F(y,x))}{Z_w(x)}\\&amp;=arg max_y exp(w \cdot F(y,x))\\&amp;=arg max_y(w \cdot F(y,x))\end{align}$$ 于是，条件随机场预测问题就成为求非规范化概率最大的最优路径问题：$$max_y(w \cdot F(y,x))$$这里，路径表示标记序列，其中，$$\begin{align}&amp;w = (w_1,w_2,…,w_K)^T\\&amp;F(y,x) = (f_1(y,x),f_2(y,x),…,f_K(y,x))^T\\&amp;f_k(y,x)=\sum_{i=1}^n f_k(y_{i-1},y_i, x,i), k=1,2,…,K\end{align}$$ 注意，这时只需计算非规范化概率，而不必计算概率，可以大大提高效率。为了求解路径，将$max_y(w \cdot F(y,x))$写成如下形式：$$max_y \sum_{i=1}^n w\cdot F_i(y_{i-1,y_i,x})$$其中，$$F_i(y_{i-1},y_i,x) = (f_1(y_{i-1},y_i,x,i), f_2(y_{i-1}, y_i, x,i), … , f_K(y_{i-1},y_i,x,i))^T$$是局部特征向量。下面叙述维特比算法，首先求出位置1的各个标记j=1,2,…,m的非规范化概率：$$\delta_1(j) = w \cdot F_1(y_0=start, y_1=j,x), j=1,2,…,m$$一般地，由地推公式，求出到位置i的各个标记l=1,2,..,m的非规范化概率的最大值，同时记录非规范化概率最大值的路径：$$\delta_i(l) = max_{1\le j \le m} {\delta_{i-1}(j)+w \cdot F_i(y_{i-1}=j, y_i=l,x)},l=1,2,…,m\\\Psi_i(l) = arg max_{1 \le j \le m} { \delta_{i-1}(j) + w \cdot F_i(y_{i-1}=j, y_i=l,x)}, l =1,2,…,,m$$直到i=n时终止。这时求得非规范化概率的最大值为：$$max_y (w \cdot F(y,x)) = max_{1 \le j \le m}\delta_n(j)$$及最优的终点：$$y_n^ = \Psi_{i+1}(y_{i+1}^),i = n-1,n-2,…,1$$求得最优路径$y^ = (y_1^, y_2^, …, y_n^)^T$维特比算法如下：]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理基础-最大熵模型]]></title>
    <url>%2F2018%2F05%2F13%2F%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[基本知识熵和条件熵熵(entropy)原是一个热力学中的概念，后由香农引入到信息论中。在信息论和概率统计中，熵用来表示随机变量的不确定性的度量。其定义如下：设$X \in {x_1,x_2,…,x_n}$为一个离散随机变量，其概率分布为$p(X=x_i)=p_i,i=1,2…,n$，则X的熵为：$$H(X)=-\sum_{i=1}^n p_ilogp_i$$其中，当$p_i=0$时，定义$0log0=0$。上式中的对数log以2为底或者以e为底，分别对应bit或nat，熵的两种单位。由熵的公式可知H(X)仅依赖于X的分布，而与X的具体取值无关，因此，常将H(X)记为H(p)。熵的意义：可以认为熵是描述事物无序性的参数，熵越大则无序性不确定性越大。一个孤立系统的熵，自发性地趋于极大，随着熵的增加，有序状态逐步变为混沌状态，不可能自发的产生新的有序结构。当熵处于最小值，即能量集中程度最高、有效能量处于最大值时，那么整个系统也处于最有序的状态，相反为最无序状态。熵增原理预示着自然界越变越无序。s由熵的计算公式可知熵的取值范围为：$$0 \leq H(X) \leq log n$$ 左边的等号在X为确定值的时候成立，即X没有变化的可能； 右边的等号在X为均匀分布的时候成立； 条件熵的定义：设$X \in {x_1,x_2,…,x_n},Y \in {y_1,y_2,…,y_n)}$为离散的随机变量。在已知X的条件下，Y的条件熵（conditional entropy）可以定义为：$$H(Y|X)=\sum_i^n p(x_i)H(Y|X=x_i)=-\sum_{i=1}^n p(x_i) \sum_{j=1}^m p(y_j|x_i)log p(y_j|x_i)$$它表示已知X的情况下，Y的条件概率分布的熵对X的数学期望。 最大熵原理最大熵的理论基础是熵增理论，即在无外力作用下，事物总是朝着最混乱的方向发展。同时，事物是约束和自由的统一体。事物总是在约束下争取最大的自由权，这其实也是自然界的根本原则。在已知条件下，熵最大的事物，最可能接近它的真实状态。最大熵原理是在1957年由E.T.Jaynes提出的，其主要思想是，在只掌握关于未知分布的部分知识时，应该选取符合这些知识但熵值最大的概率分布。因为在这种情况下符合已知知识的概率分布可能不止一个。常在[3]中写道：“我们知道熵定义的实际上是一个随机变量的不确定性，熵最大的时候，说明随机变量最不确定，换句话说，也就是随机变量最随机，对其行为做准确的预测最困难。从这个意义上讲，那么最大熵原理的实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的唯一不偏不倚的选择，任何其他的选择都意味着我们增加了其他的约束和假设，这些约束和假设根据我们掌握的信息无法做出。”吴军老师的《数学之美》第20章中提到了一个掷骰子的例子可以很好的解释最大熵原理。对于一个骰子，每面向上的概率是多少，可能我们会不加思索会说是$\frac {1}{6}$，但是，如果说骰子的其中四点被做过特殊处理，四点向上的概率为$\frac {1}{3}$，那么其他点向上的概率则变为$\frac {2}{15}$。虽然，这是一个很简单的问题，却隐含着最大熵的原理。首先，在骰子没做任何处理之前，我们认为骰子的各个面出现的概率是相同的，即符合均匀分布，此时熵最大。然后，当增加四点被做过特殊处理后，其他面的向上的概率变为$\frac {2}{15}$，在这里四点被做过特殊处理，即所谓的约束条件，满足约束条件之后，而对其它则不做任何假设其他面向上的概率是相同的，即为$\frac {2}{15}$，也是熵最大的。在这个例子中，不作任何假设就是使用“等概率”，这个时候概率分布最均匀，从而使得概率分布的熵最大，即最大熵原理。 最大熵模型的定义最大熵原理是统计学学习的一般原理，将它应用到分类得到最大熵模型。假设分类模型是一个条件概率分布$P(Y|X)$，$X \in \cal X \subseteq R^*$表示输入，$Y \in \cal Y $表示输出，$\cal X和\cal Y$表示输入和输出的集合。这个模型表示的是对于给定的输入X，以条件概率$P(Y|X)$输出Y。给定义一个训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，学习的目标是用最大熵原理选择最好的分类模型。首先，考虑模型应该满足的条件。给定训练数据集，可以确定联合概率分布P(X,)的经验分布和边缘分布P(X)经验分布，分别以$\hat P(X,Y)$和$\hat P(X)$表示，这里：$$\begin{align} &amp;\hat P(X=x,Y=y)=\frac {v(X=x,Y=y)}{N} \\ &amp;\hat P(X=x)=\frac {v(X=x)}{N}\end{align}$$其中，$v(X=x,Y=y)$表示训练数据中样本(x,y)出现的频数，$v(X=x)$表示训练数据中输入x出现的频数，N表示训练样本的容量。 用特征函数(feature function)$f(x,y)$描述输入x和输出y之间的某一事实，其定义为：$$f(x,y)=\begin{cases} 1, &amp; x与y满足某一事实 \\ 0, &amp; 否则\end{cases}\\$$它是一个二值函数，当x和y满足这个事实时取值为1，否则取值为0。特征函数$f(x,y)$在训练数据集上关于经验分布$\hat P(X,Y)$的期望值，用$E_{\hat p}(f)$表示：$$E_{\hat p}(f) = \sum_{x,y} \hat P(x, y)f(x,y)$$特征函数$f(x,y)$关于模型上关于$p(x,y)$的数学期望为：$$E_{p}(f) = \sum_{x,y}p(x,y)f(x,y)$$$p(y|x)$与经验分布$\hat p(x)$的期望值用$E_p(f)$表示。需要注意的是$p(x,y)$是未知的，而且建模的目标是$p(y|x)$。因此，我们希望将排$p(x,y)$表示成$p(y|x)$的函数。利用贝叶斯定理有，$p(x,y)=p(x)p(y|x)$，但$p(x)$依然是未知的，此时，只好利用$\hat p(x)$来近似了。这样$E_p(f)$可以重新定义为：$$E_p(f) = \sum_{x,y}\hat P(x)P(y|x)f(x,y)$$对于概率分布$p(y|x)$，我们希望特征$f$的期望值应该和从训练数据中得到的特征期望值是一致的，因此，提出约束：$$E_p(f) = E_{\hat p}(f)$$即：$$\sum_{x,y}\hat P(x)P(y|x)f(x,y)=\sum_{x,y}p(x,y)f(x,y)$$假设从训练数据集中抽取了n个特征，相应地，便有n个特征函数$f_i(i=1,2,…,n)$以及n个约束条件：$$C_i:E_p(f_i)=E_{\hat p}(f_i),i=1,2,…,n.$$最大熵模型定义：利用最大熵原理选择一个最好的分类模型，即对于任意一个给定的输入$x \in \cal X$，可以以概率$p(y|x)$输出$y \in \cal Y$。前面已经讨论了特征函数和约束条件，要利用最大熵原理，还差一个熵的定义，注意，我们的目标是获取一个条件分布，因此，这里也采用相应的条件熵，如下：$$H(p(y|x))= \sum_{x,y} \hat p(x)p(y|x)log p(y|x)$$下文将$H(p(y|x))$简记为$H(p)$，为了后面计算方便，上式中的对数为自然对数，即以e为底。至此，可以给出最大熵模型的完整描述了。对于给定的训练数据T，特征函数$f_i(x,y),i=1,2,…,n$，最大熵模型就是求解：$$\begin{align}&amp;max_{p\in C} H(p)=(-\sum_{x,y} \hat p(x)p(y|x)log p(y|x)),\\&amp;s.t. \\&amp;\sum_y p(y|x)=1\end{align}$$或者:$$\begin{align}&amp;min_{p\in C} -H(p)=(-\sum_{x,y} \hat p(x)p(y|x)log p(y|x)),\\&amp;s.t. \sum_y p(y|x)=1\end{align}$$则模型集合C中条件熵$H(p)$最大的模型称为最大熵模型。 最大熵模型的学习最大熵模型的学习过程就是最大熵模型的求解过程。最大熵模型的学习可以形式化为约束最优化问题，主要思路和步骤如下： 利用拉格朗日乘子法将最大熵模型由一个带约束的最优化问题转化为一个与之等价的无约束的最优化问题，它是一个极小极大问题（min max)； 利用对偶问题等价性，转化为求解上一步得到的极小极大问题的对偶问题，它是一个极大极小问题（max min） 在求解内层的极小问题时，可以导出最大熵模型的解具有指数形式，而在求解最外层的极大问题时，还将意外地发现其与最大似然估计的等价性。对于给定的训练数据$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$以及特征函数$f_i(x,y),i=1,2,…n$，最大熵模型的学习等价于约束最优化问题：$$\begin{align}&amp;max_{p\in C} H(p)=-\sum_{x,y}\hat p(x)p(y|x)logp(y|x)\\&amp;s.t. E_p(f_i)=E_{\hat p}(f_i),i=1,2,…,n\\&amp; \sum_yp(y|x)=1\end{align}$$按照最优化问题的习惯，将求最大值问题转换为求最小值的问题：$$\begin{align}&amp;min_{p\in C} -H(p)=\sum_{x,y}\hat p(x)p(y|x)logp(y|x)\\&amp;s.t. E_p(f_i)-E_{\hat p}(f_i)=0,i=1,2,…,n\\&amp; \sum_yp(y|x)=1\end{align}$$求解约束最优化问题，所得出的解，就是最大熵模型学习的解，下面给出具体的推导。这里先将约束最优化的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题求解原始问题。首先，引入拉格朗日乘子$\lambda_{0},\lambda_{1},…,\lambda_n$，记为$\lambda=(\lambda_{0},\lambda_{1},…,\lambda_n)^T$，定义拉格朗日函数为$L(p,\lambda)$，则：$$\begin{align}L(p,\lambda)&amp;=-H(p)+\lambda_0(1-\sum_yp(y|x))+\sum_{i=1}^n \lambda_{i}(E_{\hat p}(f_i)-E_p(f_i))\\&amp;=\sum_{x,y}\hat p(x)p(y|x)log p(y|x)+\lambda_0(1-\sum_y p(y|x))+\sum_{i=1}^n(E_{\hat p}(f_i)-E_p(f_i))\end{align}$$利用拉格朗日对偶性，可知最大熵模型等价于求解：$$min_{p\in C} max_{\lambda} L(p,\lambda)$$我们把上式称为原始问题（primary problem)极小极大问题；通过交换极大和极小的位置，可以得到原始问题的对偶问题（dual problem）为$$max_{\lambda}min_{p\in C}L(p,\lambda)$$由于$H(p)$是关于p的凸函数，因此，原始问题和对偶问题是等价的。这样可以通过求解对偶问题来求解原始问题。首先，对于对偶问题内层的极小问题$min_{p\in C}L(p,\lambda)$是关于参数$\lambda$的函数，将其记为$\psi(\lambda)$，即：$$\psi(\lambda)=min_{p\in C}L(p,\lambda)=L(p_{\lambda},\lambda)$$其中，$$p_{\lambda} = arg min_{p\in C}L(p,\lambda)$$首先，计算拉格朗日函数$L(p,\lambda)$对$p(y|x)$的偏导数。$$\begin{align}\frac {\partial L(p,\lambda)}{\partial p(y|x)}&amp;=\sum_{x,y}\hat p(x)(logp(y|x)+1)-\sum_y \lambda_0 - \sum_{i=1}^n \lambda_i(\sum_{x,y}\hat p(x)f_i(x,y))\\&amp;=\sum_{x,y}\hat p(x)(logp(y|x)+1)-\sum_x \hat p(x)\sum_y \lambda_0 - \sum_{x,y}\hat p(x)\sum_{i=1}^n \lambda_i f_i(x,y)\\&amp;=\sum_{x,y} \hat p(x)(log p(y|x)+1) - \sum_{x,y}\hat p(x)\lambda_0 - \sum_{x,y}\hat p(x)\sum_{i=1}^n \lambda_i f_i(x,y)\\&amp;=\sum_{x,y}\hat p(x)(log p(y|x)+1 - \lambda_0 - \sum_{i=1}^n \lambda_if_i(x,y))\end{align}$$另偏导数等于0，在$\hat p(x)&gt;0$的情况下，进一步，令$\frac {\partial L(p, \lambda)}{\partial p(y|x)}=0$可得：$$log p(y|x)+1 - \lambda_0 - \sum_{i=1}^n \lambda_if_i(x,y)=0$$从而得到：$$p(y|x)=e^{\lambda_0 - 1} \cdot e^{\sum_{i=1}^n \lambda_i f_i(x,y)}$$将上式代入约束条件$\sum_y p(y|x)=1$，即：$$\sum_y p(y|x)=e^{\lambda_0 - 1} \cdot \sum_y e^{\sum_{i=1}^n \lambda_i f_i(x,y)=1}$$可得：$$e^{\lambda_0-1} = \frac {1}{\sum_y e^{\sum_{i=1}^n \lambda_i f_i(x,y)}}$$将上式代入到$p(y|x)=e^{\lambda_0 - 1} \cdot e^{\sum_{i=1}^n \lambda_i f_i(x,y)}$得到：$$p_{\lambda} = \frac {1}{Z_{\lambda}(x)}e^{\sum_{i=1}^n \lambda_if_i(x,y)}$$其中，$$Z_{\lambda}(x)=\sum_{y} e^{\sum_{i=1}^n \lambda_if_i(x,y)}$$称为规范化因子，注意，此时的$\lambda=(\lambda_1,\lambda_2,…,\lambda_n)^T$，不再包含$\lambda_0$了。 $p_{\lambda}$就是最大熵模型的解，它具有指数形式，其中$\lambda_1,\lambda_2,…,\lambda_n$为参数，分别对应$f_1,f_2,…,f_n$的权重，$\lambda_i$越大，表示特征$f_i$越重要。之后求解对偶问题外部的极大化问题：$$max_{\lambda} \psi (\lambda)$$设其解为：$$\lambda^{*} = argmax_{\lambda} \psi (\lambda)$$则，最大熵模型的解为$p^* = p_{\lambda^*}$为此，首先要给出$\psi(\lambda)$的表达式：$$\begin{align} \psi(\lambda) &amp;=L(p_{\lambda},\lambda)\\ &amp;=\sum_{x,y}\hat p(x)p_{\lambda}(y|x)logp_{\lambda}(y|x)+\sum_{i=1}^n \lambda_{i}(E_{\hat p}(f_i) - E_{p}(f_i))\\ &amp;=\sum_{x,y}\hat p(x)p_{\lambda}(y|x)logp_{\lambda}(y|x)+\sum_{i=1}^n \lambda_{i}(\sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_{x,y}\hat p(x)p_{\lambda}(y|x)f_i(x,y))\\ &amp;=\sum_{i=1}^n \lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) + \sum_{x,y} \hat p(x)p_{\lambda}(y|x)(log p_{\lambda}(y|x) - \sum_{i=1}^n \lambda_if_i(x,y))\end{align}$$由于，$$log p_{\lambda}(y|x) = \sum_{i=1}^n \lambda_if_i(x,y) - log Z_{\lambda}(x)$$带入前面的$\psi (\lambda)$可得：$$\begin{align}\psi(\lambda)&amp;=\sum_{i=1}^n\lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_{x,y}\hat p(x)p_{\lambda}(y|x)log Z_{\lambda}(x)\\&amp;=\sum_{i=1}^n\lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_x \hat p(x)log Z_{\lambda}(x)\sum_y p_{\lambda}(y|x)\\&amp;=\sum_{i=1}^n\lambda_i \sum_{x,y}\hat p(x,y)f_i(x,y) - \sum_x \hat p(x)log Z_{\lambda}(x)\end{align}$$有了$\psi(\lambda)$的具体表达式，就可以求解其最大值点$\lambda^*$了。对$\psi (\lambda)$的最大化等价于最大熵模型的极大似然估计。已知训练数据的经验分布为$\hat p(x,y)$，条件概率分布$p(y|x)$的对数似然函数表示为：$$L_{\hat p}(p) = log \prod_{x,y}p(y|x)^{\hat p(x,y)}=\sum_{x,y} \hat p(x,y)logp(y|x)$$将$p_{\lambda}$带入似然函数，可得：$$\begin{align}L_{\hat p}(p_{\lambda})&amp;=\sum_{x,y}\hat p(x,y)log p_{\lambda}(y|x)\\&amp;=\sum_{x,y}\hat p(x,y)(\sum_{i=1}^n \lambda_if_i(x,y) - logZ_{\lambda}(x))\\&amp;=\sum_{x,y}\hat p(x,y)\sum_{i=1}^n \lambda_if_i(x,y) - \sum_{x,y}logZ_{\lambda}(x)\\&amp;=\sum_{i=1}^n \lambda_i(\sum_{x,y} \hat p(x,y)f_i(x,y)) - \sum_{x,y} \hat p(x,y)log Z_{\lambda}(x)\\&amp;=\sum_{i=1}^n \lambda_i \sum_{x,y} \hat p(x,y)f_i(x,y) - \sum_x \hat p(x)log Z_{\lambda}(x)\end{align}$$可以看到公式25和公式30是相等的，这说明最大化$\psi(\lambda)$和最大化似然估计是等价的。最大熵模型最优化方法有多种，比如GIS算法、IIS算法以及梯度下降法等方法，这里就不具体介绍了，具体可参考《统计学习》方法。 最大熵模型的优缺点最大熵模型的优点如下： 建模时，实验者只需要集中精力选择特征，而不需要花费精力考虑如何使用这些特征； 特征选择灵活，且不需要额外的独立假定或者内在约束； 模型应用在不同领域时的可移植性强； 可结合更丰富的信息； 缺点： 时空开销大； 数据稀疏问题严重； 对语料的依赖性较强； 特征引入算法在使用最大熵模型的时候，往往会构建很多特征，但并不是所有的特征都是有用的，所以需要作特征选择。尤其对于自然语言处理问题，这些特征的选取往往具有很大的主观性。同时，因为特征数量往往很多，必须引入一个客观标准自动计算以引入特征到模型中，同时针对特征进行参数估计。Della Pietra et al[6]对自然语言处理中随机域的特征选择进行了描述，在进行特征选取时，是由特征的信息增益作为衡量标准的。一个特征对所处理问题带来的信息越多，该特征越适合引入到模型中。一般我们首先形式化一个特征空间，所有可能的特征都为后补特征，然后从这个后补特征集内选取对模型最为有用的特征集合。特征引入的算法如下： 算法： 特征引入算法输入：候选特征集合F，经验分布$\hat p(x,y)$输出：模型选用的特征集合S，集合这些特征的模型$P_s$ 初始化：特征集合S为空，它所对应的模型$P_s$均匀分布，n=0; 对于候选特征集合F中的每一个特征$f\in F$，计算加入该特征后为模型带来的增益值$G_f$； 选择具有最大增益值$G(S,f)$的特征$f_n$; 把特征$f_n$加入到集合S中，$S=(f_1,f_2,…,f_n)$; 重新调整参数值，使用GIS算法计算模型$P_s$; n=n+1，返回到第2步； 在算法的第2步中，要计算每个特征的增益值，这里是根据Kullback-Leibler（简称KL）距离来计算的。衡量两个概率分布p和q的KL距离，公式如下：$$D(p||q) = \sum_x p(x)ln \frac {p(x)}{q(x)}$$距离的大小与两种分布的相似程度成反比，距离越小表示两种分布越逼近。因此，在加入第n个特征前后，求的模型分布与样本分布之间的KL距离为：$$D(\hat p||p^{(n-1)}) = \sum_x \hat p(x) ln \frac {\hat p(x)}{p^{(n-1)}(x)}\\D(\hat p||p^{(n)}) = \sum_x \hat p(x) ln \frac {\hat p(x)}{p^{(n)}(x)}$$这样，我们定义引入第n个特征$f^{(n)}$后的增益值为：$$f^{(n)} = D(\hat p||p^{(n-1)}) - D(\hat p||p^{(n)})$$因此，选择第n个特征为：$$f^{(n)} = arg max_{f \in F}G(p, f^{(n)})$$ 参考文献 https://www.cnblogs.com/pinard/p/6093948.html； https://blog.csdn.net/itplus/article/details/26550597； 自然语言处理的最大熵模型-常宝宝； 统计学习方法第6章； 自然语言处理技术中的最大熵模型方法-李素建、刘群等； Stephen Della Pietra, Vincent Della Pietra, and John Lafferty, Inducing features of random fields, IEEE Transactions on Pattern Analysis and Machine Intelligence 19:4, pp.380–393, April, 1997]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计语言模型]]></title>
    <url>%2F2018%2F05%2F07%2F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[语言模型概述语言模型(Language Model)，就是用来计算一个句子概率的模型。从统计的角度看，自然语言中的一个句子可以由任何词串构成。不过P(s)有大有小。比如： s1 = 我 刚 吃 过 晚饭 s2 = 刚 我 过 晚饭 吃 可以看出P(s1)&gt;P(s2)。对于给定的句子而言，通常P(s)是未知的。对于一个服从某个概率分布P的语言L，根据给定的语言样本估计P的过程被称作语言建模。根据语言样本估计出的概率分布P就称为语言L的语言模型。$$\sum_{s\in L}P(s)=1$$语言建模技术首先在语音识别研究中提出，后来陆续用到OCR、手写体识别、机器翻译、信息检索等领域。在语音识别中，如果识别结果有多个，则可以根据语言模型计算每个识别结果的可能性，然后挑选一个可能性较大的识别结果。语言模型也可以用于汉语歧义消解。那么如何计算一个句子的概率呢？对于给定的句子:$$S=w_1w_2,…,w_n$$它的概率可以表示为：$$P(S)=P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,w_2,…,w_{n-1})$$由于上面的式子参数过多，因此需要近似的计算方法，常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机场(CRF)方法、神经网络方法等。本篇文章主要记录n-gram模型方法。 n-gram模型对于给定的句子$S=w_1w_2…w_n,$,根据链式规则:$$P(S)=P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,w_2,…,w_{n-1})=\prod_{i=1}^np(w_i|w_1…w_{i-1})$$P(S)就是语言模型，即用来计算一个句子S概率的模型。那么，如何计算$p(w_i|w_1,w_2,…,w_{i-1})$呢？最简单、直接的方法是计数后做除法，即最大似然估计(Maximum Likelihood Estimate，MLE)，如下：$$p(w_i|w_1,w_2,…,w_{i-1})=\frac {count(w_1,w_2,…,w_{i-1},w_i)}{count(w_1,w_2,…,w_{i-1})}$$其中，$count(w_1,w_2,…,w_{i-1},w_i)$表示次序列$(w_1,w_2,…,w_{i-1},w_i)$在预料库中出现的频率。 这里面临两个重要的问题：数据稀疏严重和参数空间过大，导致无法实用。实际中，我们一般较长使用N语法模型(n-gram)，它采用了马尔科夫假设，即认为语言中的每个词只与其前面长度为n-1的上下文有关。 假设下一个词的出现不依赖前面的词，即为uni-gram，则有:$$ \begin {aligned} P(S)&amp;=P(w_1)P(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_1,w_2,…,w_{n-1})\\\\&amp;=p(w_1)p(w_2)…p(w_n)\end{aligned}$$ 假设下一个词的出现只依赖前面的一个词，即为bi-gram，则有：$$ \begin {aligned} P(S)&amp;=P(w_1)P(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_1,w_2,…,w_{n-1})\\\\&amp;=p(w_1)p(w_2|w_1)p(w_3|w_2)…p(w_n|w_{n-1})\end{aligned}$$ 假设下一个词的出现依赖它前面的两个词，即为tri-gram，则有：$$\begin {aligned} P(S)&amp;=P(w_1)P(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_1,w_2,…,w_{n-1})\\\\&amp;=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)…p(w_n|w_{n-2},w_{n-1})\end {aligned}$$ 实际上常常在对数空间里面计算概率，原因有两个： 防止溢出；如果计算的句子很长，那么最后得到的结果将非常小，甚至会溢出，比如计算得到的概率是0.001，那么假设以10为底取对数的结果就是-3，这样就不会溢出； 对数空间里面加法可以代替乘法，因为log(p1p2) = logp1 +logp2，在计算机内部，显然加法比乘法执行更快； n-gram中的n如何选择？ n较大时：提供了更多的上下文语境信息，语境更具有区别性；但是，参数个数多、计算代价大、训练预料需要多，参数估计不可靠； n较小时：提供的上下文语境少，不具有区别性；但是，参数个数小、计算代价小、训练预料无须太多、参数估计可靠； 理论上，n越大越好，经验上tri-gram用的最多，尽管如此，原则上，能用bi-gram解决，绝不使用tri-gram。 建立n-gram语言模型构建n-gram语言模型，通过计算最大似然估计构造语言模型。一般的过程如下： 1、数据准备： 确定训练语料 对语料进行tokenization或切分 句子边界，增加特殊的词和开始和结束 2、参数估计： 利用训练语料，估计模型参数 令$c(w_1,…,w_n)$表示n-gram $w_1,…,w_n$在训练语料中出现的次数，则：$$ P_{MLE}(w_n|w_1,…,w_{n-1})=\frac {c(w_1,…,w_n)}{c(w_1,…,w_{n-1})}$$ 语言模型效果评估目前主要有两种方法判断建立的语言模型的好坏： 实用方法：通过查看该模型在实际应用（如拼写检查、机器翻译）中的表现来评价，优点是直观、实用，缺点是缺乏针对性、不够客观； 理论方法：困惑度(preplexity)，其基本思想是给测试集赋予较高概率值的语言模型较好； 平滑方法最大似然估计给训练样本中未观察到的事件赋以0概率。如果某个n-gram在训练语料中没有出现，则该n-gram的概率必定是0。这就会使得在计算某个句子S的概率时，如果某个词没有在预料中出现，那么该句子计算出来的概率就会变为0，这是不合理的。解决的办法是扩大训练语料的规模。但是无论怎样扩大训练语料，都不可能保证所有的词在训练语料中均出现。由于训练样本不足而导致估计的分布不可靠的问题，称为数据稀疏问题。在NLP领域中，稀疏问题永远存在，不太可能有一个足够大的语料，因为语言中的大部分词都属于低频词。 Zipf定律Zipf定律描述了词频以及词在词频表中的位置之间的关系。针对某个语料库，如果某个词w的词频是f，并且该词在词频表中的序号为r(即w是所统计的语料中第r常用词)，则：$$f*r=k(k是一个常数)$$若$w_i$在词频表中的排名50，$w_j$在词频表中排名为150，则$w_i$的出现频率大约是$w_j$的频率的3倍。Zipf定律告诉我们： 语言中只有很少的常用词 语言中的大部分词都是低频词（不常用的词) Zipf的解释是Principle of Lease effort 说话的人只想使用少量的常用词进行交流 听话的人只想使用没有歧义的词（量大低频）进行交流 Zipf定律告诉我们： 对于语言中的大多数词，它们在语料中出现是稀疏的 只有少量的词语料库可以提供它们规律的可靠样本 平滑技术对于语言而言，由于数据稀疏的存在，MLE不是一种很好的参数估计方法。为了解决数据稀疏问题，人们为理论模型实用化而进行了众多的尝试，出现了一系列的平滑技术，它们的基本思想是降低已出现n-gram的条件概率分布，以使未出现的n-gram条件概率分布为非零，且经过平滑后保证概率和为1。目前，已经提出了很多数据平滑技术，如下： Add-one平滑 Add-delta平滑 Good-Turing平滑 Interpolation平滑 回退模型-Katz平滑 … Add-one平滑加一平滑法，又称为拉普拉斯定律，其规定任何一个n-gram在训练预料中至少出现一次（即规定没有出现过的n-gram在训练预料中出现了1次） $$P_{Add1}(w_1,w_2,…,w_n)=\frac {C(w_1,w_2,…,w_n)+1}{N+V}$$ N:为训练预料中所所有的n-gram的数量(token); V:所有的可能的不同的n-gram的数量(type);下面两幅图分别演示了未平滑和平滑后的bi-gram示例： (注：上图均来自《计算语言学-常宝宝》的课件) 训练语料中未出现的n-gram的概率不再为0，而是一个大于0的较小的概率值。但是，由于训练预料中未出现的n-gram数量太多，平滑后，所有未出现的n-gram占据了整个概率分布中的一个很大的比例。因此，在NLP中，Add-one给训练预料中没有出现过的n-gram分配太多的概率空间。同时，它认为所有未出现的n-gram概率相等，这是否合理？出现在训练语料中的哪些n-gram，都增加同样频度值，不一定合理。 Add-delta平滑Add-delta平滑，不是加1，而是加一个比1小的整数$\lambda$:$$P_{AddD}(w_1,w_2,…,w_n)=\frac {C(w_1,w_2,…,w_n)+\lambda}{N+\lambda V}$$通常$\lambda =0.5$，此时又称为Jeffreys-Perks Law或ELE。它的效果要比Add-one好，但是仍然不理想。 Good-Turing平滑其基本思想是利用频率的类别信息对频率进行平滑。假设N是样本数据的大小，$n_r$是在样本中正好出现r次的事件的数目(在这里，事件为n-gram $w_1,w_2,…,w_n$)。即：出现1的$n_1$个，出现2次的$n_2$个,…。那么：$$N=\sum_{r=1}^{\infty} n_r r$$由于，$$N=\sum_{r=0}^{\infty}n_r r^*=\sum_{r=0}^{\infty}(r+1)n_{r+1}$$，所以，$$r^* = (r+1)\frac {n_{r+1}}{n_r}$$ 那么，Good-Turing估计在样本中出现r次的事件的概率为:$$P_r = \frac {r^*}{N}$$实际应用中，一般直接使用$n_{r+1}$代替$E(n_{r+1})$，$n_r$代替$E(n_r)$。这样，样本中所有事件的概率之和为：$$\sum_{r&gt;0} n_r * P_r = 1 - \frac {n_1}{N} &lt;1$$因此，有$\frac {n_1}{N}$的剩余的概率量就可以均分给所有未出现事件(r=0)。Good-Turing估计适用于大词汇集产生的符合多项式分布的大量的观察数据。在估计频度为r的n-gram的概率$p_r$时，如果数据集中没有频度为r+1的n-gram怎么办？此时，$N_{r+1}=0$导致$p_r=0$。解决的办法是对$N_r$进行平滑，设S(.)是平滑函数，S(r)是$N_r$的平滑值。$$r^* = (r+1)\frac {S(r+1)}{S(r)}$$ Interpolation平滑不管是Add-one，还是Good Turing平滑技术，对于未出现的n-gram都一视同仁，难免存在不合理性。所以介绍一种线性差值的的平滑技术，其基本思想是将高阶模型和低阶模型作线性组合，利用低阶n-gram模型对高阶n-gram模型进行线性差值。因为没有足够的数据对高阶n-gram模型进行概率估计时，低阶的n-gram模型通常可以提供有用的信息。因此，可以把不同阶的n-gram模型组合起来产生一个更好的模型。 把不同阶别的n-gram模型线性加权组合：$$P(w_n|w_{n-1},w_{n-2})=\lambda_1P(w_n)+\lambda_2P(w_n|w_{n-1})+\lambda_3P(w_n|w_{n-1}w_{n-2})$$其中，$0&lt;=\lambda_i&lt;=1,\sum_i \lambda_i=1$。$\lambda_i$可以根据实验凭经验设定，也也可以通过应用某些算法确定，例如EM算法。 回退模型-Katz平滑回退模型-Katz平滑，其基本思想是：当某一事件在样本中出现的概率大于K(通常K为0或1)，运用最大似然估计减值来估计其概率，否则使用低阶的，即(n-1)gram概率代替n-gram概率。而这种替代必须受归一化因子$\alpha$的作用。回退模型的一般形式如下：$$p_{smooth}(w_i|w_{i-n+1}^{i-1})=\begin{cases} \hat p(w_i|w_{i-n+1}^{i-1}), &amp; if c(w_{i-n+1}^i)&gt;0 \\ \alpha(w_{i-n+1}^{i-1})\cdot p_{smooth}(w_i|w_{i-n+2}^{i-1}), &amp; if c(w_{i-n+1}^{i-1})=0 \end{cases}$$参数$\alpha(w_{i-n+1}^{i-1})$是归一化因子，以保证$$\sum_{w_i}p_{smooth}(w_i|w_{i-n+1}^{i-1})=1$$以bi-gram为例，令$r=c(w_{i-1}w_i)$，如果r&gt;0，则$p_{katz}(w_i|w_{i-1})=d_r\cdot p_{ML}(w_i|w_{i-1})$，$d_r$称为折扣率，给定$w_{i-1}$，从r&gt;0的bi-grams中折除的概率为：$$S(w_{i-1}) = 1 - \sum_{w_i \in M(w_{i-1})} p_{katz}(w_i|w_{i-1}) \\其中，M(w_{i-1})={w_i|c(w_{i-1}w_i)&gt;0}$$ 对于给定的$w_{i-1}$，令：$$Q(w_{i-1}) = {w_i|c(w_{i-1}w_i)=0}$$如何把$S(w_{i-1})$分配给集合$Q(w_{i-1})$中的那些元素？ 对于$w_i \in Q(w_{i-1})$，如果$p_{ML}(w_i)$比较大，则应该分配更多的概率给它。所以，若r=0，则：$$p_{katz}(w_i|w_{i-1})=\frac {p_{ML}(w_i)}{\sum_{w_j\in Q}p_{ML}(w_j)} \cdot S(w_{i-1})$$对于bi-gram模型，Katz平滑为：$$p_{katz}(w_i|w_{i-1})= \begin{cases} d_r\cdot p_{ML}(w_i|w_{i-1}), &amp; if r&gt;0 \\ \alpha(w_{i-1}) \cdot p_{ML}(w_i), &amp; if r=0 \end{cases}\\其中，\alpha(w_{i-1}) = \frac {1 - \sum_{w_j\in M} p_{katz}(w_j|w_{j-1}) }{\sum_{w_j\in Q} p_{ML}(w_j)}$$ 如何计算$d_r$? 如果$r&gt;k$，不折扣，即$d_r=1$(Katz提出k=5) 如果$0&lt;r \leq k$，按照和Good-Turing估计同样的方式折扣，即按照$\frac {r^{*}}{r}$进行折扣。严格说，要求$d_r$满足，$1-d_r=u(1-\frac {r^{*}}{r})$ 根据Good-Turing估计，未出现的n元组估计出现频次是$n_1$，$\sum_{r=1}^k n_r(1-d_r)r=n$ 具体而言，若$0&lt;r \leq k$，有$$d_r = \frac {\frac {r^*}{r} - \frac {(k+1)n_{k+1}}{n_1}} {1 - \frac {(k+1)n_{k+1}}{n!}}$$ big-gram的Katz平滑模型最终可描述为：$$p_{katz}(w_i|w_{i-1})=\begin{cases} c(w_{i-1}w_i)/c(w_{i-1}), &amp; if r&gt;k \\ d_rc(w_{i-1}w_i)/c(w_{i-1}), &amp; if k \geq r &gt; 0 \\ \alpha (w_{i-1})p_{katz}(w_i) &amp; r=0 \end{cases}$$n-gram模型的Katz平滑可以此类推。在回退模型和线性插值模型中，当高阶n-gram未出现时，使用低阶n-gram估算高阶n-gram的概率分布。在回退模型中，高阶n-gram一旦出现，就不再使用低阶n-gram进行估计。在线性插值模型中，无论高阶n-gram是否出现，低阶n-gram都会被用来估计高阶n-gram的概率分布。 大规模n-gram的优化如果不想自己动手实现n-gram语言模型，推荐几款开源的语言模型项目： SRILM(http://www.speech.sri.com/projects/srilm/) IRSTLM(http://hlt.fbk.eu/en/irstlm) MITLM(http://code.google.com/p/mitlm/) BerkeleyLM(http://code.google.com/p/berkeleylm/) 在使用 n-gram 语言模型时，也有一些技巧在里面。例如，面对 Google N-gram 语料库，压缩文件大小为 27.9G，解压后 1T 左右，如此庞大的语料资源，使用前一般需要先剪枝（Pruning）处理，缩小规模，如仅使用出现频率大于 threshold 的 n-gram，过滤高阶的 n-gram（如仅使用 n&lt;=3 的资源），基于熵值剪枝，等等。 另外，在存储方面也需要做一些优化: 采样Trie数的数据结构，可以优化时间复杂度为$O(log_{|V|}m)$|V|为字母的个数； 借助Bloom filter辅助查询，把String映射为int类型处理； 利用郝夫曼树对词进行编码，将词作为索引值而不是字符串进行存储，能将所有词编码成包含在2个字节内的索引值； 优化概率值存储，概率值原使用的数据类型是（float），用4-8bit来代替原来8Byte的存储内容； N-gram模型的缺陷 数据稀疏问题：利用平滑技术解决； 空间占用大； 长距离依赖问题； 多义性； 同义性；如 “鸡肉”和“狗肉”属于同一类词，p(肉|鸡)应当等于p(肉|狗)，而在训练集中学习到的概率可能相差悬殊； 语言模型应用n-gram距离假设有一个字符串s，那么该字符串的n-gram就表示按长度n切分原词得到的词段，也就是s中长度为n的子串。假设有两个字符串，然后分别求它们的n-gram，那么就可以从它们的共有子串的数量这个角度定义两个字符串间的n-gram距离。但是仅仅是简单地对共有子串进行计数显然也存在不足，这种方案忽略了两个字符长度的差异，可能导致的问题。比如，字符串girl和girlfriend，二者拥有的公共子串数量显然与girl和其自身所拥有的公共子串数量相等，但是不能认为girl和girlfriend是两个等同的匹配。为解决该问题，有学者便提出以非重复的n-gram分词为基础来定义n-gram距离这一概念，可以用下面的公式来表述： $$|G_N(s)|+|G_N(t)|-2*|G_N(s)\cap G_N(t)|$$ 例如，字符串s=”ABC”，t=”AB”，分别在字符串首尾加上begin和end，采用二元语言模型，字符串s产生的bi-gram为：(begin,A),(A,B),(B,C),(C,end)；字符串t产生的bi-gram为：(begin,A),(A,B),(B,end)。采用上面公式定义:4+3 - 2*2 = 3显然，字符串之间的距离越小，它们就越接近。当两个字符串完全相等的时候，它们之间的距离就是0。 分词分词是NLP中一项比较基础且重要的任务。对于X=”我爱中国”这样一句话，有多种切分方案，对于$y_i$这种分词方案，如,$Y_0=(“我”，“爱”，“中国”),Y_1=(“我”，“爱中”，“国”)$，利用贝叶斯公式可以计算出每种切分的概率:$$P(Y_i|X)=\frac {P(X|Y_i)P(Y_i)}{P(X)}\propto P(X|Y_i)P(Y_i),i=1,2,3,…$$无论在哪种$Y_i$下，最终都能生成句子X，因此$P(X|Y_i)=1$，所以$P(Y_i|S)\propto P(Y_i),i=1,2,3…$。所以，只需要最大化$P(Y_i)即可$。例如，根据bi-gram语言模型，$P(Y_0|X)\propto P(Y_0)=P(我)P(爱|我)P(中国|爱)$，$P(Y_1|X)\propto P(Y_1)=P(我)P(爱中|我)P(国|爱中)$，然后利用计算出的概率，选择最大的作为分词方案。 词性标注词性标注（POS tagging）是一个典型的多分类问题，将每个词标注为名词、动词、形容词、副词、代词等。例如，在“我/爱/中国”句话中，“爱”有很多词性，比如，名词、动词。最简单的标注其语义的方案就是，看语料库中“爱”出现的次数，以及其词性，即：$$P(POS_i|爱)=\frac {c(“爱”作为POS_i )}{c(爱),i=1,2,…,k，k为词性总数}$$但是，这种简单的词性标注的方案依赖人工，且未考虑上下文。考虑到在一个句子中当前词的词性和前面一两个词关系比较大。因此，可以借用n-gram模型的思路进行求解。比如，在考虑“爱”的词性时，以前面一个词的词性作为参考，即“我”的词性，则，当前这个“爱”的词性概率分布为:$$P(POS_i|我，爱)=P(POS_i|Pron.,爱)=\frac {前面被“副词”修饰的“爱”的POS_i}{c(前面被“副词”修饰的“爱”)},i=1,2,…,k,k为词性总数$$ 计算这个概率需要对语料库进行统计。但是，前提是先判断好“我”的词性。因为，采用的是bi-gram模型，由于“我”已经是第一个词，在二元模型中主需要级简的方案判断即可。 n-gram作为文本特征在处理文本的特征的时候，通常一个关键词作为一个特征。但是，在某些场景下可能词的表达能力不够，需要提取更多的特征，n-gram就是一个很好的特征。以bi-gram为例，在原始文本中，每个关键词可以作为一个特征，将每个关键词两两组合，得到一个bi-gram组合，在根据n-gram语言模型，计算各个bi-gram组合的概率，作为新的特征。 英语介词短语消歧参考 我们是这样理解语言的； 《计算语言学-常宝宝》的课件 https://www.cnblogs.com/ljy2013/p/6425277.html https://blog.csdn.net/TiffanyRabbit/article/details/72654180]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP基础-熵]]></title>
    <url>%2F2018%2F05%2F02%2FNLP%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E7%86%B5%2F</url>
    <content type="text"><![CDATA[一些基础互斥如果事件A和B不可能同时发生，即$AB=\Phi$，则称A与B是互斥的。 对立如果A与B互斥，又在每次试验中不是出现A就是出现B，即$AB=\Phi$且$A+B=\Omega$，则称B是A的对立事件。 条件概率在事件B发生的条件下，事件A发生的概率称为事件A在事件B已发生的条件下的条件概率，记作P(A|B)。当P(B)&gt;0时，规定:$$P(A|B)=\frac {P(AB)}{P(B)}$$当P(B)=0时，规定P(A|B)=0。由条件概率的定义，可以得到乘法公式：$$\begin {aligned}&amp;P(AB)=P(A)P(B|A)\\\\&amp;P(A_1A_2…A_n)=P(A_1)P(A_2|A_1)P(A_3|A_2A_1)…P(A_n|A_{n-1}A_{n-2}…A_1)=\prod_i^n P(A_i|A_{i-1}A_{i-2}…A_1)\end {aligned}$$一般而言，条件概率P(A|B)与概率P(A)是不等的。但在某些情况下，它们是相等的。根据条件概率的定义和乘法公式有:$$P(AB)=P(A)P(B)$$这时，称事件A与B是相互独立的。 贝叶斯公式根据乘法公式，可以的得到下面重要的公式，该公式称为贝叶斯公式：$$P(A|B)=\frac {P(B|A)(A)}{P(B)}$$一般地，事件$A_1,A_2,…,A_n$两两互斥，事件B满足$B\subset A_1+A_2+…+A_n$且$P(A_i)&gt;0(i=1,2,…,n),P(B)&gt;0$，贝叶斯公式可以推广为：$$p(A_j|B)=\frac{P(A_j)P(B|A_j)}{P(A_1)P(B|A_1)+…+P(A_n)P(B|A_n)}=\frac {P(A_j)P(B|A_j)}{\sum_i^n P(A_i)P(B|A_i)}$$实用上称，$P(A_1),P(A_2),…,P(A_n)$的值称为先验概率，称$P(A_1|B),P(A_2|B),…,P(A_n|B)$的值称为后验概率，贝叶斯公式便是从先验概率计算后验概率的公式。 各种熵(entropy)在信息论中，如果发送一个消息所需要的编码的长度较大，则可以理解为消息所蕴涵的信息量较大，如果发送一个消息所需要的编码长度较小，则该消息所蕴涵的信息量较小，平均信息量即为发送一个消息的平均编码长度，可以用熵的概念来描述。 自信息自信息是熵的基础，自信息表示某一事件发生时所带来的信息量的多少。当事件发生的概率越大，则自信息越小。当一件事发生的概率非常小，并且实际上也发生了（观察结果），则此时的自信息较大。某一事件发生的概率非常大，并且实际上也发生了，则此时的自信息较小。如何度量它？现在要寻找一个函数，满足条件：事件发生的概率越大，则自信息越小；自信息不能是负值，最小是0；自信息应该满足可加性，并且两个对立事件的自信息应该等于两个事件单独的自信息。自信息的公式如下：$$I(p_i)=-log(p_i)$$其中，$p_i$表示随机变量的第i个事件发生的概率，自信息单位是bit,表征描述该信息需要多少位。可以看出，自信息的计算和随机变量本身数值没有关系，只和其概率有关。 熵的定义设X是取有限个值的随机变量，它的分布密度为$p(x)=P(X=x),且x\in X$，则X的熵的定义为：$$H(x)=-\sum _{x \in X}p(x)log_ap(x)$$熵描述了随机变量的不确定性。一般也说，熵给出随机变量的一种度量。对于数底a可以是任何正数，对数底a决定了熵的单位，如果a=2，则熵的单位称为比特(bit)。 熵的基本性质 $H(x)&lt;=log|X|$，其中等号成立当且仅当$p(x)=\frac {1}{|x|}$，这里|X|表示集合X中的元素个数。该性质表明等概场具有的最大熵； $H(X)&gt;=0$，其中等号成立的条件当且仅当对某个i,$p(x_i)=1$，其余的$p(x_k)=0 (k!=i)$。这表明确定场(无随机性)的熵最小； 熵越大，随机变量的不确定性就越大，分布越混乱，随机变量状态数越多； 联合熵设X,Y是两个离散随机变量，它们的联合分布密度为p(x,y)，则给定X时Y的条件熵定义为：$$\begin{aligned}H(Y|X) &amp;=-\sum_{x\in X}p(x)H(Y|X=x)\\\\&amp;=\sum_{x\in X}p(x)[-\sum_{y \in Y}p(y|x)log p(y|x)]\\\\&amp;=-\sum_{x \in X}\sum_{y \in Y} p(x,y)log p(y|x)\end{aligned}$$联合熵和条件熵的关系可以用下面的公式来描述，该关系一般也称为链式规则：$$H(X,Y)=H(X)+H(Y|X)$$信息量的大小随着消息的长度增加而增加，为了便于比较，一般使用熵率的概率。熵率一般也称为字符熵(per-letter entropy)或词熵(per-word entropy)。 熵率对于长度为n的消息，熵率的定义为：$$H_{rate}=\frac{1}{n}H(x_{1n}) = -\frac {1}{n}\sum_{x_{1n}}p(x_{1n})log p(x_{1n})$$这里的$x_{1n}$表示随机变量序列$X_1,X-2…X_n,p(x_{1n})表示分布密度p(x_1,x_2,…,x_n)$。可以把语言看做一系列语言单位构成的一个随机变量序列$L={X_1X_2…X_n}$，则语言L的熵可以定义这个随机变量序列的熵率:$$H_{rate}=\lim_{x \to +\infty}\frac{1}{n}H(H_1,H_2,…,H_n)$$ 互信息根据链式规则，有:$$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$可以推导出：$$H(X)-H(X|Y)=H(Y)-H(Y|X)$$H(X)与H(X|Y)的差称为互信息，一般记作I(X;Y)。I(X;Y)描述了包含在X中的有关Y的信息量，或包含在Y中的有关X的信息量。下图很好的描述了互信息和熵之间的关系。$$\begin{aligned} I(X;Y)&amp;=H(X)-H(X|Y)\\\\&amp;=H(X)+H(Y)-H(X,Y)\\\\ &amp;=\sum_x p(x)log \frac {1}{p(x)}+\sum_x p(y)log \frac {1}{p(y)}+\sum_{x,y} p(x,y)log p(x, y)\\\\&amp;=\sum_{x,y}log \frac {p(x,y)}{p(x)p(y)}\end{aligned}$$互信息(mutual information),随机变量X,Y之间的互信息定义为：$$I(X;Y)=\sum_{x,y}p(x,y)log \frac {p(x,y)}{p(x)p(y)}$$互信息的性质： $I(X;Y)&gt;=0$，等号成立当且仅当X和Y相互独立。 $I(X;Y)=I(Y;X)$说明互信息是对称的。 互信息相对于相对熵的区别就是，互信息满足对称性；互信息的公式给出了两个随机变量之间的互信息。在计算语言学中，更为常用的是两个具体事件之间的互信息，一般称之为点式互信息。点式互信息(pointwise mutual information)，事件x,y之间的互信息定义为：$$I(x,y) = log \frac {p(x,y)}{p(x)p(y)}$$一般而言，点间互信息为两个事件之间的相关程度提供一种度量，即： 当$I(x,y)&gt;&gt;0$时，x和y是高度相关的； 当$I(x,y)=0$时，x和y是高度相互独立； 当$I(x,y)&lt;&lt;0$时，x和y呈互补分布； 交叉熵(cross entropy)交叉熵的概念是用来衡量估计模型与真实概率分布之间差异情况的，其定义为，设随机变量X的分布密度p(x)，在很多情况下p(x)是未知的，人们通常使用通过统计的手段得到X的近似分布q(x),则随机变量X的交叉熵定义为：$$H(p,q) = -\sum_{x \in X} p(x)log q(x)$$其中，p是真实样本的分布，q为预测样本分布。在信息论中，其计算数值表示：如果用错误的编码方式q去编码真实分布p的事件，需要多少bit数，是一种非常有用的衡量概率分布相似的数学工具。 相对熵(relative entropy)相对熵，设p(x),q(x)是随机变量X的两个不同的分布密度，则它们的相对熵定义为:$$D(p||q)=\sum_{x \in X}p(x) log \frac {p(x)}{q(x)}=H(p,q)-H(p)$$相对熵较交叉熵有更多的优异性质，主要为： 当p分布和q分布相等的时候，KL散度值为0； 可以证明是非负的； KL散度是非对称的，通过公式可以看出，KL散度是衡量两个分布的不相似性，不相似性越大，则值越大，当完全相同时，取值为0； 对比交叉熵和相对熵，可以发现仅仅差一个H(p)，如果从最优化的角度来看，p是真实分布，是固定值，最小化KL散度的情况下，H(p)可以省略，此时交叉熵等价于KL散度。 在机器学习中，何时需要使用相对熵，何时使用交叉熵？在最优化问题中，最小化相对熵等价于最小化交叉熵；相对熵和交叉熵的定义其实可以从最大似然估计得到。最大化似然函数，等价于最小化负对数似然，等价于最小化交叉熵，等价于最小化KL散度。交叉熵大量应用在Sigmoid函数和SoftMax函数中，而相对熵大量应用在生成模型中，例如，GAN、EM、贝叶斯学习和变分推导中。从这可以看出：如果想通过算法对样本数据进行概率分布建模，那么通常都是使用相对熵，因为需要明确知道生成的分布和真实分布的差距，最好的KL散度值应该是0；而在判别模型中，仅仅只需要评估损失函数的下降值即可，交叉熵可以满足要求，其计算量比KL散度小。在《数学之美》一书中是这样描述它们的区别：交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除新系统的不确定性所需要付出的努力的大小；相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异。 困惑度(perplexity)对于语言$L=(x_i)~p(x)$，与其模型q的交叉熵定义为：$$H(L,p)=-\lim_{x \to \infty} \frac {1}{n} \sum_{x_1^n}p(x_1^n)log q(x_1^n)$$其中，$x_1^n=x_1,…,x_n$为语言L的语句，$p(x_1^n)$为L中语句的概率，$q(x_1^n)$为模型q对$x_1^n$的概率估计。我们可以假设这种语言是“理想”的，即n趋于无穷大时，其全部“单词”的概率和为1。就是说，根据信息论的定理：假定语言L是稳态(stationary) ergodic随机过程， L与其模型q的交叉熵计算公式就变为：$$H(L,q)=-\lim_{x \to \infty} \frac {1}{n} log q(x_1^n)$$由此，我们可以根据模型q和一个含义大量数据的L的样本来计算交叉熵。在设计模型q时，我们的目的是使交叉熵最小，从而使模型最接近真实的概率分布p(x)。 在设计语言模型时，通常用困惑度来代替交叉熵衡量语言模型的好坏。给定语言L的样本$l_1^n=l_1,,,,l_n$，L的困惑度为$PP_q$定义为：$$PP_q = 2^{H(L,q)} \approx 2^{-\frac {1}{n}log q(l_1^n)} = [q(l_1^n)]^{- \frac {1}{n}}$$于是语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实语言的情况。从perplexity的计算式可以看出来，它是对于样本句子出现的概率，在句子长度上Normalize一下的结果。它越小，说明出现概率越大，所得模型就越好。 参考 机器学习各种熵：从入门到全面掌握：https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw##]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络中的激活函数]]></title>
    <url>%2F2018%2F04%2F21%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[激活函数简介神经网络是目前比较流行深度学习的基础，神经网络模型模拟人脑的神经元。人脑神经元接收一定的信号，对接收的信号进行一定的处理，并将处理后的结果传递到其他的神经元，数以亿计的神经元组成了人体复杂的结构。在神经网络的数学模型中，神经元节点，将输入进行加权求和，加权求和后再经过一个函数进行变换，然后输出。这个函数就是激活函数，神经元节点的激活函数定义了对神经元输入的映射关系。 激活函数的定义在ICML2016的一篇论文：Noisy Activation Functions中给出的了激活函数的定义：激活函数是实数到实数的映射，且几乎处处可导。激活函数一般具有以下性质： 非线性：弥补线性模型的不足； 几乎处处可导：反向传播时需要计算激活函数的偏导数，所以要求激活函数除个别点外，处处可导； 计算简单 单调性：当激活函数是单调的时候，单层网络能够保证是凸函数； 输出值范围有限：当激活函数的输出值有限的时候，基于梯度的优化方法会更加稳定；因为特定的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的learning rate。 激活函数的作用 神经网络中的激活函数能够引入非线性因素，提高模型的表达能力；网络中仅有线性模型的话，表达能力不够。比如一个多层的线性网络，其表达能力和单层的线性网络是相同的。网络中卷积层、池化层和全连接层都是线性的。所以，需要在网络中加入非线性的激活函数层。 一些激活函数能够起到特征组合的作用；例如，对于Sigmoid函数$\sigma(x) = \frac {1}{1+e^(-x)}$，根据泰勒公式展开:$$e^x = 1+ \frac {1}{1!}x + \frac {1}{2!}x^2 + \frac {1}{3!}x^3+O(x^3)$$对于输入特征为$x_1,x_2$，加权组合后如下：$$x = w_1x_1+w_2x_2$$将x带入到$e^x$泰勒展开的平方项，$$x^2=(w_1x_1+w_2x_2)^2 = ((w_1x_1)^2+(w_2x_2)^2 + 2w_1x_1*w_2x_2)$$可以看出，平方项起到了特征两两组合的作用，更高阶的$x^3,x^4$等，则是更复杂的特征组合。 常见非线性激活函数在介绍常见的激活函数之前，先介绍一下饱和(Saturated)的概念。 左饱和：当函数h(x)满足，$\lim \limits_{x \to +\infty}h^{‘}(x)=0$; 右饱和：当函数h(x)满足，$\lim \limits_{x \to -\infty}h^{‘}(x)=0$; 饱和：当函数h(x)既满足左饱和又满足右饱和，称h(x)是饱和的; 当激活函数是饱和的，对激活函数进行求导计算梯度时，计算出的梯度趋近于0，导致参数更新缓慢。 SigmoidSigmoid函数： 定义：$\sigma(x) = \frac {1}{1+e^{-x}}$ 值域：(0,1) 导数：$\sigma^{‘}(x) = \sigma(x)(1-\sigma(x))$ 从数学上看，非线性的Sigmoid函数对中央区域的信号增益较大，对两侧区域的信号增益较小，在信号的特征空间映射上，有很好的效果。从神经科学上来看，中央神经区酷似神经元的兴奋态，两侧区酷似神经元的抑制状态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。 Sigmoid的有以下优点： 输出值域在(0,1)之间，可以被表示为概率； 输出范围有限，数据在传递的过程中不容易发散； 求导比较方便； Sigmoid的缺点如下： Sigmoid函数是饱和的，可能导致梯度消失(两个原因:(1)Sigmoid导数值较小；(2)Sigmoid是饱和的)，导致训练出现问题； 输出不以0为中心，可能导致收敛缓慢(待思考原因)； 指数计算，计算复杂度高； TanhTanh函数的表达式为： $$tanh(x)=\frac {e^x-e^{-x}}{e^x+e^{-x}}=2\sigma(2x) - 1$$它将输入值映射到[-1,1]区间内，其函数图像为： 它的导数为$tanh^{‘}(x)=1-tanh^2(x)$Tanh函数是Sigmoid函数的一种变体；与Sigmoid不同的是，Tanh是0均值的。因此，在实际应用中，Tanh会比Sigmoid更好，但Tanh函数现在也很少使用，其优缺点总结如下： 相比Sigmoid函数，收敛速度更快； 相比Sigmoid函数，其输出是以0为中心的； 没有解决由于饱和性产生的梯度消失问题； ReLU(Rectified Linear Units)ReLU函数为现在使用比较广泛的激活函数，其表达式为：$$f(x)=max(0,x)=\begin{cases} x, &amp; if x&gt;0 \\ 0 &amp; if x\leq0 \end{cases}$$其函数图像如下： 导数图像如下： ReLU的优点如下: 相比Sigmoid和Tanh，ReLU在SGD中收敛速度要相对快一些； Sigmoid和Tanh涉及到指数运算，计算复杂度高，ReLU只需要一个阈值就可以得到激活值，加快正向传播的计算速度； 有效的缓解了梯度消失的问题； 提供了神经网络的稀疏表达能力； ReLU的缺点如下： ReLU的输出不是以0为中心的； 训练时，网络很脆弱，很容易出现很多神经元值为0，从而再也训练不动； ReLU的变体为了解决上面的问题，出现了一些变体，这些变体的主要思路是将x&gt;0的部分保持不变，$x\leq0$的部分不直接设置为0，设置为$\alpha x$，如下三种变体: L-ReLU(Leaky ReLU):$\alpha$固定为比较小的值，比如：0.01，0.05； P-ReLU(Parametric ReLU):$\alpha$作为参数，自适应地从数据中学习得到； R-ReLU(Randomized ReLU):先随机生成一个$\alpha$，然后在训练过程中再进行修正； ELUELU的函数形式如下：$$f(x)=\begin{cases} x, &amp; if x&gt;0 \\ \alpha(e^x-1) &amp; if x\leq0 \end{cases}$$ 其函数图像如下： ELU也是为了解决ReLU存在的问题而提出的，它具有ReLU的基本所有优点，以及： 不会有神经元死亡的问题； 输出的均值接近于0，zero-centered; 计算量稍大，理论上虽然好于ReLU，但在实际使用中，目前并没有好的证据证明ELU总是优于ReLU; MaxOutMaxOut函数定义如下：$$y=f(x)=max_{j\in[1,k]}z_j \\z_j = w_jx+b_j$$一个比较容易的介绍，如下图： 假设w是二维的，那么有:$$f(x)=max(w_1^Tx+b_1,w_2^Tx+b_2)$$可以看出，ReLU及其变体都是它的一个变形(当$w_1,b_1=0的时候，就是ReLU$) 激活函数使用建议 如果想让结果在(0,1)之间，使用Sigmoid(如LSTM的各种Gates); 如果想神经网络训练的很深，不要使用S型的激活函数； 如果使用ReLU，要注意初始化和Learning Rates的设置； 如果使用ReLU，出现很多神经元死亡的问题，且无法解决，可以尝试使用L-ReLU、P-ReLU等ReLU的变体； 最好不要使用Sigmoid，可以尝试使用Tanh;]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 变量管理]]></title>
    <url>%2F2018%2F03%2F11%2FTensorflow%E5%8F%98%E9%87%8F%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[摘自：《TensorFlow实战Google深度学框架》一书，5.3节。 Tensorflow提供了通过变量名称来创建或者获取一个变量的机制。通过这个机制，在不同的函数中可以直接通过变量的名字来使用变量，而不需要将变量通过参数的形式到处传递。TensorFlow中通过变量名获取变量的机制主要是通过tf.get_variable和tf.variable_scope函数实现的。下面将分别介绍如何使用这两个函数。通过tf.Variable()函数可以创建一个变量。除了tf.Variable()函数，TensorFlow还提供了tf.get_variable函数来创建或者获取变量。当tf.get_variable用于创建变量时，它和tf.Variable()的功能是基本等价的。下面的代码给出通过这两个函数创建同一个变量的示例：123# 下面这两个定义是等价的v = tf.get_variable("v",shape=[1],initializer=tf.constant_initializer(1.0))v = tf.Variable(tf.constant(1.0, shape=[1]),name='v') 从上面的代码可以看出，通过tf.Variable和tf.get_variable函数创建变量的过程基本上是一样的。tf.get_variable函数调用时提供的维度(shape)信息以及初始化方法(initializer)的参数和tf.Variable函数调用时提供的初始化过程中的参数也类似。TensorFlow中提供的initializer函数和随机数以及常量生成函数大部分是一一对应的。比如，在上面的样例程序中使用的常数初始化函数tf.constant_initializer和常数生成的函数tf.constant功能上是一致的。Tensorflow提供了7种不同的初始化函数，如下表所示： 初始化函数 功能 主要参数 tf.constant_initializer 将变量初始化为给定常量 常量的取值 tf.random_normal_initializer 将变量初始化为满足正态分布的随机值 正态分布的均值和标准差 tf.truncated_normal_initializer 将变量初始化为满足正态分布的随机值，但如果随机出来的值偏离平均值超过2个标准差，那么这个数将会被重新随机 正态分布的均值和标准差 tf.random_uniform_initializer 将变量初始化为满足均匀分布的随机值 最大值、最小值 tf.uniform_unit_scaling_initializer 将变量初始化为满足均匀分布但不影响输出数量级的随机值 factor(产生随机数时乘以的系数) tf.zeros_initializer 将被变量设置为0 变量维度 tf.ones_initializer 将变量设置为1 变量的维度 tf.get_variable函数与tf.Variable函数最大的区别在于指定变量名称的参数。对于tf.Variable函数， 变量名称是一个可选的参数，通过name=“v”的形式给出。但是对于tf.get_variable函数，变量名称是一个必填的参数。tf.get_variable会根据这个名字去创建或者获取变量。在上面的示例程序中，tf.get_variable首先会试图创建一个名字为v的参数，如果创建失败（比如已经有同名的参数），那么这个程序会报错。这是为了避免无意识的变量复用造成的错误。比如在定义神经网络参数时，第一层网络的权重已经叫weights了，如果创建第二层的神经网络时，如果参数名仍然叫weights，那么就会触发变量重用的错误。否则两层神经网络公用一个权重会出现一些比较难以发现的错误。如果需要通过tf.get_variable获取一个已经创建的变量，需要通过tf.variable_scope函数来生成一个上下文管理器，并明确指定在这个上下文管理器中，tf.get_variable将直接获取已经生成的变量。下面给出一段代码说明如何通过tf.variable_scope函数来控制tf.get_variable函数获取已经创建过的变量。123456789101112131415161718# 在名字为foo的命名空间内创建名字为v的变量with tf.variable_scope("foo"): v = tf.get_variable("v", [1], initializer=tf.constant_initializer(1.0))# 因为在命名空间foo中已经存在名为v的变量，所以下面的代码将会报错with tf.variable_scope("foo"): v = tf.get_variable("v",[1])# 在生成上下文管理器时，将参数reuse设置为True。这样tf.get_vaiable函数将直接获取# 已经声明的变量。with tf.variable_scope("foo",reuse=True): v1 = tf.get_variable("v",[1]) print v==v1 #输出为True，v和v1代表的是相同的变量# 将参数reuse设置为True时，tf.variable_scope将只能获取已经创建过的变量。因为在命名# 空间bar中还没有创建变量v，所以下面的代码将会报错with tf.variable_scope("bar",reuse=True): v = tf.get_variable("v",[1]) 上面的样例简单地说明了通过tf.variable_scope函数可以控制tf.get_variable函数的语义。当tf.variable_scope函数使用参数reuse=True生成上下文管理器时，这个上下文管理器内所有的tf.get_variable函数会直接获取已经创建的变量。如果变量没有被创建，则tf.get_variable将会报错；相反如果tf.variable_scope函数使用参数reuse=None或者reuse=False创建上下文管理器，tf.get_variable操作将创建新的变量。如果同名变量已经存在，则tf.get_variable函数将会报错。TensorFlow中tf.variable_scope函数是可以嵌套的。下面的程序说明了当tf.variable_scope函数嵌套时，reuse参数的取值时如何确定的。 123456789101112with tf.variable_scope("root"): # 可以通过tf.get_variable_scope().reuse函数来获取当前上下文管理器中reuse参数的取值 print tf.get_variable_scope().reuse #输出False，即最外层reuse是False with tf.variable_scope("foo",reuse=True): # 新建一个嵌套的上下文管理器， # 并指定reuse为True print tf.get_variable_scope().reuse # 输出为True with tf.variable_scope("bar"): # 新建一个嵌套的上下文管理器 # 但不指定reuse的取值，和外层的保持一致 print tf.get_variable_scope().reuse # 输出为True print tf.get_variable_scope().reuse # 输出False，退出reuse设置为True # 的上下文之后，reuse的值又回到了False tf.variable_scope函数生成的上下文管理器也会创建一个TensorFlow中的命名空间，在命名空间内创建的变量名称都会带上这个命名空间名作为前缀。所以，tf.variable_scope函数除了控制tf.get_variable执行的功能之外，这个函数也提供了一个管理变量命名空间的方式。下面的代码显示如何通过tf.variable_scope来管理变量的名称。1234567891011121314151617181920212223v1 = tf.get_variable("v",[1])print v1.name # 输出v:0,"v"为变量名称，“：0”表示这个变量时生成变量这个运算的第一个结果with tf.variable_scope("foo"): v2 = tf.get_variable("v",[1]) print v2.name # 输出为foo/v:0。在tf.variable_scope中创建的变量，名称前面会 # 加入命名空间的名称，通过/来分隔命名空间的名称和变量的名称。with tf.variable_scope("foo"): with tf.variable_scope("bar"): v3 = tf.get_variable("v",[1]) print v3.name # 输出为foo/bar/v:0。命名空间可以嵌套，同时变量的名称也会 # 加入所有命名空间的名称作为前缀。 v4 = tf.get_variable("v1",[1]) print v4.name # 输出foo/v1:0。当命名空间退出之后，变量名称也就不会再被加入其前缀了。# 创建一个名称为空的命名空间，并设置为reuse=Truewith tf.variable_scope("", reuse=True): v5 = tf.get_variable("foo/bar/v",[1]) # 可以直接通过带命名空间名称的变量名来 # 获取其他命名空间下的变量 print v5 == v3 # 输出为True v6 = tf.get_variable("foo/v1",[1]) print v6 == v4 # 输出为True]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention Model 注意力机制]]></title>
    <url>%2F2018%2F02%2F12%2FAttention%20Model%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本文是对:https://blog.heuritech.com/2016/01/20/attention-mechanism/ 的翻译。这篇文章对Attention Model原理进行了一个比较清晰的阐述，所以记录一下。由于本人英语能力有限，翻译不周的地方，还请见谅。在2015年，随着DeepLearning和AI的发展，神经网络中的注意力机制引起了许多研究者的兴趣。这篇博文的目的是从一个高的层次上对注意力机制进行解释，以及详细介绍attention的一些计算步骤。如果你要更多关于attention的公式或例子，文后的参考文献提供了一下，特别注意Cho et al[3]这篇文章。不幸的是，这些模型往往你自己实现不了，仅仅有一些开源的实现。 Attention神经科学和计算神经科学中的neural processes已经广泛研究了注意力机制[1,2]。视觉注意力机制是一个特别值得研究的方向：许多动物专注于视觉输入的特定部分，去计算适当的反映。这个原理对神经计算有很大的影响，因为我们需要选择最相关的信息，而不是使用所有可用的信息，所有可用信息中有很大一部分与计算神经元反映无关。一个类似于视觉专注于输入的特定部分，也就是注意力机制已经用于深度学习、语音识别、翻译、推理以及视觉识别。 Attention for Image Captioning我们通过介绍一个例子，去解释注意力机制。这个任务是我们想实现给图片加标题：对于给定的图片，根据图片中的内容给图片配上标题(说明/描述)。一个典型的image captioning系统会对图片进行编码，使用预训练的卷积神经网络产生一个隐状态h。然后，可以使用RNN对这个隐状态h进行解码，生成标题。这种方法已经被不少团队采用，包括[11]，如下图所示： 这种方法的问题是：当模型尝试去产生标题的下一个词时，这个词通常是描述图片的一部分。使用整张图片的表示h去调节每个词的生成，不能有效地为图像的不同部分产生不同的单词。这正是注意力机制有用的地方。使用注意力机制，图片首先被划分成n个部分，然后我们使用CNN计算图像每个部分的表示$h_1,…,h_n$，也就是对n个部分的图像进行编码。当使用RNN产生一个新的词时，注意力机制使得系统只注意图片中相关的几个部分，所以解码仅仅使用了图片的特定的几个部分。如下图所示，我们可以看到标题的每个词都是用图像(白色部分)的一部分产生的。 更多的例子如下图所示，我们可看到图片相关的部分产生标题中划线的单词。 现在我们要解释注意力机制是怎么工作的，文献[3]详细介绍了基于注意力机制的Encoder-Decoder Network的实现。 What is an attention model在一般情况下，什么是注意力机制？ 一个attention model通常包含n个参数$y_1,..,y_n$(在前面的例子中，$y_i$可以是$h_i$) ,和一个上下文c。它返回一个z向量，这个向量可以看成是对$y_i$的总结，关注与上下文c相关联的信息。更正式地说是，它返回的$y_i$的加权算术平均值，并且权重是根据$y_i$与给定上下文c的相关性来选择的。 在上面的例子中，上下文是刚开始产生的句子，$y_i$是图像的每个部分的表示($h_i$)，输出是对图像进行一定的过滤（例如：忽略图像中的某些部分）后的表示，通过过滤将当前生成的单词的重点放在感兴趣的部分。注意力机制有一个有趣的特征：计算出的权重是可访问的并且可以被绘制出来。这正是我们之前展示的图片，如果权重越高，则对应部分的像素越白。 但是，这个黑盒子做了什么？下图能够清晰的表示Attention Model的原理： 可能这个网络图看起来比较复杂，我们一步一步来解释这个图。首先，我们能够看出 一个输入c，是上下文，$y_i$是我们正在研究的“数据的一部分”。 然后，网络计算$m_1,…,m_n$通过一个tanh层。这意味着我们计算$y_i$和c的一个”聚合”。重要的一点是，每个$m_i$的计算都是在不考虑其他$y_j,j \neq i$的情况下计算出来的。它们是独立计算出来的。 $$m_i = tanh(W_{cm}c+W_{ym}y_i)$$ 然后，我们计算每个weight使用softmax。softmax，就像他的名字一样，它的行为和argmax比较像，但是稍有不同。$argmax(x_1,…,x_n)=(0,..,0,1,0,..,0)$,在输出中只有一个1，告诉我们那个是最大值。但是，softmax的定义为：$softmax(x_1,…,x_n)=(\frac {e^{x_i}}{\sum_j e^{x_j}})_i$。如果其中有一个$x_i$比其他的都大，$softmax(x_1,…,x_n)将会非常接近argmax(x_1,…,x_n)$。 $$s_i 正比于 exp(w_m, m_i)\\\ \sum_i s_i=1$$这里的$s_i$是通过softmax进行归一化后的值。因此，softmax可以被认为是“相关性”最大值的变量，根据上下文。输出$z$是所有$y_i$的算术平均，每个权重值表示$y_i,..,y_n$和上下文c的相关性。 $$z = \sum_i s_iy_i$$ An other computation of “relevance”上面介绍的attention model是可以进行修改的。首先，tanh层可以被其他网络或函数代替。重要的是这个网络或函数可以综合c和$y_i$。比如可以只使用点乘操作计算c和$y_i$的内积，如下图所示： 这个版本的比较容易理解。上面介绍的Attention是softly-choosing与上下文最相关的变量（$y_i$）。据我们所知，这两种系统似乎都能产生类似的结果。另外一个比较重要的改进是hard attention。 Soft Attention and Hard Attention上面我们描述的机制称为Soft Attention，因为它是一个完全可微的确定性机制，可以插入到现有的系统中，梯度通过注意力机制进行传播，同时它们通过网络的其余部分进行传播。Hard Attention是一个随机过程：系统不使用所有隐藏状态作为解码的输入，而是以概率$s_i$对隐藏状态$y_i$进行采样。为了进行梯度传播，使用蒙特卡洛方法进行抽样估计梯度。 这两个系统都有自己的优缺点，但是研究的趋势是集中于Soft Attention，因为梯度可以直接计算，并不是通过随机过程来估计的。 Return to the image captioning现在，我们来理解一下image captioning 系统是怎样工作的。 从上面的图我们可以看到image captioning的典型模型，但是添加了一个新的关于attention model的层。当我们想要预测标题的下一个单词时，发生了什么？如果我们要预测第i个词，LSTM的隐藏状态是$h_i$。我们选择图像相关的部分通过把$h_i$作为上下文。然后，attention model的输出是$z_i$，这是被过滤的图像的表示，只有图像的相关部分被保留，用作LSTM的输入。然后，LSTM预测一个下一个词，并返回一个隐藏状态$h_{i+1}$。 Learning to Align in Machine TranslationBahdanau, et al[5]中的工作提出了一个神经翻译模型将句子从一种语言翻译成另一种语言，并引入注意力机制。在解释注意力机制之前，vanillan神经网络翻译模型使用了编码-解码(Encoder-Decoder)框架。编码器使用循环神经网络(RNN,通常GRU或LSTM)将用英语表示的句子进行编码，并产生隐藏状态h。这个隐藏状态h用于解码器RNN产生正确的法语的句子。 编码器不产生与整个句子对应的单个隐藏状态，而是产生与一个词对应的隐藏状态$h_j$。每当解码器RNN产生一个单词时，取决于每个隐藏状态作为输入的贡献，通常是一个单独的参数（参见下图）。这个贡献参数使用Softmax进行计算：这意味着attention weights $a_j$在$\sum a_j=1$的约束下进行计算并且所有的隐藏状态$h_j$给解码器贡献的权重为$a_j$。在我们的例子中，注意力机制是完全可微的，不需要额外的监督，它只是添加在现有的编码-解码框架的顶部。这个过程可以看做是对齐，因为网络通常在每次生成输出词时都会学习集中于单个输入词。这就意味着大多数的注意力权重是0(黑)，而一个单一的被激活(白色)。下面的图像显示了翻译过程中的注意权重，它揭示了对齐方式，并使解释网络所学的内容成为可能（这通常是RNNs的问题！）。 Attention without Recurrent Neural Networks到现在为止，我们仅介绍了注意力机制在编码-解码框架下的工作。但是，当输入的顺序无关紧要时，可以考虑独立的隐藏状态$h_j$。这个在Raffel et Al[10]中进行了介绍，这里attention model是一个前向全连接的网络。同样的应用是Mermory Networks[6]（参见下一节）。 From Attention to Memory AddressingNIPS 2015会议上提出了一个非常有趣的工作叫做 RAM for Reasoning、Attention and Memory。它的工作包含Attention，但是也包括Memory Networks[6],Neural Turing Machines[7]或 Differentiable Stack RNNS[8]以及其他的工作。这些模型都有共同之处，它们使用一种外部存储器的形式，这种存储器可以被读取（最终写入）。比较和解释这些模型是超出了这个本文的范围, 但注意机制和记忆之间的联系是有趣的。例如，在Memory Networks中，我们认为外部存储器-一组事实或句子$x_i$和一个输入q。网络学习对记忆的寻址，这意味着选择哪个事实$x_i$去关注来产生答案。这对应了一个attention model在外部存储器上。In Memory Networks, the only difference is that the soft selection of the facts (blue Embedding A in the image below) is decorrelated from the weighted sum of the embeddings of the facts (pink embedding C in the image).（PS：实在读不懂了）。在Neural Turing Machine中，使用了一个Soft Attention机制。这些模型将会是下个博文讨论的对象。 Final Word注意力机制和其他完全可微寻址记忆系统是目前许多研究人员广泛研究而的热点。尽管它们仍然年轻, 在现实世界系统中没有实现, 但它们表明,它们可以处理在编码-解码框架下以前遗留的许多问题，它们可以被用来击败最先进的系统。在Heuritech，几个月前，我们对注意力机制产生了兴趣，组织了一个小组，去实现带注意力机制的编码-解码器。虽然我们还没有在生产中使用注意机制,但我们设想它在高级文本理解中有一个重要的作用, 在某些推理是必要的, 以类似的方式，Hermann et al[9]中的工作和此类似。在另一个单独的博客帖子中,我将详细阐述我们在研讨会上所学到的内容以及在RAM研讨会上提出的最新进展。Léonard Blier et Charles Ollion Acknowledgments在此感谢Mickael Eickenberg 和 Olivier Grisel的有益的讨论。 Bibliography[1] Itti, Laurent, Christof Koch, and Ernst Niebur. « A model of saliency-based visual attention for rapid scene analysis. » IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 11 (1998): 1254-1259. [2] Desimone, Robert, and John Duncan. « Neural mechanisms of selective visual attention. » Annual review of neuroscience 18.1 (1995): 193-222. [3] Cho, Kyunghyun, Aaron Courville, and Yoshua Bengio. « Describing Multimedia Content using Attention-based Encoder–Decoder Networks. » arXiv preprint arXiv:1507.01053 (2015) [4] Xu, Kelvin, et al. « Show, attend and tell: Neural image caption generation with visual attention. » arXiv preprint arXiv:1502.03044 (2015). [5] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. « Neural machine translation by jointly learning to align and translate. » arXiv preprint arXiv:1409.0473 (2014). [6] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. « End-to-end memory networks. » Advances in Neural Information Processing Systems. (2015). [7] Graves, Alex, Greg Wayne, and Ivo Danihelka. « Neural Turing Machines. » arXiv preprint arXiv:1410.5401 (2014). [8] Joulin, Armand, and Tomas Mikolov. « Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets. » arXiv preprint arXiv:1503.01007 (2015). [9] Hermann, Karl Moritz, et al. « Teaching machines to read and comprehend. » Advances in Neural Information Processing Systems. 2015. [10] Raffel, Colin, and Daniel PW Ellis. « Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems. » arXiv preprint arXiv:1512.08756 (2015). [11] Vinyals, Oriol, et al. « Show and tell: A neural image caption generator. » arXiv preprint arXiv:1411.4555 (2014).]]></content>
      <categories>
        <category>翻译文章</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>Attention Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GRU神经网络]]></title>
    <url>%2F2018%2F01%2F20%2FGRU%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[GRU神经网络GRU(Gated Recurrent Unit)是LSTM的一种变体，它对LSTM做了很多简化，同时却保持着和LSTM几乎相同的效果。因此，GRU最近变得非常流行。下图是GRU的网络架构图。GRU对LSTM做了两个大得改动： 将输入门、遗忘门和输出门改变为两个门：更新门（Update Gate)$z_t$和重置门(Reset Gate)$r_t$。 将单元状态与输出合并为一个状态：h。 根据上图的架构图可以得出GRU的前向计算公式：$$\begin{aligned}&amp;r_t = \sigma(W_r \cdot [h_{t-1},x_t])\\\\&amp;z_t =\sigma(W_z \cdot [h_{t-1},x_t])\\\\&amp;\hat {h_t}=tanh(W_{\hat {h}} \cdot [r_t \bigodot h_{t-1},x_t])\\\\&amp;h_t =(1-z_t)\bigodot h_{t-1}+z_t \bigodot \hat {h_t}\\\\&amp;y_t=\sigma(W_o \cdot h_t)\end{aligned}$$ GRU的反向传播梯度计算GRU的参数更新方式同样是基于沿时间反向传播的算法（BPTT）,为了为了更清晰的推导GRU反向传播梯度计算，对上文的GRU前向计算公式进行一定的改写，实质上还是一样的，只不过是将参数分开写而已。具体如下：假设，对于t时刻，GRU的输出为$\hat {y_t}$，输入为$x_t$，前一时刻的状态为$s_{t-1}$，则可以得出如下的前向计算的公式：$$\begin{aligned} &amp;z_t = \sigma(U_zx_t+W_zs_{t-1}+b_z)\\\\ &amp;r_t = \sigma(U_r x_t+W_rs_{t-1}+b_r)\\\\ &amp;h_t = tanh(U_hx_t+W_h(s_{t-1}\bigodot r_t)+b_h)\\\\ &amp;s_t = (1-z_t)\bigodot h_t + z_t \bigodot s_{t-1}\\\\ &amp;\hat {y_t}=softmax(Vs_t+b_V)\end{aligned}$$其中，$\bigodot$表示向量的点乘；$z_t$表示更新门；$r_t$表示重置门；$\hat {y_t}$表示t时刻的输出。如果采用交叉熵损失函数，那么在t时刻的损失为$L_t$:$$L_t =sumOfAllElements(-y_t\bigodot log(\hat {y_t}))$$为了训练GRU，需要把所有时刻的损失加在一起，并最小化损失$L=\sum_{t=1}^T L_t$:$$argmin_{\Theta}L$$其中，$\Theta={U_z,U_r,U_c,W_z,W_r,W_c,b_z,b_r,b_c,V,b_V}$。 这是一个非凸优化问题，通常采用随机梯度下降法去解决问题。因此，需要计算$\partial L/ \partial U_z,\partial L/ \partial U_r,\partial L/ \partial U_h,\partial L/ \partial W_z,\partial L/ \partial W_r,\partial L/ \partial W_h,\partial L/ \partial b_z,\partial L/ \partial b_r,\partial L/ \partial b_h,\partial L/ \partial V,\partial L/ \partial b_v$。计算上面的梯度，最好的方式是利用链式法则从输出到输入一步一步去计算，为了更好得看清输入、中间值以及输出之间的关系，画了一张GRU的计算图，如下图所示： 根据计算图，利用链式法则计算梯度，需要从上至下沿着边进行计算。如果节点X有多条出边和目标节点T相连，如果要计算$\partial T / \partial X$，需要分别计算每条边对X的梯度，并将梯度进行相加。 以计算$\frac {\partial L}{\partial U_z}$为例，其他的计算方式和其相似。因为$L=\sum_{t=1}^T L_t$，所以，$\frac {\partial L}{\partial U_z}=\sum_{t=1}^T \frac {\partial L_t}{\partial U_z}$，因此，可以先计算$\frac {\partial L_t}{\partial U_z}$，然后将不同时刻结果加起来就可以。 根据链式法则:$$\frac {\partial L_t}{\partial U_z} = \frac {\partial L_t}{\partial s_t} \frac{\partial s_t}{\partial U_z} (公式1)$$公式1右边的第一个式子的计算如下：$$\frac {\partial L_t}{\partial s_t}=V(\hat {y_t}-y_t) (公式2)$$对于$\frac {\partial z}{\partial U_z}$，一些人可能会直接进行如下的求导计算$$\frac {\overline{\partial s_t}}{\partial U_z}=((s_{t-1}-h_t)\bigodot z_t \bigodot (1-z_t))x_t^T (公式3)$$在$s_t$的计算公式里有$1-z$和$z\bigodot s_{t-1}$两个公式都会影响到$\frac {\partial s_t}{\partial U_z}$。正确的方法是分别计算每条边的偏导数，并将它们相加，因此，需要引入$\frac {\partial s_t}{\partial s_{t-1}}$。但是，公式3只计算了部分的梯度，因此用$\frac {\overline{\partial s_t}}{\partial U_z}$表示。 因为$s_{t-1}$同样依赖于$U_z$，因此我们不能把$s_{t-1}$作为一个常量处理。$s_{t-1}$同样会受到$s_i,i=1,…,t-2$的影响，因此，需要将公式1进行扩展，如下：$$\begin{aligned} \frac {\partial L_t}{\partial U_z} = &amp;\frac {\partial L_t}{\partial s_t} \frac{\partial s_t}{\partial U_z}\\\\ =&amp;\frac {\partial L_t}{\partial s_t}\sum_{i=1}^t(\frac{\partial s_t}{\partial s_i}\frac{\overline {\partial s_i}}{\partial U_z})\\\\ =&amp;\frac {\partial L_t}{\partial s_t}\sum_{i=1}^t ((\prod_{j=i}^{t-1} \frac {\partial s_{j+1}}{\partial s_j})\frac{\overline {\partial s_i}}{\partial U_z})\end{aligned} (公式4)$$其中，$\frac{\overline {\partial s_i}}{\partial U_z}$是$s_i$对$U_z$的梯度，其计算公式如公式3所示。$\frac {\partial s_t}{\partial s_{t-1}}$的计算和$\frac {\partial s_t}{\partial z}$的计算相似。因为从$s_{t-1}$到$s_t$有四条边，直接或间接相连，通过$z_t,r_t和h_t$，因此，需要计算这四条边上的梯度，然后进行相加，计算公式如下：$$\begin{aligned}\frac {\partial s_t}{\partial s_{t-1}}=&amp;\frac {\partial s_t}{\partial h_t} \frac {\partial h_t}{\partial s_{t-1}}+\frac{\partial s_t}{\partial z_t}\frac{\partial z_t}{\partial s_{t-1}} + \frac {\overline {\partial s_t}}{\partial s_{t-1}}\\\\=&amp;\frac {\partial s_t}{\partial h_t}(\frac {\partial h_t}{\partial r_t}\frac {\partial r_t}{\partial s_{t-1}}+\frac {\overline {\partial h_t}}{\partial s_{t-1}}) + \frac {\partial s_t}{\partial z_t}\frac{\partial z_t}{\partial s_{t-1}}+\frac {\overline {\partial s_t}}{\partial s_{t-1}}\end{aligned} (公式5)$$其中，$\frac {\overline {\partial s_t}}{\partial s_{t-1}}$是对$s_t$关于$s_{t-1}$的导数，并将$h_t,z_t$看做常量。同样，$\frac {\overline {\partial h_t}}{\partial s_{t-1}}$是$h_t$关于$s_{t-1}$的导数，将$r_t$看做常量。最终，可以得到:$$ \frac {\partial s_t}{\partial s_{t-1}}=(1-z_t)(W_r^T((W_h^T(1-h\bigodot h))\bigodot s_{t-1}\bigodot r \bigodot (1-r))+((W_h^T(1-h \bigodot h))\bigodot r_t))+\\\\W_z^T((s_{t-1}-h_t)\bigodot z_t \bigodot (1-z_t))+z (公式6)$$到此为止，$\frac {\partial L}{\partial U_z}$的计算已经完成，而其余的参数的计算和它的计算方式类似，沿着计算图一步一步计算，这里就不一一计算了。 参考 A Tutorial On Backward Propagation Through Time (BPTT) In The Gated Recurrent Unit (GRU) RNN]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM参数更新推导]]></title>
    <url>%2F2018%2F01%2F06%2FLSTM%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[本文转自：https://zybuluo.com/hanbingtao/note/581764，对其进行一定的整理。 LSTM前向计算 在Understanding LSTM Networks一文中，介绍了LSTM的基本原理。LSTM网络使用了门(gete)的概念，门实际上就是一层全连接，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，b是偏置项，那么门可以表示为：$$g(x) =\sigma(Wx+b)$$门的使用，就是用门的输出向量按元素乘以要控制的向量。因为门的输出是0到1之间的实数向量。所以，当门输出为0时，任何向量与之相乘都会得到0向量，这就相当于不能通过；当输出为1时，任何向量与之相乘都不会有任何改变，相当都通过。因为$\sigma$函数的值域是(0,1)，所以门的状态都是半开半闭的。 典型的LSTM的网络架构图如下图所示： 相比RNN网络，LSTM新增加状态C，称为单元状态(cell state)，如下图所示： 图引自：https://zybuluo.com/hanbingtao/note/581764 从图中可以看出，LSTM的输入有三个：当前时刻网络的输入$x_t$、上一时刻LSTM的输出值$h_{t-1}$以及上一时刻的单元状态$c_{t-1}$。LSTM的输出有两个：当前时刻LSTM输出值$h_t$和当前时刻的单元状态$c_t$。在这里x、h、c都是向量。 LSTM中引入了三个门：遗忘门(forget gate)、输入门(input gate)和输出门(output gate)。 遗忘门：它决定了上一时刻的单元状态$c_{t-1}$有多少保留到当前时刻$c_t$; 输入门：它决定了当前时刻网络的输入$x_t$有多少保留到单元状态$c_t$。 输出门：来控制单元状态$c_t$有多少输出到LSTM的当前输出值$h_t$。 遗忘门计算$$f_t = \sigma(W_f\cdot[h_{t-1},x_t]+b_f) (公式1)$$其中，$W_f$是遗忘门的权重矩阵，$[h_{t-1},x_t]$是表示把两个向量连接成一个更长的向量，$b_f$是遗忘门的偏置项，$\sigma$是sigmoid函数。事实上权重矩阵$W_f$是由两个矩阵拼接而成的，一个是$W_{fh}$，它对应着输入项$h_{t-1}$，一个是$W_{fx}$。$W_f$可以写为：$$[W_f]\begin{bmatrix}h_{t-1}\\\\x_t\end{bmatrix}=\begin{bmatrix} W_{fh}&amp;W_{fx}\end{bmatrix}\begin{bmatrix}h_{t-1}\\\\x_t\end{bmatrix}=W_{fh}h_{t-1}+W_{fx}x_t$$ 输出门计算$$i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i) (公式2)$$ 单元状态$\hat{c_t}$$$\hat{c_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c) (公式3)$$单元状态$c_t$,它是有上一时刻的单元状态$c_{t-1}$按元素乘以遗忘门$f_t$，再用当前输入的单元状态$\hat{c_t}$按元素乘以输入门$i_t$，再将两个积加和产生的：$$c_t = f_t \bigodot c_{t-1}+i_t \bigodot \hat{c_t} (公式4)$$其中$\bigodot$表示按位相乘。这样就把LSTM关于当前的记忆$\hat{c_t}$和长期的记忆$c_{t-1}$组合在一起，形成了新的状态单元$c_t$。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，又可以避免当前无关紧要的内容进入记忆。下面看一下输出门，它控制了长期记忆对当前输出的影响：输出门$$o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o) (公式5)$$ LSTM最终的输出，是由输出门和单元状态共同确定的：$$h_t = o_t \bigodot tanh(c_t) (公式6)$$从公式1到公式6就是LSTM的前向计算的全部公式。LSTM前向传播的更新过程如下： 更新遗忘门的输出：$$f_t = \sigma(W_f\cdot[h_{t-1},x_t]+b_f)$$ 更新输入门的两部分输出:$$i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\\\\hat{c_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)$$ 更新细胞状态:$$c_t = f_t \bigodot c_{t-1}+i_t \bigodot \hat{c_t}$$ 更新输出门状态:$$o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\\\h_t = o_t \bigodot tanh(c_t)$$ 更新当前序列输出:$$\hat {y_t}=\sigma(Vh_t+b_y)$$ LSTM反向传播LSTM的训练算法，仍然是反向传播算法，主要有三个步骤： 前向计算每个神经元的输出值，对于LSTM来说，即$f_t、i_t、c_t、o_t、h_t$ 5组向量。 反向计算每个神经元的误差项$\delta$。与循环神网络一样，LSTM误差项的反向传播也是包括两个方向：一个是沿时间的反向传播，即从当前时刻t开始，计算每个时刻的误差项；一个是将误差项向上一层传播。 根据相应的误差项，计算每个权重的梯度。 LSTM需要学习的参数共有8组，分别是：遗忘门的权重矩阵$W_f$和偏置项$b_f$、输入门的权重矩阵$W_i$和偏置项$b_i$、输出门的权重矩阵$W_o$和偏置项$b_o$以及计算单元状态的权重矩阵$W_c$和偏置项$b_c$。因为权重矩阵的两部分在反向传播中使用不同的公式，因此，权重矩阵$W_f$、$W_i$、$W_c$、$W_o$都将被写为分开的两个矩阵:$W_{fh}$、$W_{fx}$、$W_{ih}$、$W_{ix}$、$W_{ch}$、$W_{cx}$、$W_{oh}$、$W_{ox}$。 在t时刻，LSTM的输出值为$h_t$，定义t时刻的误差项为$\delta_t$为:$$\delta_t = \frac {\partial E}{\partial h_t}$$ 因为LSTM有四个加权输入，分别为$f_t、i_t、c_t、o_t、h_t$，定义这四个加权输入，以及他们对应的误差项。$$net_{f,t} = W_f[h_{t-1},x_t]+b_f=W_{fh}h_{t-1}+W_{fx}x_t+b_f\\\\net_{i,t} = W_i[h_{t-1},x_t]+b_i=W_{ih}h_{t-1}+W_{ix}x_t+b_i\\\\net_{\hat{c},t} = W_c[h_{t-1},x_t]+b_f=W_{ch}h_{t-1}+W_{cx}x_t+b_c\\\\net_{o,t} = W_o[h_{t-1},x_t]+b_o=W_{oh}h_{t-1}+W_{ox}x_t+b_o\\\\\delta_{f,t}=\frac {\partial E}{\partial net_{f,t}}\\\\\delta_{i,t}=\frac {\partial E}{\partial net_{i,t}}\\\\\delta_{\hat{c},t}=\frac {\partial E}{\partial net_{\hat{c},t}}\\\\\delta_{o,t}=\frac {\partial E}{\partial net_{o,t}}$$ 误差项沿时间反向传递沿时间反向传递误差项，就是要计算出t-1时刻的误差项$\delta_{t-1}$。$$\delta_{t-1}^T=\frac {\partial E}{\partial h_{t-1}}=\frac {\partial E}{\partial h_{t}}\frac {\partial h_t}{\partial h_{t-1}}=\delta_t^T\frac {\partial h_t}{\partial h_{t-1}}$$其中，$\frac {\partial h_t}{\partial h_{t-1}}$是一个Jacobian矩阵。如果隐藏层h的维度是N的话，那么它就是一个N*N的矩阵。为了求出它，先列出$h_t$的计算公式，即公式6和公式4：$$h_t=o_t\bigodot tanh(c_t)\\\\c_t=f_t \bigodot c_{t-1}+i_t \bigodot \hat {c_t}$$可以看出，$o_t、f_t、i_t、\hat{c_t}$都是$h_{t-1}$的函数，那么利用全导数公式可得：$$\delta_t^T\frac {\partial h_t}{\partial h_{t-1}}=\delta_t^T\frac {\partial h_t}{\partial o_t}\frac {\partial o_t}{\partial net_{o,t}}\frac {\partial net_{o,t}}{\partial h_{t-1}}+\delta_t^T\frac {\partial h_t}{\partial c_t}\frac {\partial c_t}{\partial f_t}\frac{\partial f_t}{\partial net_{f,t}}\frac {\partial net_{f,t}}{\partial h_{t-1}}+\delta_t^T\frac{\partial h_t}{\partial c_t}\frac {\partial c_t}{\partial i_t}\frac {\partial i_t}{\partial net_{i,t}}\frac {\partial net_{i,j}}{\partial h_{t-1}}\\\\=\delta_{o,t}^T\frac {\partial net_{o,t}}{\partial h_{t-1}}+\delta_{f,t}^T\frac {\partial net_{f,t}}{\partial h_{t-1}}+\delta_{i,t}^T\frac {\partial net_{i,t}}{\partial h_{t-1}}+\delta_{\hat{c_t},t}^T\frac{\partial net_{\hat{c_t},t}}{\partial h_{t-1}}\\\(公式7)$$下面要把公式7中的每个偏导数都求出来，根据公式6，我们可以求出：$$\frac {\partial h_t}{\partial o_t}= diag[tanh(c_t)]\\\\\frac {\partial h_t}{\partial c_t}= diag[o_t\bigodot(1-tanh(c_t)^2)]$$根据公式4，可以求出：$$\frac {\partial c_t}{\partial f_t} = diag[c_{t-1}]\\\\\frac {\partial c_t}{\partial i_t} = diag[\hat{c_t}]\\\\\frac {\partial c_t}{\partial \hat{c_t}} =diag[i_t]$$因为：$$\begin{aligned} &amp;o_t = \sigma(net_{o,t})\\\\&amp;net_{o,t} = W_{oh}h_{t-1}+W_{ox}x_t+b_o\\\ &amp;f_t = \sigma(net_{f,t})\\\ &amp;net_{f,t} = W_{ft}h_{t-1}+W_{fx}x_t+b_f\\\ &amp;i_t = \sigma(net_{i,t})\\\\&amp;net_{i,t}=W_{ih}h_{t-1}+W_{ix}x_t+b_i\\\ &amp;\hat{c_t}=tanh(net_{\hat{c},t})\\\\&amp;net_{\hat{c},t}=W_{ch}h_{t-1}+W_{cx}x_t+b_c\end{aligned}$$很容易得出:$$\begin{aligned}&amp;\frac {\partial o_t}{\partial net_{o,t}}= diag[o_t\bigodot(1-o_t)]\\\\&amp;\frac {\partial_{o,t}}{\partial h_{t-1}}=W_{oh}\\\\&amp;\frac {\partial f_t}{\partial net_{f,t}}=diag[f_t\bigodot(1-f_t)]\\\\&amp;\frac {\partial net_{f,t}}{\partial h_{t-1}}=W_{fh}\\\\&amp;\frac {\partial i_t}{\partial net_{i,j}}=diag[i_t\bigodot(1-i_t)]\\\\&amp;\frac {\partial net_{i,t}}{\partial h_{t-1}}=W_{ih}\\\\&amp;\frac {\partial \hat{c_t}}{\partial net_{\hat{c},t}}=diag[1-\hat{c_t}^2]\\\\&amp;\frac {\partial net_{\hat{c},t}}{\partial h_{t-1}}=W_{ch}\end{aligned}$$将上述偏导数带入公式7，可以得到：$$\delta_{t-1} = \delta_{o,t}^T\frac {\partial net_{o,t}}{\partial h_{t-1}}+\delta_{f,t}^T\frac {\partial net_{f,t}}{\partial h_{t-1}}+\delta_{i,t}^T\frac {\partial net_{i,t}}{\partial h_{t-1}}+\delta_{\hat{c},t}^T\frac {\partial net_{\hat{c},t}}{\partial h_{t-1}}=\delta_{o,t}^TW_{oh}+\delta_{f,t}^TW_{fh}+\delta_{i,t}^TW_{i,h}+\delta_{\hat{c},t}^TW_{ch} (公式8)$$ 根据$\delta_{o,t}、\delta_{f,t}、\delta_{i,t}、\delta_{\hat{c},t}$的定义，可知：$$\begin{aligned}&amp;\delta_{o,t}^T = \delta_t^T\bigodot tanh(c_t)\bigodot o_t\bigodot(1-o_t)(公式9)\\\\&amp;\delta_{f,t}^T =\delta_t^T \bigodot o_t\bigodot(1-tanh(c_t)^2)\bigodot c_{t-1}\bigodot f_t \bigodot (1-f_t)(公式10)\\\\&amp;\delta_{i,t}^T = \delta_t^T \bigodot o_t \bigodot(1-tanh(c_t)^2)\bigodot \hat{c_t}\bigodot i_t \bigodot (1-i_t)(公式11)\\\\&amp;\delta_{\hat{c},t}^T=\delta_t^T\bigodot o_t\bigodot(1-tanh(c_t)^2)\bigodot i_t\bigodot (1-\hat{c}^2)(公式12)\end{aligned}$$公式8到公式12就是将误差沿时间反向传播的一个时刻的公式。有了它，我们可以写出将误差向前传递到任意时刻k的公式：$$\delta_k^T = \prod_{j=k}^t-1\delta_{o,j}^TW_{oh}+\delta_{f,j}^TW_{fh}+\delta_{i,j}^TW_{ih}+\delta_{\hat{c},j}^TW_{ch}$$ 将误差传递到上一层假设当前层为第l层，定义第l-1层的误差项是误差函数l-1层加权输入的导数，即:$$\delta_t^{l-1}=\frac {\partial E}{\partial net_t^{l-1}}$$LSTM的输入$x_t$由下面的公式计算：$$x_t^l = f^{l-1}(net_t^{l-1})$$上式中，$f^{l-1}$表示第l-1层的激活函数。因为$net_{f,t}^l、net_{i,t}^l、net_{\hat{c},t}^l、net_{o,t}^l$都是$x_t$的函数，$x_t$又是$net_t^{l-1}$的函数，因此，要求出E对$net_t^{l-1}$的导数，就需要使用全导数公式：$$\begin{aligned}\frac {\partial E}{\partial net_t^{l-1}}&amp;=\frac {\partial E}{\partial net_{f,t}^l}\frac {\partial net_{f,t}^l}{\partial x_t^l}\frac {\partial x_t^l}{\partial net_t^{l-1}}+\frac {\partial E}{\partial net_{i,t}^l}\frac{\partial net_{i,t}^l}{\partial x_t^l}\frac {\partial x_t^l}{\partial net_{i,t}^{l-1}}+\frac {\partial E}{\partial net_{\hat{c},t}^l}\frac{\partial net_{\hat{c},t}^l}{\partial x_t^l}\frac {\partial x_t^l}{\partial net_t^{l-1}}+\frac{\partial E}{\partial net_{o,t}^l} \frac{\partial net_{o,t}^l}{\partial x_t^l}\frac{\partial x_t^l}{\partial net_{o,t}^{l-1}}\\\\&amp;=\delta_{f,t}^TW_{fx}\bigodot f^{\prime}(net_t^{l-1})+\delta_{i,t}^TW_{ix}\bigodot f^{\prime}(net_t^{l-1})+\delta_{\hat{c},t}^TW_{cx}\bigodot f^{\prime}(net_t^{l-1})+\delta_{o,t}^TW_{ox}\bigodot f^{\prime}(net_t^{l-1})\\\\&amp;=(\delta_{f,t}^TW_{fx}+\delta_{i,t}^TW_{ix}+\delta_{\hat{c},t}^TW_{cx}+\delta_{o,t}^TW_{ox})\bigodot f^{\prime}(net_l^{l-1})\end{aligned}(公式14)$$ 公式14就是将误差传递到上一层的公式。 权重梯度计算对度$W_{fh}、W_{ih}、W_{ch}、W_{oh}$的权重梯度，我们知道我们知道它的梯度是各个时刻梯度之和，我们首先求出它们在t时刻的梯度，然后再求出他们最终的梯度。我们已经求得误差项$\delta_{o,t}、\delta_{f,t}、\delta_{i,t}、\delta_{\hat{c},t}$很容易求出t时刻$W_{fh}、W_{ih}、W_{ch}、W_{oh}$的梯度。$$\begin{aligned} &amp;\frac{\partial E}{\partial W_{oh,t}}=\frac{\partial E}{\partial net_{o,t}}\frac{\partial net_{o,t}}{\partial W_{oh,t}}=\delta_{o,t}h_{t-1}^T\\\\ &amp;\frac{\partial E}{\partial W_{fh,t}}=\frac{\partial E}{\partial net_{f,t}}\frac{\partial net_{f,t}}{\partial W_{fh,t}}=\delta_{f,t}h_{t-1}^T\\\\ &amp;\frac{\partial E}{\partial W_{ih,t}}=\frac{\partial E}{\partial net_{i,t}}\frac{\partial net_{i,t}}{\partial W_{ih,t}}=\delta_{i,t}h_{t-1}^T\\\\ &amp;\frac{\partial E}{\partial W_{ch,t}}=\frac{\partial E}{\partial net_{\hat{c},t}}\frac{\partial net_{\hat{c},t}}{\partial W_{ch,t}}=\delta_{\hat{c},t}h_{t-1}^T\end{aligned}$$将各个时刻的梯度加在一起，就能得到最终的梯度：$$\begin{aligned}&amp;\frac{\partial E}{\partial W_{oh}}=\sum_{j=1}^t\delta_{o,j}h_{j-1}^T\\\\&amp;\frac{\partial E}{\partial W_{fh}}=\sum_{j=1}^t\delta_{f,j}h_{j-1}^T\\\\&amp;\frac{\partial E}{\partial W_{ih}}=\sum_{j=1}^t\delta_{i,j}h_{j-1}^T\\\\&amp;\frac{\partial E}{\partial W_{ch}}=\sum_{j=1}^t\delta_{\hat{c},j}h_{j-1}^T\\\\\end{aligned}$$对于偏置项$b_f、b_i、b_c、b_o$的梯度，也是将各个时刻的梯度加在一起，下面是各个时刻的偏置梯度:$$\begin{aligned}&amp;\frac{\partial E}{\partial b_{o,t}}=\frac {\partial E}{\partial net_{o,t}}\frac{\partial net_{o,t}}{\partial b_{o,t}}=\delta_{o,t}\\\\&amp;\frac{\partial E}{\partial b_{f,t}}=\frac {\partial E}{\partial net_{f,t}}\frac{\partial net_{f,t}}{\partial b_{f,t}}=\delta_{f,t}\\\\&amp;\frac{\partial E}{\partial b_{i,t}}=\frac {\partial E}{\partial net_{i,t}}\frac{\partial net_{i,t}}{\partial b_{i,t}}=\delta_{i,t}\\\\&amp;\frac{\partial E}{\partial b_{c,t}}=\frac {\partial E}{\partial net_{\hat{c},t}}\frac{\partial net_{\hat{c},t}}{\partial b_{c,t}}=\delta_{\hat{c},t}\end{aligned}$$下面是最终的偏置项的梯度，即将各个时刻的偏置项梯度加在一起：$$\begin{aligned}&amp;\frac {\partial E}{\partial b_o}=\sum_{j=1}^t\delta_{o,j}\\\\&amp;\frac {\partial E}{\partial b_i}=\sum_{j=1}^t\delta_{i,j}\\\\&amp;\frac {\partial E}{\partial b_f}=\sum_{j=1}^t\delta_{f,j}\\\\&amp;\frac {\partial E}{\partial b_c}=\sum_{j=1}^t\delta_{\hat{c},j}\end{aligned}$$对于$W_{fx}、W_{ix}、W_{cx}、W_{ox}$的权重梯度，只需要根据相应的误差项直接计算即可:$$\begin{aligned}&amp;\frac {\partial E}{\partial W_{ox}}=\frac{\partial E}{\partial net_{o,t}}\frac{\partial net_{o,t}}{\partial W_{ox}}=\delta_{o,t}x_t^T\\\\&amp;\frac {\partial E}{\partial W_{fx}}=\frac{\partial E}{\partial net_{f,t}}\frac{\partial net_{f,t}}{\partial W_{fx}}=\delta_{f,t}x_t^T\\\\&amp;\frac {\partial E}{\partial W_{ix}}=\frac{\partial E}{\partial net_{i,t}}\frac{\partial net_{i,t}}{\partial W_{ix}}=\delta_{i,t}x_t^T\\\\&amp;\frac {\partial E}{\partial W_{cx}}=\frac{\partial E}{\partial net_{\hat{c},t}}\frac{\partial net_{\hat{c},t}}{\partial W_{cx}}=\delta_{\hat{c},t}x_t^T\\\\\end{aligned}$$ &lt;–end–&gt;]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>LSTM</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解LSTM]]></title>
    <url>%2F2017%2F12%2F31%2F%E7%90%86%E8%A7%A3LSTM%2F</url>
    <content type="text"><![CDATA[本文是http://colah.github.io/posts/2015-08-Understanding-LSTMs/的译文，这篇文章对LSTM的原理讲解的非常清楚，故存下来。 Recurrent Neural Networks人类并非每一秒都从头开始思考问题。当你阅读这篇文章时，你是基于之前的单词来理解，每个单词。你并不会把所有的内容都抛弃掉，然后从头开始理解。你的思考具有持久性。传统的神经网络并不能做到这一点，这似乎是其一个主要的缺点。例如，想象你要把一部电影里面的时间点正在发生的事情进行分类。传统神经网络并不知道怎样才能把关于之前事件的推理运用到之后的事件中去。RNN神经网络解决了这个问题。它们是一种具有循环的网络，具有保持信息的能力。如下图所示： 如上图所示，神经网络的模块A输入为$x_i$，输出为$h_t$。模块A的循环结构使得信息从网络的上一步传到了下一步。这个循环使得RNN看起来有点神秘。然而，如果你仔细想想就会发现它与普通的神经网络并没有太大不同。RNN可以被认为是相同网络的多重复制结构，每一个网络把消息传给其继承者。如果我们把循环体展开就是这样，如下图所示： 这种链接属性表明，RNN与序列之间有着紧密的连续。这也是运用这类数据最自然的结构。当然它们已经得到了应用。过去几年中，RNNs已经被成功用于各式各样的问题中：语音识别、语言建模、翻译、图像标注..等等。RNNs取得的各种瞩目成果可以参考Andrej Karpathy的博客：The Unreasonable Effectiveness of Recurrent Neural Networks。确实效果让人非常吃惊。 取得这项成功的一个要素是LSTMs，这是一种非常特殊的周期神经网络，对于许多任务，比标准版要有效得多。几乎所有基于RNN的好成果都使用了它们。本文将着重介绍LSTMs。 长期依赖问题(The Problem of Long-Term Dependencies)RNNs的一个想法是，它们可能会能够将之前的信息连接到现在的任务之中。例如，用视频前一帧的信息可以用于理解当前帧的信息。如果RNNs能够做到这些，那么将会非常有用。但是它们可以吗？ 这要看情况。 有时候，我们处理当前任务仅需要查看当前信息。例如，设想用一个语言模型基于当前单词尝试着去预测下一个单词。如果我们尝试着预测”the clouds are in the”的最后一个单词，我们并不需要任何额外的信息，很显然下一个但是sky。这样的话，如果目标预测的点与其他相关信息的点之间的间隔较小，RNNs可以学习利用过去的信息。 但是，也有时候我们需要更多的上下文信息。设想预测这句话的最后一个单词：”I grew up in France… I speak fluent French“。最近的信息表明下一个单词似乎是一种语言的名字，但是如果我们希望缩小确定语言类型的范围，我们需要更早之前作为France 的上下文。而且需要预测的点与其相关点之间的间隔非常有可能变得很大，如图所示： 不幸的是，随着间隔增长，RNNs变得难以学习连接之间的关系，如下图所示： 理论上来说，RNNs绝对能够处理这种『长期依赖』。人们可以小心选取参数来解决这种类型的小模型。悲剧的是，事实上，RNNs似乎并不能学习出来这些参数。这个问题已经在Hochreiter (1991) German与Bengio, et al. (1994),中被深入讨论，他们发现了为何RNNs不起作用的一些基本原因。幸运的是，LSTMs可以解决这个问题! LSTM网络长短时间记忆网络(Long Short Term Memory networks, LSTMs)，是一种特殊的RNN，它能够学习长时间依赖。它们由Hochreiter &amp; Schmidhuber (1997)，后来由很多让人加以改进和推广。它们在大量的问题上都取得了巨大的成功，现在已经被广泛应用。 LSTMs是专门设计用来避免长期依赖问题的。记忆长期信息是LSTMs的默认行为，而不是它们努力学习的东西！所有RNN都具有链式的重复模块神经网络。在标准的RNNs中，这种重复模块具有非常简单的结构，比如是一个tanh层，如下图所示： LSTMs同样具有链式结构，但是其重复模块却有着不同的结构。不同于单独的神经网络层，它具有4个以特殊方式相互影响的神经网络层，如图所示： 不要担心接下来涉及到的细节。我们将会一步步讲解LSTM的示意图。下面是我们将要用到的符号，如图所示： 在上图中，每一条线代表一个完整的向量，从一个节点的输出到另一个节点的输入。粉红色圆形代表了逐点操作。例如，向量求和；黄色方框代表学习出的神经网络层；聚拢的线代表了串联，而分开的线代表了内容复制去了不同的地方。 LSTMs背后的核心思想LSTMs的关键在于细胞状态，在图中以水平线表示。细胞状态就像一个传送带。它顺着整个链条从头到尾运行，中间只有少许线性的交互。信息很容易顺着它流动而保持不变。如图所示： LSTM通过称之为门(gates)的结构来对细胞状态增加或删除信息。门是选择性的让信息通过的方式。它们的输出有一个sigmoid层和逐点乘积操作，如图所示： Sigmoid 层的输出在0到1之间，定义了各成分被放行通过的程度。0值意味着『不让任何东西过去』；1值意味着『让所有东西通过』。LSTM具有3种门，用于保护和控制细胞状态。 逐步讲解LSTMLSTM的第一步是决定我们要从细胞中抛弃何种信息。这个决定是由叫做『遗忘门』的sigmoid层决定的。它以$h_{t-1}$和$x_i$为输入，在$C_{t-1}$细胞输出一个介于0和1之间的数。其中，1代表『完全保留』，0代表『完全遗忘』。 让我们回到之前那个语言预测模型的例子，这个模型尝试着根据之前的单词学习预测下一个单词。在这个问题中，细胞状态可能包括了现在的主语的性别，因此能够使用正确的代词。当我们见到一个新的主语时，我们希望它能够忘记之前主语的性别。如下图所示： 下一步是决定细胞要存储何种信息。它有2个组成部分，首先一各叫做『输入门层』的sigmoid层决定我们将要更新哪些值。其次，一个tanh层创建一个新的候选向量$\hat{C}_t$，它可以加在状态之中。在下一步我们将结合两者来生成状态的更新。在语言模型的例子中，我们希望把新主语的性别加入到状态之中，从而取代我们打算遗忘的旧主语的性别，如下图所示： 现在我们可以将旧细胞状态$C_{t-1}$更新为$C_t$了。之前的步骤已经决定了该怎么做，我们现在实际操作一下。我们把旧状态乘以$f_t$，用以遗忘之前我们决定忘记的信息。然后我们加上$i_t*\hat{C}_t$。这是新的候选值，根据我们决定更新状态的程度来作为缩放系数。在语言模型中，这里就是我们真正丢弃关于旧主语性信息以及添加新信息的地方，如下图所示： 最终，我们可以决定输出哪些内容。输出取决于我们的细胞状态，但是以一个过滤后的版本。首先，我们使用sigmoid层来决定我们要输出细胞状态的哪些部分。然后，把用tanh处理细胞状态（将状态值映射到-1至1之间）。最后，将其与sigmoid门的输出值相乘，从而我们能够输出我们决定输出的值。如下图所示： 对于语言模型，在预测下一个单词的例子中，当它输入一个主语，它可能会希望输出相关的动词。例如，当主语时单数或复数时，它可能会以相应形式的输出。 各种LSTM的变化形式目前，我所描述的都是普通的LSTM，然而并非所有的LSTM都是一样的。事实上，似乎每一篇使用LSTMs的文章都有细微的差别。这些差别很小，但是值得一提。 其中一个流行的LSTM变化形式是由Gers &amp; Schmidhuber (2000)提出的，增加了『窥视孔连接（peephole connections）』。如下图所示： 在上图中，所有的门都加上了窥视孔，但是许多论文中只在其中一些装了窥视孔。另一个变种是使用了配对遗忘与输入门。与之前分别决定遗忘与添加信息不同，我们同时决定两者。只有当我们需要输入一些内容的时候我们才需要忘记。只有当早前信息被忘记之后我们才会输入。如图所示： LSTM 一个更加不错的变种是 Gated Recurrent Unit（GRU），是由Cho, et al. (2014)提出的。这个模型将输入门与和遗忘门结合成了一个单独的『更新门』。而且同时还合并了细胞状态和隐含状态，同时也做了一下其他的修改。因此这个模型比标准LSTM模型要简单，并且越来越收到欢迎。如下图所示： 这些仅仅只是LSTM的少数几个著名变种。还有很多其他的种类，例如由Yao, et al. (2015) 提出的Depth Gated RNNs 。以及处理长期依赖问题的完全不同的手段，如Koutnik, et al. (2014)提出的Clockwork RNNs。 那种变种是最好的？这些不同重要吗？Greff, et al. (2015)将各种著名的变种做了比较，发现其实基本上是差不多的。Jozefowicz, et al. (2015)测试了超过一万种RNN结构，发现了一些在某些任务上表现良好的模型。 结论最开始我提到的杰出成就都是使用的是RNNs。本质上所有的这些成果都是使用了LSTMs。在大多数任务中，确实它们的表现非常优秀。 以公式的形式写下来，LSTMs看起来非常令人胆怯。然而本文的逐步讲解使得LSTM变得平易近人了。 LSTMs 是我们使用RNNs的重要一步。我们很自然地想到：还有下一个重要的一大步吗？研究者的普遍观点是：『有！下一大步就是「注意力」。』其基本思想就是让RNN的每一步从更大范围的信息中选取。例如，假设你为图片打标签，它可能会为它输出的每一个词语选取图片的一部分作为输入。事实上，Xu, et al. (2015)就是这么做的——如果你想探索『注意力』的话，这是个有趣的引子！已经有大量使用『注意力』得到的良好成果，而且似乎更多的陈果也将要出现…… 『注意力』并非是RNN研究中唯一一个激动人心的方向。例如，Kalchbrenner, et al. (2015)做出的Grid LSTMs 似乎很有前途。在生成模型中使用RNNs－例如Gregor, et al. (2015)，Chung, et al. (2015)以及Bayer &amp; Osendorfer (2015)－似乎也很有趣。过去几年是RNN激动人心的阶段，未来几年将会更加如此！]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[循环神经网络RNN 梯度推导(BPTT)]]></title>
    <url>%2F2017%2F12%2F30%2FRNN%E7%AE%80%E5%8D%95%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[循环神经网络简介循环神经网络(Recurrent Neural Network,RNN)，是一种sequence model，它的思想就是使用序列信息。在前馈、卷积神经网络中，认为输入（和输出）彼此之间是互相独立的。但是对很多任务而言，这种处理方式很不合理。同时，在前馈、卷积神经网络中，输入和输出的维数都是固定的，不能任意改变，且无法处理变长的序列数据。循环神经网络，它对于序列中的每个元素都执行相同的任务，输出依赖于之前的计算。另一种思考循环神经网络的方法是，它们有一个记忆，记忆可以捕获迄今为止已经计算过的信息。理论上循环神经网络可以利用任意长度的序列信息。但是，在实际应用中，由于梯度传播的原因，它们仅能利用有限步长。循环神经网络的网络结构图如下： 在上图的网络中，网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。$h_t$的值不仅依赖于$x_t$，还取决于$s_{t-1}$。循环神经网络的计算方法如下：$$o_t = g(V_{h_t}) (公式1)\\\\s_t = f(Ux_t+Ws_{t-1}) (公式2)$$ 其中，公式1是输出层的计算公式，输出层可以是一个全连接层，V是输出层的权重矩阵，g是相应的激活函数。公式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一层的输出值$s_{t-1}$作为这一次的输入的权重矩阵，f是激活函数。 可以看出，循环层和全连接层的区别就是多了一个权重矩阵W。如果把公式2反复带到公式1，我们可以得到：$$\begin{aligned}o_t &amp;= g(Vs_t)\\\\&amp;=Vf(Vs_t)\\\\&amp;=Vf(Ux_t+Ws_{t-1})\\\\&amp;=Vf(Ux_t+Wf(Ux_{t-1}+Ws_{t-2}))\\\\&amp;=Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Ws_{t-3})))\\\\&amp;=Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Wf(Ux_{t-3}+…))))\end {aligned}$$ 从上面可以看出，循环神经网络的输出值$o_t$，是受前面的历次输入值$x_t、x_{t-1}、x_{t-2}、…$影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。循环神经网络的隐藏层的输出可以用于预测词汇/标签等符号的分布，隐藏层状态保留了到目前为止的历史信息。 循环神经网络的训练算法BPTT介绍循环神网络的训练算法是Backpropagation Through Time,BPTT算法，其基本原理和反向传播算法是一样的，只不过反向传播算法是按照层进行反向传播，BPTT是按照时间t进行反向传播。对于下图所示的循环神经网络： 这个图和上面的图所示的架构没有区别，只不过是把隐藏层的状态用$h_t$表示，t时刻的输出用$y_t$表示。$$ h_t = f(Uh_{t-1}+Wx_t+b)\\\\y_t=softmax(Vh_t)$$假设循环神经网络在每个时刻t都有一个监督信息，损失为$J_t$，则整个序列的损失为$J=\sum_{t=1}^T J_t$。$$J_t = -y_tlogy_t\\\\ J = -\sum_{t=1}^T y_tlogy_t$$ 1、J关于V的梯度计算 $$\frac {\partial J}{\partial V} = \frac {\partial}{\partial V} \sum_{t=1}^T J_t=\sum_{t=1}^T \frac {\partial J_t}{\partial V}$$令$$y_t = softmax(z_t)\\\ z_t = Vh_t$$，则$\frac {\partial J_t}{\partial V}$的计算公式如下： $$\frac {\partial J_t}{\partial V} = \frac {\partial J_t}{\partial y_t}\frac {\partial y_t}{\partial V}=\frac {\partial J_t}{\partial y_t} \frac {\partial y_t}{\partial z_t} \frac {\partial z_t}{\partial V}$$ 2、损失J关于U的梯度计算 $$\frac {\partial J}{\partial U} = \frac {\partial}{\partial U} \sum_{t=1}^T \frac {\partial J_t}{\partial U}=\sum_{t=1}^T \frac {\partial h_t}{\partial U} \frac {\partial J_t}{\partial h_t}$$其中，$h_t$是关于U和$h_{t-1}$的函数，而$h_{t-1}$又是关于U和$h_{t-2}$的函数。 用链式法则可以得到：$$\frac {\partial J}{\partial U} = \sum_{t=1}^T\sum_{k=1}^t \frac {\partial h_k}{\partial U} \frac {\partial h_t}{\partial h_k} \frac {\partial y_t}{\partial h_t}\frac {\partial J_t}{\partial y_t}\\\\h_t = f(Uh_{t-1} + Wx_t+b)$$其中，$$\frac {\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac {\partial h_i}{\partial h_{i-1}} = \prod_{i=k+1}^t U^Tdiag[f^{\prime}(h_{i-1})]$$ 则J对U的梯度为：$$\frac {\partial J}{\partial U} = \sum_{t=1}^T \sum_{k=1}^t \frac {\partial h_k}{\partial U}(\prod_{i=k+1}^t U^T diag[f^{\prime}(h_{i-1})])\frac {\partial y_t}{\partial h_t}\frac {\partial J_t}{\partial y_t}$$ 3、损失J关于W的梯度计算 $$\frac {\partial J}{\partial W} = \frac {\partial}{\partial W}\sum_{t=1}^T J_t=\sum_{t=1}^T \frac {\partial J_t}{\partial W}=\sum_{t=1}^T \frac {\partial h_t}{\partial W} \frac {\partial J_t}{\partial h_t}$$ 用链式法则可以得到：$$\frac {\partial J}{\partial W}=\sum_{t=1}^T \sum_{k=1}^t \frac{\partial h_k}{\partial W}\frac{\partial h_t}{\partial h_k}\frac {\partial y_t}{\partial h_t} \frac {\partial J_t}{\partial y_t}\\\\h_t = f(Uh_{t-1} + Wx_t+b)$$ 其中，$$\frac {\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac {\partial h_i}{\partial h_{i-1}} = \prod_{i=k+1}^t U^Tdiag[f^{\prime}(h_{i-1})]$$则，对W的梯度为：$$\frac {\partial J}{\partial W} = \sum_{t=1}^T\sum_{k=1}^t \frac {\partial h_k}{\partial W}(\prod_{i=k+1}^t U^Tdiag[f^{\prime}(h_{i-1})])\frac {\partial y_t}{\partial h_t}\frac {\partial J_t}{\partial y_t}$$ 如果定义$\gamma=||U^T diag(f^{\prime}(h_{i-1})||$，则在上面公式中的括号里面$\gamma^{t-k}$。如果$\gamma &gt; 1$，则当$t-k \rightarrow \infty$时，$\gamma^{t-k} \rightarrow \infty$，会造成系统的不稳定，也就是所谓的梯度爆炸问题；相反，如果$\gamma &lt; 1$，$t-k \rightarrow \infty$，$\gamma^{t-k} \rightarrow 0$，会出现和深度前馈神经网络类似的梯度消失的问题。 因此，虽然简单循环网络可从理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度爆炸或消失的存在，实际上只能学习到短期的依赖关系，这就是所谓的长期依赖问题。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络学习笔记]]></title>
    <url>%2F2017%2F12%2F23%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[本文是对近期学习的卷积神经网络相关知识的简单记录和梳理。 卷积神经网络简介卷积神经网络(Convolution Neural Network，CNN或ConvNet)是一种前馈神经网络。卷积神经网络是受生物学上感受野(Receptive Field)的机制提出来的。一个神经元的感受野是指特定区域，只有这个区域内的刺激才能够激活该神经元。 感受野，主要是指听觉、视觉等神经系统中一些神经元的特性，即神经元只接受其所支配的刺激区域内的信号。在视觉神经系统中，视觉皮层中的神经细胞的输出依赖于视网膜上的光感受器。视网膜上的光感受器受刺激兴奋时，将神经冲动信号传到视觉皮层，但不是所有的视觉皮层中的神经元都会接受这些信号。一个神经元的感受野指视网膜上的特定区域，只有这个区域内的刺激才能够激活该神经元。 卷积神经网络最早是主要处理图像信息。如果用全连接前馈神经网络来处理图像时，会存在以下两个问题： 参数太多：如果图像的输入大小为100x100x3，在使用全连接前馈神经网络中，第一个隐藏层的每个神经元到输入层都有100x100x3=30,000个相互独立的连接，每个连接都对应一个权重参数。随着隐藏层神经元数量增多，参数的规模也会增加。这会导致整个神经网络的训练效率会非常低下，也会很容易出现过拟合。 局部不变性特征： 自然图像中的物体都具有局部特征不变性，比如在尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈神经网络很难提取这些局部不变性，一般需要进行数据增强来提高性能。 目前的卷积神经网络一般是由卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络，使用反向传播算法进行训练。卷积神经网络有三个结构上的特性：局部连接、权重共享以及子采样。这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数更少。 卷积运算卷积介绍卷积(convolution)，是数学分析中一种重要的运算。在信号处理或图像中，经常使用一维卷积或二维卷积。一维卷积，一维卷积常用在信号处理中，用于计算信号的延迟累积。假设一个信号发生器每个时刻t产生一个信号$x_t$,其信息的衰减率为$f_k$，即在k-1个时间步长后，信息变为原来的$f_k$倍。假设$f_1=1,f_2=1/2,f_3=1/4$，那么在时刻t收到的信号为$y_t$为当前时刻产生的信息和以前时刻延迟信息的叠加，$$y_t = 1\times x_t + 1/2 \times x_{t-1} + 1/4 \times x_{t-2} \\\\=f_1 \times x_t + f_2 \times x_{t-1} + f_3 \times x_{t-2}$$我们把$f_1,f_2…$称为滤波器(filter)或卷积核(convolution kernel)。假设滤波器的长度为m，它和一个信号序列$x_1,x_2…$的卷积为：$$y_t = \sum_{k-1}^m f_k \cdot x_{t-k+1}$$信号序列x和滤波器w的卷积定义为：$$y = w \bigotimes x$$一般情况下，滤波器的长度m远小于信号序列的长度n。当$f_k=1/m$时，卷积相当于移动平均。下图是一个一维卷积的例子： 二维卷积，卷积也经常用于图像处理中。因为图像是一个二维结构，需要将一维卷积进行扩展。给定一个图像$X \in R^{M \times N}$和滤波器$W \in R^{M \times N }$，一般m&lt;&lt;M，n&lt;&lt;N，其卷积为：$$y_{ij} = \sum_{u=1}^m \sum_{v=1}^n w_{uv}\cdot x_{i-u+1,j-v+1} (公式1)$$ 卷积的类型根据在输入信号两端的补0的情况可以将卷积分为：窄卷积、宽卷积和等长卷积。 一维卷积 窄卷积，在信号两端不补0，输出信号长度为n-m+1； 宽卷积，信号两端各补m-1个0，输出信号长度为n+m-1； 等长卷积，信号两端各补(m-1)/2个0， 输出信号长度为n; 二维卷积 窄卷积，信号四周不补0，输出信号长度为M-m+1*N-n+1; 宽卷积，信号四周补0，输出长度为M+m-1*N+n-1; 等长卷积，信号四周补0，输出长度为M*N; 卷积神经网络中的卷积在机器学和图像处理领域，卷积主要的功能是在一个图像上滑动一个卷积核，通过卷积核操作得到一组新的特征。在计算卷积的过程中，需要进行卷积的翻转。在具体的实现上，一般会以互相关操作来代替卷积，从而会减少一些不必要的操作或开销。互相关(cross-correlation)是一个衡量两个序列相关性的函数，通常是用滑动窗口的点积计算来实现。给定一个图像$x\in R^{M \times N}$和卷积核$W\in R^{m\times n}$，它们的互相关为：$$y_{ij}=\sum_{u=1}^m \sum_{v=1}^n w_{uv} \cdot x_{i+u-1,j+v-1}$$和公式1相比，互相关和卷积的主要区别在于卷积核仅仅是否进行翻转。因此，互相关也可以称为不翻转卷积。 翻转，就是从两个维度(从上到下、从左到右)颠倒次序，即旋转180度。 在神经网络中使用卷积是为了进行特征抽取，卷积核是否进行翻转和其特征的抽取的能力无关。特别是当卷积核是可学习的参数时，卷积核互相关是等价的。因此，为了实现上的方便，卷积神经网络中的卷积实际上用互相关操作来代替卷积，可以将互相关表示为$$Y=W \bigotimes X$$其中，$Y\in R^{M-m+1,N-n+1}$为输出矩阵。 互相关运算，即常用的卷积神经网络中的卷积操作示例如下图所示： 卷积运算的性质卷积具有很多很好的性质，下面就介绍一下二维卷积的数学性质，同样适用于一维卷积。 交换性 如果不限制两个卷积信号的长度，卷积是具有交换性的，即$x \bigotimes y=y\bigotimes x$。 当输入信息和卷积核有固定长度时，它们的宽卷积依然具有交换性。 对于两维的图像$x\in R^{M \times N}$和卷积核$W\in R^{m\times n}$，对图像X的两个维度进行零填充，两端各补m-1和n-1个零，得到全填充的图像$\hat{X} \in R^{(M+2m-2) \times (N+2n-2)}$ 图像X和卷积核W的宽卷积定义为：$$\hat{Y}=W\hat{\bigotimes}X$$其中，$\hat{\bigotimes}$ 为宽卷积操作。宽卷积具有交换性，即：$$W\hat{\bigotimes}X=X\hat{\bigotimes}W$$ 导数 假设$Y=W\bigotimes X$，其中$x\in R^{M \times N}$，$W\in R^{m\times n}$，函数$f(Y)\in R$为一个标量函数，则：$$\frac {\partial f(Y)}{\partial w_{uv}}=\sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}\frac{\partial y_{ij}}{\partial w_{uv}}\frac {\partial f(Y)}{\partial y_{ij}}\\\\=\sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}x_{i+u-1,j+v-1}\frac {\partial f(Y)}{\partial y_{ij}}\\\\=\sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}\frac {\partial f(Y)}{\partial y_{ij}}x_{u+i-1,v+j-1}$$可以看到，f(Y)关于W的偏导数为X和$\frac {\partial f(Y)}{\partial Y}$的卷积$$\frac {\partial f(Y)}{\partial (W)} = \frac {\partial f(Y)}{\partial Y} \bigotimes X (公式2)$$同理可以得到：$$\frac {\partial f(Y)}{\partial x_{st}} = \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1} \frac {\partial y_{ij}}{\partial x_{st}} \frac {\partial f(Y)}{\partial {y_{ij}}}\\\ = \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1} w_{s-i+1,t-j+1}\frac {\partial f(Y)}{\partial y_{ij}} (公式3)$$其中，当(s-i+1)m，或(t-j+1)n时，$w_{s-i+1,t-j+1}$，相当于对W进行了p=(M-m,N-n)的零填充。 可以看到，f(Y)关于X的偏导数为W和$\frac {\partial f(Y)}{\partial Y}$的宽卷积。公式3中的卷积是真正的卷积，而不是互相关，为了一致性，我们用互相关的“卷积”，即：$$\frac {\partial f(Y)}{\partial X} = rot180(\frac {\partial f(Y)}{\partial Y}) \hat {\bigotimes}W \\\\=rot180(W) \hat {\bigotimes} \frac {\partial f(Y)}{\partial Y}$$ 卷积神经网络结构首先，看一下典型的卷积神经网络的结构： 图片引自：https://www.zybuluo.com/hanbingtao/note/485480如上图所示，一个卷积神经网络由若干卷积层、Pooling层、全连接层组成。通过设置不同的卷积层、Pooling层以及全连接层，可以构建不同的卷积神经网络结构，它常用的架构模式为：1Input -&gt; [[卷积层]*N-&gt;Pooling ?] * M -&gt; [全连接层]*K也就是N个卷积层叠加，然后(可选)叠加一个Pooling层，重复这个结构M次，最后叠加K个全连接层。下面介绍每个层的作用。 卷积层全连接前馈神经网络中，第l-1层的$n^{(l-1)}$个神经元和第l层的$n^{(l)}$个神经元的连接采用的是全连接的方式，则权重矩阵有$n^{(l)}\times n^{(l-1)}$个参数，当l-1层和l层的神经元过多时，权重矩阵的参数非常多，训练的效率会非常低。 如果采用卷积来代替全连接，第l层的净输入$z^{(l)}$为第l-1层激活值$a^{(l-1)}$和滤波器$w^{(l)}$的卷积，即：$$z^{(l)} = w^{(l)} \bigotimes a^{(l-1)} + b^{(l)} (公式4)$$其中，滤波器$w^{(l)}$为权重向量，$b^{(l)}\in R^{(n^{l-1})}$为偏置。 根据卷积的定义，卷积层具有如下两个性质： 局部连接 在卷积层中的每一个神经元都和下一层某个局部窗口内的神经元相连，构成一个局部连接网络。如下图所示，卷积层和下一层之间的连接数大大减少，由原来的$n^{(l)}\times n^{(l-1)}$个连接变为$n^{(l)} \times m$个连接，m为滤波器的大小。 权重共享 从公式4可以看出，作为参数的滤波器$w^{(l)}$对于第l层的所有的神经元都是相同的。从上图也可以看出，所有同颜色连接上的权重是相同的。 卷积层的主要作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。上面介绍的卷积层的神经元和全连接都是一维结构。对于常见的图像为二维结构，因此为了更充分的利用图像的局部信息，通常将神经元组织为三维结构，其大小为$宽度M \times 高度N \times 深度D$，有D个$M\times N$的特征映射构成。 特征映射（Feature Map) 为了增强卷积层的表示能力，我们可以使用K个不同的滤波器来得到K组不同的输出。每组输出都共享一个滤波器。如果我们把滤波器看成是一个特征提取器，每一组输出都可以看成是输入图像经过一个特征抽取后得到的特征。因此，在卷积神经网络中每一组输出也叫作一组特征映射。 图片引自： https://nndl.github.io/ 卷积神经网络 在输入层，特征映射就是图像本身，如果图像是灰度图像，就是一个特征映射，深度为D=1；如果是彩色图像，分别有RGB三个颜色通道的特征映射，输入深度D=3。 假设一个卷积层的结构如下： 输入特征映射组：$X \in R^{M \times N \times D}$为三维张量(tensor)，每个切片为矩阵$X^d \in R^{M \times N}$为一个输入特征映射，1&lt;=d&lt;=D； 输出特征映射组：$Y \in R^{M^{\prime}\times N^{\prime} \times P^{\prime}}$为三维张量，其中每个切片矩阵$Y^p \in R^{M^{\prime}\times N^{\prime}}，$1&lt;=p&lt;=P； 卷积核：$W \in R^{m\times n \times D \times P}$为四维张量，其中每个切片矩阵$W^{p,d}\in R^{m \times n}$为一个两维卷积核，1&lt;=d&lt;=D，1&lt;=p&lt;=P; 为了计算输出特征映射$Y^p$，用卷积核$W^{p,1},W^{p,2},…,W^{p,D}$分别对输入特征$X^1,X^2,…,X^D$进行卷积，然后将卷积结果相加，并加上一个标量偏置b得到卷积层的净输入$Z^P$，再经过非线性激活函数得到最终的输出特征映射$Y^p$。$$Z^p = W^p \bigotimes X + b^p = \sum_{d=i}^D W^{p,d} \bigotimes X^d + b^p\\\\Y^p = f(Z^p)$$ 其中，$W^p \in R^{m \times n \times D}$为三维卷积核，f(.)为非线性激活函数，一般用ReLU函数。整个计算的过程如下图所示。如果希望卷积层输出P个特征映射，可以将上述计算过程重复P次，得到P个输出特征映射，$Y^1,Y^2,…,Y^P$。 图片引自： https://nndl.github.io/ 卷积神经网络 在输入为$X \in R^{M \times N \times D}$，输出为$Y \in R^{M^{\prime}\times N^{\prime} \times P^{\prime}}$的卷积层中，每一个输入特征映射都需要D个滤波器以及一个偏置。假设每个滤波器的大小为$m \times n$，那么共需要$P \times D \times (m \times n)+P$个参数。 Pooling层汇聚层(Pooling Layer)，也叫子采样层(subsampling layer)，作用就是进行特征选择，降低特征数量，从而减少参数的数量。卷积层虽然可以减少网络中连接的数量，但特征映射组中的神经元个数没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合。为了解决这个问题，可以在卷积层之后加一个汇聚层，从而降低特征的维数，避免过拟合。 对于卷积层得到的一个特征映射$X^{(l)}$，我们可以将$X^{(l)}$划分为很多区域$R_K,k=1,2…,K$。区域$R_k$可以重叠，也可以不重叠，则采样层的输出有：$$X^{(l+1)} = f(W^{(l+1)}\cdot down(R_k)+b^{(l+1)})$$其中，$w^{(l+1)}$和$b^{(l+1)}$分别是可训练的权重和偏置参数。于是，可以简化成如下公式：$$X^{(l+1)} = f(W^{(l+1)}\cdot down(X^{(l)})+b^{(l+1)})$$其中，$down(X^{(l)})$是指子采样后的特征映射。 常见的采样方式如下： 最大值采样（Maximum Pooling） $pool_{max}(R_k) = max_{i\in R_k}a_i$ 最小值采样（Minimum Pooling） $pool_{min}(R_k) = min_{i\in R_k}a_i$ 平均值采样（Average pooling） $pool_{ave}(R_k) = \frac {1}{|R_k|}\sum_{i\in R_k}^{|R_k|} a_i$ TopK采样 $pool_k(R_k) = topk_{i \in R_k}a_i$ 下图为最大值采样的一个示例： 典型的汇聚层是将每个特征映射划分为2X2的大小的不重叠的区域，然后使用最大汇聚的方式进行下采样。汇聚层也可以看做是一个特殊的卷积层，卷积核的大小为$m\times m$，步长为$s \times s$，卷积核为max函数或者mean函数。过大的采样区域会急剧减少神经元的数量，造成过多的信息损失。 卷积神经网络参数学习在卷积神经网络中，参数为卷积核中的权重以及偏置。和前馈神经网络类似，卷积神经网络也可以通过误差反向传播算法来进行参数学习。 在全连接前馈神经网络中，梯度主要通过每一层的误差项$\delta$进行反向传播，并进一步计算每层参数的梯度。在卷积神经网络中，主要有两种不同功能的神经层：卷积层和汇聚层。 对第l层为卷积层，第l-1层的输入特征为$X^{(l-1)} \in R^{M \times N \times D}$，通过卷积计算得到第l层的特征净输入为$Z^{(l)}\in R^{M^{\prime} \times N^{\prime} \times P}$，第l层的第p个特征净输入为：$$Z^{(l,p)} = \sum_i^D W^{(l,p,d)} \bigotimes X^{(l-1,d)} + b^{(l,p)}$$其中，$W^{(l,p,d)}$和$b^{(l,p)}$为卷积核及偏置。第l层中共有$P \times D$个卷积核和P个偏置，可以分别使用链式法则来计算器梯度。$$\frac {\partial L(Y,\hat{Y})}{\partial W^{(l,p,d)}} = \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l,p)}} \bigotimes X^{(l-1, d)}\\\\=\delta^{l,p} \bigotimes X^{(l-1,d)}$$其中，$\delta^{(l,p)}$为损失函数关于第l层的第p个特征映射净输入$Z^{(p,l)}$的偏导数。 同理可以得到，损失函数关于第l层第p个偏置$b^{(l,p)}$的梯度为：$$\frac {\partial L(Y,\hat {Y})}{\partial b^{(l,p)}} = \sum_{i,j}[\delta^{(l,p)}]_{i,j}.$$因此，卷积网络的每层参数的梯度也依赖于其所在层的误差项$\delta^{(l,p)}$。卷积层和汇聚层中，误差项的计算有所不同，因此，需要分别计算误差项。 汇聚层 当第l+1层为汇聚层时，因为汇聚层是下采样操作，l+1层的每个神经元的误差项$\delta$对应于第l层的相应特征的一个区域。l层的第p个特征映射中的每个神经元都有一条边和l+1层的第p个特征映射中的一个神经元相连。根据链式法则，第l层的一个特征映射的误差项$\delta^{(l,p)}$，只需要将l+1层对应的特征映射误差项$\delta^{(l+1, p)}$进行上采样，再和l层的特征映射的激活值偏导数逐元素相乘就得到了$\delta^{(l,p)}$。 第l层的第p个特征映射的误差项$\delta^{(l,p)}$的具体推导如下：$$\delta^{(l,p)} = \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l,p)}}\\\\=\frac {\partial X^{(l,p)}}{\partial Z^{(l,p)}} \cdot \frac {\partial Z^{(l+1,p)}}{\partial X^{(l,p)}} \cdot frac {\partial L(Y,\hat {Y})}{\partial Z^{(l+1, p)}}\\\\=f_l^\prime(Z^{(l,p)}) \bigodot up(\delta^{(l+1, k)})$$其中，$f_l^\prime(Z^{(l,p)})$为第l层使用的激活函数导数，up为上采样函数，与汇聚层中使用的下采样操作刚好相反。如果下采样是最大汇聚，误差项$\delta^{(l+1, k)}$中的每个值将会直接传递到上一层对应区域中最大值所对应的神经元，该区域中其他神经元的误差项都设为0,。如果采用平均采样，误差项$\delta^{(l+1, k)}$中的每个值都会被平均分配到上一层对应的区域中的所有神经元上。 卷积层 当第l+1层为卷积层时，假设特征映射净输入为$Z^{(l+1)} \in R^{M^\prime \times N^\prime \times K}$，其中第k个特征映射的净输入为：$$Z^{(l+1,k)} = \sum_{p=1}^P W^{(l+1,k,p)} \bigotimes X^{(l,p)} + b^{(l+1, k)}$$ 其中，$W^{(l+1,k,p)}$和$b^{(l+1, k)}$为第l+1层的卷积核及偏置。第l+1层中共有$K\times P$个卷积核和K个偏置。第l层的第p个特征映射为误差项$\delta^{(l,p)}$的具体推导如下：$$\delta^{(l,p)} = \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l,p)}}\\\\=\frac {\partial X^{(l,p)}}{\partial Z^{l,p}}\cdot \frac {\partial L(Y,\hat{Y})}{\partial X^{(l,p)}}\\\\=f_l^\prime(Z^{(l)}) \bigodot \sum_{k=1}^K (rot180(W^{(l+1,k,p)}) \hat{\bigotimes} \frac {\partial L(Y,\hat{Y})}{\partial Z^{(l+1, k)}})\\\\=f_l^\prime(Z^{(l)}) \bigodot \sum_{k=1}^K(rot180(W^{(l+1,k,p)}) \hat{\bigotimes} \delta^{(l=1,k)})$$ 其中，$\hat{\bigotimes}$为宽卷积。 Reference https://nndl.github.io/ 卷积神经网络 https://www.zybuluo.com/hanbingtao/note/485480]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前馈神经网络反向传播推导]]></title>
    <url>%2F2017%2F12%2F17%2F%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[前馈神经网络前向传播 一个三层的前馈神经网络如下图所示： 对于第二层的输出$a_1^2,a_2^2,a_3^2$,有：$$a_1^2 = \sigma(z_1^2)=\sigma(w_{11}^2x_1+w_{12}^2x_2+w_{13}^2x_3+b_1^2)\\\\a_2^2 = \sigma(z_2^2)=\sigma(w_{21}^2x_1+w_{22}^2x_2+w_{23}^2x_3+b_3^2)\\\\a_3^2 = \sigma(z_3^2)=\sigma(w_{31}^2x_1+w_{32}^2x_2+w_{33}^2x_3+b_3^2)$$对于第三层的输出$a_1^3$，有：$$a_1^3 = \sigma(z_1^3)=\sigma(w_{11}^3x_1+w_{12}^3x_2+w_{13}^3x_3+b_1^3)$$ 用下面的符号来描述一个前馈神经网络： L:表示神经网络的层数； $n^l$：表示第l层神经元的个数； $f_l(.)$:表示第l层神经元的激活函数； $W^{(l)}\in R^{n^l\times n^{l-1}}$:表示第l-1层到第l层的权重矩阵； $b^{(l)\in R^{n^l}}$：表示第l-1层到第l层的偏置； $z^{(l)\in R^{n^l}}$:表示第l层神经元的净输入； $a^{(l)\in R^{n^l}}$:表示第l层神经元的输出（激活值）； $W_{ij}^{(l)}$:表示第l-1层第j个输入到第l层第i个神经元的权重； 前馈神经网络通过下面的公式进行信息传播：$$z^{(l)} = W^{(l)}\cdot a^{(l-1)} + b^{(l)} \\\\a^{(l)} = f_l(z^{(l)})$$上面的公式可以合并为：$$z^{(l)} = W^{(l)}\cdot f_{l-1}(z^{(l-1)}) + b^{(l)}$$ 这样，前馈神经网络可以通过逐层的信息传递，得到网络最后的输出$a^{(L)}$。整个网络可以看作一个复合函数$\phi(x;W,b)$,将输入x作为第1层的输入$a^{(0)}$，将第L层的输出作为$a^{(L)}$作为输出。$$x=a^{(0)}\rightarrow z^{(l)} \rightarrow a^{(1)}\rightarrow z^{(2)}\rightarrow … \rightarrow a^{(L-1)} \rightarrow z^{(L)} \rightarrow a^{(L-1)}=\phi(x;W,b)$$ 反向传播推导假设损失函数是$L(y,\hat{y})$，对第l层中的参数$W^{(l)}$和$b^{(l)}$计算偏导数。因为$\frac {\partial L(y,\hat{y})}{\partial W^{(l)}}$的计算涉及到矩阵的微分，十分繁琐，可以先计算偏导数$\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}}$。根据链式法则：$$\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}} =\left(\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}}\right)^T\frac {\partial L(y,\hat{y})}{\partial z^{(l)}} （1）\\\\\frac {\partial L(y,\hat{y})}{\partial b^{(l)}} =\left(\frac {\partial z^{(l)}}{\partial b^{(l)}}\right)^T\frac {\partial L(y,\hat{y})}{\partial z^{(l)}} (2)$$公式(1)和(2)都是为目标函数关于第l层神经元$z^{(l)}$的偏导数，称为误差项，因此可以共用。我们只需要计算三个偏导数，分别为$\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}},\frac {\partial L(y,\hat{y})}{\partial b^{(l)}}和\frac {\partial L(y,\hat{y})}{\partial z^{(l)}}$ 1、计算偏导数$\frac {\partial z^{(l)} }{\partial W_{ij}^{(l)}}$ 因为$z^{(l)}和W_{ij}^{(l)}$的函数关系为$z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)}$，因此，偏导数：$$\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}}=\frac {\partial (W^{(l)}a^{(l-1)}+b^{(l)})}{\partial W_{ij}^{(l)}}= a_j^{(l-1)}$$其中，$W_{i:}^{(l)}$为权重矩阵$W^{(l)}$的第i行 2、计算偏导数$\frac {\partial z^{l}}{\partial b^{(l)}}$ 因为$z^{(l)}$和$b^{(l)}$的函数关系为$z^{(l)} = W^{(l)}a^{(l-1)}+b^{(l)}$，因此偏导数：$$\frac {\partial z^{l}}{\partial b^{(l)}}=I_{n^l}$$为$n^l\times n^l$的单位矩阵。 3、计算偏导数$\frac {\partial L(y,\hat{y})}{\partial z{(l)}}$ 用$\delta^{(l)}$来定义第l层的神经元误差项：$$\delta^{(l)} = \frac {\partial L(y,\hat{y})}{\partial z^{(l)}} \in R^{n^l}$$误差项$\delta^{l}$表示第l层的神经元对最终的误差的影响，也反映了最终的输出对第l层的神经元对最终误差的敏感程度。 根据$z^{(l+1)}=W^{(l+1)}a^{(l)} + b^{(l+1)}$，有：$$\frac {\partial z^{(l+1)}}{\partial a^{(l)}} = (W^{(l+1)})^T$$根据$a^{(l)}=f(z^{(l)})$，其中，$f_l(.)$为按位计算的函数，因此有：$$\frac {\partial a^{(l)}}{\partial z^{(l)}} = \frac {\partial f_l(z^{(l)})}{\partial z^{l}} = diag(f_l^{\prime}(z^{(l)}))$$因此，根据链式法则，第l层的误差项为：$$\delta^{(l)} = \frac {\partial L(y, \hat{y})}{\partial z^{(l)}}\\\\=\frac {\partial a^{(l)}}{\partial z^{(l)}}\cdot\frac{\partial z^{(l+1)}}{\partial a^{(l)}}\cdot \frac {\partial L(y, \hat{y})}{\partial z^{(l+1)}}\\\\=diag(f_l^{\prime}(z^{(l)}))\bigodot ((W^{(l+1)})^T\delta^{(l+1)}) (3)$$其中，$\bigodot$是向量的点积运算，表示每个元素相乘。 从公式3可以看出，第l层的误差项可以通过第l+1层的误差项计算，这就是反向传播。反向传播算法的含义是：第l层的一个神经元的误差项是所有与该神经元相连的第l+1层神经元误差项的权重和，然后再乘上该神经元激活函数的梯度。 在计算出三个偏导数之后，可以得到最终的偏导数：$$\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}} = \delta_i^{(l)}a_j^{(l-1)}$$进一步，$L(y,\hat{y})$关于第l层的权重$W^{(l)}$的梯度为：$$\frac {\partial L(y,\hat{y})}{\partial W^{(l)}} = \delta^{(l)}a^{(l-1)}$$同理可得，$L(y,\hat{y})$关于第l层偏置$b^(l)$的梯度为：$$\frac {\partial L(y,\hat{y})}{\partial b^{(l)}} = \delta^{(l)}$$基于随机梯度下降的反向传播算法如下： 图片引自：https://github.com/nndl/nndl.github.io，chap-前馈神经网络.pdf。]]></content>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN学习的相关资料]]></title>
    <url>%2F2017%2F12%2F17%2FCNN%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[对卷积的理解 http://mengqi92.github.io/2015/10/06/convolution/ http://www.cnblogs.com/freeblues/p/5738987.html http://blog.csdn.net/bitcarmanlee/article/details/54729807 卷积神经网络的原理及推导 反向传播的推导：https://zhuanlan.zhihu.com/p/22473137 pinard:http://www.cnblogs.com/pinard/p/6483207.html zybuluo:https://www.zybuluo.com/hanbingtao/note/485480 一文读懂卷积神经网络CNN：http://www.sohu.com/a/126742834_473283 CS231N翻译：https://zhuanlan.zhihu.com/p/21930884 charlote:http://www.cnblogs.com/charlotte77/p/7759802.html 卷积神经网络全面解析:http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi tornadomeet:http://www.cnblogs.com/tornadomeet/p/3468450.html]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>资料</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(9) ConvNet notes]]></title>
    <url>%2F2017%2F12%2F16%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC9%E8%AF%BE-ConvNet-notes%2F</url>
    <content type="text"><![CDATA[本文转载自：https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit，原文为：http://cs231n.github.io/convolutional-networks/，并进行一定的修改。 目录 结构概述 用来构建卷积神经网络的各种层 卷积层 汇聚层 归一化层 全连接层 将全连接层转化成卷积层 卷积神经网络的结构 层的排列规律 层的尺寸设置规律 案例学习（LeNet/AlexNet/ZFNet/GoogLeNet/VGGNet） 计算上的考量 拓展资源 卷积神经网络(CNNs/ConvNets)卷积神经网络和上一章讲的常规神经网络非常相似：它们都是由神经元组成，神经元中有局域学习能力的权重和偏差。每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出的是不同类别的评分。在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或者softmax）,并且在神经网络中我们的各种技巧和要点依旧适用于卷积神经网络。 那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。 结构概述回顾：常规神经网络。在上一章中，神经网络的输入是一个向量，然后在一些列的隐层中对它做变换。每个隐层是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接。但是在一个隐藏层中，神经元相互独立不进行任何连接。最后的全连接层被称为“输出层”，在分类问题中，它输出的值被看做是不同类别的评分值。 常规神经网络对于大尺寸图像效果不尽人意。在CIFAR-10中，图像的尺寸是32x32x3（宽高均为32像素，3个颜色通道），因此，对应的的常规神经网络的第一个隐层中，每一个单独的全连接神经元就有32x32x3=3072个权重。这个数量看起来还可以接受，但是很显然这个全连接的结构不适用于更大尺寸的图像。举例说来，一个尺寸为200x200x3的图像，会让神经元包含200x200x3=120,000个权重值。而网络中肯定不止一个神经元，那么参数的量就会快速增加！显而易见，这种全连接方式效率低下，大量的参数也很快会导致网络过拟合。 神经元的三维排列。卷积神经网络针对输入全部是图像的情况，将结构调整得更加合理，获得了不小的优势。与常规神经网络不同，卷积神经网络的各层中的神经元是3维排列的：宽度、高度和深度（这里的深度指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）。举个例子，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是32x32x3（宽度，高度和深度）。我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的。下面是例子： 左边是一个3层的神经网络。右边是一个卷积神经网络，图例中网络将它的神经元都排列成3个维度（宽、高和深度）。卷积神经网络的每一层都将3D的输入数据变化为神经元3D的激活数据并输出。在这个例子中，红色的输入层装的是图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是3（代表了红、绿、蓝3种颜色通道）。 用来构建卷积神经网络的各种层一个简单的卷积神经网是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。卷积神经网络主要由三种类型的层构成：卷积层、汇聚层和全连接层（全连接层和常规的神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。 网络结构的例子：这仅仅是一个概述，下面会有更详细的介绍。一个用于CIFAP-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下： 输入[32*3]存有图像的原始像素值，本例中，图像宽高均为32，有3个颜色通道。 卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积。卷积层会计算所有的神经元的输出。如果我们使用12个滤波器（也叫核），得到的输出数据体的维度就是[323212]。 ReLU层将会逐个元素地进行激活函数操作，比如使用以0为阈值的max(0,x)作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。 汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。 全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。 由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值。其中有的层含有参数，有的没有。具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。 小结 简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）。 卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）。 每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据。 有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。 有的层有额外的参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）。 卷积层卷积层是构建卷积神经网络的核心层，它产生了网络中的大部分的计算量。 概述和直观介绍：首先讨论的是，在没有大脑和生物意义上的神经元之类的比喻下，卷积层到底在计算什么。卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案。 在每个卷积层上，我们会有一整个集合的滤波器（比如12个），每个都会生成一个不同的二维激活图。将这些激活映射在深度方向上层叠起来就生成了输出数据。 以大脑做比喻：如果你喜欢用大脑和生物神经元来做比喻，那么输出的3D数据中的每个数据项可以被看做是神经元的一个输出，而该神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。现在开始讨论神经元的连接，它们在空间中的排列，以及它们参数共享的模式。 局部连接：在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。 例1：假设输入的数据体尺寸为32x32x3，如果滤波器是5x5，那么卷积层中的每个神经元有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。 例2：假设输入数据体的尺寸是[16x16x20]，感受野尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20=180个连接。再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）。 左边：红色的是输入数据体（比如CIFAR-10中的图像），蓝色的部分是第一个卷积层中的神经元。卷积层中的每个神经元都只是与输入数据体的一个局部在空间上相连，但是与输入数据体的所有深度维度全部相连（所有颜色通道）。在深度方向上有多个神经元（本例中5个），它们都接受输入数据的同一块区域（感受野相同）。至于深度列的讨论在下文中有。右边：神经网络章节中介绍的神经元保持不变，它们还是计算权重和输入的内积，然后进行激活函数运算，只是它们的连接被限制在一个局部空间。空间排列：上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及他们的排列方式。3个超参数控制着输出数据体的尺寸：深度(depth)、步长(stride)和零填充(zero-padding)。下面是对它们的讨论：1. 首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界、或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列，也有人使用纤维(fibre)来称呼它们。2. 其次，在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。3. 在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的。这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。输出数据体在空间上的尺寸可以通过输入数据体尺寸(W)，卷积层中神经元的感受野尺寸(F)、步长(S)和零填充的数量(P)的函数来计算（假设输入数组的空间形状是正方形，即高度和宽度相等）输出数据体的空间尺寸为(W-F+2P)/S+1。比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出。如果步长为2，输出就是3x3。下面是例子： 空间排列的图示。在本例中只有一个空间维度（x轴），神经元的感受野尺寸F=3，输入尺寸W=5，零填充P=1。左边：神经元使用的步长S=1，所以输出尺寸是(5-3+2)/1+1=5。右边：神经元的步长S=2，则输出尺寸是(5-3+2)/2+1=3。注意当步长S=3时是无法使用的，因为它无法整齐地穿过数据体。从等式上来说，因为(5-3+2)=4是不能被3整除的。本例中，神经元的权重是[1,0,-1]，显示在图的右上角，偏差值为0。这些权重是被所有黄色的神经元共享的（参数共享的内容看下文相关内容）。使用零值填充：在上面的左边的例子中，注意输入维度是5，输出维度也是5。之所以如此，是因为感受野是3并且使用了1的零填充。如果不使用零填充，则输出数据体的空间维度就只有3，因为这就是滤波器整齐滑过并覆盖原始数据需要的数目。一般来说，当步长S=1时，零填充的值是P=（F-1）/2，这样就能保证输入和输出数据体有相同的空间尺寸。这样做非常常见，在介绍卷积神经网络的结构的时候我们会详细讨论其原因。步长的限制：注意这些空间排列的超参数之间是相互限制的。举例说来，当输入尺寸W=10，不使用零填充则P=0，滤波器尺寸F=3，这样步长S=2就行不通，因为(W-F+2P)/S+1=(10-3+0)/2+1=4.5，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体。因此，这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，或者修改零填充值来让设置合理，或者修改输入数据体尺寸来让设置合理，或者其他什么措施。在后面的卷积神经网络结构小节中，读者可以看到合理地设置网络的尺寸让所有的维度都能正常工作，这件事可是相当让人头痛的。而使用零填充和遵守其他一些设计策略将会有效解决这个问题。真实案例：Krizhevsky构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227x227x3]。在第一个卷积层，神经元使用的感受野尺寸F=11，步长S=4，不使用零填充P=0。因为(227-11)/4+1=55，卷积层的深度K=96，则卷积层的输出数据体尺寸为[55x55x96]。55x55x96个神经元中，每个都和输入数据体中一个尺寸为[11x11x3]的区域全连接。在深度列上的96个神经元都是与输入数据体中同一个[11x11x3]区域连接，但是权重不同。有一个有趣的细节，在原论文中，说的输入图像尺寸是224x224，这是肯定错误的，因为(224-11)/4+1的结果不是整数。这件事在卷积神经网络的历史上让很多人迷惑，而这个错误到底是怎么发生的没人知道。我的猜测是Alex忘记在论文中指出自己使用了尺寸为3的额外的零填充。参数共享=290,400个神经元，每个有11x11x3=364个参数和1个偏差。将这些合起来就是290400x364：在卷积层中使用参数共享是用来控制参数的数量。就用上面的例子，在第一个卷积层就有55x55x96=105,705,600个参数。单单第一层就有这么多参数，显然这个数目是非常大的。作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量。换言之，就是将深度维度上一个单独的2维切片看做深度切片（depth slice），比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]。在每个深度切片上的神经元都使用同样的权重和偏差。在这样的参数共享下，例子中的第一个卷积层就只有96个不同的权重集了，一个权重集对应一个深度切片，共有96x11x11x3=34,848个不同的权重，或34,944个参数（+96个偏差）。在每个深度切片中的55x55个权重使用的都是同样的参数。在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的卷积（这就是“卷积层”名字由来）。这也是为什么总是将这些权重集合称为滤波器（filter）（或卷积核（kernel）），因为它们和输入进行了卷积。 Krizhevsky等学习到的滤波器例子。这96个滤波器尺寸都是[11x11x3]，在一个深度切片中，每个滤波器都被55x55个神经元共享。注意参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也同样是有用的，这是因为图像结构具有平移不变性。所以，在卷积层的输出数据体的55x55个不同位置中，就没有必要重新学习探测一个水平边界了。注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为局部连接层（Locally-Connected Layer）。Numpy的例子：为了让讨论更加的具体，我们用代码来展示上述思路。假设输入数据体是numpy数组X。那么：一个位于(x,y)的深度列，将会是X[x,y,:]；在深度为d处的切片，或激活图应该是X[:,:,d]。卷积层例子：假设输入数据体的尺寸X.shape:(11,11,4)，不使用零填充（P=0），滤波器的尺寸是F=5，步长S=2。那么输出数据体的空间尺寸就是(11-5)/2+1=4，即输出数据体的宽度和高度都是4。那么在输出数据体中的激活映射（称其为V）看起来就是下面这样（在这个例子中，只有部分元素被计算）：1234V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0在numpy中，操作是进行数组间的逐元素相乘。权重向量W0是该神经元的权重，b0是其偏差。在这里，W0被假设尺寸是W0.shape: (5,5,4)，因为滤波器的宽高是5，输入数据量的深度是4。注意在每一个点，计算点积的方式和之前的常规神经网络是一样的。同时，计算内积的时候使用的是同一个权重和偏差（因为参数共享），在宽度方向的数字每次上升2（因为步长为2）。要构建输出数据体中的第二张激活图，代码应该是：123456V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1 （在y方向上）V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1 （或两个方向上同时）我们访问的是V的深度维度上的第二层（即index1），因为是在计算第二个激活图，所以这次试用的参数集就是W1了。在上面的例子中，为了简洁略去了卷积层对于输出数组V中其他部分的操作。还有，要记得这些卷积操作通常后面接的是ReLU层，对激活图中的每个元素做激活函数运算，这里没有显示。小结- 输入数据体的尺寸为：$W_1xH_1xD_1$- 4个超参数:滤波器的数量K;滤波器的空间尺寸F;步长S;零填充数量P- 输出数据的尺寸为：$W_2xH_2xD_2$，其中： - $W_2 = (W_1 - F + 2P)/S+1$; - $H_2 = (H_1 - F + 2P)/S+1$(宽度和高度的计算方法相同); - $D_2=K$- 由于参数共享，每个滤波器包含$F\cdot F\cdot D_1$个权重，卷积层一共有$F\cdot F\cdot D_1\cdot K$个权重和K个偏置。- 在输出数据体中，第d个深度的切片(空间尺寸是$W_2xH_2$)，用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长S），最后在加上第d个偏差。对这些超参数，常见的设置是F=2，S=1，P=1。同时设置这些超参数也有一些约定俗成的惯例和经验，可以在下面的卷积神经网络结构章节中查看。*卷积层演示：下面是一个卷积层的运行演示。因为3D数据难以可视化，所以所有的数据（输入数据体是蓝色，权重数据体是红色，输出数据体是绿色）都采取将深度切片按照列的方式排列展现。输入数据体的尺寸是$W_1=5,H_1=5,D_1=3$，卷积层参数K=2,F=3,S=2,P=1。就是说，有2个滤波器，滤波器的尺寸是$3\cdot 3$，它们的步长是2.因此，输出数据体的空间尺寸是(5-3+2)/2+1=3。注意输入数据体使用了零填充P=1，所以输入数据体外边缘一圈都是0。下面的例子在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来。 矩阵乘法实现：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法： 输入图像的局部区域被im2col操作拉伸为列。比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3=363的列向量。重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)/4+1=55，所以得到im2col操作的输出矩阵X_col的尺寸是[363x3025]，其中每列是拉伸的感受野，共有55x55=3,025个。注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。 卷积层的权重也同样被拉伸成行。举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵W_row，尺寸为[96x363]。 现在卷积的结果和进行一个大矩阵乘np.dot(W_row, X_col)是等价的了，能得到每个滤波器和每个感受野间的点积。在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出。 结果最后必须被重新变为合理的输出尺寸[55x55x96]。 这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在X_col中被复制了多次。但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的BLAS API）。还有，同样的im2col思路可以用在汇聚操作中。反向传播：卷积操作的反向传播（同时对于数据和权重）还是一个卷积（但是是和空间上翻转的滤波器）。使用一个1维的例子比较容易演示。 1x1卷积：一些论文中使用了1x1的卷积，这个方法最早是在论文Network in Network中出现。人们刚开始看见这个1x1卷积的时候比较困惑，尤其是那些具有信号处理专业背景的人。因为信号是2维的，所以1x1卷积就没有意义。但是，在卷积神经网络中不是这样，因为这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）。 扩张卷积：最近一个研究（Fisher Yu和Vladlen Koltun的论文）给卷积层引入了一个新的叫扩张（dilation）的超参数。到目前为止，我们只讨论了卷积层滤波器是连续的情况。但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张。举例，在某个维度上滤波器w的尺寸是3，那么计算输入x的方式是：w[0]x[0] + w[1]x[1] + w[2]x[2]，此时扩张为0。如果扩张为1，那么计算为： w[0]x[0] + w[1]x[2] + w[2]x[4]。换句话说，操作中存在1的间隙。在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为在很少的层数内更快地汇集输入图片的大尺度特征。比如，如果上下重叠2个3x3的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中5x5的区域（可以成这些神经元的有效感受野是5x5）。如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长。 汇聚层通常，在连续的卷积层之间会周期性地插入一个汇聚层。**它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。汇聚层的一些公式： 输入数据体尺寸$W_1\cdot H_1\cdot D_1$ 有两个超参数： 空间大小F 步长S 输出数据体尺寸$W_2\cdot H_2\cdot D_2$，其中 $W_2=(W_1-F)/S+1$ $H_2=(H_1-F)/S+1$ $D_2=D_1$ 因为对输入进行的是固定函数计算，所以没有引入参数。 在汇聚层中很少使用零填充。 在实践中，最大汇聚层通常只有两种形式：一种是F=3,S=2，也叫重叠汇聚（overlapping pooling），另一个更常用的是F=2,S=2。对更大感受野进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性。 普通汇聚（General Pooling）：除了最大汇聚，汇聚单元还可以使用其他的函数，比如平均汇聚（average pooling）或L-2范式汇聚（L2-norm pooling）。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。 汇聚层在输入数据体的每个深度切片上，独立地对其进行空间上的降采样。左边：本例中，输入数据体尺寸[224x224x64]被降采样到了[112x112x64]，采取的滤波器尺寸是2，步长为2，而深度不变。右边：最常用的降采样操作是取最大值，也就是最大汇聚，这里步长为2，每个取最大值操作是从4个数字中选取（即2x2的方块区域中）。 反向传播：回顾一下反向传播的内容，其中max(x,y)函数的反向传播可以简单理解为将梯度只沿着最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大的索引记录下来，这样在反向传播的时候梯度的路由就很高效。 不使用汇聚层：很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。 归一化层在卷积神经网络中的结果，提出很多不同类型的归一化层，有时候是为了实现生物大脑中观测到的抑制机制。但是，这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极有限的。对于不同类型的归一化层，可以看看Alex Krizhevsky的关于cuda-convnet library API的讨论。 全连接层在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。 把全连接层转化层卷积层全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所有它们的函数形式是一样的。因此，将此两者相互转化是可能的： 对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块（这是因为有局部连接），其余部分都是零。而在其中大部分块中，元素都是相等的（因为参数共享）。 相反，任何全连接层都可以被转化为卷积层。比如，一个K=4096的全连接层，输入数据体的尺寸是$7\times 7\times 512$，这个全连接层可以被等效地看做一个F=7,P=0,S=1,K=4096的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成$1\times 1\times$ 4096，这个结果就和使用初始的那个全连接层一样了。 全连接层转化为卷积层：在这两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是$224\times224\times3$的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层： 针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。 针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。 对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]。 实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（译者注：即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。 举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！ 面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。 自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。 最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。 卷积神经网络的结构卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。 层的排列规律卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &gt;=0,通常N&lt;=3,M&gt;=0,K&gt;=0,通常K&lt;3。例如，下面是一些常见的网络结构规律： INPUT -&gt; FC,实现一个线性分类器，此处N = M = K = 0。 INPUT -&gt; CONV -&gt; RELU -&gt; FC INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; F- C。此处在每个汇聚层之间有一个卷积层。 INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]3 -&gt; [FC -&gt; RELU]2 -&gt; FC。此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。 几个小滤波器卷积层的组合比一个大滤波器卷积层好：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7，但是就有一些缺点。首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有C个通道，那么单独的7x7卷积层将会包含C\times (7\times 7\times C)=49C^2个参数，而3个3x3的卷积层的组合仅有3\times (C\times (3\times 3\times C))=27C^2个参数。直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。 最新进展：传统的将层按照线性进行排列的方法已经受到了挑战，挑战来自谷歌的Inception结构和微软亚洲研究院的残差网络（Residual Net）结构。这两个网络（下文案例学习小节中有细节）的特征更加复杂，连接结构也不同。 层的尺寸设置规律到现在为止，我们都没有提及卷积神经网络中每层的超参数的使用。现在先介绍设置结构尺寸的一般性规则，然后根据这些规则进行讨论：输入层（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。 卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长S=1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当F=3，那就使用P=1来保持输入尺寸。当F=5,P=2，一般对于任意F，当P=(F-1)/2的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上。 汇聚层负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野（即F=2）的最大值汇聚，步长为2（S=2）。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。 减少尺寸设置的问题上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。 为什么在卷积层使用1的步长？在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。 为何使用零填充？使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。 因为内存限制所做的妥协：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。 案例学习下面是卷积神经网络领域中比较有名的几种结构： LeNet： 第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的。当然，最著名还是被应用在识别数字和邮政编码等的LeNet结构。AlexNet：AlexNet卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的ImageNet ILSVRC 竞赛中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。 ZF Net：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为 ZFNet（Zeiler &amp; Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。 GoogLeNet：ILSVRC 2014的胜利者是谷歌的Szeged等实现的卷积神经网络。它主要的贡献就是实现了一个奠基模块，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了。GooLeNet还有几种改进的版本，最新的一个是Inception-v4。 VGGNet：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为VGGNet。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的预训练模型是可以在网络上获得并在Caffe中使用的。VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。 ResNet：残差网络（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的跳跃链接，大量使用了批量归一化（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲（视频，PPT），以及一些使用Torch重现网络的实验。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，可以看论文Identity Mappings in Deep Residual Networks，2016年3月发表。 计算上的考量在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈。大部分现代GPU的内存是3/4/6GB，最好的GPU大约有12GB的内存。要注意三种内存占用来源： 来自中间数据体尺寸：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）。通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）。在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到。但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量。 来自参数尺寸：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存。因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多。 卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等。 一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位。把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量。如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了。 拓展资源和实践相关的拓展资源： Soumith benchmarks for CONV performance ConvNetJS CIFAR-10 demo 可以让你在服务器上实时地调试卷积神经网络的结构，观察计算结果。 Caffe，一个流行的卷积神经网络库。 State of the art ResNets in Torch7 ———————- end —————————]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(8) Neural Nets Notes 3]]></title>
    <url>%2F2017%2F12%2F13%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC8%E8%AF%BE-Neural-Nets-Notes-3%2F</url>
    <content type="text"><![CDATA[本文转载自：https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit，原文为：http://cs231n.github.io/neural-networks-3/，并进行一定的修改。 目录 梯度检查 合理性（Sanity）检查 检查学习过程 损失函数 训练集与验证集准确率 权重：更新比例 每层的激活数据与梯度分布 可视化 参数更新 一阶（随机梯度下降）方法，动量方法，Nestrov动量方法 学习率退火 二阶方法 逐参数适应学习率方法（Adagrad，RMSProp） 参数调优 评价 模型集成 总结 梯度检查理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较。然而从实际操作层面上来说，这个过程更加复杂且容易出错。下面是一些提示、技巧和需要仔细注意的事情。使用中心化公式，在使用有限差值近似来计算数值梯度的时候，常见的公式是：$$\frac {df(x)}{dx}=\frac {f(x+h) - f(x)}{h}(bad, do not use)$$其中，h是一个很小的数字，在实践中，近似为1e-5。在实践中证明，使用中心化公式效果更好：$$\frac {df(x)}{dx}=\frac {f(x+h) - f(x-h)}{2h}(use instead)$$该公式在检查梯度的每个维度的时候，会要求计算两次损失函数（所以计算资源的耗费也是两倍），但是梯度的近似值会准确很多。要理解这一点，对f(x+h)和f(x-h)使用泰勒展开，可以看到第一个公式的误差近似O(h)，第二个公式的误差近似$O(h^2)$（是个二阶近似）。 使用相对误差来比较，比较数值梯度$f_n^’$和解析梯度$f_a^’$的细节有哪些？如何得知此两者不匹配？你可能会倾向于监测它们的差的绝对值$|f_a^’-f_n^’|$或者差的平方值，然后定义该值如果超过某个规定阈值，就判断梯度实现失败。然而该思路是有问题的。想想，假设这个差值是1e-4，如果两个梯度值在1.0左右，这个差值看起来就很合适，可以认为两个梯度是匹配的。然而如果梯度值是1e-5或者更低，那么1e-4就是非常大的差距，梯度实现肯定就是失败的了。因此，使用相对误差总是更合适一些：$$\frac {|f_a^’-f_n^’|}{max(|f_a^’|,|f_n^’|)}$$上式考虑了差值占两个梯度绝对值的比例。注意通常相对误差公式只包含两个式子中的一个（任意一个均可），但是我更倾向取两个式子的最大值或者取两个式子的和。这样做是为了防止在其中一个式子为0时，公式分母为0（这种情况，在ReLU中是经常发生的）。然而，还必须注意两个式子都为零且通过梯度检查的情况。在实践中： 相对误差&gt;1e-2:通常就意味着梯度可能出错; 1e-2&gt;相对误差&gt;1e-4:要对这个值感到不舒服才行; 1e-4&gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高; 1e-7或者更小：好结果，可以高兴一把了。 要知道的是网络的深度越深，相对误差就越高。所以，如果你是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。相反，如果一个可微函数的相对误差值是1e-2，那么通常说明梯度实现不正确。 使用双精度：一个常见的错误是使用单精度浮点数来进行梯度检查，这样会导致即使梯度实现正确，相对误差值也会很高。在我的经验而言，出现过使用单精度浮点数时相对误差为1e-2，换成双精度浮点数时，就降低为1e-8的情况。 保持在浮点数的有效范围，建议通读《What Every Computer Scientist Should Konw About Floating-Point Artthmetic》一文，该文将阐明你可能犯的错误，促使你写下更加细心的代码。例如，在神经网络中，在一个批量的数据上对损失函数进行归一化是很常见的。但是，如果每个数据点的梯度很小，然后又用数据点的数量去除，就使得数值更小，这反过来会导致更多的数值问题。这就是我为什么总是会把原始的解析梯度和数值梯度数据打印出来，确保用来比较的数字的值不是过小（通常绝对值小于1e-10就绝对让人担心）。如果确实过小，可以使用一个常数暂时将损失函数的数值范围扩展到一个更“好”的范围，在这个范围中浮点数变得更加致密。比较理想的是1.0的数量级上，即当浮点数指数为0时。 目标函数的不可导点（kinks)，在进行梯度检查时，一个导致不准确的原因是不可导点问题。不可导点是指目标函数不可导的部分，由ReLU（max(0,x)）等函数，或SVM损失，Maxout神经元等引入。考虑当x=-1e6的时，对ReLU函数进行梯度检查。因为x1e-6)，导致了一个非零的结果。你可能会认为这是一个极端的案例，但实际上这种情况很常见。例如，一个用CIFAR-10训练的SVM中，因为有50,000个样本，且根据目标函数每个样本产生9个式子，所以包含有450,000个max(0,x)式子。而一个用SVM进行分类的神经网络因为采用了ReLU，还会有更多的不可导点。 注意，在计算损失的过程中是可以知道不可导点有没有被越过的。在具有max(x,y)形式的函数中持续跟踪所有“赢家”的身份，就可以实现这一点。其实就是看在前向传播时，到底x和y谁更大。如果在计算f(x+h)和f(x-h)的时候，至少有一个“赢家”的身份变了，那就说明不可导点被越过了，数值梯度会不准确。 使用少量数据点，解决上面的不可导点问题的一个办法是使用更少的数据点。因为含有不可导点的损失函数(例如：因为使用了ReLU或者边缘损失等函数)的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。还有，如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。 谨慎设置步长h，在实践中h并不是越小越好，因为当h特别小的时候，就可能会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将h调到1e-4或者1e-6，然后突然梯度检查就可能恢复正常。 在操作的特性模式中梯度检查，有一点必须要认识到：梯度检查是在参数空间中的一个特定（往往还是随机的）的单独点进行的。即使是在该点上梯度检查成功了，也不能马上确保全局上梯度的实现都是正确的。还有，一个随机的初始化可能不是参数空间最优代表性的点，这可能导致进入某种病态的情况，即梯度看起来是正确实现了，实际上并没有。例如，SVM使用小数值权重初始化，就会把一些接近于0的得分分配给所有的数据点，而梯度将会在所有的数据点上展现出某种模式。一个不正确实现的梯度也许依然能够产生出这种模式，但是不能泛化到更具代表性的操作模式，比如在一些的得分比另一些得分更大的情况下就不行。因此为了安全起见，最好让网络学习（“预热”）一小段时间，等到损失函数开始下降的之后再进行梯度检查。在第一次迭代就进行梯度检查的危险就在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。 不要让正则化吞没数据，通常损失函数是数据损失和正则化损失的和，需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分（正则化部分的梯度表达式通常简单很多）。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化的强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。 记得关闭随机失活（Dropout）和数据扩张（augmentation）,在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。关闭这些操作不好的一点是无法对它们进行梯度检查（例如随机失活的反向传播实现可能有错误）。因此，一个更好的解决方案就是在计算f(x+h)和f(x-h)前强制增加一个特定的随机种子，在计算解析梯度时也同样如此。 检查少量的维度，在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度，然后假设其他维度是正确的。注意：确认在所有不同的参数中都抽取一部分来梯度检查。在某些应用中，为了方便，人们将所有的参数放到一个巨大的参数向量中。在这种情况下，例如偏置就可能只占用整个向量中的很小一部分，所以不要随机的从向量中取维度，一定要把这种情况考虑到，确保所有的参数都收到了正确的梯度。 学习之前：合理性检查的提示与技巧在进行费时费力的最优化之前，最好进行一些合理性检查： 寻找特定情况的正确损失值，在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。 第二个合理性检查：提高正则化强度时导致损失值变大。 对小数据子集过拟合， 最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。除非能通过这一个正常性检查，不然进行整个数据集训练是没有意义的。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。 检查学习过程在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。在下面的图表中，x轴通常都是表示周期（epochs）单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。 损失函数训练期间第一个要跟踪的数值就是损失值，它再前向传播时对每个独立的批数据进行计算。下图是展示的是损失值随着时间的变化，尤其是曲线形状会给出关于学习率设置的情况： 左图展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。右图显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。 损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。 有的研究者喜欢用对数域对损失函数值作图。因为学习过程一般都是采用指数型的形状，图表就会看起来更像是能够直观理解的直线，而不是呈曲棍球一样的曲线状。还有，如果多个交叉验证模型在一个图上同时输出图像，它们之间的差异就会比较明显。 训练集与验证集准确率在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。这个图表能够展现知道模型过拟合的程度： 在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。 权重：更新比例最后一个应该跟踪的量是权重中更新值的数量和全部值的数量之间的比例。注意：是更新的，而不是原始梯度（比如，在普通sgd中就是梯度乘以学习率）。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子：123456# 假设参数向量为W，其梯度向量为dWparam_scale = np.linalg.norm(W.ravel())update = -learning_rate*dW # 简单SGD更新update_scale = np.linalg.norm(update.ravel())W += update # 实际更新print update_scale / param_scale # 要得到1e-3左右 相较于跟踪最大和最小值，有研究者更喜欢计算和跟踪梯度的范式及其更新。这些矩阵通常是相关的，也能得到近似的结果。 每层的激活数据与梯度分布一个不正确的初始化可能让学习过程变慢，甚至彻底停止。还好，这个问题可以比较简单地诊断出来。其中一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。 第一层可视化最后，如果数据是图像像素数据，那么把第一层特征可视化会有帮助。 将神经网络第一层的权重可视化的例子。左图中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。右图的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。 参数更新一旦能使用反向传播计算解析梯度，梯度就能被用来进行参数更新。进行参数更新有好几种方法，接下来都会进行讨论。 深度网络的最优化是现在非常活跃的研究领域。本节将重点介绍一些公认有效的常用的技巧，这些技巧都是在实践之中会遇到的。我们将简要介绍这些技巧的直观概念，但不进行细节分析。对细节感兴趣的读者，我们提供一些拓展阅读。 随机梯度下降及各种更新方法普通更新，最简单的更新形式是沿着负梯度方向改变参数（因为梯度指向的是上升的方法，但是我们通常希望最小化损失函数）。假设有一个参数向量x及其梯度dx，那么最简单的更新的形式是：12# 普通更新x += - learning_rate * dx 其中，learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。 动量（Momentum）更新是另外一个方法，这个方法在深度网络上几乎总能得到更好的收敛速度。该方法可以看成是从物理角度上对最优化问题的得到的启发。损失函数可以理解为是山的高度（因此高度的势能是U=mgh），用随机数字初始化参数等同于在某个位置给质点设定初始速度为0.这样最优化过程就可以看成是模拟参数向量（即质点）在地形上滚动的过程。 因为作用于质点的力与梯度的潜在能量($F=-\nabla U$)有关，质点所受的力就是损失函数的负梯度。还有，因为F=ma，所以在这个观点下负梯度与质点的加速度是成比例的。注意这个理解和上面的随机梯度下降SGD是不同的，在普通版本中，梯度直接影响位置。而在这个版本的更新中，物理观点建议梯度只是影响速度，然后速度再影响位置：123# 动量更新v = mu * v - learning_rate * dx # 与速度融合x += v # 与位置融合 在这里引入一个初始化为0的变量v和一个超参数mu。说的不恰当一点，这个变量mu，在最优化的过程中被看做动量（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设置为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火类似，动量随时间变化的设置有时能略微改善最优化效果，其中动量在学习过程的后阶段会上升。一个典型的设置是刚开始将动量设置为0.5,而在后面的多个周期（epoch）中慢慢提升到0.99。 通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。 Nesterov动量与普通动量有些许不同，最近变得比较流行。在理论上对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。Nesterov动量的核心思路是，当参数向量位于某个位置x时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过mu v稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置x+mu v看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，计算x+mu* v的梯度而不是“旧”位置x的梯度就有意义了。 Nesterov动量。既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个“向前看”的地方计算梯度。也就是说，添加一些注释后，实现代码如下：1234x_ahead = x + mu * v# 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)v = mu * v - learning_rate * dx_aheadx += v 然而在实践中，人们更喜欢和普通SGD或上面的动量方法一样简单的表达式。通过对x_ahead = x + mu * v使用变量变换进行改写是可以做到的，然后用x_ahead而不是x来表示上面的更新。也就是说，实际存储的参数向量总是向前一步的那个版本。x_ahead的公式（将其重新命名为x）就变成了：123v_prev = v # 存储备份v = mu * v - learning_rate * dx # 速度更新保持不变x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式 对于NAG（Nesterov’s Accelerated Momentum）的来源和数学公式推导，我们推荐以下的拓展阅读： Yoshua Bengio的Advances in optimizing Recurrent Networks，Section 3.5。 http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf在section 7.2对于这个主题有更详尽的阐述。 学习率退火在训练深度网络的时候，让学习率随着时间退火通常是有帮助的。可以这样理解：如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。知道什么时候开始衰减学习率是有技巧的：慢慢减小它，可能在很长时间内只能是浪费计算资源地看着它混沌地跳动，实际进展很少。但如果快速地减少它，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有3种方式： 随步数衰减：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体的问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。 指数衰减：数学公式是$\alpha = \alpha_0e^{-kt}$，其中,$\alpha_0,k$是超参数，t是迭代次数（也可以使用周期作为单位）。 1/t衰减：数学公式是$\alpha=\alpha_0/(1+kt)$,$\alpha_0,k$是超参数，t是迭代次数。在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。 二阶方法在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下$$x \leftarrow x - [Hf(x)]^{-1}\nabla f(x)$$这里Hf(x)是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。$\nabla f(x)$是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。然而，上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的拟-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是L-BFGS，该方法使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。 然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。 实践，在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。 参考资料： Large Scale Distributed Deep Networks 一文来自谷歌大脑团队，比较了在大规模数据情况下L-BFGS和SGD算法的表现。 SFO算法想要把SGD和L-BFGS的优势结合起来。 逐参数适应学习率方法前面讨论的所有方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作投入到发明能够适应性地对学习率调参的方法，甚至是逐个参数适应学习率调参。很多这些方法依然需要其他的超参数设置，但是其观点是这些方法对于更广范围的超参数比原始的学习率方法有更良好的表现。在本小节我们会介绍一些在实践中可能会遇到的常用适应算法： Adagrad是一个由Duchi等提出的适应性学习率算法：123# 假设有梯度和参数向量xcache += dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) 注意，变量cache的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数的梯度的平方和。这个一会儿将用来归一化参数更新步长，归一化是逐元素进行的。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强。有趣的是平方根的操作非常重要，如果去掉，算法的表现将会糟糕很多。用于平滑的式子eps（一般设为1e-4到1e-8之间）是防止出现除以0的情况。Adagrad的一个缺点是，在深度学习中单调的学习率被证明通常过于激进且过早停止学习。 RMSprop是一个非常高效，但没有公开发表的适应性学习率方法。有趣的是，每个使用这个方法的人在他们的论文中都引用自Geoff Hinton的Coursera课程的第六课的第29页PPT。这个方法用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均：12cache = decay_rate * cache + (1 - decay_rate) * dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) 在上面的代码中，decay_rate是一个超参数，常用的值是[0.9,0.99,0.999]。其中x+=和Adagrad中是一样的，但是cache变量是不同的。因此，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这同样效果不错。但是和Adagrad不同，其更新不会让学习率单调变小。 Adam是最近才提出的一种更新方法，它看起来像是RMSProp的动量版。简化的代码是下面这样：123m = beta1*m + (1-beta1)*dxv = beta2*v + (1-beta2)*(dx**2)x += - learning_rate * m / (np.sqrt(v) + eps) 注意这个更新方法看起来真的和RMSProp很像，除了使用的是平滑版的梯度m，而不是用的原始梯度向量dx。论文中推荐的参数值eps=1e-8, beta1=0.9, beta2=0.999。在实际操作中，我们推荐Adam作为默认的算法，一般而言跑起来比RMSProp要好一点。但是也可以试试SGD+Nesterov动量。完整的Adam更新算法也包含了一个偏置（bias）矫正机制，因为m,v两个矩阵初始为0，在没有完全热身之前存在偏差，需要采取一些补偿措施。建议读者可以阅读论文查看细节，或者课程的PPT。拓展阅读： Unit Tests for Stochastic Optimization一文展示了对于随机最优化的测试。 参数调优我们已经看到，训练一个神经网络会遇到很多超参数设置，神经网络最常用的设置有： 初始化学习率； 学习率衰减方式（例如一个衰减常量） 正则化强度（L2惩罚，随机失活强度）但是也可以看到，还有很多相对不那么敏感的超参数。比如在逐参数适应学习方法中，对于动量及时间表的设置等。在本节中将介绍一些额外的调参要点和技巧： 实现：更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要，因为这会影响你设计代码的思路。一个具体的设计是用仆程序持续地随机设置参数然后进行最优化。在训练过程中，仆程序会对每个周期后验证集的准确率进行监控，然后向文件系统写下一个模型的记录点（记录点中有各种各样的训练统计数据，比如随着时间的损失值变化等），这个文件系统最好是可共享的。在文件名中最好包含验证集的算法表现，这样就能方便地查找和排序了。然后还有一个主程序，它可以启动或者结束计算集群中的仆程序，有时候也可能根据条件查看仆程序写下的记录点，输出它们的训练统计数据等。 比起交叉验证最好使用一个验证集：在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。你可能会听到人们说他们“交叉验证”一个参数，但是大多数情况下，他们实际是使用的一个验证集。 超参数范围，在对数尺度上进行超参数搜索，例如，一个典型的学习率应该看起来是这样：learning_rate = 10**uniform(-6, 1)。也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有承的效果。例如：当学习率是0.001的时候，如果对其固定地增加0.001，那么对于学习进程会有很大的影响。然而当学习率是10的时候，影响就微乎其微了。。这就是因为学习率乘以了计算出的梯度。因此，比起加上或者减少某些值，思考学习率的范围是乘以或者除以某些值更加自然。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：dropout=uniform(0,1)）。 随机搜索优于网格搜索，Bergstra和Bengio在文章Random Search for Hyper-Parameter Optimization中说“随机选择比网格化的选择更加有效”，而且在实践中也更容易实现。 对于边界上的最优值要小心：这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使用learning_rate = 10**uniform(-6,1)来进行搜索。一旦我们得到一个比较好的值，一定确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。 从粗到细地分阶段搜索，在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练一个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。 贝叶斯超参数最优化是一整个研究领域，主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。基于这些模型，发展出很多的库，比较有名的有： Spearmint, SMAC, 和Hyperopt。然而，在卷积神经网络的实际使用中，比起上面介绍的先认真挑选的一个范围，然后在该范围内随机搜索的方法，这个方法还是差一些。这里有http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html更详细的讨论。 评价模型集成在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法： 同一个模型，不同的初始化，使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。这种方法的风险在于多样性只来自于不同的初始化条件。 在交叉验证中发现最好的模型，使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。 一个模型设置多个记录点，如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。 在训练的时候跑参数的平均值，和上面一点相关的，还有一个也能得到1-2个百分点的提升的小代价方法，这个方法就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样你就对前几次循环中的网络状态进行了平均。你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。 模型集成的一个劣势就是在测试数据的时候会花费更多时间。最近Geoff Hinton在“Dark Knowledge”上的工作很有启发：其思路是通过将集成似然估计纳入到修改的目标函数中，从一个好的集成中抽出一个单独模型。 总结训练一个神经网络需要： 利用小批量的数据对实现进行梯度检查，还要注意各种错误。 进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。 在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数相对于总参数的比例（一般在1e-3左右）然后如果是对于卷积神经网络，可以将第一层的权重可视化。 推荐的两个更新方法是SGD+Nesterov动量方法，或者是Adam方法。 随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。 使用随机搜索（不要使用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。 进行模型集成来获得额外的性能提高。 拓展阅读 Leon Botton的《SGD要点和技巧》：https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F192769%2Ftricks-2012.pdf Yann LeCun的《Efficient BackProp》：http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf Yoshua Bengio的《Practical Recommendations for Gradient-Based Training of Deep Architectures》。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(7) Neural Nets Notes 2]]></title>
    <url>%2F2017%2F12%2F12%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC7%E8%AF%BE-Neural-Nets-Notes-2%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-2/ 目录 设置数据和模型 数据预处理 权重初始化 批量归一化（Batch Normalization） 正则化（L1/L2/Maxnorm/Dropout） 损失函数 小结 设置数据和模型上一节中介绍了神经元的模型，它在计算内积后进行非线性激活函数计算，神经网络将这些神经元组织成各个层。这些做法共同定义了评分函数（score function）的新形式，该形式是从前面线性分类章节中的简单线性映射发展而来的。具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。本节将讨论更多的算法设计选项，比如数据预处理，权重初始化和损失函数。 数据预处理关于数据预处理我们有3个常用的符号，数据矩阵X，假设其尺寸是[N x D]（N是数据样本的数量，D是数据的维度）。 均值减法（Mean Subtraction）:是预处理最常用的形式。它对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码X-=np.mean(X,axis=0)实现。对于图像，更常用的是对所有像素都减去一个值，可以用X -= np.mean(X)实现，也可以在3个颜色通道上分别操作。 归一化操作（Normalization）:是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)。第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。 一般数据预处理流程：左边：原始的2维输入数据。中间：在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。右边：每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。PCA和白化（Whitening）:是另一种数据预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。 123# 假设输入数据矩阵X的尺寸为[N x D]X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵 数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的协方差。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和半正定的。我们可以对数据协方差矩阵进行SVD（奇异值分解）运算。1U,S,V = np.linalg.svd(cov) U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：1Xrot = np.dot(X,U) # 对数据去相关性 注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算Xrot的协方差矩阵，将会看到它是对角对称的。np.linalg.svd的一个良好性质是在它的返回值U中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有方差的维度。 这个操作也被称为主成分分析（ Principal Component Analysis 简称PCA）降维：1Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced 变成 [N x 100] 经过上面的操作，将原始的数据集的大小由[N x D]降到了[N x 100]，留下了数据中包含最大方差的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。最后一个在实践中会看见的变换是白化（whitening）。白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下：123# 对数据进行白化操作:# 除以特征值 Xwhite = Xrot / np.sqrt(S + 1e-5) 警告：夸大的噪声。注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。 PCA/白化。左边是二维的原始数据。中间：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。右边：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。 实践操作：在这个笔记中提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。 常见错误：进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练、验证、测试集，那么这个做法是错误的。应该怎么做呢？应该先分成训练、验证、测试集，只是从训练集中求图片平均值，然后各个集（训练、验证、测试集）中的图像再减去这个平均值。 权重初始化我们已经看到如何构建一个神经网络的结构并对数据进行预处理，但是在开始训练网络之前，还需要初始化网络的参数。 错误：全零初始化：让我们从应该避免的错误开始。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。 小随机数初始化：因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是：W = 0.01 * np.random.randn(D,H)。其中，randn函数是基于零均值和标准差的一个高斯分布来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。 警告：并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会出现非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。 使用1/sqrt(n)校准方差，上面的做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为：w = np.random.randn(n) / sqrt(n)。其中n是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。上述结论的推导过程如下：假设权重w和输入x之间的内积为$s=\sum^n_iw_ix_i$，这是还没有进行非线性激活函数运算之前的原始数值。我们可以检查s的方差：$$Var(s)=Var(\sum_i^n w_ix_i)\\\\=\sum_i^nVar(w_ix_i)\\\\=\sum_i^n[E(w_i)]^2Var(x_i)+E[(x_i)]^2Var(w_i)+Var(xIi)Var(w_i)\\\\=\sum_i^nVar(x_i)Var(w_i)\\\\=(nVar(w))Var(x)$$在前两步，使用了方差的性质。在第三步，因为假设输入和权重的平均值都是0，所以$E[x_i]=E[w_i]=0$。注意这并不是一般化情况，比如在ReLU单元中均值就为正。在最后一步，我们假设所有的$w_i,x_i$都服从同样的分布。从这个推导过程我们可以看见，如果想要s有和输入x一样的方差，那么在初始化的时候必须保证每个权重w的方差是1/n。又因为对于一个随机变量X和标量a，有$Var(aX)=a^2Var(X)$，这就说明可以基于一个标准高斯分布，然后乘以$a=\sqrt{1/n}$，使其方差为1/n，于是得出：w = np.random.randn(n) / sqrt(n)。 Glorot等在论文Understanding the difficulty of training deep feedforward neural networks中作出了类似的分析。在论文中，作者推荐初始化公式为 $Var(w) = 2/(n_{in} + n_{out})$ ，其中$n_{in}, n_{out}$是在前一层和后一层中单元的个数。这是基于妥协和对反向传播中梯度的分析得出的结论。该主题下最新的一篇论文是：Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification，作者是He等人。文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是2.0/n。代码为w = np.random.randn(n) * sqrt(2.0/n)。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。 稀疏初始化（Sparse initialization）:另一个处理非标定方差的方法就是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。 偏置（biases）的初始化，通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。 实践，当前的推荐是使用ReLU激活函数，并且使用w = np.random.randn(n) * sqrt(2.0/n)来进行权重初始化。 批量归一化（Batch Normalization）:批量归一化是loffe和Szegedy最近才提出的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛：）,其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层与激活函数之间添加一个BatchNorm层。对于这个技巧本节不会展开讲，因为参考文献：https://arxiv.org/abs/1502.03167中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。搞定！ 正则化Regularization有不少方法是通过控制神经网络的容量来防止其过拟合的：L2正则化可能是最常用的正则化方法了，可以通过惩罚目标函数中所有参数的平方将其实现。即对网络中的每个权重w，向目标函数中增加一个$\frac {1}{2}\lambda w^2$，其中$\lambda$是正则化强度。前面这个1/2很常见，是因为加上1/2后，该式子关于w梯度就是$\lambda w$而不是$2\lambda w$了，L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。在线性分类章节中讨论过，由于输入和权重之间的乘法操作，这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以w += -lambda * W向着0线性下降。 L1正则化是另一个相对常用的正则化方法。对于每个w我们都向目标函数增加一个\lambda|w|。L1和L2正则化也可以进行组合：$\lambda_1|w|+\lambda_2w^2$，这也被称作Elastic net regularizaton。L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。 最大范式约束（Max norm constraints),另一种形式的正则化是给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量$\overrightarrow{w}$必须满足$||\overrightarrow{w}||_2&lt;c$这一条件，一般c值为3或者4。有研究者发文称在使用这种正则化方法时效果更好。这种正则化还有一个良好的性质，即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的。 随机失活（Dropout）是一个简单又极其有效的正则化方法。该方法由Srivastava在论文Dropout: A Simple Way to Prevent Neural Networks from Overfitting中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0。 图片来自于论文：http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf。展示其核心思路，在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相对独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为对数量巨大的子网们做了模型集成，以此来计算出一个平均的预测。 一个3层神经网络的普通版随机失活可以用下面代码实现：123456789101112131415161718192021222324""" 普通版随机失活: 不推荐实现 (看下面笔记) """p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱def train_step(X): """ X中是输入数据 """ # 3层neural network的前向传播 H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = np.random.rand(*H1.shape) &lt; p # 第一个随机失活遮罩 H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = np.random.rand(*H2.shape) &lt; p # 第二个随机失活遮罩 H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # 反向传播:计算梯度... (略) # 进行参数更新... (略) def predict(X): # 前向传播时模型集成 H1 = np.maximum(0, np.dot(W1, X) + b1) * p # 注意：激活数据要乘以p H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # 注意：激活数据要乘以p out = np.dot(W3, H2) + b3 在上面的代码中，train_step函数在第一个隐层和第二个隐层上进行了两次随机失活。在输入层上面进行随机失活也是可以的，为此需要为输入数据X创建一个二值的遮罩。反向传播保持不变，但是肯定需要将遮罩U1和U2加入进去。 注意：在predict函数中不进行随机失活，但是对于两个隐层的输出都要乘以p，调整其数值范围。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以p=0.5为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。为了理解这点，先假设有一个神经元x的输出，那么进行随机失活的时候，该神经元的输出就是px+(1-p)0，这是有1-p的概率神经元的输出为0。在测试时神经元总是激活的，就必须调整$x\to px$来保持同样的预期输出。在测试时会在所有可能的二值遮罩（也就是数量庞大的所有子网络）中迭代并计算它们的协作预测，进行这种减弱的操作也可以认为是与之相关的。 上述操作不好的性质是必须在测试时对激活数据要按照p进行数值范围调整。既然测试性能如此关键，实际更倾向使用反向随机失活（inverted dropout），它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下：12345678910111213141516171819202122232425""" 反向随机失活: 推荐实现方式.在训练的时候drop和调整数值范围，测试时不做任何事."""p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱def train_step(X): # 3层neural network的前向传播 H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = (np.random.rand(*H1.shape) &lt; p) / p # 第一个随机失活遮罩. 注意/p! H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = (np.random.rand(*H2.shape) &lt; p) / p # 第二个随机失活遮罩. 注意/p! H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # 反向传播:计算梯度... (略) # 进行参数更新... (略)def predict(X): # 前向传播时模型集成 H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了 H2 = np.maximum(0, np.dot(W2, H1) + b2) out = np.dot(W3, H2) + b3 在随机失活发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献： Dropout paper by Srivastava et al. 2014. Dropout Training as Adaptive Regularization：“我们认为：在使用费希尔信息矩阵（fisher information matrix）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。” 前向传播中的噪音：在更一般化的分类上，随机失活属于网络在前向传播中有随机行为的方法。测试时，通过分析法（在使用随机失活的本例中就是乘以p）或数值法（例如通过抽样出很多子网络，随机选择不同子网络进行前向传播，最后对它们取平均）将噪音边缘化。在这个方向上的另一个研究是DropConnect，它在前向传播的时候，一系列权重被随机设置为0。提前说一下，卷积神经网络同样会吸取这类方法的优点，比如随机汇合（stochastic pooling），分级汇合（fractional pooling），数据增长（data augmentation）。我们在后面会详细介绍。 偏置正则化：在线性分类器的章节中介绍过，对于偏置参数的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差。这可能是因为相较于权重参数，偏置参数实在太少，所以分类器需要它们来获得一个很好的数据损失，那么还是能够承受的。 每层正则化：对于不同的层进行不同强度的正则化很少见（可能除了输出层以外），关于这个思路的相关文献也很少。 实践：通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。p值一般默认设为0.5，也可能在验证集上调参。 损失函数我们已经讨论过损失函数的正则化损失部分，它可以看做是对模型复杂程度的某种惩罚。损失函数的第二个部分时数据损失。它是一个有监督学习问题，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性。数据损失是对所有样本的数据损失求平均。也就是说，$L=\frac{1}{N}\sum_iL_i$中，N是训练集数据的样本数。让我们把神经网络中输出层的激活函数简写为f=f(x_i;W)，在实际中你可能需要解决以下几类问题： 分类问题我们一直讨论的。在该问题中，假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签（是固定分类标签之一）。在这类问题中，一个最常见的损失函数就是SVM（是Weston Watkins 公式）：$$L_i=\sum_{j\not=y_i}max(0,f_j-f_{y_i}+1)$$之前简要提起过，有些学者的论文中指出平方折叶损失（即使用max(0,f_j-f_{y_i}+1)^2）算法的结果会更好。第二个常用的损失函数是Softmax分类器，它使用交叉熵损失：$$\displaystyle L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})$$ 问题：类别数目巨大。当标签集非常庞大（例如字典中的所有英语单词，或者ImageNet中的22000种分类），就需要使用分层Softmax（Hierarchical Softmax）了（参考文献：https://arxiv.org/pdf/1310.4546.pdf）分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。 属性（Attribute）分类。上面两个损失公式的前提，都是假设每个样本只有一个正确的标签y_i。但是如果y_i是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间并不相互排斥呢？比如在Instagram上的图片，就可以看成是被一个巨大的标签集合中的某个子集打上标签，一张图片上可能有多个标签。在这种情况下，一个明智的方法是为每个属性创建一个独立的二分类的分类器。例如，针对每个分类的二分类器会采用下面的公式：$$L_i=\sum_j max(0,1-y_{ij}f_j)$$上式中，求和是对所有分类j，$y_{ij}$的值为1或者-1，具体根据第i个样本是否被第j个属性打标签而定，当该类别被正确预测并展示的时候，分值向量$f_j$为正，其余情况为负。可以发现，当一个正样本的得分小于+1，或者一个负样本的得分大于-1的时候，算法就会累计损失值。 另一种方法是对每种属性训练一个独立的逻辑回归分类器，二分类的逻辑回归只有两个分类（0，1），其中对于分类1的概率计算为：$$P(y=1|x;w,b) = \frac {1}{1+e^{-(w^Tx+b)}}=\sigma(w^Tx+b)$$因为类别0和类别1的概率和为1，所以类别0的概率为：$\displaystyle P(y=0|x;w,b)=1-P(y=1|x;w,b)$。这样，如果$\sigma(w^Tx+b)&gt;0.5$或者$w^Tx+b&gt;0$，那么样本就要被分类成为正样本（y=1）。然后损失函数最大化这个对数似然函数，问题可以简化为：$$L_i = \sum_j y_{ij}log(\sigma(f_j)) + (1-y_{ij})log(1-\sigma(f_j))$$式中，假设标签$y_{ij}$非0即1，$\sigma(.)$就是sigmoid函数。上面的公式看起来吓人，但是f的梯度实际上非常简单：$\displaystyle \frac{\partial L_i}{\partial f_j}=y_{ij}-\sigma(f_j)$（你可以自己求导来验证）。 回归问题是预测实数的值的问题，比如房价预测，预测图片中某个东西的长度等等。对于这种问题，通常是计算预测值和真实值之间的损失。然后用L2或者L1范数度量差异。对于某个样本，L2范数计算如下：$$L_i=||f-y_i||_2^2$$之所以在目标函数中要进行平方，是因为梯度算起来更加简单。因为平方是一个单调运算，所以不用改变最优参数。L1范式则是要将每个维度上的绝对值加起来：$$L_i = ||f - y_i||_1=\sum_j|f_j - (y_i)j|$$在上式中，如果有多个数量被预测了，就要对预测的所有维度的预测求和，即$\sum_j$。观察第i个样本的第j维，用$\delta_{ij}$表示预测值与真实值之间的差异。关于该维度的梯度（也就是$\partial L_i/\partial f_j）$能够轻松地通过被求导为L2范式的$\delta_{ij}$或$sign(\delta_{ij})$。这就是说，评分值的梯度要么与误差中的差值直接成比例，要么是固定的并从差值中继承sign。 注意：L2损失比起较为稳定的Softmax损失来，其最优化过程要困难很多。直观而言，它需要网络具备一个特别的性质，即对于每个输入（和增量）都要输出一个确切的正确值。而在Softmax中就不是这样，每个评分的准确值并不是那么重要：只有当它们量级适当的时候，才有意义。还有，L2损失鲁棒性不好，因为异常值可以导致很大的梯度。所以在面对一个回归问题时，先考虑将输出变成二值化是否真的不够用。例如，如果对一个产品的星级进行预测，使用5个独立的分类器来对1-5星进行打分的效果一般比使用一个回归损失要好很多。分类还有一个额外优点，就是能给出关于回归的输出的分布，而不是一个简单的毫无把握的输出值。如果确信分类不适用，那么使用L2损失吧，但是一定要谨慎：L2非常脆弱，在网络中使用随机失活（尤其是在L2损失层的上一层）不是好主意。 当面对一个回归任务，首先考虑是不是必须这样。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。 结构化预测（structured prediction）,结构化损失是指标签可以是任意的结构，例如图表、树或者其他复杂物体的情况。通常这种情况还会假设结构空间非常巨大，不容易进行遍历。结构化SVM背后的基本思想就是在正确的结构y_i和得分最高的非正确结构之间画出一个边界。解决这类问题，并不是像解决一个简单无限制的最优化问题那样使用梯度下降就可以了，而是需要设计一些特殊的解决方案，这样可以有效利用对于结构空间的特殊简化假设。我们简要地提一下这个问题，但是详细内容就超出本课程范围。 小结 推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化到[-1,1]范围之内。 使用标准差为\sqrt{2/n}的高斯分布来初始化权重，其中n是输入的神经元数。例如用numpy可以写作：w = np.random.randn(n) * sqrt(2.0/n)。 使用L2正则化和随机失活的倒置版本。 使用批量归一化。 讨论了在实践中可能要面对的不同任务，以及每个任务对应的常用损失函数。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(6) Neural Nets Notes 1]]></title>
    <url>%2F2017%2F12%2F10%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC6%E8%AF%BE-Neural-Nets-Notes-1%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-1/ 目录 简介 单个神经元建模 生物动机和连接 作为线性分类器的单个神经元 常用的激活函数 神经网络结构 层组织 前向传播计算例子 表达能力 设置层的数量和尺寸 小结 简介在线性分类一节中，在给出图像的情况下，使用$s=Wx$来计算不同视觉类别的评分，其中W是一个矩阵，x是一个输入列向量，它包含了图像的全部像素数据。在使用数据库CIFAR-10的案例中，x是一个[30721]的列向量，W是一个[103072]的矩阵，所有输出的评分是一个包含10个类别评分的向量。神经网络算法则不同，它的计算公式是$s=W_2max(0,W_1x)$。其中$W_1$的含义是这样的：举个例子来说，它可以是一个[100*3072]的矩阵，其作用是将图像转化为一个100维的过渡向量。函数max(0,-)是非线性的，它会作用到每个元素。这个非线性函数有多种选择，后续将会学到。但这个形式是一个最常用的选择，它就是简单地设置阈值，将所有小于0的值变成0。最终，矩阵$W_2$的尺寸是[10x100]，因此将得到10个数字，这10个数字可以解释为是分类的评分。注意非线性函数在计算上是至关重要的，如果略去这一步，那么两个矩阵将会合二为一，对于分类的评分计算将重新变成关于输入的线性函数。这个非线性函数就是改变的关键点。参数$W_1$,$W_2$将通过随机梯度下降来学习到，他们的梯度在反向传播过程中，通过链式法则来求导计算得出。 一个三层的神经网络可以类比地看做$s=W_3max(0,W_2max(0,W_1x))$，其中$W_1$,$W_2$,$W_3$是需要进行学习的参数。中间隐层的尺寸是网络的超参数，后续将学习如何设置它们。现在让我们先从神经元或者网络的角度理解上述计算。 单个神经元建模神经网络算法领域最初是对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好的效果。然而，讨论将还是从对生物系统的一个高层次的简要描述开始，因为神经网络毕竟是从这里得到了启发。 生物动机与连接大脑的基本计算单位是神经元（neuron）。人类的神经系统中大约有860亿个神经元，它们被大约10^14-10^15个突触（synapses）连接起来。下面图表的左边展示了一个生物学的神经元，右边展示了一个常用的数学模型。每个神经元都从它的树突获得输入信号，然后沿着它唯一的轴突（axon）产生输出信号。轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。 在神经元的计算模型中，沿着轴突传播信号（比如将$x_0$）将基于突触的突触强度（比如$w_0$），与其他神经元的树突进行乘法交互（比如$w_0x_0$）。其观点是，突触的强度（也就是权重w），是可学习的且可以控制一个神经元对于另一个神经元的影响强度（还可以控制影响方向：使其兴奋（正权重）或使其抑制（负权重））。在基本模型中，树突将信号传递到细胞体，信号在细胞体中相加。如果最终之和高于某个阈值，那么神经元将会激活，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号的频率在交流信息。基于这个速率编码的观点，将神经元的激活率建模为激活函数（activation function）f，它表达了轴突上激活信号的频率。由于历史原因，激活函数常常选择使用sigmoid函数$\sigma$，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。在本节后面部分会看到这些激活函数的各种细节。一个神经元前向传播的实例代码如下：123456class Neuron(object): def forward(inputs): """ 假设输入和权重是1-D的numpy数组，偏差是一个数字 """ cell_body_sum = np.sum(inputs * self.weights) + self.bias firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid激活函数 return firing_rate 换句话说，每个神经元都对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数（或称为激活函数）。本例中使用的是sigmoid函数$\sigma(x)=1/(1+e^{-x})$。在本节的末尾部分将介绍不同激活函数的细节。 作为线性分类器的单个神经元神经元模型的前向计算数学公式看起来可能比较眼熟。就像在线性分类器中看到的那样，神经元有能力”喜欢”（激活函数值接近1），或者不喜欢（激活函数值接近0）输入空间中的某些线性区域。因此，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。 二分类Softmax分类器，举例来说，可以把$\sigma(\sum_i w_ib_i+b)$看做其中一个分类的概率$P(y_i=1|x_i;w)$，其他分类的概率为$P(y_i=0|x_i;w)=1-P(y_i=1|x_i;w)$，因为它们加起来必须为1。根据这种理解，可以得到交叉熵损失，这个在线性分一节中已经介绍。然后将它最优化为二分类的Softmax分类器（也就是逻辑回归）。因为sigmoid函数输出限定在0-1之间，所以分类器做出预测的基准是神经元的输出是否大于0.5。 二分类SVM分类器，或者可以在神经元的输出外增加一个最大边界折叶损失（max-margin hinge loss）函数，将其训练成一个二分类的支持向量机。 一个单独的神经元可以用来实现一个二分类器，比如二分类的Softmax或者SVM分类器。 常用的激活函数每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数： 左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。 Sigmoid函数Sigmoid非线性函数的数学公式是$\sigma(x) = \frac {1}{1+e^{-x}}$，函数图像如上图的左边所示。在前面一节中已经提到，它输入实数值，并将其“挤压”到0到1范围内。更具体的说很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活(0)到在求和后的最大频率处的完全饱和的激活（1）。然而，现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点： Sigmoid函数饱和使梯度消失：Sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。 Sigmoid函数的输出不是零中心的：这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在$f=w^Tx+b$中每个元素都x&gt;0），那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式f而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。 Tanh函数Tanh函数图像如上图右边所示。它将实数值压缩到[-1,1]之间。和Sigmoid神经元一样，它也存在饱和的问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。注意tanh神经元是一个简单放大的sigmoid神经元，具体说来就是：$tanh(x)=2\sigma(2x)-1$。 左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当x=0时函数值为0。当x&gt;0函数的斜率为1。右边是从Krizhevsky等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。 ReLU:在近些年ReLU变得非常流行。它的函数公式是$f(x)=max(0,x)$。换句话说，这个激活函数就是一个关于0的阈值。使用Relu有以下一些优缺点： 优点：相较于Sigmoid和Tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用。据称这是由它的线性，非饱和的公式导致的。 优点：Sigmoid和Tanh神经元含有指数运算等耗费计算资源的操作，而Relu可以简单地对一个矩阵进行阈值计算得到。 缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。 Leaky ReLU:Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x=0)(x)$其中$\alpha$是一个小的常量。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。 Maxout:一些其他类型的单元被提了出来，它们对于权重和数据的内积结果不再使用$f(w^Tx+b)$函数形式。一个相关的流行选择是Maxout（最近由Goodfellow等发布）神经元。Maxout是对ReLU和leaky ReLU的一般化归纳，它的函数是：$max(w^T_1x+b_1,w^T_2x+b_2)$。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当$w_1,b_1=0$的时候）。这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。 以上就是一些常用的神经元及其激活函数。最后需要注意一点：在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有什么根本性问题来禁止这样做。 一句话：“那么该用那种呢？”用ReLU非线性函数。注意设置好学习率，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。 神经网络结构灵活地组织层将神经网络算法以神经元的形式图形化。神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接。也就是说，一些神经元的输出是另一些神经元的输入。在网络中是不允许循环的，因为这样会导致前向传播的无限循环。通常神经网络模型中神经元是分层的，而不是像生物神经元一样聚合成大小不一的团状。对于普通神经网络，最普通的层的类型是全连接层（fully-connected layer）。全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接。下面是两个神经网络的图例，都使用的全连接层： 左边是一个2层神经网络，隐层由4个神经元（也可称为单元（unit））组成，输出层由2个神经元组成，输入层是3个神经元。右边是一个3层神经网络，两个含4个神经元的隐层。注意：层与层之间的神经元是全连接的，但是层内的神经元不连接。命名规则：当我们说N层神经网络的时候，我们没有把输入层算入。因此，单层的神经网络就是没有隐层的（输入直接映射到输出）。因此，有的研究者会说LR或者SVM只是单层神经网络的一个特例。研究者们也会使用人工神经网络或者多层感知器来指代神经网络。很多研究者并不喜欢神经网络算法和人类大脑之间的类比，它们更倾向于用单元(unit)而不是神经元作为术语。输出层：和神经网络中其他层不同，输出层的神经元一般是不会有激活函数的（或者也可以认为它们有一个线性相等的激活函数）。这是因为最后的输出层大多用于表示分类评分值，因此是任意值的实数，或者某种实数值的目标数（比如在回归中）。确定网络尺寸：用来度量神经网络的尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数，用上面图示的两个网络举例： 第一个网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。 第二个网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。 为了方便对比，现代卷积神经网络能包含约1亿个参数，可由10-20层构成（这就是深度学习）。然而，有效（effective）连接的个数因为参数共享的缘故大大增多。在后面的卷积神经网络内容中我们将学习更多。 前向传播计算举例不断重复的矩阵乘法与激活函数交织。将神经网络组织成层状的一个主要原因，就是这个结构让神经网络算法使用矩阵向量操作变得简单和高效。用上面用上面那个3层神经网络举例，输入是[3x1]的向量。一个层所有连接的强度可以存在一个单独的矩阵中。比如第一个隐层的权重W1是[4x3]，所有单元的偏置储存在b1中，尺寸[4x1]这样，每个神经元的权重都在W1的一个行中，于是矩阵乘法np.dot(W1, x)就能计算该层中所有神经元的激活数据。类似的，W2将会是[4x4]矩阵，存储着第二个隐层的连接，W3是[1x4]的矩阵，用于输出层。完整的3层神经网络的前向传播就是简单的3次矩阵乘法，其中交织着激活函数的应用。123456# 一个3层神经网络的前向传播:f = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)h1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)h2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)out = np.dot(W3, h2) + b3 # 神经元输出(1x1) 在上面的代码中，W1，W2，W3，b1，b2，b3都是网络中可以学习的参数。注意x并不是一个单独的列向量，而可以是一个批量的训练数据（其中每个输入样本将会是x中的一列），所有的样本将会被并行化的高效计算出来。注意神经网络最后一层通常是没有激活函数的（例如，在分类任务中它给出一个实数值的分类评分）。 全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。 表达能力理解具有全连接层的神经网络的一个方式是：可以认为它们定义了一个由一系列函数组成的函数族，网络的权重就是每个函数的参数。如此产生的问题是：该函数族的表达能力如何？存在不能被神经网络表达的函数吗？ 现在看来，拥有至少一个隐层的神经网络是一个通用的近似器。在研究（例如1989年的论文Approximation by Superpositions of Sigmoidal Function，或者Michael Nielsen的这个直观解释。）中已经证明，给出任意连续函数f(x)和任意$\epsilon &gt;0$，均存在一个至少含1个隐层的神经网络g(x)（并且网络中有合理选择的非线性激活函数，比如sigmoid，对于$\forall x$，使得$|f(x)-g(x)|&lt;\epsilon$。换句话说，神经网络可以近似任何连续函数。 既然一个隐层就能近似任何函数，那为什么还要构建更多层来将网络做得更深？ 答案是：虽然一个2层网络在数学理论上能完美地近似所有连续函数，但在实际操作中效果相对较差。在一个维度上，虽然以a,b,c为参数向量“指示块之和”函数$g(x)=\sum_ic_i1(a_i&lt;x&lt;b_i)$ 也是通用的近似器，但是谁也不会建议在机器学习中使用这个函数公式。神经网络在实践中非常好用，是因为它们表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，网络通过最优化算法（例如梯度下降）能比较容易地学习到这个函数。类似的，虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好。 另外，在实践中3层的神经网络会比2层的表现好，然而继续加深（做到4，5，6层）很少有太大帮助。卷积神经网络的情况却不同，在卷积神经网络中，对于一个良好的识别系统来说，深度是一个极端重要的因素（比如数十(以10为量级)个可学习的层）。对于该现象的一种解释观点是：因为图像拥有层次化结构（比如脸是由眼睛等组成，眼睛又是由边缘组成），所以多层处理对于这种数据就有直观意义。 设置层的数量和尺寸在面对一个具体问题的时候该确定网络结构呢？到底是不用隐层呢？还是一个隐层？两个隐层或更多？每个层的尺寸该多大？ 首先，要知道当我们增加层的数量和尺寸时，网络容量上升了。即神经元们可以合作表达许多复杂的函数，所以表达函数的空间增加。例如，如果有一个在二维平面上的二分类问题，我们可以训练3个不同的神经网络，每个网络都只有一个隐藏层，但是每层的神经元数目不同： 数据是用不同颜色的圆点表示他们的不同类别，决策边界是由训练过的神经网络做出的。 在上图中，可以看见有更多神经元的神经网络可以表达更复杂的函数。然而，这既是优势也是不足，优势是可以分类更复杂的数据，不足是可能造成对训练数据的过拟合。过拟合是网络对数据中的噪音有很强的拟合能力，而没有重视数据间（假设）的潜在基本关系。举例来说，有20个神经元隐层的网络拟合了所有的训练数据，但是其代价是把决策边界变成了许多不相连的红绿区域。而有3个神经元的模型的表达能力只能用比较宽泛的方式去分类数据。它将数据看做是两个大块，并把个别在绿色区域内的红色点看做噪声。在实际中，这样可以在测试数据中获得更好的泛化（generalization）能力。 基于上面的讨论，看起来如果数据不是足够复杂，则似乎小一点的网络更好，因为可以防止过拟合。然而并非如此，防止神经网络的过拟合有很多方法（L2正则化，dropout和输入噪音等），后面会详细讨论。在实践中，使用这些方法来控制过拟合比减少网络神经元数目要好得多。 不要减少网络神经元的数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练。虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。因为神经网络是非凸的，就很难从数学上研究这些特性。即便如此，还是有一些文章尝试对这些目标函数进行理解，例如The Loss Surfaces of Multilayer Networks这篇论文。在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。重申一下，正则化强度是控制神经网络过拟合的好方法。看下图结果： 不同正则化强度的效果：每个神经网络都有20个隐层神经元，但是随着正则化强度增加，它的决策边界变得更加平滑。 需要记住的是：不应该因为害怕出现过拟合而使用小网络。相反，应该尽可能使用大网络，然后使用正则化技术来控制过拟合。 小结本节课主要介绍如下内容： 介绍了生物神经元的粗略模型； 讨论了几种不同类型的激活函数，其中ReLU是最佳推荐； 介绍了神经网络，神经元通过全连接层连接，层间神经元两两相连，但是层内神经元不连接； 理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算； 理解了神经网络是一个通用函数近似器，但是该性质与其广泛使用无太大关系。之所以使用神经网络，是因为它们对于实际问题中的函数的公式能够某种程度上做出“正确”假设。 讨论了更大网络总是更好的这一事实。然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合。在后续章节中我们讲学习更多正则化的方法，尤其是dropout。 参考资料 使用Theano的deeplearning.net tutorial:http://www.deeplearning.net/tutorial/mlp.html]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(5) Backprop Note]]></title>
    <url>%2F2017%2F12%2F09%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC5%E8%AF%BE-Backprop-Note%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/optimization-2/ 简介目的:本节帮助读者对反向传播形成直观而专业的理解。反向传播是利用链式法则递归计算梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。问题描述：核心问题是：给定函数f(x),其中x是输入数据向量，需要计算函数f关于x的梯度，也就是$\nabla f(x)$ 原因：之所以关注上述问题，是因为在神经网络中f对应的是损失函数L，输入x里面包含训练数据和神经网络权重。举个例子，损失函数可以是SVM的损失函数，输入则包含了训练数据$(x_i,y_i),i=1,…N$，权重W和偏差b。给定训练数据，权重是可以控制的变量。因此，即使使用反向传播计算输入数据$x_i$上的梯度，但在实践上为了进行参数更新，通常也只计算参数(W和b)的梯度。然而$x_i$ 的梯度有时仍然是有用的：比如将神经网络所做的事情可视化便于直观理解的时候，就能用上。 梯度的简单表达、解释首先，考虑一个简单的二元函数f(x,y)=xy。对两个输入变量分别求偏导数，能够很简单求出：$$f(x,y)=xy \rightarrow \frac {df}{dx}=y \frac {df}{dy}=x$$解释：要牢记导数的意义：函数变量在某个点周围的极小区域内变化，而导数就是变量变化导致的函数在该方向上的变化率。$$\frac {df(x)}{dx} = \lim_{h\rightarrow0} \frac {f(x+h)-f(x)}{h}$$对于上述公式，当h的值非常小时，函数可以被一条直线近似，而导数就是这条直线的斜率。换句话说，每个变量的导数指明了整个表达式对于该变量的值的敏感程度。例如，若x=4,y=-3，则f(x,y)=-12,x的导数$\frac {\partial f}{\partial x}=-3$，这就说明将变量x的值变大一点，整个表达式的值就会变小，而且变小的量是x变大的量的三倍。 如上所述，梯度$\nabla f$是偏导数的向量，所以有$\nabla f(x)=[\frac {\partial f}{\partial x},\frac {\partial f}{\partial y}] = [y,x]$。我们可以对加法操作进行求导：$$f(x,y)=x+y \rightarrow \frac {df}{dx}=1 \frac {df}{dy}=1$$这就是说，无论其值如何，x,y的导数均为1。这是有道理的，因为无论增加x,y中任一个的值，函数f的值都会增加，并且增加的变化率独立于x,y的具体值（情况和乘法操作不同）。取最大值操作也是常常使用的：$$f(x,y)=max(x,y)\rightarrow \frac {df}{dx}=1(x&gt;=y) \frac {df}{dy}=1(y&gt;=x)$$上式是说，如果该变量比另一个变量大，那么梯度是1，反之为0。例如，若x=4,y=2，那么max是4，所以函数对于y就不敏感。也就是说，在y上增加h，函数还是输出为4，所以梯度是0：因为对于函数输出是没有效果的。当然，如果给y增加一个很大的量，比如大于2，那么函数f的值就变化了，但是导数并没有指明输入量有巨大变化情况对于函数的效果，他们只适用于输入量变化极小时的情况，因为定义已经指明：$lim_{h\to 0}$。 使用链式法则计算复杂表达式的导数现在考虑更复杂的包含多个函数的复合函数，比如$f(x,y,z)=(x+y)z$。虽然，这个表达式足够简单，可以直接进行微分，但是在此使用一种有助于直观理解反向传播的算法。将公式分为两部分：$q=x+y,f=qz$。在前面已经介绍过如何对这分开的两个公式进行计算导数：$$\frac {\partial f}{\partial q} = z,\frac {\partial f}{\partial z}=q$$因为，q=x+y，所以，$$\frac {\partial q}{\partial x}=1,\frac {\partial q}{\partial y}=1$$然而，并不需要关心中间量q的梯度，因为$\frac {\partial f}{\partial q}$没有用。相反，函数f关于x、y、z的梯度才是需要关注的。链式法则指出将这些梯度表达式链接起来的正确方式是相乘，比如$\frac {\partial f}{\partial x}=\frac {\partial f}{\partial q} \frac {\partial q}{\partial x}$在实际的操作中，只是简单地将两个梯度数值相乘。最后得到变量的梯度[dfdx, dfdy, dfdz]，它们告诉我们函数f对于变量[x, y, z]的敏感程度。这是一个最简单的反向传播。一般会使用一个更简洁的表达符号，这样就不用写df了。这就是说，用dq来代替dfdq，且总是假设梯度是关于最终输出的。这次计算可以被可视化为如下计算线路的图像： 上图的真实值计算线路展示了计算的视觉化过程。前向传播从输入计算到输出（绿色），反向传播从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。 反向传播的直观理解反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输出并立即计算两个东西： 这个门的输出值； 其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算路线中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。 这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等等。 下面通过例子来对这一过程进行理解。加法门收到了输入[-2, 5]，计算输出是3。既然这个门是加法操作，那么对于两个输入的局部梯度都是+1。网络的其余部分计算出的最终值为-12。在反向传播时将递归地使用链式法则，算到加法门的时候，知道加法门的输出梯度是-4。如果网络想要输出值更高，那么可以认为它会想要加法门的输出更小一点，而且还有一个4的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让-4乘以x和y的局部梯度，x和y的局部梯度都是1，所以最终都是-4）。可以看到得到了想要的效果：如果x，y减小（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。 因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高。 模块化：Sigmoid例子上面介绍的门是相对随意的。任何可微分的函数都可以看做门。可以将多个门组合成一个门，也可以根据需求将一个函数拆成多个门。现在看一个表达式：$$f(w, x) = \frac {1}{1+e^{-(w_0x_0+w_1x_1+w_2}}$$在后面的课程中可以看到，这个表达式描述了一个含输入x和权重w的2维的神经元，该神经元使用了sigmoid激活函数。但是现在只是看做是一个简单的输入为x和w，输出为一个数字的函数。这个函数是由多个门组成的。除了上文介绍的加法门，乘法门，取最大值门，还有下面这4种：$$f(x) = \frac {1}{x} \rightarrow \frac{df}{dx} = - \frac {1}{x^2}\\\\f_c(x) = c+x \rightarrow \frac{df}{dx} = 1 \\\\f(x) = e^x \rightarrow \frac{df}{dx} = e^x\\\\f_a(x) = ax \rightarrow \frac{df}{dx} = a\\\\$$其中，函数$f_c$使用对输入值进行了常量c的平移，$f_a$将输入值扩大了常量a倍。它们是加法和乘法的特例，但是这里将其看做一元门单元，因为确实需要计算常量c，a的梯度，整个计算的线路如下： 在上面的例子中可以看见一个函数操作的长链条，链条上的门都对w和x的点积结果进行操作。该函数被称作为sigmoid函数，sigmoid函数关于其输入的求导是可以简化的：$$\sigma(x) = \frac {1}{1+e^{-x}}\\\\\frac {d\sigma(x)}{dx} = \frac {e^{-x}}{(1+e^{-x})^2}=(\frac {1+e^{-x}-1}{1+e^{-x}})(\frac {1}{1+e^{-x}}) = (1-\sigma(x))\sigma(x)$$可以看到梯度计算简单了很多。举个例子，sigmoid表达式输入为1.0，则在前向传播中计算出输出为0.73。根据上面的公式，局部梯度为(1-0.73)*0.73~=0.2，和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。 反向传播实践：分段计算看另外一个例子，假设有如下函数：$$f(x,y) = \frac {x+\sigma(y)}{\sigma(x)+(x+y)^2}$$首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子，需要强调的是，如果对x或y进行微分运算，运算结束后会得到一个巨大而复杂的表达式。然而做如此复杂的运算实际上并无必要，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码模式：1234567891011x = 3 # 例子数值y = -4# 前向传播sigy = 1.0 / (1 + math.exp(-y)) # 分子中的sigmoi #(1)num = x + sigy # 分子 #(2)sigx = 1.0 / (1 + math.exp(-x)) # 分母中的sigmoid #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # 分母 #(6)invden = 1.0 / den #(7)f = num * invden #(8) 到了表达式最后，就完成了前向传播。注意在构建代码s时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。这样计算反向传播就简单了：我们对前向传播时产生每个变量(sigy, num, sigx, xpy, xpysqr, den, invden)进行回传。我们会有同样数量的变量，但是都以d开头，用来存储对应变量的梯度。注意在反向传播的每一小块中都将包含了表达式的局部梯度，然后根据使用链式法则乘以上游梯度。对于每行代码，我们将指明其对应的是前向传播的哪部分。123456789101112131415161718192021# 回传 f = num * invdendnum = invden # 分子的梯度 #(8)dinvden = num #(8)# 回传 invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# 回传 den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# 回传 xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# 回传 xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# 回传 sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# 回传 num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# 回传 sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# 完成! 需要注意的一些事情：对前向传播变量进行缓存：计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际的操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播时也能用上。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。 在不同分支的梯度要相加：如果变量x、y在前向传播的表达式中出现多次，那么进行反向传播时要非常小心使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。 回传流中的模式一个有趣的现象是在多数情况下，反向传播中的梯度可以被很直观的解释。例如，神经网络中最常用的加法、乘法和取最大值的这三个门单元，它们在反向传播过程中的行为都非常简单的解释，先看下面的这个例子： 一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。从此例可知： 加法门单元：把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门就把梯度2.0不变且相等地路由给了两个输入。 取最大值门单元：对梯度做路由，和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余是0。上例中，取最大值门将梯度2.0转给类z变量，因为z的值比w高，于是w的梯度保持为0。 乘法门单元：相对不容易解释，它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.0*2.0 = -8.0。 非直观影响及其结果。注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，那么乘法门的操作将会不是那么直观：它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。在线性分类器中，权重和输入是进行点积$w^Tx_i$，这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本$x_i$乘以1000，那么权重的梯度将会增大1000倍，这样就必须降低学习率来弥补。这就是为什么数据预处理关系重大，它即使只是有微小变化，也会产生巨大影响。对于梯度在计算线路中是如何流动的有一个直观的理解，可以帮助读者调试网络。 小结 对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。 讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(4) Optimization Note]]></title>
    <url>%2F2017%2F12%2F04%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC4%E8%AF%BE-Optimization%2F</url>
    <content type="text"><![CDATA[本文转载自：https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit，原文：http://cs231n.github.io/optimization-1/ 简介图像分类任务中的两个关键部分： 基于参数的评分函数。该函数将原始图像像素映射为分类评分值（例如，一个线性函数）。 损失函数。该函数能够根据评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。上节，线性函数的形式是$f(x_i,W)=Wx_i$，而SVM实现的公式是：$$L=\frac {1}{N}\sum_i\sum_{j \neq y_i} [max(0,f(x_i;W)_j - f(x_i;W)_{y_i}+1)]+\alpha R(W)$$对于图像数据$x_i$，如果基于参数集W做出的分类预测与真实情况比较一致，那么计算出来的损失值L就很低。现在介绍第三个，也是最后一个关键部分：最优化Optimization。最优化是寻找能使得损失函数值最小化的参数W的过程。损失函数可视化课程讨论的损失函数一般都是定义在高维度的空间中，这样要将其进行可视化就很困难。然而办法还是有的，在1个维度或者2个维度的方向上对高维空间进行切片，就能够得到一些直观的感受。例如，随机生成一个权重矩阵W，该矩阵就与高维空间中的一个点对应。然后，沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向$W_1$并且沿着某个维度方向计算损失值，计算方法是根据不同的a值来计算$L(W+aW_1)$。这个过程将生成一个图标，x轴为a值，y轴为损失函数值。同样的方法还可以用在两个维度上，通过改变a、b来计算损失函数值$L(W+aW_1+bW_2)$，从而给出二维的图像。在图像中，a、b可以分别用x轴和y轴表示，而损失函数的值可以用颜色变化表示： 一个无正则化的多类SVM的损失函数的图示。左边和中间只有一个样本数据，右边是CIFAR-10中的100个数据。左：a值变化在某个维度方向上对应的的损失值变化。中和右：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。注意损失函数的分段线性结构。多个样本的损失值是总体的平均值，所以右边的碗状结构是很多的分段线性结构的平均（比如中间这个就是其中之一）。 可以通过数学公式来解释损失函数的分段线性结构，对于一个单独的数据，有损失函数的计算公式如下：$$L_i = \sum_{j \neq y_i}[max(0, W_j^Tx_i - W_{y_i}^Tx_i + 1)]$$通过公式可见，每个样本的数据损失值是以W为参数的线性函数的总和（0阈值来源于max(0,-)函数）。W的每一行（即$w_j$）,有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有3个只有1个维度的点，数据集数据点有3个类别。那么完整的无正则化SVM的损失值计算如下：$$L_0 = max(0, w_1^Tx_0 - w_0^Tx_0 + 1)+max(0, w_2^Tx_0 - w_0^Tx_0 + 1)\\\\L_1 = max(0, w_0^Tx_1 - w_1^Tx_1 + 1)+max(0, w_2^Tx_1 - w_1^Tx_1 + 1)\\\\L_2 = max(0, w_0^Tx_2 - w_2^Tx_2 + 1)+max(0, w_1^Tx_2 -w_2^Tx_2 + 1)\\\\L=(L_0+L_1+L_2)/3$$因为这些例子都是一维的，所以数据$x_i$和权重$w_j$都是数字。观察$w_0$，可以看到上面的式子中一些项是$w_0$的线性函数，且每一项都会与0比较，取两者的最大值。可作图如下： 从一个维度方向上对数据损失值的展示。x轴方向就是一个权重，y轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与0阈值的比较。完整的SVM数据损失就是这个形状的30730维版本。 最优化损失函数可以量化某个具体权重集W的质量，而最优化的目标就是找到能够最小化损失函数值的W。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于一些有经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM损失函数）是一个凸函数。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。策略1：随机搜索既然确认参数集W的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。蒙眼徒步者的比喻：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是30730维的（因为W是3073x10）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。 策略2：随机本地搜索第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机W开始，然后生成一个随机的扰动$\delta W$ ，只有当$W+\delta W$的损失值变低，我们才会更新。 策略3：梯度跟随前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的梯度（gradient）。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为倒数）。对一维函数的求导公式如下：$$\frac {df(x)}{dx} = \lim_{h\rightarrow0}\frac {f(x+h)-f(x)}{h}$$当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。 梯度计算计算梯度有两种方法：一个是缓慢的近似方法（数值梯度法），实现相对简单。另一个方法是（分析梯度法）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对这两种方法进行介绍：利用有限差值计算梯度上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数f和向量x，计算f的梯度的通用函数，它返回函数f在点x处的梯度：12345678910111213141516171819202122def eval_numerical_gradient(f, x): """ 一个f在x处的数值梯度法的简单实现 - f是只有一个参数的函数 - x是计算梯度的点 """ fx = f(x) # 在原点计算函数值 grad = np.zeros(x.shape) h = 0.00001 # 对x中所有的索引进行迭代 it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # 计算x+h处的函数值 ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # 增加h fxh = f(x) # 计算f(x + h) x[ix] = old_value # 存到前一个值中 (非常重要) # 计算偏导数 grad[ix] = (fxh - fx) / h # 坡度 it.iternext() # 到下个维度 return grad 根据上面的梯度公式，代码对所有维度进行迭代，在每个维度上产生一个很小的变化h，通过观察函数值变化，计算函数在该维度上的偏导数。最后，所有的梯度存储在变量grad中。 实践考虑：注意在数学公式中，h的取值是趋近于0的，然而在实际中， 用一个很小的数值就足够了。而不产生数值计算出错的理想前提下，使用尽可能小的h。还有，实际中用中心差值公式（centered difference formula）[f(x+h)-f(x-h)]/2h效果较好。 在扶梯度方向上更新：要注意我们是向着梯度df的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。步长的影响：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。选择步长（也叫作学习率）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。从某个具体的点W开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为学习率）将会是我们在调参中最重要的超参数之一。 微分分析计算梯度使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于h值是选取了一个很小的数值，但真正的梯度定义中h趋向0的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做梯度检查。用SVM的损失函数在某个数据点上的计算来举例：$$L_i = \sum_{j\neq y_i} [max(0, w_j^Tx_i - w_{y_i}^Tx_i)+\Delta]$$可以对函数进行微分，比如，对$w_{y_i}$进行微分得到：$$\nabla_{w_{y_i}}L_i = -(\sum_{j\neq y_i} 1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0))x_i$$其中1是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以x_i就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些$j\neq =y_i$行的梯度是：$$\nabla_{w_j}L_i=1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0)x_i$$一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。 梯度下降现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为梯度下降，他的普通版本是这样的：1234# 普通的梯度下降while True: weights_grad = evaluate_gradient(loss_fun, data, weights) weights += - step_size * weights_grad # 进行梯度更新 这个简单的循环在所有的神经网络核心库中都有。虽然也有其他实现最优化的方法（比如LBFGS），但是到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。 小批量数据梯度下降(Mini-batch gradient descent):在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的小批量（batches）数据。这个方法之所以效果不错，是因为训练集中的数据都是相关的。 小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为随机梯度下降（Stochastic Gradient Descent 简称SGD），有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据 比100次计算1个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。 小结 将损失函数比作了一个高维度的最优化地形，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。 提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。 函数的梯度给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是h，用来计算数值梯度）。 参数更新需要有技巧地设置步长。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。 讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用梯度检查来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(3) 线性分类器]]></title>
    <url>%2F2017%2F12%2F03%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC3%E8%AF%BE-Linear-Classification%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit，原文：http://cs231n.github.io/linear-classify/，并进行一定修改。 本节课主要介绍线性分类器相关的知识，并将其用于图像分类。 线性分类器简介评分函数（score function):它是原始图像数据到类别分值的映射。损失函数（loss function）：它是用来量化预测分类标签的得分与真实值之间的一致性。 从图像到标签分值的参数化映射该方法的第一步就是定义一个评分函数，这个函数将图像的像素值映射为各个类别的得分，得分的高低代表图像属于该类别的可能性高低。假设一个包含很多图像的训练集$x_i \in R^D，i=1,..,N$，每个图像都对应一个分类标签$y_i，i=1,…,K$。在CIFAR-10中，N=50000的训练集，每个图像有D=32*32*3=3072像素，而K=10。因为，图片被分为10个不同的类别。定义评分函数：$f:R^D\rightarrow R^K$，该函数是原始图像像素得到分类分值的映射。线性分类器定义一个线性映射：$$f(x_i,W,b)=Wx_i+b$$其中，参数W称为权重，b称为偏差向量，这是因为它影响输出数值，但是并不和原始数据$x_i$产生关联。同时，需要注意以下几点： 一个单独的矩阵乘法$Wx_i$就可以高效并行评估10个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是W的一个行向量。 注意我们认为输入数据$(x_i,y_i)$是给定且不可改变的，但参数W和b是可控制改变的。我们通过设置这些参数，使得计算出的分类分值情况和训练集中图像数据的真实类别标签相符。 该方法的一个优势是训练数据是用来学习到参数W和b的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。 意只需要做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比k-NN中将测试图像和所有训练数据做比较的方法快多了。 理解线性分类器线性分类器计算图像中3个颜色通道中所有像素的值与权重的矩阵乘，从而得到分类分值。根据我们对权重设置的值，对于图像中的某些位置的某些颜色，函数表现出喜好或者厌恶（根据每个权重的符号而定）。举个例子，可以想象“船”分类就是被大量的蓝色所包围（对应的就是水）。那么“船”分类器在蓝色通道上的权重就有很多的正权重（它们的出现提高了“船”分类的分值），而在绿色和红色通道上的权重为负的就比较多（它们的出现降低了“船”分类的分值）。 将图像看做高维度的点：既然图像被伸展成为一个高维度的列向量，那么我们可以把图像看做这个高维度空间的一个点。整个数据集就是一个点的集合，每个点都带有1个分类标签。既然定义每个分类类别的分值是权重和图像的矩阵乘法，那么每个分类类别的分数就是这个空间中的一个线性函数的函数值。我们没办法可视化3072维空间中的线性函数，但假设把这些维度挤压到二维，那么就可以看出线性分类器在做什么了： 在上图中，每个图像是一个点，有3个分类器，以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。从上面可以看到，W的每一行都是一个分类类别的分类器。对于这些数字的几何解释是：如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转。而偏差b，则允许分类器对应的直线平移。需要注意的是，如果没有偏差，无论权重如何，在$x_i=0$时分类分值始终为0。这样所有分类器的线都不得不穿过原点。偏差和权重的合并技巧：之前评分函数定义为：$$f(x_i,W,b)=Wx_i+b$$分开处理这两个参数有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时$x_i$向量就要增加一个维度，这个维度的数值是常量1，这就是默认的偏差维度的权重。这样新的公式就简化成下面这样：$f(x_i,W)=Wx_i$如下图所示，左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加1个含常量1的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的，通过右边这样做，我们只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵。 图像数据处理：在图像分类的例子中，图像上的每个像素可以看做一个特征，在实践中，对每个特征减去平均值来中心化数据是非常重要的。在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127, 127]之间了。下一个常见步骤是，让所有数值分布的区间变为[-1, 1]。零均值的中心化是很重要的。 损失函数损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值$\Delta$。假设，第i个数据中包含图像$x_i$的像素和代表正确类别的标签$y_i$。评分函数输入像素数据，然后通过公式$f(x_i,W)$来计算不同分类类别的分值。这里将分值简写为s。比如，针对第j个类别的得分就是第j个元素：$s_j=f(x_i,W)_j$。针对第i个数据的多类SVM的损失函数定义如下：$$L_i=\sum_{j \neq y_i} max(0, s_j - s_{y_i}+\Delta)$$举个例子，假设有3个类别，并且得到了分值s=[13,-7,11]。其中第一个类别是正确的类别，即$y_i=0$。同时，假设$\Delta=10$。上面的公式是将所有不正确分类($j\neq y_i$)加起来，所以我们会得到两个部分：$$L_i=max(0,-7-13+10) + max(0, 11-13+10)$$可以看到，第一个部分的结果是0，这一对类别分数和标签的损失值为0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10，而SVM只关心差距至少要大于10，更大差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13&gt;11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。那么，我们面对的是线性评分函数($f(x_i,W)=Wx_i$)，所以我们可以将损失函数稍微改写一下：$$L_i = \sum_{j \neq y_i} max(0, w_j^Tx_i-w_{y_i}^Tx_i+\Delta)$$其中，$w_j$是权重W的第j行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数f，这样做就不是必须的了。max(0,-)函数，它常被称为折页损失(hinge loss)，有时候会听到使用平方折页损失SVM（即L2-SVM）,它使用的是$max(0,-)^2$，将更强烈地惩罚过界的边界值。不使用平方的更标准的版本，但是在某些数据集中，平方折页损失会工作得更好。可以通过交叉验证来决定到底使用哪个。 多类SVM“想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出delta的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。 正则化上面的损失函数有一个问题，假设有一个数据集和权重集W能够正确分类每个数据。问题在于这个W并不唯一：可能有很多相似的W都能正确地分类所有的数据。一个简单的例子：如果W能够正确分类所有数据，即对于每个数据，损失值都是0。那么当$\lambda&gt;1$时，任何数乘$\lambda W$都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对W乘以2将使得差距变成30。 换句话说，我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚R(W)部分。最长用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：$$R(W)=\sum_k\sum_l W_{k,l}^2$$上面的表达式中，将W中所有元素平方求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数，它由两部分组成：数据损失，即所有样例的平均损失$L_i$，以及正则化损失。完整公式如下： 将其展开完整公式是：$$L = \frac {1}{N}\sum_i\sum_{j\neq y_i}[max(0,f(x_i;W)_j - f(x_i;W)y_i+\Delta)] + \lambda \sum_k\sum_l W_{k,l}^2$$其中，N是训练数据集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数$\lambda$来计算权重。该超参数无法简单确定，需要通过交叉验证来获取。正则化最好的性质就是对大数值权重进行惩罚，可以提升泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值由过大的影响。 举个例子，假设输入向量x=[1,1,1,1]，两个权重向量$w_1=[1,0,0,0]，w_2=[0.25,0.25,0.25,0.25]$。那么，$w_1^T=w_2^T=1$，两个权重向量都得到同样的内积，但是$w_1$的L2惩罚是1.0，而$w_2$的L2惩罚是0.25。因此，根据L2惩罚来看，$w_2$更好，因为它的正则化损失更小。从直观上来看，这是因为w_2的权重值更小且更分散。既然L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免过拟合。 需要注意的是，和权重不同， 偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此，通常只对权重W正则化，而不正则化偏差b。在实际操作中，可以发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当W=0的特殊情况下，才能得到损失值为0。 设置$\Delta$$\Delta$这个超参数在绝大多数情况下被设置为$\Delta=1.0$。超参数$\Delta$和$\lambda$看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重W的大小对于分类分值有直接影响：当我们将W中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如$\Delta=1$或$\Delta=100$）从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度$\lambda$来控制)。 Softmax分类器SVM是最常用的两个分类器之一，而另一个就是Softmax分类器，它的损失函数和SVM的损失函数不同。Softmax分类器可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出$f(x_i,W)$作为每个分类的评分。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释。在Softmax分类器中，函数映射保持不变$f(x_i;W)=Wx_i$,但将这些评分视为每个分类的未归一化的对数概率，并将hinge loss替换为交叉熵损失(cross-entropy loss)。公式如下：$$L_i=-log(\frac {e^{f_{y_i}}}{\sum_j e^{f_j}})$$在上式中，$f_j$表示评分向量f中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值$L_i$的均值与正则化损失R(W)之和。其中函数$f_j(z) = \frac {e^{z_j}}{\sum_k e^{z_k}}$被称作为Softmax函数：其输入值是一个向量，向量中的元素为任意实数的评分值，函数对其进行压缩，输出一个向量，其中每个元素在0到1之间，且所有的元素之和为1。信息理论视角：在”真实”分布p和评估分布q之间的较差熵定义如下：$$H(p,q)=-\sum_x p(x)logq(x)$$因此，Softmax分类器所做的就是最小化在估计分类概率和在“真实”分布之间的较差熵。在这个解释中，“真实”分布就是所有概率密度都分布在正确的类别上（比如：p=[0,…,1,…,0]在某个位置就有一个单独的1）。还有，既然交叉熵可以写成熵和相对熵$H(p,q)=H(p)+D_{KL}(p||q)$，并且delta函数p的熵是0，那么就能等价的看做是对两个分别之间的相对熵做最小化操作。换句话说，交叉熵损失函数“想要”预测分布的所有概率密度都在正确分类上。 概率论解释：先看下面的公式:$$P(y_i|x_i,W)=\frac {e^{f_{y_i}}}{\sum_j e^{f_j}}$$可以解释为是给定图像数据$x_i$，以W为参数，分配给正确分类标签$y_i$的归一化概率。为了理解这点，请回忆一下Softmax分类器将输出向量f中的评分值解释为没有归一化的对数概率。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行了归一化处理，使得这些概率的和为1。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE）。该解释的另一个好处是，损失函数中的正则化部分R(W)可以被看做是权重矩阵W的高斯先验，这里进行的是最大后验估计（MAP）而不是最大似然估计。 SVM 和 Softmax的比较下图有助于区分Softmax和SVM这两类分类器： 针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量f（本节中是通过矩阵乘来实现）。不同之处在于对f中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的对数概率，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。 Softmax分类器为每个分类提供了“可能性”：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的“可能性”是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在“可能性”上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：$$[1,-2,0]\rightarrow[e^1,e^{-2},e^0]=[2.71,0.14,1]\rightarrow[0.7,0.004,0.26]$$现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么Softmax函数的计算就是：$$[0.5,-1,0]\rightarrow[e^{0.5},e^{-1},e^0]=[1.65,0.73,1]\rightarrow[0.55,0.12,0.33]$$现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的置信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释。 在实际使用中，SVM和Softmax经常是相似的:通常来说，这两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加”局部目标化”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（$\Delta =1$）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。 对于Softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。 小结 与kNN分类器不同，参数方法的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重W进行一个矩阵乘法运算。 偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。 损失函数（SVM和Softmax线性分类器最常用的2个损失函数）。损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(2) 图像分类]]></title>
    <url>%2F2017%2F12%2F02%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC2%E8%AF%BE-Image-Calssification%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit，原文：http://cs231n.github.io/classification/，并根据原文添加了部分内容。 图像分类图像分类问题，就是对已有的固定的分类标签集合，对于输入的图像，从分类标签集合中找到一个分类标签，最后把分类标签分配给该输入图像。虽然看起来很简单，但是是计算机视觉领域的核心问题之一，并且有着广泛的应用。例如，以下图为例，图像分类模型读取该多，并生成该图片属于{cat,dog,hat,mug}中各个标签的概率。人类可以直接看到这个图像，但对于计算机来说，看到的确是由数字组成的巨大的3维数组。在此例中，图片的大小为248*400，有3个颜色通道，分别是红、绿和蓝。如此，图像就包含了248*400*3=297600个数字，每个数字都在0-255之间，图像分类的任务就是从这些数字中学习到知识，把该图像分类到“猫”。 困难和挑战：对于人类来说，从图片中识别“猫”非常简单，但是对于计算机而言却不是这么简单了。计算机视觉算法在图像识别方面遇到的一些困难： 视角变化（Viewpoint variation） 大小变化（Scale variation） 形变（Deformation) 遮挡（Occlusion) 光照条件（Illumination conditions) 背景干扰（Background clutter） 类内差异（Intra-class variation） 数据驱动方法（Data-driven approach）数据驱动方法类似和教小孩看图识物类似：给计算机很多数据，让计算机进行学习，从而进行分类。该方法的流程如下： 输入：输入包含N个图像的集合，每个图像的标签是k种分类标签中的一种。这个集合称为训练集。 学习：使用训练集来学习每个类到底长什么样。一般称为训练分类器或学习一个模型。 评价：让分类器来预测它未见过的图像的分类标签，以此评价分类器的好坏。分类正确的图像数量越多，则分类器的性能越好。 Nearest Neighbor分类器图像分类数据集：CIFAR-10，包含60000张32*32的小图像，数据集包含10中类别，60000张图片被分为包含50000张图片的训练集和包含10000张图片的测试集。下图就是10类的10张随机图片。 上图的左边是训练样本集，右边：第一列是测试图像，然后第一列的每个图像的右边是使用Nearest Neighbor算法，根据像素的差异，从训练样本集中选出的10张最类似的图片。 Nearest Neighbor算法，假设我们拿到包含50000张图片的训练集，对于一个要预测的图片，Nearest Neighbor算法会拿这张测试的图片和训练集中的每个图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。至于判断两张图片是否相似，以及最相似，在本例中，就是比较32*32*3的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。也就是说，将两张图片先转化为向量$I_1$和$I_2$，然后计算$L_1$距离：$$d_1(I_1,I_2)=\sum_p |I_1^p-I_2^p|$$以图片中的一个颜色通道为例来进行说明，两张图片使用$L_1$距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么$L_1$距离为0，但是如何两张图片很是不同，那么$L_1$值将会非常大。 距离选择：计算向量间的距离方法有很多种，另一个常用的方法为$L_2$距离，从几何的角度看，可以理解为计算两个向量间的欧式距离。 $L_1$和$L_2$比$L_2$的比较：在面对两个向量之间的差异时，$L_2$比$L_1$更不能容忍差异。也就是说，相对于一个巨大的差异，$L_2$距离更倾向于接受多个中等程度的差异。$L_1$和$L_2$都是在p-norm常用的特殊形式。 k-Nearest Neighbor分类器Nearest Neighbor算法使用最相似的1张图片作为最终的预测结果。k-Nearest Neighbor算法则是找到最相似的k张图片，然后让它们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的标签。当k=1时，K-Nearest Neighbor就变为Nearest Neighbor。从直观上可以看到，更高的k值可以让效果更平滑，使得分类器对于异常值更有抵抗力。 上图显示了Nearest Neighbor分类器和5-Nearest Neighbor分类器的区别。图中使用了2维的点来表示，分成3类。不同的颜色区域代表使用$L_2$距离的分类器的决策边界。白色的区域是分类模糊的例子（即图像与两个以上的分类标签绑定）。需要注意的是，在NN分类器中，异常的数据制造出一个不正确预测的孤岛。5—NN分类器将这些不规则都平滑了，使得针对测试数据的泛化能力更好。 用于超参数调优的验证集k-NN分类器需要设定k值，选择哪个k值最合适呢?同样也距离函数也是可选择的，那么选哪个好？这些选择，被称为超参数（hyperparameter)。在基于数据进行学习的机器学习算法设计中，朝参数时非常常见的，但是如何选择这些超参数？ 我们可能会尝试不同的值，看哪个值表现最好就选哪个。但这样做的时候要非常小心，特别注意：决不能使用测试集来进行调优。在训练机器学习模型时，应该把测试集看做非常宝贵的资源，不到最后一步，绝不使用它。如果使用测试集进行调优，而且算法看起来效果不错，但算法实际部署后，性能可能会远低于预期。这种情况，称之为过拟合。从另一个角度来说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实是过于乐观了，实际部署起来效果就会差很多。所以，最终测试的时候再使用测试集，可以很好地近似度量你所设计的分类器的泛化性能。 测试集只能使用一次，即在训练完成后评价最终的模型时使用。 实际在进行参数调优的过程中，是从训练集中抽取一部分数据用来调优，称之为验证集（validation set）。以CIFAP-10数据集为例，我们可以用49000个图像作为训练集，用1000个图像作为验证集。验证集其实就是作为假的测试集来调优。 把训练集分为训练集和验证集，使用验证集来对超参数进行调优，最后只在测试集上对模型进行评价。 交叉验证，有时候训练集较小，就会导致验证集的数量更小，人们会使用一种称为交叉验证的方法。这种方法把训练集评价分为K份，用其中的k-1份来训练，另1份来验证。然后循环着取其中的k-1份来训练，其中1份来验证。最后取所有k次验证的结果的平均值作为算法验证结果。在实际的应用中，一般会直接把训练集按照50%-90%的比例分为训练集和验证集。但这也是根据具体情况来定的：如果超参数数量较多，可能就想用更大的验证集，而验证集的数量不够，最好还是使用交叉验证。 Nearest Neighbor分类器的优劣 易于理解，实现简单； 算法的训练不需要花时间，其训练只需要和所有存储的训练图像进行比较（缺点）； 虽然训练花费很多时间，但是一旦训练完成，对新的测试数据进行分类非常快。 Nearset Neighbor分类器在某些特定的情况下，可能是不错的选择。但是在实际的图像分类工作中，很少使用。 实际应用K-NN的流程 预处理数据：对数据中的特征进行归一化，让其具有0均值和单位方差（unit variance)。 如果数据是高维数据，考虑使用降维方法。 将数据随机分为训练集和测试集。 在验证机上进行调优，尝试足够多的k值，尝试$L_1$和$L_2$两种距离。 如果分类器跑得太慢，尝试使用Approximate Nearest Neighbor库（比如FLANN）来加速这个过程，其代价是降低一些准确率。 对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上运行并再次训练呢？因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。在实践中，不要这样做。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。直接使用测试集来测试用最优参数设置好的最优模型，得到测试集数据的分类准确率，并以此作为你的kNN分类器在该数据上的性能表现。 小结 介绍了图像分类问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。 Nearest Neighbor分类器，分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。 选取超参数的正确方法是：将原始训练集分为训练集和验证集，我们在验证集上尝试不同的超参数，最后保留表现最好那个。 如果训练数据量不够，使用交叉验证方法，它能帮助我们在选取最优超参数的时候减少噪音。一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。 最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。 —end—]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《神经网络与深度学习》读书笔记]]></title>
    <url>%2F2017%2F11%2F30%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第一章 使用神经网络识别手写数字感知器 感知器在20世纪五、六十年代由科学家Frank Rosenblatt发明，其受到Warren McCulloch和Walter Pitts早期的工作影响。一个感知器接受几个二进制输入，$x_1,x_2,…,$,并产生一个二进制的输出： 示例中的感知器有三个输入，$x_1,x_2,x_3$,通常可以有更多或更少的输入，通过引入相应的权重，$w_1,w_2,…$表示相应输入对于输出的重要性的实数。神经元的输出，0或者是1，则由分配权重后的总和$\sum_{j}w_jx_j$小于或大于一些给定的阈值决定。和权重一样，阈值是一个实数，一个神经元的参数。可以用下面的形式来表示： 简化一下感知器的数学描述，用$w·x=\sum_{j}w_jx_j,b=-threshold$,则感知器的规则可以重写为： S型神经元单个感知器上一个权重或偏置的微小改动有时会引起那个感知器的输出完全翻转，如从0变到1。这样的翻可能接下来引起其余网络的行为以极其复杂的方式完全改变。因此，虽然有时可以正确对一类进行分类，但网络在其他图像的行为很可能以一些很难控制的方式被完全改变。这使得逐步修改权重和偏置来让网络接近期望行为变得很困难。 可以引入一种称为S型神经元来克服这个问题。S型神经元和感知器类似，但是被修改为权重和偏置的微小改动只引起输出的微小变化。这对于让神经元网络学习起来是很关键。S型神经元同样有多个输入，$x_1,x_2,…$，对应每个输入有权重，$w_1,w_2,…$和一个总的偏置，b。但是输出不是0和1，而是$\sigma(w·x+b)$,$\sigma$被称为S型函数，定义为：$$\sigma(z)=\frac {1}{1+e^{-z}}$$可以看到：当$z=w·x+b$为很大的正数时，S型神经元的输出近似为1；当$z=w·x+b$为很小的负数时，S型神经元的输出近似为0。这种行为与感知器很像，只有在$w·x+b$取中间值时，和感知器模型有比较大的偏离。利用S型神经元，会得到一个平滑的感知器。$\sigma$是平滑的，利用其特性，权重和偏置的微小变化会输出一个为微小的变化，不会像感知器那样变化剧烈。S型函数的形状如下： 神经网络的架构一个典型的神经网络架构如下图所示：主要包括：输入神经元、隐藏层和输出神经元三个部分。以上一层的输出作为下一层的输入，这种网络被称为前馈神经网络。这意味着网络中是没有回路的-信息总是向前传播，从不反向回馈。 使用梯度下降算法进行学习使用神经网络构建手写体识别分类算法，使用二次代价函数作为损失函数，训练神经网络找到最小化二次代价函数C(w,b)的权重和偏置。： 为什么使用二次代价，而不是直接最大化正确分类图像的数量？因为，在神经网络中，被正确分类的图像数量关联权重和偏置的函数并不是一个平滑的函数。大多数情况下，对权重和偏置做出的微小变动完全不会影响被正确分类的图像的数量。这会导致很难去解决如何改变权重和偏置来取得改进的性能。而用一个类似二次代价的平滑代价函数则能更好地解决如何用权重和偏置中的微小改变来取得更好的效果。梯度下降法假设我们要最小化某些函数，$C(v)$。它可以是任意的多元实值函数，$v=v_1,v_2,…$，用v代替w和b以强调它可能是任意的函数，并不局限于神经网络的环境。为了最小化$C(v)$，想象C是一个只有两个变量$v_1$和$v_2$的函数，如下图所示： 如果在$v_1$和$v_2$方向上分别改变很小的量，即$\Delta v_1$和$\Delta v_2$，微积分告诉我们$C$将会有如下变化：$$\Delta C \approx \frac {\partial C}{\partial v_1}\Delta v_1 + \frac {\partial C}{\partial v_2}\Delta v_2 $$因为，我们要最小化$C$，所以要寻找中选择$\delta v_1$和$\delta v_2$的方法，使得$\Delta C$为负；为了弄明白如何选择，需要定义$\delta v$为v变化的向量，$\Delta v= (\Delta v_1,\Delta v_2)^T$；定义$C$的梯度为偏导数的向量$\nabla C$:$$\nabla C =(\frac {\partial C}{\partial v_1},\frac {\partial C}{\partial v_2})^T$$那么，$$\Delta C \approx \nabla C · \Delta v$$这个表达式解释了为什么$\nabla C$被称为梯度向量：$\nabla C$把v的变化关联为$C$的变化，正如我们期望的用梯度来表示。如果选取：$$\Delta v=-\eta \nabla C$$那么，可以看到$\Delta C \approx -\eta \nabla C · \nabla C = -\eta ||\nabla C||^2$ 是永远小于等于0的，如果按照这种方式去改变v，那么$C$会一直减小，不会增加。因此，可以用如下方式更新$v$:$$v \rightarrow v^{new} = v - \eta \nabla C$$然后用它再次更新规则来计算下一次的更新，如果反复持续这样做，我们将持续减小C直到获得一个全局的最小值。如果$C$是一个具有更多变量的函数，也同样适用。梯度下降法就是通过这种方式重复改变$v$来找到函数$C$的最小值。如何在神经网络中使用梯度下降法去学习呢？其思想就是利用梯度下降算法去寻找能使得代价函数取得最小值的权重$w_k$和偏置$b_l$。我们将权重和偏置代替变量$v_j$，那么可以得到：$$w_k \rightarrow w_k^{new} = w_k - \eta \frac {\partial C}{\partial w_k}\\\\b_l = b_l^{new}=b_l - \eta \frac {\partial C}{\partial b_l}$$随机梯度下降，就是通过随机选取小量的训练输入样本来计算$\nabla C_x$,进而估计梯度$\nabla C$。通过计算少量的样本的平均值可以快速得到一个对于实际梯度$\nabla C$的很好的估算，有助于加速梯度下降，进而加速学习过程。批量梯度下降，随机选取小批量的数据，在这批数据上计算梯度，并更新参数。 第二章 反向传播算法是如何工作的反向传播算法的推导首先，定义神经网络中的权重，我们使用$w_{jk}^l$表示从(l-1)层的第k个神经元到l层的第j个神经元的连接上的权重。如下图所示：同时，我们使用$b_{j}^l$表示在第l层第j个神经元的偏置，使用$a_j^l$表示第l层第j个神经元的激活值，下图清楚地解释了这样表示的含义： 于是，第l层的第j个神经元的激活值$a_j^l$就和第(l-1)层的激活值通过下面的公式联系起来$$a_j^l = \sigma(\sum_k w_{jk}^la_k^{l-1} + b_j^l)$$对每一层l都定义一个权重矩阵$w^l$,表示连接到第l层神经元的权重，同时，每一层，定义一个偏置向量，$b^l$。于是，第l层的激活值向量$a^l$可以写成如下形式：$$a^l = \sigma(w^l a^{l-1} + b^l)$$这个表达式给出了一种更加全局的思考每层的激活值和前一层激活值的关联方式：我们仅仅用权重矩阵作用在激活值上，然后加上一个偏置向量，最后作用于$\sigma$函数。在计算$a^l$的过程中，我们计算了中间量$z^l = w^la^{l-1}+b^l$，我们称其为l层神经元的带权输入，这样$a^l = \sigma(z^l)$。其次，看一下神经网络代价函数的两个假设。上一章使用的是二次代价函数： 其中，n是训练样本总数；求和运算遍历了每个训练样本x；y=y(x)是对应的目标输出；L表示网络的层数；$a^L=a^L(x)$是当输入是x时网络输出的激活值向量。反向传播的目标是计算代价函数$C$分别关于w和b的偏导数，为了让反向传播可行需要作出以两个假设：第一个假设就是代价函数可以被写成一个在每个训练样本x上的点检函数$C_x$的均值$C=\frac {1}{n} \sum_x C_x$。需要这个假设的原因是反向传播实际上是对一个独立的训练样本计算了$\frac {\partial C_x}{\partial w}$和$\frac {\partial C_x}{\partial b}$。然后，我们通过在所有训练样本上进行平均化获得$\frac {\partial C}{\partial w}$和$\frac {\partial C}{\partial b}$。实际上，有了这个假设，我们会认为训练样本x已经被固定住了，丢掉了其下标，将代价函数$C_x$看做$C$。第二个假设就是代价函数可以写成神经网络输出的函数：最后，反向传播算法的推导。反向传播其实是对权重和偏置变化影响代价函数的过程的理解。最终极的含义其实就是计算偏导数$\frac {\partial C}{\partial w_{jk}^j}$和$\frac {\partial C}{\partial b_j^l}$。为了计算这些值，首先引入一个中间量$\delta_j^l$，称为在第l层第j个神经元上的误差。反向传播将给出计算误差$\delta_j^l$的流程，然后将其关联到计算$\frac {\partial C}{\partial w_{jk}^j}$和$\frac {\partial C}{\partial b_j^l}$上。反向传播的4个基本方程：(1) 输出层误差的方程，$\delta^L$,每个元素定义如下：$$\delta_j^L = \frac {\partial C}{\partial a_j^L}\sigma^\prime{(z_j^L)} (BP1)$$(2) 使用下一层的误差$\delta^{l+1}$ 来表示当前的误差$\delta^l$:$$\delta ^l = ((w^{l+1})^T \delta^{l+1})\bigodot\sigma^\prime(z^l) (BP2)$$其中，$(w^{l+1})^T$是第l+1层的权重矩阵$w^{l+1}$的转置。(3) 代价函数关于网络中任意偏置的改变率：$$\frac {\partial C}{\partial b_j^l} = \delta_j^l (BP3)$$(4) 代价函数关于任何一个权重的改变率：$$\frac {\partial C}{\partial w_{jk}^l} = a_k^{l-1}\delta_j^l (BP4)$$现在证明这四个基本的方程，所有的这些都是多元微积分的链式法则的推论。首先，从方程(BP1)开始，它给出了输出误差$\delta^L$的表达式。首先有如下的定义：$$\delta_j^L=\frac {\partial C}{\partial z_j^L}$$表示L层上第j个神经元的误差，应用链式法则，可以用输出激活值的偏导数的形式重新表示上面的偏导数：$$\delta_j^L=\sum_k \frac {\partial C}{\partial a_k^L}\frac {\partial a_k^L}{\partial z_j^L}$$这里求和是在输出层的所有神经元k上运行的。当然，第k个神经元的输出激活值$a_k^L$只依赖于当k=j时第j个神经元的输入权重$z_j^L$。所以，当k不等于j时$\partial a_k^L/\partial z_j^L$消失了。结果我们可以简化上一个方程为：$$\delta_j^L=\frac {\partial C}{\partial a_j^L}\frac {\partial a_j^L}{\partial z_j^L}$$于是，由$a_j^L=\sigma(z_j^L)$,上式右边的第二项可以写成$\sigma^\prime(z_j^L)$，方程就变成：$$\delta_j^L = \frac {\partial C}{\partial a_j^L}\sigma^\prime{(z_j^L)}$$下一步，证明（BP2），它给出了以下一层误差$\delta ^{l+1}$的形式表示误差$\delta^l$。为此，我们想要以$\delta_k^{l+1}=\partial C/ \partial z_k^{l+1}$的形式重写$\delta_j^l=\partial C/\partial z_j^l$。我们可以应用链式法则：$$\delta_j^l = \frac {\partial C}{\partial z_j^l}=\sum_k \frac {\partial C}{\partial z_k^{l+1}} \frac {\partial z_k^{l+1}}{\partial z_j^l}=\sum_k \frac {\partial z_k^{l+1}}{\partial z_j^l}\delta_k^{l+1} $$这里，最后一行交换了右边两项，并用$\delta_k^{l+1}$的定义带入。为了对最后一样的最后一项进行求值，注意：$$z_k^{l+1}=\sum_j w_{kj}^{l+1}a_j^l = \sum_j w_{kj}^{l+1}\sigma(z_j^l)+b_k^{l+1}$$做微分，我们得到：$$\frac {\partial z_k^{l+1}}{\partial z_j^l}=w_{kj}^{l+1}\sigma^\prime(z_j^l)$$带入，就可得到：$$\sigma_j^l = \sum_k w_{kj}^{l+1}\sigma_k^{l+1}\sigma^\prime(z_j^l)$$总结一下 反向传播算法反向传播算法给出了一种计算代价函数梯度的方法。用算法描述出来就是： 第三章 改进神经网络的学习方法交叉熵代价函数使用sigmod作为激活函数，使用二次代价函数作为损失函数时，当神经元的输出接近1的时候，曲线变得相当平，$\sigma^\prime(z)$就很小，相应计算出的梯度也会非常小，学习的速度就会变慢。假设神经元的输出为$a=\sigma(z)$,其中，$z=\sum_j w_jx_j+b$是输入的带权和。我们定义这个神经元的交叉熵代价函数：$$C = -\frac {1}{n}\sum_x[yln a + (1-y)ln(1-a)]$$其中，n是训练样本总数，求和是在所有的训练输入x上进行的，y是对应的目标输出。交叉熵为何能够解释成一个代价函数？将交叉熵看做是代价函数有两点原因。第一，它是非负的，C&gt;0。第二，如果对于所有的训练输入x，神经元实际的输出接近目标值，那么交叉熵接近0，假设在这个例子中，y=0而a接近0，交叉熵的值为接近于0，反之y=1而a接近于1，交叉熵的值也接近于0，所以，在实际输出和目标输出之间的差距越少，最终的较差熵的值就越低。综上所述，交叉熵是非负的，在神经元达到很好的正确率的时候接近0。交叉熵代价函数有一个比二次代价函数更好的特性就是它避免了学习速度下降的问题。首先，看一下交叉熵函数关于权重的偏导数，将$a=\sigma(z)$带入交叉熵损失函数，应用链式法则，得到： 将结果合并一下，简化成： 最终，可以得到：可以看到，权重的学习的速度受到$\sigma(z)-y$，也就是输出中的误差的控制。更大的误差，更快的学习速度。特别地，这个代价函数还避免了像二次代价函数类似方程中$\sigma^\prime(z)$导致学习的缓慢。交叉熵代价函数的学习曲线：特别地，当我们使用二次代价函数时，学习在神经元犯了明显的错误时却比学习快接近真实值的时候缓慢；而使用交叉熵学习正是在神经元犯了明显错误的时候速度更快。特别地，当我们使用二次代价函数时，当神经元在接近正确的输出前犯了明显错误的时候，学习变得更加缓慢；而使用交叉熵，在神经元犯明显错误时学习得更快。这些现象不依赖于如何设置学习速率。### 过度拟合和规范化过度拟合是神经网络的一个主要问题。这在现代网络中特别正常，因为网络权重和偏置数量巨大。检测过度拟合的明显方法是使用上面的方法-跟踪测试数据集合的准确率随训练变化情况。如果我们看到测试数据上的准确率不再提升，那么我们就停止训练。严格地说，这其实并非是过度拟合的一个必要现象，因为测试集和训练集上的准确率可能会同时停止提升。当然，采用这样的策略是可以防止过度拟合的。使用validation_data而不是test_data来防止过度拟合，在每个迭代期的最后都计算在validation_data上的分类准确率。一旦分类准确率已经饱和，就停止训练。这个策略被称为early stopping。在实际应用中，我们不会立即知道什么时候准确率会饱和。相反，我们会一直训练直到我们确信准确率已经饱和。为何要使用validation_data来代替test_data防止过度拟合问题？实际上，这是一个更为一般的策略的一部分，这个一般的策略就是使用validation_data来衡量不同的超参数（如迭代次数、学习速率、最好的网络架构等等）的选择的效果。如果设置超参数是基于test_data的话，可能最终我们得到过度拟合于test_data的超参数，这些超参数符合test_data的特点，但是网络的性能并不能够泛化到其他的数据集合上。我们借助validation_data来克服这个问题，一旦获得了想要的超参数，最终我们就使用test_data进行准确率测量。规范化增加训练样本的数量是一种减轻过度拟合的方法。还有其他的一些方法能够减轻过度拟合，就是规范化，有时被称为权重衰减(weight decay)或者L2规范化。L2规范化的思想就是增加一个额外的项到代价函数上，这个项叫做规范化项。下面是规范化的较差熵:可以看出，规范化的效果是让网络倾向于学习小一点的权重，其他的东西都是一样的。大的权重只有能够给出代价函数第一项足够的提升时才被允许。换而言之，规范化可以当做一种寻找小的权重和最小化原始的代价函数之间的折中。这两部分之前相对的重要性就由$\lambda$的值来控制了：$\lambda$越小，就偏向于最小的原始代价函数，反之，倾向于小的权重。新的权重的学习规则就变成：这正和通常的梯度下降学习规则相同，除了通过一个因子$1-\frac {n\lambda}{n}$重新调整了权重$w_0$。这种调整有时被称为权重衰减，因为它使得权重变小。我们已经把规范化描述为一种减轻过度拟合和提高分类准确率的方法。实际上，这不是仅有的好处。实践表明，在使用不同的权重初始化进行多次MNIST网络训练的时候，我们发现无规范化的网络会偶然被限制住，明显困在了代价函数的局部最优值处。结果就是不同的运行会给出相差很大的结果。对比看来，规范化的网络能够提供更容易复制的结果。为什么会这样子呢？，从经验上来看，如果代价函数是无规范化的，那么权重向量的长度可能会增长，而其他的东西都保持一样。随着时间的推移，这会导致权重向量变得非常大。所以会使得权重向量卡在朝着更多还是更少的方向上变化，因为当长度很大的时候梯度下降带来的变化仅仅会引起在那个方向发生微小的变化。我们相信这个现象让我们的学习算法更难有效地探索权重空间，最终导致很难找到代价函数的最优值。为何规范化可以帮助减轻过度拟合？通常的说法是：小的权重在某种程度上，意味着更低的复杂性，也就对数据给出了一种更简单却更强大的解释，因此应该优先选择。L2规范化没有限制偏置，实际上可以对偏置进行限制，但在某种程度上，对不对偏置进行规范化其实就是一种习惯了。然而，需要注意的是，有一个大的偏置并不会向大的权重那样会让神经元对输入太过敏感。所以，我们不需要对大的偏置带来的学习训练数据的噪声太过担心。同时，允许大的偏置能够让网络更加灵活，因为，大的偏置让神经元更加容易饱和，这有时候是我们所要达到的效果。所以，我们通常不会对偏置进行规范化。规范化的其他技术除了L2外还有很多规范化技术，另外给出三种减轻过拟合的其他方法：L1规范化、弃权和人为增加训练样本。L1规范化：这个方法是在未规范化的代价函数上加上一个权重绝对值的和：$$C=C_0 + \frac{\lambda}{n} \sum_w |w|$$在L1规范化中，权重通过一个常量向0进行缩小。在L2规范化中，权重通过一个和w成比例的量进行缩小。所以，当一个特定的权重绝对值|w|很大时，L1规范化的权重缩小得要远比L2规范化要小的多、相反，当一个特定的权重绝对值|w|很小时，L1规范化的权重缩小得要比L2规范化大的多。最终的结果是：L1规范化倾向于聚集于网络的权重在相对少量的高重要度连接上，而其他权重就会被驱使向0接近。弃权(DropOut)是一种相当激进的技术。和L1、L2规范化不同，弃权技术并不依赖对代价函数的修改。而是，在弃权中，我们改变了网络本身。弃权技术在训练大规模深度网络时尤其有用，这样的网络中过度拟合问题经常特别突出。人为扩展训练数据### 权重初始化在创建了神经网络后，我们需要进行权重和偏置的初始化。之前的方式是根据独立高斯随机变量来选择权重和偏置，其被归一化为均值0，标准差1。假设我们使用一个有大量输入神经元的网络，比如说1000个。假设，我们已经使用归一化的高斯分布初始化了连接的第一个隐藏层的权重。现在我们将注意力集中在这一层的连接权重上，忽略网络的其他部分： 为了简化，假设我们使用训练输入x,其中一半的输入神经元的值为1，另一半为0。以下的论点更普遍适用，让我们考虑隐藏神经元输入的带权和$z=\sum_j w_jx_j+b$。其中，500个项消去了，因为对应的输入$x_j$为0。所有z是遍历总共501个归一化的高斯随机变量的和，包含500个权重项和额外的1个偏置项。因此，z本身是一个均值为0,标准差为22.4的高斯分布。z其实有一个非常宽的高斯分布，完全不是非常尖的形状： 尤其是，我们可以从上图看出|z|会变得非常大，如果这样，隐藏神经元的输出$\sigma(z)$就会接近1或0。也就表示我们的隐藏神经元会饱和。所以，当出现这样的情况时，在权重中进行微小的调整仅仅会给隐藏神经元的激活值带来极其微弱的改变。而这种微弱的改变也会影响网络中剩下的神经元，然后会带来相应的代价函数的改变。结果就是，这些权重在我们进行梯度下降算法时会学习得非常缓慢。我们可以进行更好地初始化，能够避免这种类型的饱和，最终避免学习速度的下降，假设我们有n个输入神经元，我们可以使用均值为0标准差为$1/\sqrt[]{n}$的高斯随机分布初始化这些权重。也就是说，我们会向下挤压高斯分布，让我们的神经元更不可能饱和。 第五章 深度神经网络为何很难训练当神经网络层数加深，先前的层可能学习的比较好，但是后面的层却停滞不变。实际上，我们发现，在深度神经网络中使用基于梯度下降的学习方法本身存在着内在的不稳定性。这种不稳定性使得先前或者后面的层的学习过程阻滞。 消失的梯度问题在某些深度神经网络中，在我们在隐藏层BP的时候梯度倾向于变小。这意味着在前面的隐藏层中的神经元学习速度要慢与后面的隐藏层。在很多神经网络中存在着更加根本的导致这个现象出现的原因。这个现象也被称作是消失的梯度问题（vanishing gradient problem） 更一般地说，在深度神经网络中的梯度是不稳定的，在前面的层中或会消失或会爆炸。这种不稳定性才是神经网络中基于梯度学习的根本问题。下图是一个有三层隐藏层的神经网络： 其中，$w_1,w_2,…$是权重,而$b_1,b_2,…$是偏置，C是某个代价函数。现在看一下关联于第一个隐藏层神经元梯度$\partial C/ \partial b_1$。下图给出了具体的表达式：为何会出现梯度消失：现在把梯度的整个表达式写下来： 除了最后一项，该表达是一系列形如$w_j\sigma^\prime(z_j)$的乘积。如果我们使用标准方法来初始化网络中的权重，那么会使用一个均值为0标准差为1的高斯分布。因此，所有的权重通常会满足$|w_j|&lt;1$。有了这些信息，我们就会发现$w_j\sigma^\prime(z_j)&lt;1/4$。并且在我们进行了所有这些项的乘积时，最终的结果肯定会指数级下降：项越多，乘积下降的越快。这样就能够解释消失的梯度问题。 不稳定的梯度问题：根本问题其实并非是消失的梯度问题或者爆炸的梯度问题，而是在前面的层上的梯度是来自后面的层上的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡。如果没有某种机制或者更加本质的保证来达成平衡，那网络就很容易不稳定了。简而言之，真实的问题就是神经网络受限于不稳定梯度的问题。所以，如果我们使用标准的基于梯度的学习算法，在网络中的不同层会出现按照不同学习速度学习的情况。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识图谱技术综述]]></title>
    <url>%2F2017%2F11%2F11%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[本文主要是对《知识图谱技术综述》 徐增林等人的这篇论文阅读时的一个整理和记录。知识图谱技术是AI技术的重要组成部分，建立的具有语义处理能力与开放互联能力的知识库，可在智能搜索、智能问答、个性化推荐等智能信息服务中产生应用价值。这篇论文全面介绍了知识图谱的定义、架构以及知识图谱的知识抽取、知识表示、知识融合、知识推理四大核心技术的研究进展。 思维导图 知识图谱的定义与架构知识图谱的定义知识图谱是Google用于增强其搜索引擎功能的知识库。本质上，知识图谱是一种揭示实体之间关系的语义网络，可以对现实世界的事物及其相互关系进行形式化地描述。现在知识图谱以被用来泛指各种大规模的知识库。三元组是知识图谱的一种通用表示方式，即G=(E,R,S)，其中： G：表示知识库 E：表示知识库中的所有实际的集合 R：表示知识库中的关系集合 S：表示知识库中的三元组集合，$S \subseteq E \times R \times S $ 三元组的基本形式主要包括实体1、关系、实体2和概念、属性、属性值等，实体是知识库中最基本的元素，不同的实体之间存在不同的关系。概念主要指集合、类别、对象类型、事物的种类，例如人物、地理等；属性主要指对象可能具有的属性、特征、特性、特点及参数，例如国籍、生日等；属性值主要指对象指定属性的值，例如中国、1988-09-08等。每个实体可以用一个全局唯一确定的ID来标识，每个属性-属性值对（atrribute-value pair，AVP）可用来刻画实体的内在特性，而关系可用来连接两个实体，刻画它们之间的关联。 从覆盖范围而言，知识图谱可以分为通用知识图谱和行业知识图谱。通用知识图谱注重广度，强调融合更多的实体，较行业知识图谱而言，其准确度不够高，并且受概念范围的影响，很难借助本体库对公理、规则以及约束条件的支持能力规范其实体、属性、实体间的关系等。通用的知识库主要用于智能搜索等领域。行业知识图谱通常需要依靠特定的行业的数据来构建，具有特定的行业意义。行业知识图谱中，实体的属性与数据模式往往比较丰富，需要考虑到不同的业务场景与使用人员。 知识图谱的架构知识图谱的架构主要包括自身的逻辑结构以及体系架构。 1）知识图谱的逻辑结构知识图谱在逻辑上可以分为模式层与数据层两个层次。数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。可选择图数据库作为存储介质，例如开源的Neo4j、Twitter的FlockDB、seones的GraphDB。模式层构建在数据层之上，主要是通过本体库来规范数据层的一系列事实表达。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度叫较小。 2）知识图谱的体系架构知识图谱的体系架构是指构建模式结构，如下图所示（图引自该论文） 知识图谱主要有自顶向下(top-down)与自底向上(bottom-up)两种构建方式。自顶向下指是先为知识图谱定义好本体与数据模式，再将实体加入到知识库。该构建方式需要利用一些现有的结构化知识库作为其基础的知识库，例如Freebase就是采用这种方式，它的绝大部分数据是从维基百科中得到的。自底向上指的是从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式。目前，大多数知识图谱都采用自底向上的方式进行构建，其中最典型的就是Google的Knowledge Vault。 大规模知识库案例随着语义web资源数量激增、大量的RDF数据被发布和共享、LOD(linked open data)等项目的全面展开，学术界和工业界的研究人员花费了大量的精力构建各种结构化知识库。主要包括开放链接知识库、行业知识库两类。 开放链接知识库在LOD项目的云图中FreeBase、Wikidata、DBpedia、YAGO这4个是比较重要的大规模知识库。它们中不仅包含大量的半结构化、非结构化数据，是知识图谱数据的重要来源。而且具有较高的领域覆盖面，与领域知识库存在大量的关系。 垂直行业知识库行业知识库也可以称为垂直型知识库，这类知识库的描述目标是特定的行业领域，通常需要依靠特定行业的数据才能构建，因此其描述范围极为有限。下面几个就是比较典型的垂直行业知识库。 IMDB，是一个关于电影演员、电影、电视节目、电视明星以及电影制作的治疗库。 MusicBrainz,是一个结构化的音乐维基百科，致力于收藏所有的音乐元数据，并向大众用户开放。 ConceptNet,是一个语义知识网络，主要由一系列代表概念的结点构成，这些概念将主要采用自然语言单词或短语的表达形式，通过相互连接建立语义联系。 知识图谱的关键技术大规模知识库的构建与应用需要多种智能信息处理技术的支持。主要包括以下技术： 知识抽取：从一些公开的半结构化、非结构化的数据中提取出实体、关系、属性等知识要素。 知识融合：消除实体、关系、属性等指称项与事实对象之间的奇异，形成高质量的知识库。 知识推理：在已有的知识库的基础上，进一步挖掘隐含的知识，从而丰富、扩展知识库。 知识表示：以某种方式表示知识 下面就分别介绍相关的技术。 知识抽取知识抽取主要是面向开放的链接数据，通过自动化的技术抽取出可用的知识单元，知识单元主要包括实体、关系以及属性3个知识要素，并以此为基础，形成一系列高质量的事实表达，为上层模式层的构建奠定基础。 (1) 实体抽取实体抽取，也称为命名实体识别，指的是从原始预料自动识别出命名实体。实体抽取主要分为3种方法： 基于规则与词典的方法 基于统计机器学习的方法 面向开放域的抽取方法 (2) 关系抽取关系抽取的目标是解决实体间的语义链接问题，早期的关系抽取主要是通过人工构造语义规则以及模板的方法识别实体关系。随后，实体间的关系模型逐渐代替了人工预定义的语法与规则。但是，仍需要提前定义实体间的关系类型。后来出现面向开放域的信息抽取框架(open information extraction,OIE),这是抽取模式上一个巨大进步。但OIE方法在对实体的隐含关系抽取方面性能低下，因此，部分研究者提出基于马尔科夫逻辑网、基于本体推理的深层隐含关系抽取方法。 (3) 属性抽取属性抽取主要是针对实体而言的，通过属性可形成对实体的完整描述。由于实体的属性可以看成是实体与属性值之间的一种名称关系，因此可以将实体属性的抽取问题转换为关系抽取问题。大量的属性数据主要存在于半结构化、非结构化的大规模开放域数据集中。抽取这些属性的方法，一种是将上述从百科网站上抽取的结构化数据作为可用于属性抽取的训练集，然后再将该模型应用于开放域中的实体属性抽取；另一种，根据实体属性与属性之间的关系模式，直接从开放域数据集上抽取属性。但是，由于属性值附近普遍存在一些限定属性值定义的属性名等，所以该抽取方法的准确率并不高。 知识表示虽然，基于三元组的知识表示形式比较直观，但是其在计算效率、数据稀疏性等方面面临着诸多问题。以Deep Learning为代表的表示学习技术可以将实体的语义信息表示为稠密低维实值向量，进而在低维空间中高效计算实体、关系及其之间的复杂语义关联，对知识库的构建、推理、融合及应用均具有重要的意义。 (1) 应用场景分布式表示旨在用一个综合向量表示实体对象的语义信息，是一种模仿人脑工作的表示机制，通过知识表示而得到的分布式表示形式在知识图谱的计算、补全、推理等方面将起到重要作用： 1）语义相似度计算。由于实体通过分布式表示而形成的是一个个低维的实值向量，所以，可使用熵权系数法、余弦相似性等方法计算它们之间的相似性。这种相似性刻画了实体之间的语义关联程度，为自然语言处理等提供了极大的便利。 2）链接预测。通过分布式表示模型，可以预测图谱中任意两个实体之间的关系，以及实体间存在的关系的正确性。尤其是在大规模知识图谱上下文中，需要不断补充其中的实体关系，所以链接预测又被称为知识图谱的不全。 (2) 代表模型知识表示学习的代表模型主要包括距离模型、双线性模型、神经张量模型、矩阵分解模型、翻译模型(TransE模型)等。 (3) 复杂关系模型知识库中的实体关系类型也可以分为1-to-1、1-to-N、N-to-1、N-to-N四种类型，复杂的关系主要指的是1-to-N、N-to-1、N-to-N的3种关系类型。由于TransE模型不能用在处理复杂关系上，一系列基于它的扩展模型纷纷被提出，主要包括：TransH模型、TransR模型、TransD模型、TransG模型和KG2E模型。 (4) 多源信息融合三元组作为知识库的一种通用的表示形式，通过表示学习，能够以较为直接的方式表说实体、关系及其之间的复杂语义关联。然而，互联网中仍蕴含着大量与知识库实体、关系有关的信息未被考虑或有效利用，如何充分融合、利用这些多源异质的相关信息，将有利于进一步提升现有知识表示模型的区分能力以及性能。目前，多源异质信息融合模型方面的研究尚处于起步阶段，涉及的信息来源也极为有限。 知识融合由于知识图谱中的知识来源广泛，存在知识质量良莠不齐、来自不同数据源的知识重复、知识间的关联不够明确等问题，所以需要进行知识的融合。知识融合是高层次的知识组织，使来自不同的知识源的知识在同一框架规范下进行异构数据整合、消歧、加工、推理验证、更新等步骤，达到数据、信息、方法、经验以及人的思想的融合，形成高质量的知识库。 实体对齐实体对齐(entity alignment)，也称为实体匹配或实体解析，主要用于消除异构数据中实体冲突、指向不明等不一致性问题，可以从顶层创建一个大规模的统一知识库，从而帮助机器理解多源异质的数据，形成高质量的知识。 在大数据环境下，受知识库规模的影响，在进行知识库实体对齐时，主要会面临以下3个方面的挑战：（1）计算复杂度；（2）数据质量，不同知识库的构建目的与方式不同，可能存在知识质量良莠不齐、相似重复数据、孤立数据、数据时间粒度不一致等等问题。（3）先验训练数据，在大规模知识库中想要获得这种先验数据却非常困难。通常情况下，需要手工构造训练数据。 基于上述，知识库实体对齐的主要流程包括： 将待对齐数据分区索引，以降低计算的复杂度； 利用相似度函数或相似性计算算法查找匹配实例； 使用实体对齐算法进行实例融合； 将步骤2和3的结果结合起来，形成最终的对齐结果。 对齐算法可以分为成对实体对齐和集体实体对齐两大类，而集体实体对齐又可分为局部集体对齐和全局集体对齐。 （1）成对实体对齐基于传统概率模型的实体对齐方法，主要是考虑两个实体各自属性的相似性，而不考虑实体之间的关系。基于机器学习的实体对齐方法，主要是将实体对齐问题转化为二分类问题。根据是否使用标注数据可分为有监督学习与无监督学习两类。 （2）局部集体实体对齐方法局部集体实体对齐方法为实体本身的属性以及与它有关联的实体的属性分别设置不同的权重，并通过加权求和计算总体的相似度，还可使用向量空间模型以及余弦相似度来判别大规模知识库中的实体的相似程度。 （3）全局集体实体对齐方法 知识加工通过实体对齐，可以得到一系列的基本事实表达或初步的本体雏形，然而事实并不等于知识，它只是知识的基本单位。要形成高质量的知识，还需要经过知识加工的过程，从层次上形成一个大规模的知识体系，统一对知识进行管理。知识加工主要包括本体构建与质量评估两方面的内容。 （1）本体构建本体是同一领域内不同主体之间进行交流、连通的语义基础，其主要呈现树状结构，相邻的层次结点或概念之间具有严格的”IsA”关系，有利于进行约束、推理等，却不利于表达概念的多样性。本体在知识图谱中的地位相当于知识库的模具，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。 本体可以通过人工编辑的方式手动构建，也可以通过数据驱动自动构建，然后再经质量评估方法与人工审核相结合的方式加以修正和确认。数据驱动的本体自动构建过程主要可以分为以下3个阶段： 纵向概念间的并列关系计算。通过计算任意两个实体间并列关系的相似度，可辨析它们在语义层面是否属于同一个概念。计算方法主要包括模式匹配和分布相似度两种。 实体上下位关系抽取。上下位关系抽取方法包括基本语法的抽取与基本语义的抽取两种方式。 本体生成。对各个层次得到的概念进行聚类，并为每一类的实体指定1个或多个公共上位词。 （2）质量评估对知识库的质量评估任务通常是与实体对齐任务一起进行的，其意义在于，可以对知识的可信度进行量化，保留置信度较高的，舍弃置信度较低的，有效确保知识的质量。 知识更新人类的认知能力、知识储备以及企业需求都会随着时间而不断递增。因此，知识图谱的内容也需要与时俱进，不论是通用的知识图谱，还是行业知识图谱，它们都需要不断地迭代更新，扩展现有的知识，增加新的知识。 根据知识图谱的逻辑结构，其更新主要包括模式层的更新和数据层的更新。模式层的更新是指本体中元素的更新，包括概念的增加、修改、删除，概念属性的更新以及概念间上下位关系的更新。通常来说，模式层的增量更新方式消耗资源较少，但是多数情况下是在人工干预的情况下完成的。例如，需要人工定义规则，人工处理冲突等。数据层的更新指的是实体元素的更新，包括实体的增加、修改、删除，以及实体的基本信息和属性值。由于数据层的更新一般影响面较小，因此，通常以自动的方式完成。 知识推理知识推理则是在已有的知识库基础上进一步挖掘隐含的知识，从而丰富、扩展知识库。在推理的过程中，往往需要关联规则的支持。对于推理规则的挖掘，主要还是依赖于实体以及关系间的丰富同现情况。知识推理的对象可以是实体、实体的属性、实体间的关系、本体库中的概念的层次结构等。知识推理方法主要可以分为基于逻辑的推理和基于图的推理两种类别。 知识图谱的典型应用智能搜索基于知识图谱的智能搜索是一种基于长尾的搜索，搜索引擎以知识卡片的形式将搜索结果展现出来。用户的查询请求将经过查询式语义理解与知识检索两个阶段。 查询式语义理解。知识图谱对查询式的语义分析主要包括：（1）对查询请求文本进行分词、词性标注以及纠错；（2）描述归一化，使其与知识库中的相关知识进行匹配；（3）语境分析，在不同的语境下，用户查询式中的对象会有所差别，因此，知识图谱需要结合用户当时的情感，将用户此时需要的答案及时反馈给用户；（4）查询扩展，明确了用户的查询意图以及相关概念后，需要加入当前语境下的相关概念进行扩展。 知识检索。经过查询式语义分析后，标准查询语境进行知识库检索引擎，引擎会在知识库中检索相应的实体以及与其在类别、关系、相关性等方面匹配度较高的实体。通过对知识库的深层挖掘与提炼后，引擎将给出具有重要性排序的完整知识体系。 智能搜索引擎主要以3种形式展现知识： 集成的语义数据。例如，搜索梵高，搜索引擎将以知识卡片的形式给出梵高的生平，并配合图片等信息。 直接给出用户查询问题的答案。例如，用户搜索“姚明的身高是多少？”，搜索引擎的结果是“226cm”。 根据用户的查询给出推荐的列表等。 深度问答问答系统是信息检索系统的一种高级形式，能够以准确简洁的自然语言为用户提供问题的解答。多数问答系统更倾向于将给定的问题分解为多个小的问题，然后逐一去知识库中抽取匹配的答案，并自动检测其在时间与空间上的吻合度等，最后将答案进行合并，以直观的方式展现给用户。 社交网络知识图谱的挑战(1)知识获取(2)知识表示 复制关系中的知识表示 多源信息融合中的知识表示 (3)知识融合 并行与分布式算法 众包算法 跨语言知识对齐 (4)知识应用]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采样方法]]></title>
    <url>%2F2017%2F11%2F04%2F%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在前两周组内的技术分享中，分享了采样方法。之前在研究生阶段就对采样方法很是疑惑，特别是在看LDA时，用到的Gibbs采样，很多次尝试去学习这一知识点，但都一知半解。所以，借这个机会认真学习一下采样方法相关的知识。本文主要是记录一下自己在学习采样方法时，对不同采样方法原理的理解，主要包括蒙特卡洛方法介绍和5中不同的采样方法。 蒙特卡洛方法首先说一下蒙特卡洛方法，Monte Carlo方法，又称为统计模拟法、随机抽样技术，是一种随机模型方法，以概率和统计理论为基础的一种计算方法。是使用随机数（或伪随机数）来解决很多复杂问题的计算方法。其核心就是通过将所要求解的问题同一定的概率模型相联系，利用计算机进行模拟或抽样，以获得问题的近似解。简单地理解蒙特卡洛方法，其实就是将要解决的采用一定的方式进行转换，转换之后的问题通过利用计算机实现统计模拟或抽样，从而获得问题是近似解。至于为什么叫Monte Carlo方法，可能是和闻名世界的赌城——摩纳哥的一个小山城有关吧！具体是什么原因，不必去深究。 来看一个典型的用Monte Carlo方法解决实际问题的例子。计算圆周率Π的值，如果采用Monte Carlo方法进行计算，可以进行如下的转换： 首先，设置一个边长为1的正方形，其内有一圆，圆的半径为0.5，如下图所示： 随机的向正方形内打点，打的点在圆内的概率等于圆与正方形的面积之比0.25Π 随机产生M个点(x,y)，其中，x和y都是区间[0,1]内的符合均匀分布的随机数 假设落在圆内的点有N个，当M足够大时，根据大数定理，频率等于概率，就有：$$\frac {N} {M} = 0.25Π$$，$$Π=\frac {4N} {M}$$ 实际用代码去模拟一下，得出的圆周率的值为：3.143748，和3.1415926非常接近。 12345678def samplePI(maxCnt = 1000000):accCnt = 0for i in range(maxCnt): x = np.random.random() y = np.random.random() if np.sqrt(x**2 + y**2) &lt; 1: accCnt += 1print "PI=", float(accCnt) / maxCnt *4 通过一个简单的例子，可以直观地理解Monte Carlo方法。采样Monte Carlo方法去解决实际问题一般可以分为如下的步骤： 对复杂的问题进行转换，构造或描述随机过程。例如，将计算圆周率的问题，转换为在一个正方形内进行打点，将圆周率的值和点在圆内的概率联系起来。 从已知的概率分布中进行采样,采样出符合特定分布的样本。例如，从符合[0,1]均匀分布中采样出x和y。 建立估计量进行计算。 Monte Carlo方法是一种通用的计算技术，可以解决如下问题: 随机模拟：从一个pdf产生”典型”的样本 计算积分：在高维空间中的积分 优化问题 学习：MLE:f(x;Θ) 利用Monte Carlo方法去解决实际问题的第2步是从符合特定分布中采样出样本。我们知道，计算机本身无法产生真正的随机数，但可以根据一定的算法产生伪随机数。比如，通过线性同余发生器可以生成伪随机数，我们可以用确定性的算法生成[0,1]之间符合均匀分布的随机数，可以被当成真正的随机数使用。而至于产生符合比较复杂分布的随机数，则是以均匀分布为基础进行采样，以获得符合特定复杂分布的样本，这就是采样方法。不同的采样方法，采样过程不同，但大都基于均匀分布来进行的。下面就分别介绍，几种不同的采样方法。 Inverse Transform Sampling在介绍不同的采样方法之前，先说一下概率分布，概率分布一般分为连续型和离散型两种，离散型的概率分布一般用概率质量分布函数(pmf)表示，而连续分布用概率密度(pdf)表示,对pmf进行累加或对pdf进行积分的函数，对应于累积分布函数(cdf)。所有pmf的取值之和为1，对pdf在其定义域上进行积分，积分的值为1。对于一些简单的分布p(x),我们可以直接进行采样，比如，采样符合p(x)=[0.1,0.3,0.5,0.1]分布的样本 $x=x_1,x_2,x_3,x_4$分布，我们可以直接从Uniform(0,1)采样出一个数，x小于0.1，则采样的值为$x_1$,x等于0.1小于0.4则为$x_2$，以此进行类推。对于比较复杂的分布p(x)，不能直接进行采样。如果要采样的目标分布为p(x),它的累积分布函数F(x)能够求出来，F(x)的反函数也能求出来，那么，Inverse Transform Sampling的采样过程如下：123Inverse Transform Sampling1. 从Uniform(0,1)中随机采样一个点，用u表示。2. 计算$F^-1(u)$的值为x,则x就是服从p(x)的分布的一个样本点。 简单的证明一下：设F(x)为目标采样分布P(x)的累积分布，$x=F^{-1}(u) u\in[0,1]$为F(x)的反函数。因为F(x)是单调递增的(累积函数的性质)，所以$x=F^{-1}(u)$也是单调递增函数，对于如下不等式:$$F^{-1}(u)\leq F^{-1}(F(x)), if u \leq F(x) （1）$$根据反函数的定义有：$$F^{-1}(u) \leq x, if u \leq F(x) （2）$$根据Uniform(0,1)的定义，其累积分布函数如下： 所以，采样出的点的$F^{-1}(u)$累积分布函数如下：$$P(F^{-1}(u) \leq x)=P(u \leq F(x))=H(F(x))=F(x)$$由此可见，采样出的点的累积分布函数为F(x),所以，为符合p(x)分布的样本。虽然，利用这种方法可以采样出符合特定分布的样本，但是这种采样方法存在一个缺陷，即，如果要采样的分布p(x)累积分布函数不能够求出来或者累积分布函数没有反函数，这种方法就失效了 Acceptance-Rejection Sampling对于采样的目标分布p(x),如果其比较复杂，可以采用Acceptance-Rejection Sampling接受-拒绝采样，来进行采样。这种采样方法不直接对p(x)进行采样，而是选择另一个分布q(x)，存在常数M，使得$p(x)\leq Mq(x),\forall x $,称为建议分布(Proposal Density),这个分布容易进行采样，通过对q(x)的采样，来实现对p(x)的采样。其具体的采样过程如下：1234Acceptance-Rejection Sampling的采样过程：1. 从q(x)中随机采样出一个样本点x02. 在从Uniform(0, Mq(x0))中采样出另一个样本点u3. 如果u&gt;p(x0),则拒绝x0,并且重复前面的步骤。否则接受x0为符合p(x)分布的样本 这种采样方法可以用计算圆周率的方法来进行理解。把正方形看做一个分布Mq(x),圆形看做要采样的目标分布q(x),通过对Mq(x)进行随机采样，即打点，如点果落在圆内，表明该点符合圆形这个分布，即可以做符合p(x)的样本。即，如下图所示的圆内的点都是符合圆形这个分布的点。 Acceptance-Rejection Sampling，通过对q(x)的采样，实现了对p(x)的采样。这种采样方法的接受率正比于$\frac {1} {M}$,等于p(x)下面的面积除以Mq(x)下面的面积。可以看出，只有当M尽可能的小时，采样出的点被接受的概率才会大，从而可以提升采样的效率，因此在使用该方法进行采样是M的选择比较重要。同时,可以看出，如果q(x)和p(x)的形相似时，M就会越小，接受率就会越高。然而，对于高维的目标采样分布，q(x)可能不容易寻找，且M也可能会很大，此时，接受率就会变小，采样效率会变差。 Importance SamplingImportance Sampling 这种采样方法，其并不是为了获得符合特定分布p(x)的样本，而是为了解决当p(x)不容易进行采样时，计算E[f(x)]，x符合p(x)分布，的问题。其可以进行如下的转换：首先，根据期望的定义，E[f(x)]的计算公式如下：$$E[f(x)] = \int_x f(x)p(x)dx$$因为，p(x)的样本不容易获取，Importance Sampling同样引入一个建议分布q(x)，比较容易获取符合q(x)分布的样本，进行如下转换（以下内容参考：http://blog.csdn.net/dark_scope/article/details/70992266）：$$\int_x f(x)p(x)dx = \int_x f(x)\frac {p(x)} {q(x)} q(x) dx= \int_x g(x)q(x)dx where g(x)=f(x) \frac {p(x)} {q(x)} = f(x)w(x)$$可以看出，通过上面的转化，可以将计算f(x)在p(x)下的期望转化为求g(x)在q(x)分布下的期望。其中，$w(x) = \frac {p(x)} {q(x)}$被称为Importance Weight。但是，有些时候p(x)也是很难计算的，更常见的情况是比较方便的计算$\hat p (x)$和$\hat q(x)$$$p(x) = \frac {\hat p(x) }{Z_p}$$$$p(x) = \frac {\hat p(x)}{Z_p}$$其中，$Z_{p/q}$是一个标准化项，可以看成是一个常数，是的$\hat p(x)$或者$\hat q(x)$等比例变化为一个概率分布，也可以理解为softmax里的分母。其中：$$Z_p=\int_x \hat p(x)dx$$$$Z_q=\int_x \hat q(x)dx$$在这种情况下，Importance Sampling 可以进行如下的转换：$$ \int_x f(x)p(x)dx=\int_x f(x)\frac {p(x)} {q(x)} q(x)dx\\\\ =\int_x f(x) \frac {\hat p(x)/Z_p} {\hat q(x)/Z_q} q(x)dx\\\\=\frac {Z_q}{Z_p} \int_x f(x) \frac {\hat p(x)}{\hat q(x)}\\\\=\frac {Z_q}{Z_p} \int_x \hat g(x)q(x)dx\\\\其中，\hat g(x) = f(x)\frac {\hat g(x)}{\hat q(x)}=f(x)\hat w(x)$$而$\frac {Z_q}{Z_p}$直接计算不太好计算，而它的倒数：$$\frac {Z_p}{Z_q}=\frac {1}{Z_q}\int_x \hat p(x)dx, Z_q=\frac {\hat q(x)} {q(x)} $$所以：$$\frac {Z_p}{Z_q}=\frac {\hat p(x)}{\hat q(x)}q(x)dx = \int_x \hat w(x)q(x)dx$$这样，假设能方便从q(x)进行采样，所以上式又被转换为一个Monte Carlo可解的问题，也就是说：$$\frac {Z_p}{Z_q}=\frac {1}{m} \sum_{i=1}^m \hat w(x_i), x_i符合q(x)分布。$$最终，求解E[f(x)]的问题可以转换为：$$E[f(x)]=\frac {1}{m} \sum_{i=1}^m \hat w(x_i)f(x_i), 其中，x_i为符合q(x)的样本\\\ \hat w(x_i)=\frac {\hat w(x_i)}{\sum_{i=1}^m \hat w(x_i)}$$所以，我们可以在不用知道q(x)确切值的情况下，就可以近似地计算得到E[f(x)]。其计算过程如下：Importance Sampling采样过程： 首先为p(x)找到一个建议分布q(x),q(x)比较容易采样。 然后从q(x)中采样出m个点x 带入$E[f(x)] = \frac {1}{m} \sum_{i=1}^m \hat w(x_i)f(x_i)$ 计算期望。虽然这种方法能够work,但是在高维空间里找到一个这样合适的q(x)非常难。即使有 Adaptive importance sampling 和 Sampling-Importance-Resampling的出现，要找到一个同时满足容易抽样并且和目标分布相似的建议分布，通常是不可能的！ MCMC: Markov Chain Monte CarloImportance Sampling和Acceptance-Rejection Sampling虽然能够实现对一些分布的采样，但是只有当选取的建议分布q(x)和要进行采样的目标分布p(x)很近似时才表现好，所以选取合适的q(x)是非常关键。当在高维空间进行采样，标准的采样方法会失败，对于Acceptance-Rejection Sampling,当目标分布的维数增高时，拒绝率会趋近于100%，采样的效率会很低。对于Importance Sampling，大多数的样本的权重值会趋近于0。对于高维复杂问题，可以用马尔科夫链（Markov Chain)产生一系列相关样本，实现对目标分布的采样。MCMC是一种用一定范围内的均匀分布的随机数对高维空间概率进行采样的通用技术，其基本思想是设计一个马尔科夫链，使得其稳定概率分布为要采样的目标分布$\pi(x)$ 首先来看一下马尔科夫链的定义及其平稳分布： 马尔科夫性质：某一时刻状态转移的概率只依赖于它前一个状态 定义：假设存在状态序列$… X_{t-2},X_{t-1},X_t,X_{t+1}…$,时刻t+1的状态的条件概率只依赖于t时刻的状态$x_t$,即：$$P(X_{t+1}|…X_{t-2},X_{t-1},X_t)=P(X_{t+1}|X_t)$$ 马尔科夫链：满足马尔科夫性质的随机过程 以天气变化来解释一下上面的定义，假设每天的天气是一个状态的话，状态转移可以看成是天气的变化，比如从晴天变成阴天、从阴天变成雨天等。马尔科夫性质讲的是，今天的天气情况只依赖于昨天的天气，和前天以及之前的天气状况没有任何关系。马尔科夫链可以看成每天的天气按照这个规律进行变化的一个过程。一个马尔科夫链可以由下面的公式定义： 一个马尔科夫链一般由三部分构成： 状态空间：可以理解为天气状况的所有情况 {阴天，晴天，雨天，…} 初始状态：可以理解为第一天的天气情况 状态转移矩阵：可以理解为所有由一种天气状况变为另一种天气状况的概率 马尔科夫链具有一个非常重要的性质：马尔科夫链的平稳分布。来看一个例子，假设一个国家的人口地域分布分为：农村、城镇和城市3种状态。每年人口流动情况如下图： 上图表示每种状态转移到另一种状态的概率。如果定义矩阵P，P的某一位置P(i,j)的值为P(j|i)，表示从状态i转换为状态j的概率，则根据上图可以得到马尔科夫链的状态转移矩阵为： 假设初始状态的人口地域分布为$\pi_0=[\pi_0(1),\pi_0(2),\pi_0(3)]$，每年人口按照状态转移矩阵P进行转移，n年后人口的地域分布为$\pi_n=\pi_{n-1}P=…=\pi P^n$。假设存在如下两种初始的人口分布: $\pi_0=[0.5,0.4,0.1]$ $\pi_0=[0.3,0.4,0.3]$ 按照状态转移矩阵进行转移一定年数后的人口分布情况分布如下图所示： 可以看出，尽管采用了不同的初始化状态，但最终的概率分布都趋近于一个稳定的概率分布[0.167,0.388,0.444]。可以看到，马尔科夫链模型的状态转移矩阵收敛到稳定概率分布和初始状态概率分布无关。也就是说，如果得到了稳定概率分布对应的马尔科夫链模型的状态转移矩阵，我们可以从任意的概率分布样本开始，带入马尔科夫链模型的状态转移矩阵，经过一系列的状态转移，最终样本的分布会趋近于稳定的概率分布。用数学的语言来定义一下马尔科夫链的收敛性质：如果一个非周期的马尔科夫链，其状态转移矩阵为P，并且它的任何两个状态之间是联通的，那么$\lim_{n \rightarrow +\infty} P_{ij}^n$ 与i无关，则有： $\lim_{n \rightarrow +\infty} P^n = \begin{bmatrix}{\pi(1)}&amp;{\pi(2)}&amp;{\cdots}&amp;{\pi(n)}\\\\{\pi(1)}&amp;{\pi(2)}&amp;{\cdots}&amp;{\pi(n)}\\\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\\\{\pi(1)}&amp;{\pi(2)}&amp;{\cdots}&amp;{\pi(n)}\\\\\end{bmatrix}$ $\pi(j)=\sum_{i=0}^\infty P_{ij}$，$p_{ij}$表示状态i转移到状态j的概率 $\pi$是方程$\pi P = \pi$的唯一非负解，其中，$$\pi=[\pi(1),\pi(2),….,\pi(j),…] \sum_{i=0}^\infty \pi(i)=1$$ 上面的性质中有如下的说明： 非周期的马尔科夫链：主要是指马尔科夫链的状态转化不是循环的，如果是循环的则永远不会收敛。我们一般遇到的马尔科夫链一般都是非周期性的。用数学方式可以表述为：对于任意某一状态i,d为集合${n | n\geq 1,P_{ij}^n &gt; 0}$的最大公约数为1，如果d=1，则该状态为非周期的。 任何两个状态是联通的：指从任意一个状态可以通过有限步到达其他的任意一个状态，不会出现条件概率为0导致不可达的情况。 马尔科夫链的状态可以是有限的，也可以是无限的。因此，可以用于连续概率分布和离散概率分布。 $\pi$通常称为马尔科夫链的平稳分布。 基于马尔科夫链的采样方法从马尔科夫链的收敛性质可以看出，如果我们能够得到某个平稳分布对应的马尔科夫链状态转移矩阵，就很容易采样出这个平稳分布的样本。假设任意初始化的概率分布为$\pi_0(x)$，经过第一轮转移后的概率分布是$\pi_0(x)$，经过i轮后的概率分布为$\pi_i(x)$。假设经过n轮后马尔科夫链收敛到平稳分布$\pi(x)$,即：$$\pi_n(x)=\pi_{n+1}(x)=…=\pi(x)$$那么经过n轮之后的，沿着状态转移矩阵进行转移得到的样本都是符合$\pi(x)$分布的样本。也就是说，如果从一个具体的初始状态$x_0$出发，沿着马尔科夫链按照状态转移矩阵进行跳转，假设n次跳转后收敛，那么得到一个转移状态序列$X_0,X-1,…,X_n,X_{n+1},….$，则$X_n,X_{n+1},…$都是符合平稳分布$\pi(x)$的样本。 基于马尔科夫链的采样过程如下： 输入： 状态转移矩阵P，设定状态转移次数$n_1$，需要采样的样本个数$n_2$ 从任意概率分布采样得到一个初始状态值$x_0$ for t=0 to $n_1+n_2 - 1$：从条件概率分布$P(x|x_t)$中采样出样本为$x_{t+1}$ 输出：样本$(x_{n_1},x_{n_1+1},…,x_{n_1+n_2})$即为符合平稳分布$\pi(x)$的样本。 解释一下上面的从条件概率分布$P(x|x_t)$中采样出样本为$x_{t+1}$。为什么要从$P(x|x_t)$中进行采样？因为，当前的状态为$x_t$，从当前状态$x_t$进行转移，可转移到的状态为状态空间集合，所以要以当前状态为条件向其他状态进行转移，所以要从$P(x|x_t)$中进行采样。例如，人口分布的例子，假设当前的状态是农村，那么从农村转移到农村、城镇和城市的概率分别为0.5、0.4和0.1，即$P(x|x_t)=[0.5,0.4,0.1]$，转移时要按照这个分布进行转移，即采样时要按照这个分布进行采样，使用均匀分布可以很容易实现。而状态转移为连续的情况，则$P(x|x_t)$则是一个具体的连续分布，通过对$P(x|x_t)$的采样，实现转移。可以看出，我们要采样的目标分布为$\pi(x)$,如果我们能够构建一个状态转移矩阵P，使得其马氏链上的平稳分布是$\pi(x)$，那么初始化一个状态，按照状态转移矩阵P进行转移，经过n次后收敛，第n+1次后的样本都是符合$\pi(x)$分布的。所以，MCMC采样方法的关键是构建状态转移矩阵P。如何构建这个状态转移矩阵P呢？首先看一下马尔科夫链的细致平稳条件：如果一个非周期的马尔科夫链状态转移矩阵为P和概率分布$\pi(x)$，对于所有的i，j满足：$$\pi(i)P(i,j)=\pi(j)P(j,i)$$其中，P(i,j)表示状态i转移到状态j的概率,则称$\pi(x)$是状态转移矩阵P的平稳分布。证明：$$\sum_{i=1}^\infty \pi(i)P(i,j)=\sum_{i=1}^\infty \pi(j)P(j,i)=\pi(j)\sum_{i=1}^\infty P(j,i) = \pi(j)$$写成矩阵的形式，即：$$\pi P=\pi$$由于$\pi$是$\pi P=\pi$的解，所以$\pi$是一个平稳分布。上式被称为细致平稳条件(detailed balance condition)。其实这个定理是显而易见的，因为细致平稳条件的物理含义就是对于任何两个状态i,j 从i转移出去到j而丢失的概率质量，恰好会被从j转移回i的概率质量补充回来，所以状态i上的概率质量$\pi(i)$是稳定的，从而$\pi(x)$是马尔科夫链的平稳分布。从细致平稳条件可以看出，只要找到可以使概率分布$\pi(x)$满足细致平稳分布的矩阵P即可。假设在进行采样之前已经存在一个状态转移矩阵Q，Q(i,j)表示从状态i转移到状态q的概率，也可以表示为Q(j|i),通常情况下：$$\pi(i)Q(i,j)\neq \pi(j)Q(j,i)$$也就是说不满足细致平稳条件。不过可以对其进行改造，使其满足细致平稳条件。具体的是引入$\alpha$使得上面的公式成立，即：$$\pi(i)Q(i,j)\alpha(i,j)=\pi(j)Q(j,i)\alpha(j,i)$$什么样的$\alpha$能够使上式成立呢？最简单的，按照对称性，可以取如下的值：$$\alpha(i,j)=\pi(j)Q(j,i)\\\\\alpha(j,i)=\pi(i)Q(j,i)$$在改造Q的过程中，引入的$\alpha(i,j)$称为接受率，取值在[0,1]之间，物理意义可以理解为在原来的马尔科夫链上，从状态i以Q(i,j)的概率转跳转到状态j的时候，我们以$\alpha(i,j)$的概率接受这个转移。这样就可以得到新的马尔科夫链的转移概率为$Q(i,j)\alpha(i,j)$。通过这种改造，就能够进行采样了。MCMC采样过程如下：MCMC采样算法 初始化马尔科夫链初始状态$X_0=x_0$ fo t=0,1,2,… do:2.1 在t时刻马尔科夫链状态为$X_t=x_t$，从$Q(x|x_t)$中采样出一个样本y2.2 从均匀分布中采样出u~Uniform(0,1)2.3 如果$u&lt;\alpha(x_t,y)=\pi(y)Q(x_t|y)$，则接受转移$x_{t+1}=y$2.4 否则，不接受转移，$x_{t+1}=x_t$ 上面的MCMC采样算法已经能够很好的工作了，但是它存在一个缺陷:马氏链在转移的过程中的接受率$\alpha(i,j)$可能偏小，这样采样过程中，马氏链不容易转移，一直处于原地，导致收敛到平稳分布的速度偏慢。假设$\alpha(i,j)=0.1,\alpha(j,i)=0.2$，假设在此时满足细致平稳条件，于是有：$$\pi(i)Q(i,j).0.1 = \pi(j)\alpha(j,i).0.2$$将上式两边同时扩大5倍，等式变为：$$\pi(i)Q(i,j).0.5 = \pi(j)\alpha(j,i).1$$可以看到细致平稳条件并没有被打破，而接受率变大了。因此，我们可以把细致平稳条件中的$\alpha(i,j)$和$\alpha(j,i)$同比例放大，使得两个数中较大的那个放大到1，这样就提高了采样中的跳转的接受率。于是$\alpha(i,j)$可以取下面的值:$$\alpha(i,j) = min{\frac {\pi(j)Q(j,i)} {\pi(i)Q(i,j)},1}$$这样，就完成了对MCMC采样算法的改造，这就是Metropolis-Hastings采样算法，其采样过程如下：Metropolis-Hastings采样算法 初始化马尔科夫链初始状态$X_0=x_0$ for t=0,1,2,… do:2.1 在t时刻马尔科夫链状态为$X_t=x_t$，从$Q(x|x_t)$中采样出一个样本y2.2 从均匀分布中采样出u~Uniform(0,1)2.3 如果$u&lt;\alpha(x_t,y)=min{\frac {\pi(y)Q(y,x_t)} {\pi(x_t)Q(x_t,y)},1}$，则接受转移$x_{t+1}=y$2.4 否则，不接受转移，$x_{t+1}=x_t$ Gibbs Sampling虽然MCMC采样和Metropolis-Hasting采样算法已经能够解决蒙特卡罗方法中需要的任意概率分布的样本的问题。但是还是存在一定的缺陷，首先是采样过程中要计算接受率，在高维时，计算量大，可能存在辛辛苦苦计算出的接受率，最终被拒绝，不跳转。并且由于接受率的原因导致算法的收敛时间变长。其次是，对于高维空间，状态的条件概率分布好求解，但是联合分布不好求。针对这一问题，对于高维空间的数据采样，Stuart Geman和Donald Geman这两兄弟于1984年提出来了Gibbs Sampling算法。Gibbs Sampling算法思想是通过构建状态转移矩阵，使得接受率为1，从而提升了接受的效率。对于二维情形，假设存在一个概率分布P(x,y)，对于x坐标相同的两个点$A(x_1,y_1),B(x_1,y_2)$可以得到如下公式：$$p(x_1,y_1)p(y_2|x_1) = p(x_1)p(y_1|x_1)p(y_2|x_1)\\\\p(x_1,y2)p(y_1|x_1)=p(x_1)p(y_2|x_1)p(y_1|x_1)$$上面的转换是利用乘法公式进行转换的。于是可以得到：$$p(x_1,y_1)p(y_2|x_1) = p(x_1,y_2)p(y_1,x_1)\\\\p(A)p(y_2|x_1) = p(B)p(y_1|x_1)$$可以看到，在$x=x_1$这条直线上，如果使用条件概率分布$p(y|x_1)$作为任意两点转移概率，那么任何两点之间的转移概率满足细致平稳条件。同样，在$y=y_1$这条直线上任取两点$A(x_1,y_1),C(x_2,y_1)$，同样有：$$p(A)p(x_2|y_1)=p(C)p(x_1|y_1)$$于是可以构造平面上任意两点之间的转移概率矩阵Q:$$Q(A-&gt;B)=p(y_B|x_1), if x_A=x_B=x_1\\\\Q(A-&gt;C)=P(x_C|y_1), if y_A=y_C=y_1\\\\Q(A-&gt;D)=0，其他$$则对于平面上任意两点X,Y,很容易验证是否满足细致平稳条件：$$p(X)Q(X-&gt;Y)=P(Y)Q(Y-&gt;X)$$于是二维的Gibbs Sampling采样算法，采样过程如下： 随机初始化$X_0=x_0, Y_0=y_0$ 对于t=1,2,…循环采样：2.1 从条件概率分布$p(y|x_0)$中采样得到$y_1$2.2 从条件概率分布$p(x|y_1)$中采样得到$x_1$ 对于多维的情况，算法也是成立的。例如，一个n维的概率分布$\pi(x_1,x_2,…,x_n)$，可以通过在n个坐标轴上轮换进行采样，得到新的样本集。对于轮换到的任意一个坐标轴$x_i$上的转移，马尔科夫链的状态转移概率为$p(x_i|x_1,x_2,…,x_{i-1},x_{i+1},…,x_n)$。即固定n-1个坐标轴，在某一个坐标轴上移动，同样是满足细致平稳条件。多维的Gibbs Sampling算法采样过程如下： 随机初始化${x_i:i=1,…,n}$ 对于t=0,1,2,…循环采样：2.1 $X_{1}^{t+1} ~p(x_1|x_{2}^{(t)},x_{3}^{(t)},…,x_n^{(t)}$2.2 $X_{2}^{t+1} ~p(x_2|x_{1}^{(t+1)},x_{3}^{(t)},…,x_n^{(t)}$2.3 …2.4 $X_{j}^{t+1} ~p(x_j|x_{1}^{(t+1)},x_{2}^{(t+1)},…,x_{j-1}^{(t+1)},x_{j+1}^{(t)},…,x_n^{(t)}$2.5 …2.5 $X_{n}^{t+1} ~p(x_1|x_{1}^{(t+1)},x_{2}^{(t+1)},…,x_{n-1}^{(t+1)}$ 同样的，轮换坐标轴不是必须的，可以随机选择某一个坐标轴进行状态转移，只不过常用的Gibbs采样的实现都是基于坐标轴轮换的。 Reference Pattern Recognition and Machine Learning， Christopher Bishop，Chapter 11 LDA数学八卦 http://blog.csdn.net/dark_scope/article/details/70992266 http://www.cnblogs.com/pinard/p/6625739.html https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling http://www.jdl.ac.cn/user/lyqing/StatLearning/10_08_MonteCarlo-blue.pdf http://www.jdl.ac.cn/user/lyqing/StatLearning/10_13_MonteCarlo2.pdf]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Sampling Methods</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编写可读代码的艺术-读书笔记]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%BC%96%E5%86%99%E5%8F%AF%E8%AF%BB%E4%BB%A3%E7%A0%81%E7%9A%84%E8%89%BA%E6%9C%AF-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第二章 把信息装到名字里​ 主题是：把信息塞入名字中。即，读者仅通过读到名字就可以获得大量的信息。主要有以下几点： 使用专业的单词–例如，不用Get，而用Fetch或者Download可能会更好，这由上下文决定。 避免空泛的名字，像tmp和retval，除非使用它们有特殊的理由。 使用具体的名字来更细致地描述事物——ServerCanStart() 这个名字就比CanListenOnPort更不清楚。 给变量名带上重要的细节——例如，在值为毫秒的变量后加上_ms，或者在还需要转义的、未处理的变量前加上raw_。 为作用域大的名字采用更长的名字——不要用让人费解的一个或两个字母的名字来命名在几屏之间都可见的变量。对于只存在于几行之间的变量用短一点的名字更好。 有目的地使用大小写、下划线等——例如，可以在类成员和局部变量后面加上“_”来区分它们。第三章 不会误解的名字​ 不要误解的名字是最好的名字——阅读你代码的人应该理解你的本意，并且不会有其他的理解。 在决定使用一个名字以前，要想象一下你的名字被误解成什么。最好的名字是不会误解的。 当要定义一个值的上限或下限时，max_和min_是很好的前缀。对弈包含的范围，first和last是好的选择。对于包含/排除范围，begin和end是好的选择。 当为布尔值命名时，使用is和has这样的词来明确表示它是一个布尔值，避免使用反义的词。 要小心用户对特定词的期望。例如，用户会期望get()或者size()是轻量的方法。第四章 审美​ 通过把代码用一致的、有意义的方式“格式化”，可以把代码变得更容易读，并且可以读得更快。下面是讨论过的一些具体技巧: 如果多个代码块做相似的事情，尝试让他们有同样的剪影。 把代码按“列”对齐可以让代码更容易浏览。 如果在一段代码中提到A、B和C，那么不要在另一段中说B、C和A。选择一个有意义的顺序，并始终用这样的顺序。 用空行来把大块分成逻辑上的“段落”。第五章 该写出什么样的注释 注释的目的是帮助读者了解作者在写代码时已经知道的那些事情。本章主要介绍如何发现所有的不那么明显的信息块并把它们写下来。什么地方不需要注释: 能从代码本身中迅速地推断的事实。 用来粉饰烂代码的“拐杖式注释”——应该把代码改好。应该记录下来的想法包括: 对于为什么代码写成这样而不是那样的内在理由（“指导性批注”）。 代码中的缺陷，使用像TODO:或者XXX:这样的标记。（TODO:还没有处理的事情；FIXME：已知的无法运行的代码；HACK：对一个问题不得不采用的比较粗暴的解决方案；XXX：危险！这里有重要的问题） 常量背后的故事，为什么是这个值。站在读者的立场上思考: 预料到代码中哪些部分会让读者产生疑问，并且给它们加上注释。 为普通读者意料之外的行为加上注释。 在文件/类的级别上使用“全局观”注释来解释所有的部分是如何一起工作的。 用注释来总结代码块，使读者不致迷失在细节中。第六章 如何写出言简意赅的注释 当像“it”和“this”这样的代词可能指代多个事物时，避免使用它们。 尽量精确地描述函数的行为。 在注释中用精心挑选的输入/输出例子进行说明。 声明代码的高层次意图，而非明显的细节。 用嵌入的注释来解释难以理解的函数参数。 用含义丰富的词来使注释简洁。第七章 把控制流变得易读有几种方法可以让代码的控制流更易读 在写一个比较时（while （bytes_expected &gt; bytes_received）），把改变的值写在左边，并且把更稳定的值写在右边更好一些（while （bytes_received &lt; bytes_expected））。 可以重新排列if/else语句中的语句块。通常来讲，先处理正确的/简单的/有趣的情况。有时这种准则会冲突，但是当不冲突时，这是要遵守的经验法则。 某些编程结构，像三目运算符(:?)、do/while循环，以及goto经常会导致代码的可读性变差。最好不要使用它们，因为总是有更整洁的代替方式。 嵌套的代码块需要更加集中精力去理解。每层新的嵌套都需要读者把更多的上下文“压入栈”。应该把它们改写成更加“线性”的代码来避免深嵌套。 通常来讲提早返回可以减少嵌套并让代码整洁。“保护语句”（在函数顶部处理简单的情况时）尤其有用。第八章 拆分超长的表达式 关键思想：把超长的表达式拆分为更容易理解的小块 引入“解释变量”来代表较长的子表达式。这种方式有三个好处： 它把巨大的表达式拆成小段； 它通过用简单的名字描述子表达式来让代码文档化； 它帮助读者识别代码中的主要概念。 用德摩根定理来操作逻辑表达式——这个技术可以把布尔表达式用更整洁的方式重写； 任何复杂逻辑的地方都可以进行拆分。第九章 变量与可读性减少变量 删除没有价值的临时变量 减少中间结果 减少控制变量缩小变量的作用域 关键思想:让你的变量对尽量少的代码行可见。 只写一次的变量更好总结 本章是关于程序中的变量是如何快速累积而变得难以跟踪的。你可以通过减少变量的数量和让它们尽量“轻量级”来让代码更有可读性。具体有： 减少变量。 减少每个变量的作用域，越小越好。把变量移到一个有最少代码可以看到的地方。 只写一次的变量更好。那些只设置一次的变量（或者const、final、常量）使得代码更容易理解。第十章 抽取不相关的子问题​ 本章一个简单的总结就是“把一般代码和项目专有的代码分开“。其结果是，大部分的代码都是一般代码。通过建立一大组库和辅助函数来解决一般问题，剩下的只是让你的程序与众不同的核心部分。​ 这个技巧有帮助的原因是它使程序员关注小而定义良好的问题，这些问题已经同项目的其他部分脱离。其结果是，对于这些子问题的解决方案倾向于更加完整和正确。你也可以再以后重用它们。第十一章 一次只做一件事 关键思想：应该把代码组织得一次只做一件事情 ​ 如果你有很难读的代码，尝试把它所做的所有任务列出来。其中一些任务可以很容易地变成单独的函数。其他的可以简单地变成为一个函数中的逻辑”段落”。具体如何拆分这些任务没有它们已经分开这个事实那样重要。难的是要准确地面描述你的程序所做的所有这些小事情。 第十二章 把想法变成代码​ 本章讨论了一个简单的技巧，用自然语言描述程序然后用这个描述来帮助你写出更自然的代码。这个技巧出人意料地简单，但很强大。看到你在描述中所用的词和短语还可以帮助你发现哪些子问题可以拆分出来。但是这个“用自然语言说事情”的过程不仅可以用于写代码。另一个看待这个问题的角度是：如果你不能把问题说明白或者用词语来做设计，估计是缺少什么东西或者什么东西缺少定义。把一个我那天变成语言可以让它变得更具体。 第十三章 少写代码 关键思想:最好的代码就是没有代码。 质疑和拆分你的需求 保持小的代码库 ​ 随着项目的增长，项目加进来的越来越多的源文件。项目很大，没有一个人自己全部理解它。增加新功能会变得很痛苦，而且使用这些代码还很费力还令人不快。最好的解决办法就是”让你的代码库越小，越轻量级越好“，可以尝试如下方法： 1. 创建越多越好的”工具“代码来减少重复代码； 2. 减少无用代码或没用的功能； 3. 让你的项目保持分开的子项目状态； 4. 总的来说，要小心代码的”重量“。让它保持又轻又灵。 熟悉你周边的库 总结 ​ 本章是关于写越少代码越好的。每行新的代码都需要测试、写文档和维护。另外，代码库中的代码越多，它就越”重“，而且在其上开发就越难。可以通过以下方法来避免重新编写新代码： 从项目中消除不必要的功能，不要过度设计； 重新考虑需求，解决版本最简单的问题，只要能完成工作就行； 经常性地通读标准库的整个API，保持对他们的熟悉程度。 第十四章 测试与可读性在测试代码中，可读性仍然很重要。如果测试的可读性很好，其结果是他们也会变得很容易写，因此大家会写更多的测试。并且，如果你把事实代码设计得容易测试，代码设计会变得更好。以下是如何改进测试的具体要点： 每个测试的最高一次应该越简明越好。最好每个测试的输入/输出可以用一行代码描述； 如果测试失败了，它所发出的错误消息应该能让你容易跟踪并修正这个bug； 使用最简单的并且能够完整运用代码的测试输入； 给测试函数取一个有完整描述性的名字，以使每个测试所测到的东西很明确。不要用Test1()，而要像Test___这样的名字。 最重要的是，要使它易于改动和增加新的测试。]]></content>
      <categories>
        <category>程序猿的自我修养</category>
      </categories>
      <tags>
        <tag>程序猿的自我修养</tag>
      </tags>
  </entry>
</search>
