<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[前馈神经网络反向传播推导]]></title>
    <url>%2F2017%2F12%2F17%2F%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[前馈神经网络前向传播 一个三层的前馈神经网络如下图所示： 对于第二层的输出$a_1^2,a_2^2,a_3^2$,有：$$a_1^2 = \sigma(z_1^2)=\sigma(w_{11}^2x_1+w_{12}^2x_2+w_{13}^2x_3+b_1^2)\\\\a_2^2 = \sigma(z_2^2)=\sigma(w_{21}^2x_1+w_{22}^2x_2+w_{23}^2x_3+b_3^2)\\\\a_3^2 = \sigma(z_3^2)=\sigma(w_{31}^2x_1+w_{32}^2x_2+w_{33}^2x_3+b_3^2)$$对于第三层的输出$a_1^3$，有：$$a_1^3 = \sigma(z_1^3)=\sigma(w_{11}^3x_1+w_{12}^3x_2+w_{13}^3x_3+b_1^3)$$ 用下面的符号来描述一个前馈神经网络： L:表示神经网络的层数； $n^l$：表示第l层神经元的个数； $f_l(.)$:表示第l层神经元的激活函数； $W^{(l)}\in R^{n^l\times n^{l-1}}$:表示第l-1层到第l层的权重矩阵； $b^{(l)\in R^{n^l}}$：表示第l-1层到第l层的偏置； $z^{(l)\in R^{n^l}}$:表示第l层神经元的净输入； $a^{(l)\in R^{n^l}}$:表示第l层神经元的输出（激活值）； $W_{ij}^{(l)}$:表示第l-1层第j个输入到第l层第i个神经元的权重； 前馈神经网络通过下面的公式进行信息传播：$$z^{(l)} = W^{(l)}\cdot a^{(l-1)} + b^{(l)} \\\\a^{(l)} = f_l(z^{(l)})$$上面的公式可以合并为：$$z^{(l)} = W^{(l)}\cdot f_{l-1}(z^{(l-1)}) + b^{(l)}$$ 这样，前馈神经网络可以通过逐层的信息传递，得到网络最后的输出$a^{(L)}$。整个网络可以看作一个复合函数$\phi(x;W,b)$,将输入x作为第1层的输入$a^{(0)}$，将第L层的输出作为$a^{(L)}$作为输出。$$x=a^{(0)}\rightarrow z^{(l)} \rightarrow a^{(1)}\rightarrow z^{(2)}\rightarrow … \rightarrow a^{(L-1)} \rightarrow z^{(L)} \rightarrow a^{(L-1)}=\phi(x;W,b)$$ 反向传播推导假设损失函数是$L(y,\hat{y})$，对第l层中的参数$W^{(l)}$和$b^{(l)}$计算偏导数。因为$\frac {\partial L(y,\hat{y})}{\partial W^{(l)}}$的计算涉及到矩阵的微分，十分繁琐，可以先计算偏导数$\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}}$。根据链式法则：$$\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}} =\left(\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}}\right)^T\frac {\partial L(y,\hat{y})}{\partial z^{(l)}} （1）\\\\\frac {\partial L(y,\hat{y})}{\partial b^{(l)}} =\left(\frac {\partial z^{(l)}}{\partial b^{(l)}}\right)^T\frac {\partial L(y,\hat{y})}{\partial z^{(l)}} (2)$$公式(1)和(2)都是为目标函数关于第l层神经元$z^{(l)}$的偏导数，称为误差项，因此可以共用。我们只需要计算三个偏导数，分别为$\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}},\frac {\partial L(y,\hat{y})}{\partial b^{(l)}}和\frac {\partial L(y,\hat{y})}{\partial z^{(l)}}$ 1、计算偏导数$\frac {\partial z^{(l)} }{\partial W_{ij}^{(l)}}$ 因为$z^{(l)}和W_{ij}^{(l)}$的函数关系为$z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)}$，因此，偏导数：$$\frac {\partial z^{(l)}}{\partial W_{ij}^{(l)}}=\frac {\partial (W^{(l)}a^{(l-1)}+b^{(l)})}{\partial W_{ij}^{(l)}}= a_j^{(l-1)}$$其中，$W_{i:}^{(l)}$为权重矩阵$W^{(l)}$的第i行 2、计算偏导数$\frac {\partial z^{l}}{\partial b^{(l)}}$ 因为$z^{(l)}$和$b^{(l)}$的函数关系为$z^{(l)} = W^{(l)}a^{(l-1)}+b^{(l)}$，因此偏导数：$$\frac {\partial z^{l}}{\partial b^{(l)}}=I_{n^l}$$为$n^l\times n^l$的单位矩阵。 3、计算偏导数$\frac {\partial L(y,\hat{y})}{\partial z{(l)}}$ 用$\delta^{(l)}$来定义第l层的神经元误差项：$$\delta^{(l)} = \frac {\partial L(y,\hat{y})}{\partial z^{(l)}} \in R^{n^l}$$误差项$\delta^{l}$表示第l层的神经元对最终的误差的影响，也反映了最终的输出对第l层的神经元对最终误差的敏感程度。 根据$z^{(l+1)}=W^{(l+1)}a^{(l)} + b^{(l+1)}$，有：$$\frac {\partial z^{(l+1)}}{\partial a^{(l)}} = (W^{(l+1)})^T$$根据$a^{(l)}=f(z^{(l)})$，其中，$f_l(.)$为按位计算的函数，因此有：$$\frac {\partial a^{(l)}}{\partial z^{(l)}} = \frac {\partial f_l(z^{(l)})}{\partial z^{l}} = diag(f_l^{\prime}(z^{(l)}))$$因此，根据链式法则，第l层的误差项为：$$\delta^{(l)} = \frac {\partial L(y, \hat{y})}{\partial z^{(l)}}\\\\=\frac {\partial a^{(l)}}{\partial z^{(l)}}\cdot\frac{\partial z^{(l+1)}}{\partial a^{(l)}}\cdot \frac {\partial L(y, \hat{y})}{\partial z^{(l+1)}}\\\\=diag(f_l^{\prime}(z^{(l)}))\bigodot ((W^{(l+1)})^T\delta^{(l+1)}) (3)$$其中，$\bigodot$是向量的点积运算，表示每个元素相乘。 从公式3可以看出，第l层的误差项可以通过第l+1层的误差项计算，这就是反向传播。反向传播算法的含义是：第l层的一个神经元的误差项是所有与该神经元相连的第l+1层神经元误差项的权重和，然后再乘上该神经元激活函数的梯度。 在计算出三个偏导数之后，可以得到最终的偏导数：$$\frac {\partial L(y,\hat{y})}{\partial W_{ij}^{(l)}} = \delta_i^{(l)}a_j^{(l-1)}$$进一步，$L(y,\hat{y})$关于第l层的权重$W^{(l)}$的梯度为：$$\frac {\partial L(y,\hat{y})}{\partial W^{(l)}} = \delta^{(l)}a^{(l-1)}$$同理可得，$L(y,\hat{y})$关于第l层偏置$b^(l)$的梯度为：$$\frac {\partial L(y,\hat{y})}{\partial b^{(l)}} = \delta^{(l)}$$基于随机梯度下降的反向传播算法如下： 图片引自：https://github.com/nndl/nndl.github.io，chap-前馈神经网络.pdf。]]></content>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN学习的相关资料]]></title>
    <url>%2F2017%2F12%2F17%2FCNN%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[对卷积的理解 http://mengqi92.github.io/2015/10/06/convolution/ http://www.cnblogs.com/freeblues/p/5738987.html http://blog.csdn.net/bitcarmanlee/article/details/54729807 卷积神经网络的原理及推导 反向传播的推导：https://zhuanlan.zhihu.com/p/22473137 pinard:http://www.cnblogs.com/pinard/p/6483207.html zybuluo:https://www.zybuluo.com/hanbingtao/note/485480 一文读懂卷积神经网络CNN：http://www.sohu.com/a/126742834_473283 CS231N翻译：https://zhuanlan.zhihu.com/p/21930884 charlote:http://www.cnblogs.com/charlotte77/p/7759802.html 卷积神经网络全面解析:http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi tornadomeet:http://www.cnblogs.com/tornadomeet/p/3468450.html]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>资料</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(9) ConvNet notes]]></title>
    <url>%2F2017%2F12%2F16%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC9%E8%AF%BE-ConvNet-notes%2F</url>
    <content type="text"><![CDATA[本文转载自：https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit，原文为：http://cs231n.github.io/convolutional-networks/，并进行一定的修改。 目录 结构概述 用来构建卷积神经网络的各种层 卷积层 汇聚层 归一化层 全连接层 将全连接层转化成卷积层 卷积神经网络的结构 层的排列规律 层的尺寸设置规律 案例学习（LeNet/AlexNet/ZFNet/GoogLeNet/VGGNet） 计算上的考量 拓展资源 卷积神经网络(CNNs/ConvNets)卷积神经网络和上一章讲的常规神经网络非常相似：它们都是由神经元组成，神经元中有局域学习能力的权重和偏差。每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出的是不同类别的评分。在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或者softmax）,并且在神经网络中我们的各种技巧和要点依旧适用于卷积神经网络。 那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。 结构概述回顾：常规神经网络。在上一章中，神经网络的输入是一个向量，然后在一些列的隐层中对它做变换。每个隐层是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接。但是在一个隐藏层中，神经元相互独立不进行任何连接。最后的全连接层被称为“输出层”，在分类问题中，它输出的值被看做是不同类别的评分值。 常规神经网络对于大尺寸图像效果不尽人意。在CIFAR-10中，图像的尺寸是32x32x3（宽高均为32像素，3个颜色通道），因此，对应的的常规神经网络的第一个隐层中，每一个单独的全连接神经元就有32x32x3=3072个权重。这个数量看起来还可以接受，但是很显然这个全连接的结构不适用于更大尺寸的图像。举例说来，一个尺寸为200x200x3的图像，会让神经元包含200x200x3=120,000个权重值。而网络中肯定不止一个神经元，那么参数的量就会快速增加！显而易见，这种全连接方式效率低下，大量的参数也很快会导致网络过拟合。 神经元的三维排列。卷积神经网络针对输入全部是图像的情况，将结构调整得更加合理，获得了不小的优势。与常规神经网络不同，卷积神经网络的各层中的神经元是3维排列的：宽度、高度和深度（这里的深度指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）。举个例子，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是32x32x3（宽度，高度和深度）。我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的。下面是例子： 左边是一个3层的神经网络。右边是一个卷积神经网络，图例中网络将它的神经元都排列成3个维度（宽、高和深度）。卷积神经网络的每一层都将3D的输入数据变化为神经元3D的激活数据并输出。在这个例子中，红色的输入层装的是图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是3（代表了红、绿、蓝3种颜色通道）。 用来构建卷积神经网络的各种层一个简单的卷积神经网是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。卷积神经网络主要由三种类型的层构成：卷积层、汇聚层和全连接层（全连接层和常规的神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。 网络结构的例子：这仅仅是一个概述，下面会有更详细的介绍。一个用于CIFAP-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下： 输入[32*3]存有图像的原始像素值，本例中，图像宽高均为32，有3个颜色通道。 卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积。卷积层会计算所有的神经元的输出。如果我们使用12个滤波器（也叫核），得到的输出数据体的维度就是[323212]。 ReLU层将会逐个元素地进行激活函数操作，比如使用以0为阈值的max(0,x)作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。 汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。 全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。 由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值。其中有的层含有参数，有的没有。具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。 小结 简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）。 卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）。 每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据。 有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。 有的层有额外的参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）。 卷积层卷积层是构建卷积神经网络的核心层，它产生了网络中的大部分的计算量。 概述和直观介绍：首先讨论的是，在没有大脑和生物意义上的神经元之类的比喻下，卷积层到底在计算什么。卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案。 在每个卷积层上，我们会有一整个集合的滤波器（比如12个），每个都会生成一个不同的二维激活图。将这些激活映射在深度方向上层叠起来就生成了输出数据。 以大脑做比喻：如果你喜欢用大脑和生物神经元来做比喻，那么输出的3D数据中的每个数据项可以被看做是神经元的一个输出，而该神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。现在开始讨论神经元的连接，它们在空间中的排列，以及它们参数共享的模式。 局部连接：在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。 例1：假设输入的数据体尺寸为32x32x3，如果滤波器是5x5，那么卷积层中的每个神经元有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。 例2：假设输入数据体的尺寸是[16x16x20]，感受野尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20=180个连接。再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）。 左边：红色的是输入数据体（比如CIFAR-10中的图像），蓝色的部分是第一个卷积层中的神经元。卷积层中的每个神经元都只是与输入数据体的一个局部在空间上相连，但是与输入数据体的所有深度维度全部相连（所有颜色通道）。在深度方向上有多个神经元（本例中5个），它们都接受输入数据的同一块区域（感受野相同）。至于深度列的讨论在下文中有。右边：神经网络章节中介绍的神经元保持不变，它们还是计算权重和输入的内积，然后进行激活函数运算，只是它们的连接被限制在一个局部空间。空间排列：上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及他们的排列方式。3个超参数控制着输出数据体的尺寸：深度(depth)、步长(stride)和零填充(zero-padding)。下面是对它们的讨论：1. 首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界、或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列，也有人使用纤维(fibre)来称呼它们。2. 其次，在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。3. 在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的。这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。输出数据体在空间上的尺寸可以通过输入数据体尺寸(W)，卷积层中神经元的感受野尺寸(F)、步长(S)和零填充的数量(P)的函数来计算（假设输入数组的空间形状是正方形，即高度和宽度相等）输出数据体的空间尺寸为(W-F+2P)/S+1。比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出。如果步长为2，输出就是3x3。下面是例子： 空间排列的图示。在本例中只有一个空间维度（x轴），神经元的感受野尺寸F=3，输入尺寸W=5，零填充P=1。左边：神经元使用的步长S=1，所以输出尺寸是(5-3+2)/1+1=5。右边：神经元的步长S=2，则输出尺寸是(5-3+2)/2+1=3。注意当步长S=3时是无法使用的，因为它无法整齐地穿过数据体。从等式上来说，因为(5-3+2)=4是不能被3整除的。本例中，神经元的权重是[1,0,-1]，显示在图的右上角，偏差值为0。这些权重是被所有黄色的神经元共享的（参数共享的内容看下文相关内容）。使用零值填充：在上面的左边的例子中，注意输入维度是5，输出维度也是5。之所以如此，是因为感受野是3并且使用了1的零填充。如果不使用零填充，则输出数据体的空间维度就只有3，因为这就是滤波器整齐滑过并覆盖原始数据需要的数目。一般来说，当步长S=1时，零填充的值是P=（F-1）/2，这样就能保证输入和输出数据体有相同的空间尺寸。这样做非常常见，在介绍卷积神经网络的结构的时候我们会详细讨论其原因。步长的限制：注意这些空间排列的超参数之间是相互限制的。举例说来，当输入尺寸W=10，不使用零填充则P=0，滤波器尺寸F=3，这样步长S=2就行不通，因为(W-F+2P)/S+1=(10-3+0)/2+1=4.5，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体。因此，这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，或者修改零填充值来让设置合理，或者修改输入数据体尺寸来让设置合理，或者其他什么措施。在后面的卷积神经网络结构小节中，读者可以看到合理地设置网络的尺寸让所有的维度都能正常工作，这件事可是相当让人头痛的。而使用零填充和遵守其他一些设计策略将会有效解决这个问题。真实案例：Krizhevsky构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227x227x3]。在第一个卷积层，神经元使用的感受野尺寸F=11，步长S=4，不使用零填充P=0。因为(227-11)/4+1=55，卷积层的深度K=96，则卷积层的输出数据体尺寸为[55x55x96]。55x55x96个神经元中，每个都和输入数据体中一个尺寸为[11x11x3]的区域全连接。在深度列上的96个神经元都是与输入数据体中同一个[11x11x3]区域连接，但是权重不同。有一个有趣的细节，在原论文中，说的输入图像尺寸是224x224，这是肯定错误的，因为(224-11)/4+1的结果不是整数。这件事在卷积神经网络的历史上让很多人迷惑，而这个错误到底是怎么发生的没人知道。我的猜测是Alex忘记在论文中指出自己使用了尺寸为3的额外的零填充。参数共享=290,400个神经元，每个有11x11x3=364个参数和1个偏差。将这些合起来就是290400x364：在卷积层中使用参数共享是用来控制参数的数量。就用上面的例子，在第一个卷积层就有55x55x96=105,705,600个参数。单单第一层就有这么多参数，显然这个数目是非常大的。作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量。换言之，就是将深度维度上一个单独的2维切片看做深度切片（depth slice），比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]。在每个深度切片上的神经元都使用同样的权重和偏差。在这样的参数共享下，例子中的第一个卷积层就只有96个不同的权重集了，一个权重集对应一个深度切片，共有96x11x11x3=34,848个不同的权重，或34,944个参数（+96个偏差）。在每个深度切片中的55x55个权重使用的都是同样的参数。在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的卷积（这就是“卷积层”名字由来）。这也是为什么总是将这些权重集合称为滤波器（filter）（或卷积核（kernel）），因为它们和输入进行了卷积。 Krizhevsky等学习到的滤波器例子。这96个滤波器尺寸都是[11x11x3]，在一个深度切片中，每个滤波器都被55x55个神经元共享。注意参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也同样是有用的，这是因为图像结构具有平移不变性。所以，在卷积层的输出数据体的55x55个不同位置中，就没有必要重新学习探测一个水平边界了。注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为局部连接层（Locally-Connected Layer）。Numpy的例子：为了让讨论更加的具体，我们用代码来展示上述思路。假设输入数据体是numpy数组X。那么：一个位于(x,y)的深度列，将会是X[x,y,:]；在深度为d处的切片，或激活图应该是X[:,:,d]。卷积层例子：假设输入数据体的尺寸X.shape:(11,11,4)，不使用零填充（P=0），滤波器的尺寸是F=5，步长S=2。那么输出数据体的空间尺寸就是(11-5)/2+1=4，即输出数据体的宽度和高度都是4。那么在输出数据体中的激活映射（称其为V）看起来就是下面这样（在这个例子中，只有部分元素被计算）：1234V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0在numpy中，操作是进行数组间的逐元素相乘。权重向量W0是该神经元的权重，b0是其偏差。在这里，W0被假设尺寸是W0.shape: (5,5,4)，因为滤波器的宽高是5，输入数据量的深度是4。注意在每一个点，计算点积的方式和之前的常规神经网络是一样的。同时，计算内积的时候使用的是同一个权重和偏差（因为参数共享），在宽度方向的数字每次上升2（因为步长为2）。要构建输出数据体中的第二张激活图，代码应该是：123456V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1 （在y方向上）V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1 （或两个方向上同时）我们访问的是V的深度维度上的第二层（即index1），因为是在计算第二个激活图，所以这次试用的参数集就是W1了。在上面的例子中，为了简洁略去了卷积层对于输出数组V中其他部分的操作。还有，要记得这些卷积操作通常后面接的是ReLU层，对激活图中的每个元素做激活函数运算，这里没有显示。小结- 输入数据体的尺寸为：$W_1xH_1xD_1$- 4个超参数:滤波器的数量K;滤波器的空间尺寸F;步长S;零填充数量P- 输出数据的尺寸为：$W_2xH_2xD_2$，其中： - $W_2 = (W_1 - F + 2P)/S+1$; - $H_2 = (H_1 - F + 2P)/S+1$(宽度和高度的计算方法相同); - $D_2=K$- 由于参数共享，每个滤波器包含$F\cdot F\cdot D_1$个权重，卷积层一共有$F\cdot F\cdot D_1\cdot K$个权重和K个偏置。- 在输出数据体中，第d个深度的切片(空间尺寸是$W_2xH_2$)，用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长S），最后在加上第d个偏差。对这些超参数，常见的设置是F=2，S=1，P=1。同时设置这些超参数也有一些约定俗成的惯例和经验，可以在下面的卷积神经网络结构章节中查看。*卷积层演示：下面是一个卷积层的运行演示。因为3D数据难以可视化，所以所有的数据（输入数据体是蓝色，权重数据体是红色，输出数据体是绿色）都采取将深度切片按照列的方式排列展现。输入数据体的尺寸是$W_1=5,H_1=5,D_1=3$，卷积层参数K=2,F=3,S=2,P=1。就是说，有2个滤波器，滤波器的尺寸是$3\cdot 3$，它们的步长是2.因此，输出数据体的空间尺寸是(5-3+2)/2+1=3。注意输入数据体使用了零填充P=1，所以输入数据体外边缘一圈都是0。下面的例子在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来。 矩阵乘法实现：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法： 输入图像的局部区域被im2col操作拉伸为列。比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3=363的列向量。重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)/4+1=55，所以得到im2col操作的输出矩阵X_col的尺寸是[363x3025]，其中每列是拉伸的感受野，共有55x55=3,025个。注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。 卷积层的权重也同样被拉伸成行。举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵W_row，尺寸为[96x363]。 现在卷积的结果和进行一个大矩阵乘np.dot(W_row, X_col)是等价的了，能得到每个滤波器和每个感受野间的点积。在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出。 结果最后必须被重新变为合理的输出尺寸[55x55x96]。 这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在X_col中被复制了多次。但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的BLAS API）。还有，同样的im2col思路可以用在汇聚操作中。反向传播：卷积操作的反向传播（同时对于数据和权重）还是一个卷积（但是是和空间上翻转的滤波器）。使用一个1维的例子比较容易演示。 1x1卷积：一些论文中使用了1x1的卷积，这个方法最早是在论文Network in Network中出现。人们刚开始看见这个1x1卷积的时候比较困惑，尤其是那些具有信号处理专业背景的人。因为信号是2维的，所以1x1卷积就没有意义。但是，在卷积神经网络中不是这样，因为这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）。 扩张卷积：最近一个研究（Fisher Yu和Vladlen Koltun的论文）给卷积层引入了一个新的叫扩张（dilation）的超参数。到目前为止，我们只讨论了卷积层滤波器是连续的情况。但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张。举例，在某个维度上滤波器w的尺寸是3，那么计算输入x的方式是：w[0]x[0] + w[1]x[1] + w[2]x[2]，此时扩张为0。如果扩张为1，那么计算为： w[0]x[0] + w[1]x[2] + w[2]x[4]。换句话说，操作中存在1的间隙。在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为在很少的层数内更快地汇集输入图片的大尺度特征。比如，如果上下重叠2个3x3的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中5x5的区域（可以成这些神经元的有效感受野是5x5）。如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长。 汇聚层通常，在连续的卷积层之间会周期性地插入一个汇聚层。**它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。汇聚层的一些公式： 输入数据体尺寸$W_1\cdot H_1\cdot D_1$ 有两个超参数： 空间大小F 步长S 输出数据体尺寸$W_2\cdot H_2\cdot D_2$，其中 $W_2=(W_1-F)/S+1$ $H_2=(H_1-F)/S+1$ $D_2=D_1$ 因为对输入进行的是固定函数计算，所以没有引入参数。 在汇聚层中很少使用零填充。 在实践中，最大汇聚层通常只有两种形式：一种是F=3,S=2，也叫重叠汇聚（overlapping pooling），另一个更常用的是F=2,S=2。对更大感受野进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性。 普通汇聚（General Pooling）：除了最大汇聚，汇聚单元还可以使用其他的函数，比如平均汇聚（average pooling）或L-2范式汇聚（L2-norm pooling）。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。 汇聚层在输入数据体的每个深度切片上，独立地对其进行空间上的降采样。左边：本例中，输入数据体尺寸[224x224x64]被降采样到了[112x112x64]，采取的滤波器尺寸是2，步长为2，而深度不变。右边：最常用的降采样操作是取最大值，也就是最大汇聚，这里步长为2，每个取最大值操作是从4个数字中选取（即2x2的方块区域中）。 反向传播：回顾一下反向传播的内容，其中max(x,y)函数的反向传播可以简单理解为将梯度只沿着最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大的索引记录下来，这样在反向传播的时候梯度的路由就很高效。 不使用汇聚层：很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。 归一化层在卷积神经网络中的结果，提出很多不同类型的归一化层，有时候是为了实现生物大脑中观测到的抑制机制。但是，这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极有限的。对于不同类型的归一化层，可以看看Alex Krizhevsky的关于cuda-convnet library API的讨论。 全连接层在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。 把全连接层转化层卷积层全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所有它们的函数形式是一样的。因此，将此两者相互转化是可能的： 对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块（这是因为有局部连接），其余部分都是零。而在其中大部分块中，元素都是相等的（因为参数共享）。 相反，任何全连接层都可以被转化为卷积层。比如，一个K=4096的全连接层，输入数据体的尺寸是$7\times 7\times 512$，这个全连接层可以被等效地看做一个F=7,P=0,S=1,K=4096的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成$1\times 1\times$ 4096，这个结果就和使用初始的那个全连接层一样了。 全连接层转化为卷积层：在这两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是$224\times224\times3$的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层： 针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。 针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。 对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]。 实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（译者注：即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。 举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！ 面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。 自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。 最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。 卷积神经网络的结构卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。 层的排列规律卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &gt;=0,通常N&lt;=3,M&gt;=0,K&gt;=0,通常K&lt;3。例如，下面是一些常见的网络结构规律： INPUT -&gt; FC,实现一个线性分类器，此处N = M = K = 0。 INPUT -&gt; CONV -&gt; RELU -&gt; FC INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; F- C。此处在每个汇聚层之间有一个卷积层。 INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]3 -&gt; [FC -&gt; RELU]2 -&gt; FC。此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。 几个小滤波器卷积层的组合比一个大滤波器卷积层好：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7，但是就有一些缺点。首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有C个通道，那么单独的7x7卷积层将会包含C\times (7\times 7\times C)=49C^2个参数，而3个3x3的卷积层的组合仅有3\times (C\times (3\times 3\times C))=27C^2个参数。直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。 最新进展：传统的将层按照线性进行排列的方法已经受到了挑战，挑战来自谷歌的Inception结构和微软亚洲研究院的残差网络（Residual Net）结构。这两个网络（下文案例学习小节中有细节）的特征更加复杂，连接结构也不同。 层的尺寸设置规律到现在为止，我们都没有提及卷积神经网络中每层的超参数的使用。现在先介绍设置结构尺寸的一般性规则，然后根据这些规则进行讨论：输入层（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。 卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长S=1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当F=3，那就使用P=1来保持输入尺寸。当F=5,P=2，一般对于任意F，当P=(F-1)/2的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上。 汇聚层负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野（即F=2）的最大值汇聚，步长为2（S=2）。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。 减少尺寸设置的问题上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。 为什么在卷积层使用1的步长？在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。 为何使用零填充？使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。 因为内存限制所做的妥协：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。 案例学习下面是卷积神经网络领域中比较有名的几种结构： LeNet： 第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的。当然，最著名还是被应用在识别数字和邮政编码等的LeNet结构。AlexNet：AlexNet卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的ImageNet ILSVRC 竞赛中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。 ZF Net：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为 ZFNet（Zeiler &amp; Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。 GoogLeNet：ILSVRC 2014的胜利者是谷歌的Szeged等实现的卷积神经网络。它主要的贡献就是实现了一个奠基模块，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了。GooLeNet还有几种改进的版本，最新的一个是Inception-v4。 VGGNet：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为VGGNet。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的预训练模型是可以在网络上获得并在Caffe中使用的。VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。 ResNet：残差网络（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的跳跃链接，大量使用了批量归一化（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲（视频，PPT），以及一些使用Torch重现网络的实验。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，可以看论文Identity Mappings in Deep Residual Networks，2016年3月发表。 计算上的考量在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈。大部分现代GPU的内存是3/4/6GB，最好的GPU大约有12GB的内存。要注意三种内存占用来源： 来自中间数据体尺寸：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）。通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）。在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到。但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量。 来自参数尺寸：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存。因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多。 卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等。 一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位。把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量。如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了。 拓展资源和实践相关的拓展资源： Soumith benchmarks for CONV performance ConvNetJS CIFAR-10 demo 可以让你在服务器上实时地调试卷积神经网络的结构，观察计算结果。 Caffe，一个流行的卷积神经网络库。 State of the art ResNets in Torch7 ———————- end —————————]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(8) Neural Nets Notes 3]]></title>
    <url>%2F2017%2F12%2F13%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC8%E8%AF%BE-Neural-Nets-Notes-3%2F</url>
    <content type="text"><![CDATA[本文转载自：https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit，原文为：http://cs231n.github.io/neural-networks-3/，并进行一定的修改。 目录 梯度检查 合理性（Sanity）检查 检查学习过程 损失函数 训练集与验证集准确率 权重：更新比例 每层的激活数据与梯度分布 可视化 参数更新 一阶（随机梯度下降）方法，动量方法，Nestrov动量方法 学习率退火 二阶方法 逐参数适应学习率方法（Adagrad，RMSProp） 参数调优 评价 模型集成 总结 梯度检查理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较。然而从实际操作层面上来说，这个过程更加复杂且容易出错。下面是一些提示、技巧和需要仔细注意的事情。使用中心化公式，在使用有限差值近似来计算数值梯度的时候，常见的公式是：$$\frac {df(x)}{dx}=\frac {f(x+h) - f(x)}{h}(bad, do not use)$$其中，h是一个很小的数字，在实践中，近似为1e-5。在实践中证明，使用中心化公式效果更好：$$\frac {df(x)}{dx}=\frac {f(x+h) - f(x-h)}{2h}(use instead)$$该公式在检查梯度的每个维度的时候，会要求计算两次损失函数（所以计算资源的耗费也是两倍），但是梯度的近似值会准确很多。要理解这一点，对f(x+h)和f(x-h)使用泰勒展开，可以看到第一个公式的误差近似O(h)，第二个公式的误差近似$O(h^2)$（是个二阶近似）。 使用相对误差来比较，比较数值梯度$f_n^’$和解析梯度$f_a^’$的细节有哪些？如何得知此两者不匹配？你可能会倾向于监测它们的差的绝对值$|f_a^’-f_n^’|$或者差的平方值，然后定义该值如果超过某个规定阈值，就判断梯度实现失败。然而该思路是有问题的。想想，假设这个差值是1e-4，如果两个梯度值在1.0左右，这个差值看起来就很合适，可以认为两个梯度是匹配的。然而如果梯度值是1e-5或者更低，那么1e-4就是非常大的差距，梯度实现肯定就是失败的了。因此，使用相对误差总是更合适一些：$$\frac {|f_a^’-f_n^’|}{max(|f_a^’|,|f_n^’|)}$$上式考虑了差值占两个梯度绝对值的比例。注意通常相对误差公式只包含两个式子中的一个（任意一个均可），但是我更倾向取两个式子的最大值或者取两个式子的和。这样做是为了防止在其中一个式子为0时，公式分母为0（这种情况，在ReLU中是经常发生的）。然而，还必须注意两个式子都为零且通过梯度检查的情况。在实践中： 相对误差&gt;1e-2:通常就意味着梯度可能出错; 1e-2&gt;相对误差&gt;1e-4:要对这个值感到不舒服才行; 1e-4&gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高; 1e-7或者更小：好结果，可以高兴一把了。 要知道的是网络的深度越深，相对误差就越高。所以，如果你是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。相反，如果一个可微函数的相对误差值是1e-2，那么通常说明梯度实现不正确。 使用双精度：一个常见的错误是使用单精度浮点数来进行梯度检查，这样会导致即使梯度实现正确，相对误差值也会很高。在我的经验而言，出现过使用单精度浮点数时相对误差为1e-2，换成双精度浮点数时，就降低为1e-8的情况。 保持在浮点数的有效范围，建议通读《What Every Computer Scientist Should Konw About Floating-Point Artthmetic》一文，该文将阐明你可能犯的错误，促使你写下更加细心的代码。例如，在神经网络中，在一个批量的数据上对损失函数进行归一化是很常见的。但是，如果每个数据点的梯度很小，然后又用数据点的数量去除，就使得数值更小，这反过来会导致更多的数值问题。这就是我为什么总是会把原始的解析梯度和数值梯度数据打印出来，确保用来比较的数字的值不是过小（通常绝对值小于1e-10就绝对让人担心）。如果确实过小，可以使用一个常数暂时将损失函数的数值范围扩展到一个更“好”的范围，在这个范围中浮点数变得更加致密。比较理想的是1.0的数量级上，即当浮点数指数为0时。 目标函数的不可导点（kinks)，在进行梯度检查时，一个导致不准确的原因是不可导点问题。不可导点是指目标函数不可导的部分，由ReLU（max(0,x)）等函数，或SVM损失，Maxout神经元等引入。考虑当x=-1e6的时，对ReLU函数进行梯度检查。因为x1e-6)，导致了一个非零的结果。你可能会认为这是一个极端的案例，但实际上这种情况很常见。例如，一个用CIFAR-10训练的SVM中，因为有50,000个样本，且根据目标函数每个样本产生9个式子，所以包含有450,000个max(0,x)式子。而一个用SVM进行分类的神经网络因为采用了ReLU，还会有更多的不可导点。 注意，在计算损失的过程中是可以知道不可导点有没有被越过的。在具有max(x,y)形式的函数中持续跟踪所有“赢家”的身份，就可以实现这一点。其实就是看在前向传播时，到底x和y谁更大。如果在计算f(x+h)和f(x-h)的时候，至少有一个“赢家”的身份变了，那就说明不可导点被越过了，数值梯度会不准确。 使用少量数据点，解决上面的不可导点问题的一个办法是使用更少的数据点。因为含有不可导点的损失函数(例如：因为使用了ReLU或者边缘损失等函数)的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。还有，如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。 谨慎设置步长h，在实践中h并不是越小越好，因为当h特别小的时候，就可能会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将h调到1e-4或者1e-6，然后突然梯度检查就可能恢复正常。 在操作的特性模式中梯度检查，有一点必须要认识到：梯度检查是在参数空间中的一个特定（往往还是随机的）的单独点进行的。即使是在该点上梯度检查成功了，也不能马上确保全局上梯度的实现都是正确的。还有，一个随机的初始化可能不是参数空间最优代表性的点，这可能导致进入某种病态的情况，即梯度看起来是正确实现了，实际上并没有。例如，SVM使用小数值权重初始化，就会把一些接近于0的得分分配给所有的数据点，而梯度将会在所有的数据点上展现出某种模式。一个不正确实现的梯度也许依然能够产生出这种模式，但是不能泛化到更具代表性的操作模式，比如在一些的得分比另一些得分更大的情况下就不行。因此为了安全起见，最好让网络学习（“预热”）一小段时间，等到损失函数开始下降的之后再进行梯度检查。在第一次迭代就进行梯度检查的危险就在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。 不要让正则化吞没数据，通常损失函数是数据损失和正则化损失的和，需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分（正则化部分的梯度表达式通常简单很多）。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化的强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。 记得关闭随机失活（Dropout）和数据扩张（augmentation）,在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。关闭这些操作不好的一点是无法对它们进行梯度检查（例如随机失活的反向传播实现可能有错误）。因此，一个更好的解决方案就是在计算f(x+h)和f(x-h)前强制增加一个特定的随机种子，在计算解析梯度时也同样如此。 检查少量的维度，在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度，然后假设其他维度是正确的。注意：确认在所有不同的参数中都抽取一部分来梯度检查。在某些应用中，为了方便，人们将所有的参数放到一个巨大的参数向量中。在这种情况下，例如偏置就可能只占用整个向量中的很小一部分，所以不要随机的从向量中取维度，一定要把这种情况考虑到，确保所有的参数都收到了正确的梯度。 学习之前：合理性检查的提示与技巧在进行费时费力的最优化之前，最好进行一些合理性检查： 寻找特定情况的正确损失值，在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。 第二个合理性检查：提高正则化强度时导致损失值变大。 对小数据子集过拟合， 最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。除非能通过这一个正常性检查，不然进行整个数据集训练是没有意义的。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。 检查学习过程在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。在下面的图表中，x轴通常都是表示周期（epochs）单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。 损失函数训练期间第一个要跟踪的数值就是损失值，它再前向传播时对每个独立的批数据进行计算。下图是展示的是损失值随着时间的变化，尤其是曲线形状会给出关于学习率设置的情况： 左图展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。右图显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。 损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。 有的研究者喜欢用对数域对损失函数值作图。因为学习过程一般都是采用指数型的形状，图表就会看起来更像是能够直观理解的直线，而不是呈曲棍球一样的曲线状。还有，如果多个交叉验证模型在一个图上同时输出图像，它们之间的差异就会比较明显。 训练集与验证集准确率在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。这个图表能够展现知道模型过拟合的程度： 在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。 权重：更新比例最后一个应该跟踪的量是权重中更新值的数量和全部值的数量之间的比例。注意：是更新的，而不是原始梯度（比如，在普通sgd中就是梯度乘以学习率）。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子：123456# 假设参数向量为W，其梯度向量为dWparam_scale = np.linalg.norm(W.ravel())update = -learning_rate*dW # 简单SGD更新update_scale = np.linalg.norm(update.ravel())W += update # 实际更新print update_scale / param_scale # 要得到1e-3左右 相较于跟踪最大和最小值，有研究者更喜欢计算和跟踪梯度的范式及其更新。这些矩阵通常是相关的，也能得到近似的结果。 每层的激活数据与梯度分布一个不正确的初始化可能让学习过程变慢，甚至彻底停止。还好，这个问题可以比较简单地诊断出来。其中一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。 第一层可视化最后，如果数据是图像像素数据，那么把第一层特征可视化会有帮助。 将神经网络第一层的权重可视化的例子。左图中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。右图的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。 参数更新一旦能使用反向传播计算解析梯度，梯度就能被用来进行参数更新。进行参数更新有好几种方法，接下来都会进行讨论。 深度网络的最优化是现在非常活跃的研究领域。本节将重点介绍一些公认有效的常用的技巧，这些技巧都是在实践之中会遇到的。我们将简要介绍这些技巧的直观概念，但不进行细节分析。对细节感兴趣的读者，我们提供一些拓展阅读。 随机梯度下降及各种更新方法普通更新，最简单的更新形式是沿着负梯度方向改变参数（因为梯度指向的是上升的方法，但是我们通常希望最小化损失函数）。假设有一个参数向量x及其梯度dx，那么最简单的更新的形式是：12# 普通更新x += - learning_rate * dx 其中，learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。 动量（Momentum）更新是另外一个方法，这个方法在深度网络上几乎总能得到更好的收敛速度。该方法可以看成是从物理角度上对最优化问题的得到的启发。损失函数可以理解为是山的高度（因此高度的势能是U=mgh），用随机数字初始化参数等同于在某个位置给质点设定初始速度为0.这样最优化过程就可以看成是模拟参数向量（即质点）在地形上滚动的过程。 因为作用于质点的力与梯度的潜在能量($F=-\nabla U$)有关，质点所受的力就是损失函数的负梯度。还有，因为F=ma，所以在这个观点下负梯度与质点的加速度是成比例的。注意这个理解和上面的随机梯度下降SGD是不同的，在普通版本中，梯度直接影响位置。而在这个版本的更新中，物理观点建议梯度只是影响速度，然后速度再影响位置：123# 动量更新v = mu * v - learning_rate * dx # 与速度融合x += v # 与位置融合 在这里引入一个初始化为0的变量v和一个超参数mu。说的不恰当一点，这个变量mu，在最优化的过程中被看做动量（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设置为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火类似，动量随时间变化的设置有时能略微改善最优化效果，其中动量在学习过程的后阶段会上升。一个典型的设置是刚开始将动量设置为0.5,而在后面的多个周期（epoch）中慢慢提升到0.99。 通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。 Nesterov动量与普通动量有些许不同，最近变得比较流行。在理论上对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。Nesterov动量的核心思路是，当参数向量位于某个位置x时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过mu v稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置x+mu v看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，计算x+mu* v的梯度而不是“旧”位置x的梯度就有意义了。 Nesterov动量。既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个“向前看”的地方计算梯度。也就是说，添加一些注释后，实现代码如下：1234x_ahead = x + mu * v# 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)v = mu * v - learning_rate * dx_aheadx += v 然而在实践中，人们更喜欢和普通SGD或上面的动量方法一样简单的表达式。通过对x_ahead = x + mu * v使用变量变换进行改写是可以做到的，然后用x_ahead而不是x来表示上面的更新。也就是说，实际存储的参数向量总是向前一步的那个版本。x_ahead的公式（将其重新命名为x）就变成了：123v_prev = v # 存储备份v = mu * v - learning_rate * dx # 速度更新保持不变x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式 对于NAG（Nesterov’s Accelerated Momentum）的来源和数学公式推导，我们推荐以下的拓展阅读： Yoshua Bengio的Advances in optimizing Recurrent Networks，Section 3.5。 http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf在section 7.2对于这个主题有更详尽的阐述。 学习率退火在训练深度网络的时候，让学习率随着时间退火通常是有帮助的。可以这样理解：如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。知道什么时候开始衰减学习率是有技巧的：慢慢减小它，可能在很长时间内只能是浪费计算资源地看着它混沌地跳动，实际进展很少。但如果快速地减少它，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有3种方式： 随步数衰减：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体的问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。 指数衰减：数学公式是$\alpha = \alpha_0e^{-kt}$，其中,$\alpha_0,k$是超参数，t是迭代次数（也可以使用周期作为单位）。 1/t衰减：数学公式是$\alpha=\alpha_0/(1+kt)$,$\alpha_0,k$是超参数，t是迭代次数。在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。 二阶方法在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下$$x \leftarrow x - [Hf(x)]^{-1}\nabla f(x)$$这里Hf(x)是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。$\nabla f(x)$是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。然而，上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的拟-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是L-BFGS，该方法使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。 然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。 实践，在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。 参考资料： Large Scale Distributed Deep Networks 一文来自谷歌大脑团队，比较了在大规模数据情况下L-BFGS和SGD算法的表现。 SFO算法想要把SGD和L-BFGS的优势结合起来。 逐参数适应学习率方法前面讨论的所有方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作投入到发明能够适应性地对学习率调参的方法，甚至是逐个参数适应学习率调参。很多这些方法依然需要其他的超参数设置，但是其观点是这些方法对于更广范围的超参数比原始的学习率方法有更良好的表现。在本小节我们会介绍一些在实践中可能会遇到的常用适应算法： Adagrad是一个由Duchi等提出的适应性学习率算法：123# 假设有梯度和参数向量xcache += dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) 注意，变量cache的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数的梯度的平方和。这个一会儿将用来归一化参数更新步长，归一化是逐元素进行的。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强。有趣的是平方根的操作非常重要，如果去掉，算法的表现将会糟糕很多。用于平滑的式子eps（一般设为1e-4到1e-8之间）是防止出现除以0的情况。Adagrad的一个缺点是，在深度学习中单调的学习率被证明通常过于激进且过早停止学习。 RMSprop是一个非常高效，但没有公开发表的适应性学习率方法。有趣的是，每个使用这个方法的人在他们的论文中都引用自Geoff Hinton的Coursera课程的第六课的第29页PPT。这个方法用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均：12cache = decay_rate * cache + (1 - decay_rate) * dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) 在上面的代码中，decay_rate是一个超参数，常用的值是[0.9,0.99,0.999]。其中x+=和Adagrad中是一样的，但是cache变量是不同的。因此，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这同样效果不错。但是和Adagrad不同，其更新不会让学习率单调变小。 Adam是最近才提出的一种更新方法，它看起来像是RMSProp的动量版。简化的代码是下面这样：123m = beta1*m + (1-beta1)*dxv = beta2*v + (1-beta2)*(dx**2)x += - learning_rate * m / (np.sqrt(v) + eps) 注意这个更新方法看起来真的和RMSProp很像，除了使用的是平滑版的梯度m，而不是用的原始梯度向量dx。论文中推荐的参数值eps=1e-8, beta1=0.9, beta2=0.999。在实际操作中，我们推荐Adam作为默认的算法，一般而言跑起来比RMSProp要好一点。但是也可以试试SGD+Nesterov动量。完整的Adam更新算法也包含了一个偏置（bias）矫正机制，因为m,v两个矩阵初始为0，在没有完全热身之前存在偏差，需要采取一些补偿措施。建议读者可以阅读论文查看细节，或者课程的PPT。拓展阅读： Unit Tests for Stochastic Optimization一文展示了对于随机最优化的测试。 参数调优我们已经看到，训练一个神经网络会遇到很多超参数设置，神经网络最常用的设置有： 初始化学习率； 学习率衰减方式（例如一个衰减常量） 正则化强度（L2惩罚，随机失活强度）但是也可以看到，还有很多相对不那么敏感的超参数。比如在逐参数适应学习方法中，对于动量及时间表的设置等。在本节中将介绍一些额外的调参要点和技巧： 实现：更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要，因为这会影响你设计代码的思路。一个具体的设计是用仆程序持续地随机设置参数然后进行最优化。在训练过程中，仆程序会对每个周期后验证集的准确率进行监控，然后向文件系统写下一个模型的记录点（记录点中有各种各样的训练统计数据，比如随着时间的损失值变化等），这个文件系统最好是可共享的。在文件名中最好包含验证集的算法表现，这样就能方便地查找和排序了。然后还有一个主程序，它可以启动或者结束计算集群中的仆程序，有时候也可能根据条件查看仆程序写下的记录点，输出它们的训练统计数据等。 比起交叉验证最好使用一个验证集：在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。你可能会听到人们说他们“交叉验证”一个参数，但是大多数情况下，他们实际是使用的一个验证集。 超参数范围，在对数尺度上进行超参数搜索，例如，一个典型的学习率应该看起来是这样：learning_rate = 10**uniform(-6, 1)。也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有承的效果。例如：当学习率是0.001的时候，如果对其固定地增加0.001，那么对于学习进程会有很大的影响。然而当学习率是10的时候，影响就微乎其微了。。这就是因为学习率乘以了计算出的梯度。因此，比起加上或者减少某些值，思考学习率的范围是乘以或者除以某些值更加自然。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：dropout=uniform(0,1)）。 随机搜索优于网格搜索，Bergstra和Bengio在文章Random Search for Hyper-Parameter Optimization中说“随机选择比网格化的选择更加有效”，而且在实践中也更容易实现。 对于边界上的最优值要小心：这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使用learning_rate = 10**uniform(-6,1)来进行搜索。一旦我们得到一个比较好的值，一定确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。 从粗到细地分阶段搜索，在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练一个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。 贝叶斯超参数最优化是一整个研究领域，主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。基于这些模型，发展出很多的库，比较有名的有： Spearmint, SMAC, 和Hyperopt。然而，在卷积神经网络的实际使用中，比起上面介绍的先认真挑选的一个范围，然后在该范围内随机搜索的方法，这个方法还是差一些。这里有http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html更详细的讨论。 评价模型集成在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法： 同一个模型，不同的初始化，使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。这种方法的风险在于多样性只来自于不同的初始化条件。 在交叉验证中发现最好的模型，使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。 一个模型设置多个记录点，如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。 在训练的时候跑参数的平均值，和上面一点相关的，还有一个也能得到1-2个百分点的提升的小代价方法，这个方法就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样你就对前几次循环中的网络状态进行了平均。你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。 模型集成的一个劣势就是在测试数据的时候会花费更多时间。最近Geoff Hinton在“Dark Knowledge”上的工作很有启发：其思路是通过将集成似然估计纳入到修改的目标函数中，从一个好的集成中抽出一个单独模型。 总结训练一个神经网络需要： 利用小批量的数据对实现进行梯度检查，还要注意各种错误。 进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。 在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数相对于总参数的比例（一般在1e-3左右）然后如果是对于卷积神经网络，可以将第一层的权重可视化。 推荐的两个更新方法是SGD+Nesterov动量方法，或者是Adam方法。 随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。 使用随机搜索（不要使用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。 进行模型集成来获得额外的性能提高。 拓展阅读 Leon Botton的《SGD要点和技巧》：https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F192769%2Ftricks-2012.pdf Yann LeCun的《Efficient BackProp》：http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf Yoshua Bengio的《Practical Recommendations for Gradient-Based Training of Deep Architectures》。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(7) Neural Nets Notes 2]]></title>
    <url>%2F2017%2F12%2F12%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC7%E8%AF%BE-Neural-Nets-Notes-2%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-2/ 目录 设置数据和模型 数据预处理 权重初始化 批量归一化（Batch Normalization） 正则化（L1/L2/Maxnorm/Dropout） 损失函数 小结 设置数据和模型上一节中介绍了神经元的模型，它在计算内积后进行非线性激活函数计算，神经网络将这些神经元组织成各个层。这些做法共同定义了评分函数（score function）的新形式，该形式是从前面线性分类章节中的简单线性映射发展而来的。具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。本节将讨论更多的算法设计选项，比如数据预处理，权重初始化和损失函数。 数据预处理关于数据预处理我们有3个常用的符号，数据矩阵X，假设其尺寸是[N x D]（N是数据样本的数量，D是数据的维度）。 均值减法（Mean Subtraction）:是预处理最常用的形式。它对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码X-=np.mean(X,axis=0)实现。对于图像，更常用的是对所有像素都减去一个值，可以用X -= np.mean(X)实现，也可以在3个颜色通道上分别操作。 归一化操作（Normalization）:是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)。第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。 一般数据预处理流程：左边：原始的2维输入数据。中间：在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。右边：每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。PCA和白化（Whitening）:是另一种数据预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。 123# 假设输入数据矩阵X的尺寸为[N x D]X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵 数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的协方差。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和半正定的。我们可以对数据协方差矩阵进行SVD（奇异值分解）运算。1U,S,V = np.linalg.svd(cov) U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：1Xrot = np.dot(X,U) # 对数据去相关性 注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算Xrot的协方差矩阵，将会看到它是对角对称的。np.linalg.svd的一个良好性质是在它的返回值U中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有方差的维度。 这个操作也被称为主成分分析（ Principal Component Analysis 简称PCA）降维：1Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced 变成 [N x 100] 经过上面的操作，将原始的数据集的大小由[N x D]降到了[N x 100]，留下了数据中包含最大方差的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。最后一个在实践中会看见的变换是白化（whitening）。白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下：123# 对数据进行白化操作:# 除以特征值 Xwhite = Xrot / np.sqrt(S + 1e-5) 警告：夸大的噪声。注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。 PCA/白化。左边是二维的原始数据。中间：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。右边：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。 实践操作：在这个笔记中提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。 常见错误：进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练、验证、测试集，那么这个做法是错误的。应该怎么做呢？应该先分成训练、验证、测试集，只是从训练集中求图片平均值，然后各个集（训练、验证、测试集）中的图像再减去这个平均值。 权重初始化我们已经看到如何构建一个神经网络的结构并对数据进行预处理，但是在开始训练网络之前，还需要初始化网络的参数。 错误：全零初始化：让我们从应该避免的错误开始。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。 小随机数初始化：因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是：W = 0.01 * np.random.randn(D,H)。其中，randn函数是基于零均值和标准差的一个高斯分布来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。 警告：并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会出现非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。 使用1/sqrt(n)校准方差，上面的做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为：w = np.random.randn(n) / sqrt(n)。其中n是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。上述结论的推导过程如下：假设权重w和输入x之间的内积为$s=\sum^n_iw_ix_i$，这是还没有进行非线性激活函数运算之前的原始数值。我们可以检查s的方差：$$Var(s)=Var(\sum_i^n w_ix_i)\\\\=\sum_i^nVar(w_ix_i)\\\\=\sum_i^n[E(w_i)]^2Var(x_i)+E[(x_i)]^2Var(w_i)+Var(xIi)Var(w_i)\\\\=\sum_i^nVar(x_i)Var(w_i)\\\\=(nVar(w))Var(x)$$在前两步，使用了方差的性质。在第三步，因为假设输入和权重的平均值都是0，所以$E[x_i]=E[w_i]=0$。注意这并不是一般化情况，比如在ReLU单元中均值就为正。在最后一步，我们假设所有的$w_i,x_i$都服从同样的分布。从这个推导过程我们可以看见，如果想要s有和输入x一样的方差，那么在初始化的时候必须保证每个权重w的方差是1/n。又因为对于一个随机变量X和标量a，有$Var(aX)=a^2Var(X)$，这就说明可以基于一个标准高斯分布，然后乘以$a=\sqrt{1/n}$，使其方差为1/n，于是得出：w = np.random.randn(n) / sqrt(n)。 Glorot等在论文Understanding the difficulty of training deep feedforward neural networks中作出了类似的分析。在论文中，作者推荐初始化公式为 $Var(w) = 2/(n_{in} + n_{out})$ ，其中$n_{in}, n_{out}$是在前一层和后一层中单元的个数。这是基于妥协和对反向传播中梯度的分析得出的结论。该主题下最新的一篇论文是：Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification，作者是He等人。文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是2.0/n。代码为w = np.random.randn(n) * sqrt(2.0/n)。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。 稀疏初始化（Sparse initialization）:另一个处理非标定方差的方法就是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。 偏置（biases）的初始化，通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。 实践，当前的推荐是使用ReLU激活函数，并且使用w = np.random.randn(n) * sqrt(2.0/n)来进行权重初始化。 批量归一化（Batch Normalization）:批量归一化是loffe和Szegedy最近才提出的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛：）,其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层与激活函数之间添加一个BatchNorm层。对于这个技巧本节不会展开讲，因为参考文献：https://arxiv.org/abs/1502.03167中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。搞定！ 正则化Regularization有不少方法是通过控制神经网络的容量来防止其过拟合的：L2正则化可能是最常用的正则化方法了，可以通过惩罚目标函数中所有参数的平方将其实现。即对网络中的每个权重w，向目标函数中增加一个$\frac {1}{2}\lambda w^2$，其中$\lambda$是正则化强度。前面这个1/2很常见，是因为加上1/2后，该式子关于w梯度就是$\lambda w$而不是$2\lambda w$了，L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。在线性分类章节中讨论过，由于输入和权重之间的乘法操作，这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以w += -lambda * W向着0线性下降。 L1正则化是另一个相对常用的正则化方法。对于每个w我们都向目标函数增加一个\lambda|w|。L1和L2正则化也可以进行组合：$\lambda_1|w|+\lambda_2w^2$，这也被称作Elastic net regularizaton。L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。 最大范式约束（Max norm constraints),另一种形式的正则化是给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量$\overrightarrow{w}$必须满足$||\overrightarrow{w}||_2&lt;c$这一条件，一般c值为3或者4。有研究者发文称在使用这种正则化方法时效果更好。这种正则化还有一个良好的性质，即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的。 随机失活（Dropout）是一个简单又极其有效的正则化方法。该方法由Srivastava在论文Dropout: A Simple Way to Prevent Neural Networks from Overfitting中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0。 图片来自于论文：http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf。展示其核心思路，在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相对独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为对数量巨大的子网们做了模型集成，以此来计算出一个平均的预测。 一个3层神经网络的普通版随机失活可以用下面代码实现：123456789101112131415161718192021222324""" 普通版随机失活: 不推荐实现 (看下面笔记) """p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱def train_step(X): """ X中是输入数据 """ # 3层neural network的前向传播 H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = np.random.rand(*H1.shape) &lt; p # 第一个随机失活遮罩 H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = np.random.rand(*H2.shape) &lt; p # 第二个随机失活遮罩 H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # 反向传播:计算梯度... (略) # 进行参数更新... (略) def predict(X): # 前向传播时模型集成 H1 = np.maximum(0, np.dot(W1, X) + b1) * p # 注意：激活数据要乘以p H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # 注意：激活数据要乘以p out = np.dot(W3, H2) + b3 在上面的代码中，train_step函数在第一个隐层和第二个隐层上进行了两次随机失活。在输入层上面进行随机失活也是可以的，为此需要为输入数据X创建一个二值的遮罩。反向传播保持不变，但是肯定需要将遮罩U1和U2加入进去。 注意：在predict函数中不进行随机失活，但是对于两个隐层的输出都要乘以p，调整其数值范围。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以p=0.5为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。为了理解这点，先假设有一个神经元x的输出，那么进行随机失活的时候，该神经元的输出就是px+(1-p)0，这是有1-p的概率神经元的输出为0。在测试时神经元总是激活的，就必须调整$x\to px$来保持同样的预期输出。在测试时会在所有可能的二值遮罩（也就是数量庞大的所有子网络）中迭代并计算它们的协作预测，进行这种减弱的操作也可以认为是与之相关的。 上述操作不好的性质是必须在测试时对激活数据要按照p进行数值范围调整。既然测试性能如此关键，实际更倾向使用反向随机失活（inverted dropout），它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下：12345678910111213141516171819202122232425""" 反向随机失活: 推荐实现方式.在训练的时候drop和调整数值范围，测试时不做任何事."""p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱def train_step(X): # 3层neural network的前向传播 H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = (np.random.rand(*H1.shape) &lt; p) / p # 第一个随机失活遮罩. 注意/p! H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = (np.random.rand(*H2.shape) &lt; p) / p # 第二个随机失活遮罩. 注意/p! H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # 反向传播:计算梯度... (略) # 进行参数更新... (略)def predict(X): # 前向传播时模型集成 H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了 H2 = np.maximum(0, np.dot(W2, H1) + b2) out = np.dot(W3, H2) + b3 在随机失活发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献： Dropout paper by Srivastava et al. 2014. Dropout Training as Adaptive Regularization：“我们认为：在使用费希尔信息矩阵（fisher information matrix）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。” 前向传播中的噪音：在更一般化的分类上，随机失活属于网络在前向传播中有随机行为的方法。测试时，通过分析法（在使用随机失活的本例中就是乘以p）或数值法（例如通过抽样出很多子网络，随机选择不同子网络进行前向传播，最后对它们取平均）将噪音边缘化。在这个方向上的另一个研究是DropConnect，它在前向传播的时候，一系列权重被随机设置为0。提前说一下，卷积神经网络同样会吸取这类方法的优点，比如随机汇合（stochastic pooling），分级汇合（fractional pooling），数据增长（data augmentation）。我们在后面会详细介绍。 偏置正则化：在线性分类器的章节中介绍过，对于偏置参数的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差。这可能是因为相较于权重参数，偏置参数实在太少，所以分类器需要它们来获得一个很好的数据损失，那么还是能够承受的。 每层正则化：对于不同的层进行不同强度的正则化很少见（可能除了输出层以外），关于这个思路的相关文献也很少。 实践：通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。p值一般默认设为0.5，也可能在验证集上调参。 损失函数我们已经讨论过损失函数的正则化损失部分，它可以看做是对模型复杂程度的某种惩罚。损失函数的第二个部分时数据损失。它是一个有监督学习问题，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性。数据损失是对所有样本的数据损失求平均。也就是说，$L=\frac{1}{N}\sum_iL_i$中，N是训练集数据的样本数。让我们把神经网络中输出层的激活函数简写为f=f(x_i;W)，在实际中你可能需要解决以下几类问题： 分类问题我们一直讨论的。在该问题中，假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签（是固定分类标签之一）。在这类问题中，一个最常见的损失函数就是SVM（是Weston Watkins 公式）：$$L_i=\sum_{j\not=y_i}max(0,f_j-f_{y_i}+1)$$之前简要提起过，有些学者的论文中指出平方折叶损失（即使用max(0,f_j-f_{y_i}+1)^2）算法的结果会更好。第二个常用的损失函数是Softmax分类器，它使用交叉熵损失：$$\displaystyle L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})$$ 问题：类别数目巨大。当标签集非常庞大（例如字典中的所有英语单词，或者ImageNet中的22000种分类），就需要使用分层Softmax（Hierarchical Softmax）了（参考文献：https://arxiv.org/pdf/1310.4546.pdf）分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。 属性（Attribute）分类。上面两个损失公式的前提，都是假设每个样本只有一个正确的标签y_i。但是如果y_i是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间并不相互排斥呢？比如在Instagram上的图片，就可以看成是被一个巨大的标签集合中的某个子集打上标签，一张图片上可能有多个标签。在这种情况下，一个明智的方法是为每个属性创建一个独立的二分类的分类器。例如，针对每个分类的二分类器会采用下面的公式：$$L_i=\sum_j max(0,1-y_{ij}f_j)$$上式中，求和是对所有分类j，$y_{ij}$的值为1或者-1，具体根据第i个样本是否被第j个属性打标签而定，当该类别被正确预测并展示的时候，分值向量$f_j$为正，其余情况为负。可以发现，当一个正样本的得分小于+1，或者一个负样本的得分大于-1的时候，算法就会累计损失值。 另一种方法是对每种属性训练一个独立的逻辑回归分类器，二分类的逻辑回归只有两个分类（0，1），其中对于分类1的概率计算为：$$P(y=1|x;w,b) = \frac {1}{1+e^{-(w^Tx+b)}}=\sigma(w^Tx+b)$$因为类别0和类别1的概率和为1，所以类别0的概率为：$\displaystyle P(y=0|x;w,b)=1-P(y=1|x;w,b)$。这样，如果$\sigma(w^Tx+b)&gt;0.5$或者$w^Tx+b&gt;0$，那么样本就要被分类成为正样本（y=1）。然后损失函数最大化这个对数似然函数，问题可以简化为：$$L_i = \sum_j y_{ij}log(\sigma(f_j)) + (1-y_{ij})log(1-\sigma(f_j))$$式中，假设标签$y_{ij}$非0即1，$\sigma(.)$就是sigmoid函数。上面的公式看起来吓人，但是f的梯度实际上非常简单：$\displaystyle \frac{\partial L_i}{\partial f_j}=y_{ij}-\sigma(f_j)$（你可以自己求导来验证）。 回归问题是预测实数的值的问题，比如房价预测，预测图片中某个东西的长度等等。对于这种问题，通常是计算预测值和真实值之间的损失。然后用L2或者L1范数度量差异。对于某个样本，L2范数计算如下：$$L_i=||f-y_i||_2^2$$之所以在目标函数中要进行平方，是因为梯度算起来更加简单。因为平方是一个单调运算，所以不用改变最优参数。L1范式则是要将每个维度上的绝对值加起来：$$L_i = ||f - y_i||_1=\sum_j|f_j - (y_i)j|$$在上式中，如果有多个数量被预测了，就要对预测的所有维度的预测求和，即$\sum_j$。观察第i个样本的第j维，用$\delta_{ij}$表示预测值与真实值之间的差异。关于该维度的梯度（也就是$\partial L_i/\partial f_j）$能够轻松地通过被求导为L2范式的$\delta_{ij}$或$sign(\delta_{ij})$。这就是说，评分值的梯度要么与误差中的差值直接成比例，要么是固定的并从差值中继承sign。 注意：L2损失比起较为稳定的Softmax损失来，其最优化过程要困难很多。直观而言，它需要网络具备一个特别的性质，即对于每个输入（和增量）都要输出一个确切的正确值。而在Softmax中就不是这样，每个评分的准确值并不是那么重要：只有当它们量级适当的时候，才有意义。还有，L2损失鲁棒性不好，因为异常值可以导致很大的梯度。所以在面对一个回归问题时，先考虑将输出变成二值化是否真的不够用。例如，如果对一个产品的星级进行预测，使用5个独立的分类器来对1-5星进行打分的效果一般比使用一个回归损失要好很多。分类还有一个额外优点，就是能给出关于回归的输出的分布，而不是一个简单的毫无把握的输出值。如果确信分类不适用，那么使用L2损失吧，但是一定要谨慎：L2非常脆弱，在网络中使用随机失活（尤其是在L2损失层的上一层）不是好主意。 当面对一个回归任务，首先考虑是不是必须这样。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。 结构化预测（structured prediction）,结构化损失是指标签可以是任意的结构，例如图表、树或者其他复杂物体的情况。通常这种情况还会假设结构空间非常巨大，不容易进行遍历。结构化SVM背后的基本思想就是在正确的结构y_i和得分最高的非正确结构之间画出一个边界。解决这类问题，并不是像解决一个简单无限制的最优化问题那样使用梯度下降就可以了，而是需要设计一些特殊的解决方案，这样可以有效利用对于结构空间的特殊简化假设。我们简要地提一下这个问题，但是详细内容就超出本课程范围。 小结 推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化到[-1,1]范围之内。 使用标准差为\sqrt{2/n}的高斯分布来初始化权重，其中n是输入的神经元数。例如用numpy可以写作：w = np.random.randn(n) * sqrt(2.0/n)。 使用L2正则化和随机失活的倒置版本。 使用批量归一化。 讨论了在实践中可能要面对的不同任务，以及每个任务对应的常用损失函数。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(6) Neural Nets Notes 1]]></title>
    <url>%2F2017%2F12%2F10%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC6%E8%AF%BE-Neural-Nets-Notes-1%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/neural-networks-1/ 目录 简介 单个神经元建模 生物动机和连接 作为线性分类器的单个神经元 常用的激活函数 神经网络结构 层组织 前向传播计算例子 表达能力 设置层的数量和尺寸 小结 简介在线性分类一节中，在给出图像的情况下，使用$s=Wx$来计算不同视觉类别的评分，其中W是一个矩阵，x是一个输入列向量，它包含了图像的全部像素数据。在使用数据库CIFAR-10的案例中，x是一个[30721]的列向量，W是一个[103072]的矩阵，所有输出的评分是一个包含10个类别评分的向量。神经网络算法则不同，它的计算公式是$s=W_2max(0,W_1x)$。其中$W_1$的含义是这样的：举个例子来说，它可以是一个[100*3072]的矩阵，其作用是将图像转化为一个100维的过渡向量。函数max(0,-)是非线性的，它会作用到每个元素。这个非线性函数有多种选择，后续将会学到。但这个形式是一个最常用的选择，它就是简单地设置阈值，将所有小于0的值变成0。最终，矩阵$W_2$的尺寸是[10x100]，因此将得到10个数字，这10个数字可以解释为是分类的评分。注意非线性函数在计算上是至关重要的，如果略去这一步，那么两个矩阵将会合二为一，对于分类的评分计算将重新变成关于输入的线性函数。这个非线性函数就是改变的关键点。参数$W_1$,$W_2$将通过随机梯度下降来学习到，他们的梯度在反向传播过程中，通过链式法则来求导计算得出。 一个三层的神经网络可以类比地看做$s=W_3max(0,W_2max(0,W_1x))$，其中$W_1$,$W_2$,$W_3$是需要进行学习的参数。中间隐层的尺寸是网络的超参数，后续将学习如何设置它们。现在让我们先从神经元或者网络的角度理解上述计算。 单个神经元建模神经网络算法领域最初是对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好的效果。然而，讨论将还是从对生物系统的一个高层次的简要描述开始，因为神经网络毕竟是从这里得到了启发。 生物动机与连接大脑的基本计算单位是神经元（neuron）。人类的神经系统中大约有860亿个神经元，它们被大约10^14-10^15个突触（synapses）连接起来。下面图表的左边展示了一个生物学的神经元，右边展示了一个常用的数学模型。每个神经元都从它的树突获得输入信号，然后沿着它唯一的轴突（axon）产生输出信号。轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。 在神经元的计算模型中，沿着轴突传播信号（比如将$x_0$）将基于突触的突触强度（比如$w_0$），与其他神经元的树突进行乘法交互（比如$w_0x_0$）。其观点是，突触的强度（也就是权重w），是可学习的且可以控制一个神经元对于另一个神经元的影响强度（还可以控制影响方向：使其兴奋（正权重）或使其抑制（负权重））。在基本模型中，树突将信号传递到细胞体，信号在细胞体中相加。如果最终之和高于某个阈值，那么神经元将会激活，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号的频率在交流信息。基于这个速率编码的观点，将神经元的激活率建模为激活函数（activation function）f，它表达了轴突上激活信号的频率。由于历史原因，激活函数常常选择使用sigmoid函数$\sigma$，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。在本节后面部分会看到这些激活函数的各种细节。一个神经元前向传播的实例代码如下：123456class Neuron(object): def forward(inputs): """ 假设输入和权重是1-D的numpy数组，偏差是一个数字 """ cell_body_sum = np.sum(inputs * self.weights) + self.bias firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid激活函数 return firing_rate 换句话说，每个神经元都对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数（或称为激活函数）。本例中使用的是sigmoid函数$\sigma(x)=1/(1+e^{-x})$。在本节的末尾部分将介绍不同激活函数的细节。 作为线性分类器的单个神经元神经元模型的前向计算数学公式看起来可能比较眼熟。就像在线性分类器中看到的那样，神经元有能力”喜欢”（激活函数值接近1），或者不喜欢（激活函数值接近0）输入空间中的某些线性区域。因此，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。 二分类Softmax分类器，举例来说，可以把$\sigma(\sum_i w_ib_i+b)$看做其中一个分类的概率$P(y_i=1|x_i;w)$，其他分类的概率为$P(y_i=0|x_i;w)=1-P(y_i=1|x_i;w)$，因为它们加起来必须为1。根据这种理解，可以得到交叉熵损失，这个在线性分一节中已经介绍。然后将它最优化为二分类的Softmax分类器（也就是逻辑回归）。因为sigmoid函数输出限定在0-1之间，所以分类器做出预测的基准是神经元的输出是否大于0.5。 二分类SVM分类器，或者可以在神经元的输出外增加一个最大边界折叶损失（max-margin hinge loss）函数，将其训练成一个二分类的支持向量机。 一个单独的神经元可以用来实现一个二分类器，比如二分类的Softmax或者SVM分类器。 常用的激活函数每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数： 左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。 Sigmoid函数Sigmoid非线性函数的数学公式是$\sigma(x) = \frac {1}{1+e^{-x}}$，函数图像如上图的左边所示。在前面一节中已经提到，它输入实数值，并将其“挤压”到0到1范围内。更具体的说很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活(0)到在求和后的最大频率处的完全饱和的激活（1）。然而，现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点： Sigmoid函数饱和使梯度消失：Sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。 Sigmoid函数的输出不是零中心的：这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在$f=w^Tx+b$中每个元素都x&gt;0），那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式f而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。 Tanh函数Tanh函数图像如上图右边所示。它将实数值压缩到[-1,1]之间。和Sigmoid神经元一样，它也存在饱和的问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。注意tanh神经元是一个简单放大的sigmoid神经元，具体说来就是：$tanh(x)=2\sigma(2x)-1$。 左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当x=0时函数值为0。当x&gt;0函数的斜率为1。右边是从Krizhevsky等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。 ReLU:在近些年ReLU变得非常流行。它的函数公式是$f(x)=max(0,x)$。换句话说，这个激活函数就是一个关于0的阈值。使用Relu有以下一些优缺点： 优点：相较于Sigmoid和Tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用。据称这是由它的线性，非饱和的公式导致的。 优点：Sigmoid和Tanh神经元含有指数运算等耗费计算资源的操作，而Relu可以简单地对一个矩阵进行阈值计算得到。 缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。 Leaky ReLU:Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x=0)(x)$其中$\alpha$是一个小的常量。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。 Maxout:一些其他类型的单元被提了出来，它们对于权重和数据的内积结果不再使用$f(w^Tx+b)$函数形式。一个相关的流行选择是Maxout（最近由Goodfellow等发布）神经元。Maxout是对ReLU和leaky ReLU的一般化归纳，它的函数是：$max(w^T_1x+b_1,w^T_2x+b_2)$。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当$w_1,b_1=0$的时候）。这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。 以上就是一些常用的神经元及其激活函数。最后需要注意一点：在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有什么根本性问题来禁止这样做。 一句话：“那么该用那种呢？”用ReLU非线性函数。注意设置好学习率，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。 神经网络结构灵活地组织层将神经网络算法以神经元的形式图形化。神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接。也就是说，一些神经元的输出是另一些神经元的输入。在网络中是不允许循环的，因为这样会导致前向传播的无限循环。通常神经网络模型中神经元是分层的，而不是像生物神经元一样聚合成大小不一的团状。对于普通神经网络，最普通的层的类型是全连接层（fully-connected layer）。全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接。下面是两个神经网络的图例，都使用的全连接层： 左边是一个2层神经网络，隐层由4个神经元（也可称为单元（unit））组成，输出层由2个神经元组成，输入层是3个神经元。右边是一个3层神经网络，两个含4个神经元的隐层。注意：层与层之间的神经元是全连接的，但是层内的神经元不连接。命名规则：当我们说N层神经网络的时候，我们没有把输入层算入。因此，单层的神经网络就是没有隐层的（输入直接映射到输出）。因此，有的研究者会说LR或者SVM只是单层神经网络的一个特例。研究者们也会使用人工神经网络或者多层感知器来指代神经网络。很多研究者并不喜欢神经网络算法和人类大脑之间的类比，它们更倾向于用单元(unit)而不是神经元作为术语。输出层：和神经网络中其他层不同，输出层的神经元一般是不会有激活函数的（或者也可以认为它们有一个线性相等的激活函数）。这是因为最后的输出层大多用于表示分类评分值，因此是任意值的实数，或者某种实数值的目标数（比如在回归中）。确定网络尺寸：用来度量神经网络的尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数，用上面图示的两个网络举例： 第一个网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。 第二个网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。 为了方便对比，现代卷积神经网络能包含约1亿个参数，可由10-20层构成（这就是深度学习）。然而，有效（effective）连接的个数因为参数共享的缘故大大增多。在后面的卷积神经网络内容中我们将学习更多。 前向传播计算举例不断重复的矩阵乘法与激活函数交织。将神经网络组织成层状的一个主要原因，就是这个结构让神经网络算法使用矩阵向量操作变得简单和高效。用上面用上面那个3层神经网络举例，输入是[3x1]的向量。一个层所有连接的强度可以存在一个单独的矩阵中。比如第一个隐层的权重W1是[4x3]，所有单元的偏置储存在b1中，尺寸[4x1]这样，每个神经元的权重都在W1的一个行中，于是矩阵乘法np.dot(W1, x)就能计算该层中所有神经元的激活数据。类似的，W2将会是[4x4]矩阵，存储着第二个隐层的连接，W3是[1x4]的矩阵，用于输出层。完整的3层神经网络的前向传播就是简单的3次矩阵乘法，其中交织着激活函数的应用。123456# 一个3层神经网络的前向传播:f = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)h1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)h2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)out = np.dot(W3, h2) + b3 # 神经元输出(1x1) 在上面的代码中，W1，W2，W3，b1，b2，b3都是网络中可以学习的参数。注意x并不是一个单独的列向量，而可以是一个批量的训练数据（其中每个输入样本将会是x中的一列），所有的样本将会被并行化的高效计算出来。注意神经网络最后一层通常是没有激活函数的（例如，在分类任务中它给出一个实数值的分类评分）。 全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。 表达能力理解具有全连接层的神经网络的一个方式是：可以认为它们定义了一个由一系列函数组成的函数族，网络的权重就是每个函数的参数。如此产生的问题是：该函数族的表达能力如何？存在不能被神经网络表达的函数吗？ 现在看来，拥有至少一个隐层的神经网络是一个通用的近似器。在研究（例如1989年的论文Approximation by Superpositions of Sigmoidal Function，或者Michael Nielsen的这个直观解释。）中已经证明，给出任意连续函数f(x)和任意$\epsilon &gt;0$，均存在一个至少含1个隐层的神经网络g(x)（并且网络中有合理选择的非线性激活函数，比如sigmoid，对于$\forall x$，使得$|f(x)-g(x)|&lt;\epsilon$。换句话说，神经网络可以近似任何连续函数。 既然一个隐层就能近似任何函数，那为什么还要构建更多层来将网络做得更深？ 答案是：虽然一个2层网络在数学理论上能完美地近似所有连续函数，但在实际操作中效果相对较差。在一个维度上，虽然以a,b,c为参数向量“指示块之和”函数$g(x)=\sum_ic_i1(a_i&lt;x&lt;b_i)$ 也是通用的近似器，但是谁也不会建议在机器学习中使用这个函数公式。神经网络在实践中非常好用，是因为它们表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，网络通过最优化算法（例如梯度下降）能比较容易地学习到这个函数。类似的，虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好。 另外，在实践中3层的神经网络会比2层的表现好，然而继续加深（做到4，5，6层）很少有太大帮助。卷积神经网络的情况却不同，在卷积神经网络中，对于一个良好的识别系统来说，深度是一个极端重要的因素（比如数十(以10为量级)个可学习的层）。对于该现象的一种解释观点是：因为图像拥有层次化结构（比如脸是由眼睛等组成，眼睛又是由边缘组成），所以多层处理对于这种数据就有直观意义。 设置层的数量和尺寸在面对一个具体问题的时候该确定网络结构呢？到底是不用隐层呢？还是一个隐层？两个隐层或更多？每个层的尺寸该多大？ 首先，要知道当我们增加层的数量和尺寸时，网络容量上升了。即神经元们可以合作表达许多复杂的函数，所以表达函数的空间增加。例如，如果有一个在二维平面上的二分类问题，我们可以训练3个不同的神经网络，每个网络都只有一个隐藏层，但是每层的神经元数目不同： 数据是用不同颜色的圆点表示他们的不同类别，决策边界是由训练过的神经网络做出的。 在上图中，可以看见有更多神经元的神经网络可以表达更复杂的函数。然而，这既是优势也是不足，优势是可以分类更复杂的数据，不足是可能造成对训练数据的过拟合。过拟合是网络对数据中的噪音有很强的拟合能力，而没有重视数据间（假设）的潜在基本关系。举例来说，有20个神经元隐层的网络拟合了所有的训练数据，但是其代价是把决策边界变成了许多不相连的红绿区域。而有3个神经元的模型的表达能力只能用比较宽泛的方式去分类数据。它将数据看做是两个大块，并把个别在绿色区域内的红色点看做噪声。在实际中，这样可以在测试数据中获得更好的泛化（generalization）能力。 基于上面的讨论，看起来如果数据不是足够复杂，则似乎小一点的网络更好，因为可以防止过拟合。然而并非如此，防止神经网络的过拟合有很多方法（L2正则化，dropout和输入噪音等），后面会详细讨论。在实践中，使用这些方法来控制过拟合比减少网络神经元数目要好得多。 不要减少网络神经元的数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练。虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。因为神经网络是非凸的，就很难从数学上研究这些特性。即便如此，还是有一些文章尝试对这些目标函数进行理解，例如The Loss Surfaces of Multilayer Networks这篇论文。在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。重申一下，正则化强度是控制神经网络过拟合的好方法。看下图结果： 不同正则化强度的效果：每个神经网络都有20个隐层神经元，但是随着正则化强度增加，它的决策边界变得更加平滑。 需要记住的是：不应该因为害怕出现过拟合而使用小网络。相反，应该尽可能使用大网络，然后使用正则化技术来控制过拟合。 小结本节课主要介绍如下内容： 介绍了生物神经元的粗略模型； 讨论了几种不同类型的激活函数，其中ReLU是最佳推荐； 介绍了神经网络，神经元通过全连接层连接，层间神经元两两相连，但是层内神经元不连接； 理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算； 理解了神经网络是一个通用函数近似器，但是该性质与其广泛使用无太大关系。之所以使用神经网络，是因为它们对于实际问题中的函数的公式能够某种程度上做出“正确”假设。 讨论了更大网络总是更好的这一事实。然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合。在后续章节中我们讲学习更多正则化的方法，尤其是dropout。 参考资料 使用Theano的deeplearning.net tutorial:http://www.deeplearning.net/tutorial/mlp.html]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(5) Backprop Note]]></title>
    <url>%2F2017%2F12%2F09%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC5%E8%AF%BE-Backprop-Note%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit，并进行一定修改。原文为：http://cs231n.github.io/optimization-2/ 简介目的:本节帮助读者对反向传播形成直观而专业的理解。反向传播是利用链式法则递归计算梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。问题描述：核心问题是：给定函数f(x),其中x是输入数据向量，需要计算函数f关于x的梯度，也就是$\nabla f(x)$ 原因：之所以关注上述问题，是因为在神经网络中f对应的是损失函数L，输入x里面包含训练数据和神经网络权重。举个例子，损失函数可以是SVM的损失函数，输入则包含了训练数据$(x_i,y_i),i=1,…N$，权重W和偏差b。给定训练数据，权重是可以控制的变量。因此，即使使用反向传播计算输入数据$x_i$上的梯度，但在实践上为了进行参数更新，通常也只计算参数(W和b)的梯度。然而$x_i$ 的梯度有时仍然是有用的：比如将神经网络所做的事情可视化便于直观理解的时候，就能用上。 梯度的简单表达、解释首先，考虑一个简单的二元函数f(x,y)=xy。对两个输入变量分别求偏导数，能够很简单求出：$$f(x,y)=xy \rightarrow \frac {df}{dx}=y \frac {df}{dy}=x$$解释：要牢记导数的意义：函数变量在某个点周围的极小区域内变化，而导数就是变量变化导致的函数在该方向上的变化率。$$\frac {df(x)}{dx} = \lim_{h\rightarrow0} \frac {f(x+h)-f(x)}{h}$$对于上述公式，当h的值非常小时，函数可以被一条直线近似，而导数就是这条直线的斜率。换句话说，每个变量的导数指明了整个表达式对于该变量的值的敏感程度。例如，若x=4,y=-3，则f(x,y)=-12,x的导数$\frac {\partial f}{\partial x}=-3$，这就说明将变量x的值变大一点，整个表达式的值就会变小，而且变小的量是x变大的量的三倍。 如上所述，梯度$\nabla f$是偏导数的向量，所以有$\nabla f(x)=[\frac {\partial f}{\partial x},\frac {\partial f}{\partial y}] = [y,x]$。我们可以对加法操作进行求导：$$f(x,y)=x+y \rightarrow \frac {df}{dx}=1 \frac {df}{dy}=1$$这就是说，无论其值如何，x,y的导数均为1。这是有道理的，因为无论增加x,y中任一个的值，函数f的值都会增加，并且增加的变化率独立于x,y的具体值（情况和乘法操作不同）。取最大值操作也是常常使用的：$$f(x,y)=max(x,y)\rightarrow \frac {df}{dx}=1(x&gt;=y) \frac {df}{dy}=1(y&gt;=x)$$上式是说，如果该变量比另一个变量大，那么梯度是1，反之为0。例如，若x=4,y=2，那么max是4，所以函数对于y就不敏感。也就是说，在y上增加h，函数还是输出为4，所以梯度是0：因为对于函数输出是没有效果的。当然，如果给y增加一个很大的量，比如大于2，那么函数f的值就变化了，但是导数并没有指明输入量有巨大变化情况对于函数的效果，他们只适用于输入量变化极小时的情况，因为定义已经指明：$lim_{h\to 0}$。 使用链式法则计算复杂表达式的导数现在考虑更复杂的包含多个函数的复合函数，比如$f(x,y,z)=(x+y)z$。虽然，这个表达式足够简单，可以直接进行微分，但是在此使用一种有助于直观理解反向传播的算法。将公式分为两部分：$q=x+y,f=qz$。在前面已经介绍过如何对这分开的两个公式进行计算导数：$$\frac {\partial f}{\partial q} = z,\frac {\partial f}{\partial z}=q$$因为，q=x+y，所以，$$\frac {\partial q}{\partial x}=1,\frac {\partial q}{\partial y}=1$$然而，并不需要关心中间量q的梯度，因为$\frac {\partial f}{\partial q}$没有用。相反，函数f关于x、y、z的梯度才是需要关注的。链式法则指出将这些梯度表达式链接起来的正确方式是相乘，比如$\frac {\partial f}{\partial x}=\frac {\partial f}{\partial q} \frac {\partial q}{\partial x}$在实际的操作中，只是简单地将两个梯度数值相乘。最后得到变量的梯度[dfdx, dfdy, dfdz]，它们告诉我们函数f对于变量[x, y, z]的敏感程度。这是一个最简单的反向传播。一般会使用一个更简洁的表达符号，这样就不用写df了。这就是说，用dq来代替dfdq，且总是假设梯度是关于最终输出的。这次计算可以被可视化为如下计算线路的图像： 上图的真实值计算线路展示了计算的视觉化过程。前向传播从输入计算到输出（绿色），反向传播从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。 反向传播的直观理解反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输出并立即计算两个东西： 这个门的输出值； 其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算路线中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。 这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等等。 下面通过例子来对这一过程进行理解。加法门收到了输入[-2, 5]，计算输出是3。既然这个门是加法操作，那么对于两个输入的局部梯度都是+1。网络的其余部分计算出的最终值为-12。在反向传播时将递归地使用链式法则，算到加法门的时候，知道加法门的输出梯度是-4。如果网络想要输出值更高，那么可以认为它会想要加法门的输出更小一点，而且还有一个4的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让-4乘以x和y的局部梯度，x和y的局部梯度都是1，所以最终都是-4）。可以看到得到了想要的效果：如果x，y减小（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。 因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高。 模块化：Sigmoid例子上面介绍的门是相对随意的。任何可微分的函数都可以看做门。可以将多个门组合成一个门，也可以根据需求将一个函数拆成多个门。现在看一个表达式：$$f(w, x) = \frac {1}{1+e^{-(w_0x_0+w_1x_1+w_2}}$$在后面的课程中可以看到，这个表达式描述了一个含输入x和权重w的2维的神经元，该神经元使用了sigmoid激活函数。但是现在只是看做是一个简单的输入为x和w，输出为一个数字的函数。这个函数是由多个门组成的。除了上文介绍的加法门，乘法门，取最大值门，还有下面这4种：$$f(x) = \frac {1}{x} \rightarrow \frac{df}{dx} = - \frac {1}{x^2}\\\\f_c(x) = c+x \rightarrow \frac{df}{dx} = 1 \\\\f(x) = e^x \rightarrow \frac{df}{dx} = e^x\\\\f_a(x) = ax \rightarrow \frac{df}{dx} = a\\\\$$其中，函数$f_c$使用对输入值进行了常量c的平移，$f_a$将输入值扩大了常量a倍。它们是加法和乘法的特例，但是这里将其看做一元门单元，因为确实需要计算常量c，a的梯度，整个计算的线路如下： 在上面的例子中可以看见一个函数操作的长链条，链条上的门都对w和x的点积结果进行操作。该函数被称作为sigmoid函数，sigmoid函数关于其输入的求导是可以简化的：$$\sigma(x) = \frac {1}{1+e^{-x}}\\\\\frac {d\sigma(x)}{dx} = \frac {e^{-x}}{(1+e^{-x})^2}=(\frac {1+e^{-x}-1}{1+e^{-x}})(\frac {1}{1+e^{-x}}) = (1-\sigma(x))\sigma(x)$$可以看到梯度计算简单了很多。举个例子，sigmoid表达式输入为1.0，则在前向传播中计算出输出为0.73。根据上面的公式，局部梯度为(1-0.73)*0.73~=0.2，和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。 反向传播实践：分段计算看另外一个例子，假设有如下函数：$$f(x,y) = \frac {x+\sigma(y)}{\sigma(x)+(x+y)^2}$$首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子，需要强调的是，如果对x或y进行微分运算，运算结束后会得到一个巨大而复杂的表达式。然而做如此复杂的运算实际上并无必要，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码模式：1234567891011x = 3 # 例子数值y = -4# 前向传播sigy = 1.0 / (1 + math.exp(-y)) # 分子中的sigmoi #(1)num = x + sigy # 分子 #(2)sigx = 1.0 / (1 + math.exp(-x)) # 分母中的sigmoid #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # 分母 #(6)invden = 1.0 / den #(7)f = num * invden #(8) 到了表达式最后，就完成了前向传播。注意在构建代码s时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。这样计算反向传播就简单了：我们对前向传播时产生每个变量(sigy, num, sigx, xpy, xpysqr, den, invden)进行回传。我们会有同样数量的变量，但是都以d开头，用来存储对应变量的梯度。注意在反向传播的每一小块中都将包含了表达式的局部梯度，然后根据使用链式法则乘以上游梯度。对于每行代码，我们将指明其对应的是前向传播的哪部分。123456789101112131415161718192021# 回传 f = num * invdendnum = invden # 分子的梯度 #(8)dinvden = num #(8)# 回传 invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# 回传 den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# 回传 xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# 回传 xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# 回传 sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# 回传 num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# 回传 sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# 完成! 需要注意的一些事情：对前向传播变量进行缓存：计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际的操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播时也能用上。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。 在不同分支的梯度要相加：如果变量x、y在前向传播的表达式中出现多次，那么进行反向传播时要非常小心使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。 回传流中的模式一个有趣的现象是在多数情况下，反向传播中的梯度可以被很直观的解释。例如，神经网络中最常用的加法、乘法和取最大值的这三个门单元，它们在反向传播过程中的行为都非常简单的解释，先看下面的这个例子： 一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。从此例可知： 加法门单元：把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门就把梯度2.0不变且相等地路由给了两个输入。 取最大值门单元：对梯度做路由，和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余是0。上例中，取最大值门将梯度2.0转给类z变量，因为z的值比w高，于是w的梯度保持为0。 乘法门单元：相对不容易解释，它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.0*2.0 = -8.0。 非直观影响及其结果。注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，那么乘法门的操作将会不是那么直观：它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。在线性分类器中，权重和输入是进行点积$w^Tx_i$，这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本$x_i$乘以1000，那么权重的梯度将会增大1000倍，这样就必须降低学习率来弥补。这就是为什么数据预处理关系重大，它即使只是有微小变化，也会产生巨大影响。对于梯度在计算线路中是如何流动的有一个直观的理解，可以帮助读者调试网络。 小结 对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。 讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(4) Optimization Note]]></title>
    <url>%2F2017%2F12%2F04%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC4%E8%AF%BE-Optimization%2F</url>
    <content type="text"><![CDATA[本文转载自：https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit，原文：http://cs231n.github.io/optimization-1/ 简介图像分类任务中的两个关键部分： 基于参数的评分函数。该函数将原始图像像素映射为分类评分值（例如，一个线性函数）。 损失函数。该函数能够根据评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。上节，线性函数的形式是$f(x_i,W)=Wx_i$，而SVM实现的公式是：$$L=\frac {1}{N}\sum_i\sum_{j \neq y_i} [max(0,f(x_i;W)_j - f(x_i;W)_{y_i}+1)]+\alpha R(W)$$对于图像数据$x_i$，如果基于参数集W做出的分类预测与真实情况比较一致，那么计算出来的损失值L就很低。现在介绍第三个，也是最后一个关键部分：最优化Optimization。最优化是寻找能使得损失函数值最小化的参数W的过程。损失函数可视化课程讨论的损失函数一般都是定义在高维度的空间中，这样要将其进行可视化就很困难。然而办法还是有的，在1个维度或者2个维度的方向上对高维空间进行切片，就能够得到一些直观的感受。例如，随机生成一个权重矩阵W，该矩阵就与高维空间中的一个点对应。然后，沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向$W_1$并且沿着某个维度方向计算损失值，计算方法是根据不同的a值来计算$L(W+aW_1)$。这个过程将生成一个图标，x轴为a值，y轴为损失函数值。同样的方法还可以用在两个维度上，通过改变a、b来计算损失函数值$L(W+aW_1+bW_2)$，从而给出二维的图像。在图像中，a、b可以分别用x轴和y轴表示，而损失函数的值可以用颜色变化表示： 一个无正则化的多类SVM的损失函数的图示。左边和中间只有一个样本数据，右边是CIFAR-10中的100个数据。左：a值变化在某个维度方向上对应的的损失值变化。中和右：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。注意损失函数的分段线性结构。多个样本的损失值是总体的平均值，所以右边的碗状结构是很多的分段线性结构的平均（比如中间这个就是其中之一）。 可以通过数学公式来解释损失函数的分段线性结构，对于一个单独的数据，有损失函数的计算公式如下：$$L_i = \sum_{j \neq y_i}[max(0, W_j^Tx_i - W_{y_i}^Tx_i + 1)]$$通过公式可见，每个样本的数据损失值是以W为参数的线性函数的总和（0阈值来源于max(0,-)函数）。W的每一行（即$w_j$）,有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有3个只有1个维度的点，数据集数据点有3个类别。那么完整的无正则化SVM的损失值计算如下：$$L_0 = max(0, w_1^Tx_0 - w_0^Tx_0 + 1)+max(0, w_2^Tx_0 - w_0^Tx_0 + 1)\\\\L_1 = max(0, w_0^Tx_1 - w_1^Tx_1 + 1)+max(0, w_2^Tx_1 - w_1^Tx_1 + 1)\\\\L_2 = max(0, w_0^Tx_2 - w_2^Tx_2 + 1)+max(0, w_1^Tx_2 -w_2^Tx_2 + 1)\\\\L=(L_0+L_1+L_2)/3$$因为这些例子都是一维的，所以数据$x_i$和权重$w_j$都是数字。观察$w_0$，可以看到上面的式子中一些项是$w_0$的线性函数，且每一项都会与0比较，取两者的最大值。可作图如下： 从一个维度方向上对数据损失值的展示。x轴方向就是一个权重，y轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与0阈值的比较。完整的SVM数据损失就是这个形状的30730维版本。 最优化损失函数可以量化某个具体权重集W的质量，而最优化的目标就是找到能够最小化损失函数值的W。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于一些有经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM损失函数）是一个凸函数。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。策略1：随机搜索既然确认参数集W的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。蒙眼徒步者的比喻：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是30730维的（因为W是3073x10）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。 策略2：随机本地搜索第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机W开始，然后生成一个随机的扰动$\delta W$ ，只有当$W+\delta W$的损失值变低，我们才会更新。 策略3：梯度跟随前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的梯度（gradient）。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为倒数）。对一维函数的求导公式如下：$$\frac {df(x)}{dx} = \lim_{h\rightarrow0}\frac {f(x+h)-f(x)}{h}$$当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。 梯度计算计算梯度有两种方法：一个是缓慢的近似方法（数值梯度法），实现相对简单。另一个方法是（分析梯度法）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对这两种方法进行介绍：利用有限差值计算梯度上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数f和向量x，计算f的梯度的通用函数，它返回函数f在点x处的梯度：12345678910111213141516171819202122def eval_numerical_gradient(f, x): """ 一个f在x处的数值梯度法的简单实现 - f是只有一个参数的函数 - x是计算梯度的点 """ fx = f(x) # 在原点计算函数值 grad = np.zeros(x.shape) h = 0.00001 # 对x中所有的索引进行迭代 it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # 计算x+h处的函数值 ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # 增加h fxh = f(x) # 计算f(x + h) x[ix] = old_value # 存到前一个值中 (非常重要) # 计算偏导数 grad[ix] = (fxh - fx) / h # 坡度 it.iternext() # 到下个维度 return grad 根据上面的梯度公式，代码对所有维度进行迭代，在每个维度上产生一个很小的变化h，通过观察函数值变化，计算函数在该维度上的偏导数。最后，所有的梯度存储在变量grad中。 实践考虑：注意在数学公式中，h的取值是趋近于0的，然而在实际中， 用一个很小的数值就足够了。而不产生数值计算出错的理想前提下，使用尽可能小的h。还有，实际中用中心差值公式（centered difference formula）[f(x+h)-f(x-h)]/2h效果较好。 在扶梯度方向上更新：要注意我们是向着梯度df的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。步长的影响：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。选择步长（也叫作学习率）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。从某个具体的点W开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为学习率）将会是我们在调参中最重要的超参数之一。 微分分析计算梯度使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于h值是选取了一个很小的数值，但真正的梯度定义中h趋向0的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做梯度检查。用SVM的损失函数在某个数据点上的计算来举例：$$L_i = \sum_{j\neq y_i} [max(0, w_j^Tx_i - w_{y_i}^Tx_i)+\Delta]$$可以对函数进行微分，比如，对$w_{y_i}$进行微分得到：$$\nabla_{w_{y_i}}L_i = -(\sum_{j\neq y_i} 1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0))x_i$$其中1是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以x_i就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些$j\neq =y_i$行的梯度是：$$\nabla_{w_j}L_i=1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0)x_i$$一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。 梯度下降现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为梯度下降，他的普通版本是这样的：1234# 普通的梯度下降while True: weights_grad = evaluate_gradient(loss_fun, data, weights) weights += - step_size * weights_grad # 进行梯度更新 这个简单的循环在所有的神经网络核心库中都有。虽然也有其他实现最优化的方法（比如LBFGS），但是到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。 小批量数据梯度下降(Mini-batch gradient descent):在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的小批量（batches）数据。这个方法之所以效果不错，是因为训练集中的数据都是相关的。 小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为随机梯度下降（Stochastic Gradient Descent 简称SGD），有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据 比100次计算1个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。 小结 将损失函数比作了一个高维度的最优化地形，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。 提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。 函数的梯度给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是h，用来计算数值梯度）。 参数更新需要有技巧地设置步长。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。 讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用梯度检查来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(3) 线性分类器]]></title>
    <url>%2F2017%2F12%2F03%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC3%E8%AF%BE-Linear-Classification%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit，原文：http://cs231n.github.io/linear-classify/，并进行一定修改。 本节课主要介绍线性分类器相关的知识，并将其用于图像分类。 线性分类器简介评分函数（score function):它是原始图像数据到类别分值的映射。损失函数（loss function）：它是用来量化预测分类标签的得分与真实值之间的一致性。 从图像到标签分值的参数化映射该方法的第一步就是定义一个评分函数，这个函数将图像的像素值映射为各个类别的得分，得分的高低代表图像属于该类别的可能性高低。假设一个包含很多图像的训练集$x_i \in R^D，i=1,..,N$，每个图像都对应一个分类标签$y_i，i=1,…,K$。在CIFAR-10中，N=50000的训练集，每个图像有D=32*32*3=3072像素，而K=10。因为，图片被分为10个不同的类别。定义评分函数：$f:R^D\rightarrow R^K$，该函数是原始图像像素得到分类分值的映射。线性分类器定义一个线性映射：$$f(x_i,W,b)=Wx_i+b$$其中，参数W称为权重，b称为偏差向量，这是因为它影响输出数值，但是并不和原始数据$x_i$产生关联。同时，需要注意以下几点： 一个单独的矩阵乘法$Wx_i$就可以高效并行评估10个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是W的一个行向量。 注意我们认为输入数据$(x_i,y_i)$是给定且不可改变的，但参数W和b是可控制改变的。我们通过设置这些参数，使得计算出的分类分值情况和训练集中图像数据的真实类别标签相符。 该方法的一个优势是训练数据是用来学习到参数W和b的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。 意只需要做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比k-NN中将测试图像和所有训练数据做比较的方法快多了。 理解线性分类器线性分类器计算图像中3个颜色通道中所有像素的值与权重的矩阵乘，从而得到分类分值。根据我们对权重设置的值，对于图像中的某些位置的某些颜色，函数表现出喜好或者厌恶（根据每个权重的符号而定）。举个例子，可以想象“船”分类就是被大量的蓝色所包围（对应的就是水）。那么“船”分类器在蓝色通道上的权重就有很多的正权重（它们的出现提高了“船”分类的分值），而在绿色和红色通道上的权重为负的就比较多（它们的出现降低了“船”分类的分值）。 将图像看做高维度的点：既然图像被伸展成为一个高维度的列向量，那么我们可以把图像看做这个高维度空间的一个点。整个数据集就是一个点的集合，每个点都带有1个分类标签。既然定义每个分类类别的分值是权重和图像的矩阵乘法，那么每个分类类别的分数就是这个空间中的一个线性函数的函数值。我们没办法可视化3072维空间中的线性函数，但假设把这些维度挤压到二维，那么就可以看出线性分类器在做什么了： 在上图中，每个图像是一个点，有3个分类器，以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。从上面可以看到，W的每一行都是一个分类类别的分类器。对于这些数字的几何解释是：如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转。而偏差b，则允许分类器对应的直线平移。需要注意的是，如果没有偏差，无论权重如何，在$x_i=0$时分类分值始终为0。这样所有分类器的线都不得不穿过原点。偏差和权重的合并技巧：之前评分函数定义为：$$f(x_i,W,b)=Wx_i+b$$分开处理这两个参数有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时$x_i$向量就要增加一个维度，这个维度的数值是常量1，这就是默认的偏差维度的权重。这样新的公式就简化成下面这样：$f(x_i,W)=Wx_i$如下图所示，左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加1个含常量1的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的，通过右边这样做，我们只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵。 图像数据处理：在图像分类的例子中，图像上的每个像素可以看做一个特征，在实践中，对每个特征减去平均值来中心化数据是非常重要的。在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127, 127]之间了。下一个常见步骤是，让所有数值分布的区间变为[-1, 1]。零均值的中心化是很重要的。 损失函数损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值$\Delta$。假设，第i个数据中包含图像$x_i$的像素和代表正确类别的标签$y_i$。评分函数输入像素数据，然后通过公式$f(x_i,W)$来计算不同分类类别的分值。这里将分值简写为s。比如，针对第j个类别的得分就是第j个元素：$s_j=f(x_i,W)_j$。针对第i个数据的多类SVM的损失函数定义如下：$$L_i=\sum_{j \neq y_i} max(0, s_j - s_{y_i}+\Delta)$$举个例子，假设有3个类别，并且得到了分值s=[13,-7,11]。其中第一个类别是正确的类别，即$y_i=0$。同时，假设$\Delta=10$。上面的公式是将所有不正确分类($j\neq y_i$)加起来，所以我们会得到两个部分：$$L_i=max(0,-7-13+10) + max(0, 11-13+10)$$可以看到，第一个部分的结果是0，这一对类别分数和标签的损失值为0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10，而SVM只关心差距至少要大于10，更大差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13&gt;11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。那么，我们面对的是线性评分函数($f(x_i,W)=Wx_i$)，所以我们可以将损失函数稍微改写一下：$$L_i = \sum_{j \neq y_i} max(0, w_j^Tx_i-w_{y_i}^Tx_i+\Delta)$$其中，$w_j$是权重W的第j行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数f，这样做就不是必须的了。max(0,-)函数，它常被称为折页损失(hinge loss)，有时候会听到使用平方折页损失SVM（即L2-SVM）,它使用的是$max(0,-)^2$，将更强烈地惩罚过界的边界值。不使用平方的更标准的版本，但是在某些数据集中，平方折页损失会工作得更好。可以通过交叉验证来决定到底使用哪个。 多类SVM“想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出delta的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。 正则化上面的损失函数有一个问题，假设有一个数据集和权重集W能够正确分类每个数据。问题在于这个W并不唯一：可能有很多相似的W都能正确地分类所有的数据。一个简单的例子：如果W能够正确分类所有数据，即对于每个数据，损失值都是0。那么当$\lambda&gt;1$时，任何数乘$\lambda W$都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对W乘以2将使得差距变成30。 换句话说，我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚R(W)部分。最长用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：$$R(W)=\sum_k\sum_l W_{k,l}^2$$上面的表达式中，将W中所有元素平方求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数，它由两部分组成：数据损失，即所有样例的平均损失$L_i$，以及正则化损失。完整公式如下： 将其展开完整公式是：$$L = \frac {1}{N}\sum_i\sum_{j\neq y_i}[max(0,f(x_i;W)_j - f(x_i;W)y_i+\Delta)] + \lambda \sum_k\sum_l W_{k,l}^2$$其中，N是训练数据集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数$\lambda$来计算权重。该超参数无法简单确定，需要通过交叉验证来获取。正则化最好的性质就是对大数值权重进行惩罚，可以提升泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值由过大的影响。 举个例子，假设输入向量x=[1,1,1,1]，两个权重向量$w_1=[1,0,0,0]，w_2=[0.25,0.25,0.25,0.25]$。那么，$w_1^T=w_2^T=1$，两个权重向量都得到同样的内积，但是$w_1$的L2惩罚是1.0，而$w_2$的L2惩罚是0.25。因此，根据L2惩罚来看，$w_2$更好，因为它的正则化损失更小。从直观上来看，这是因为w_2的权重值更小且更分散。既然L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免过拟合。 需要注意的是，和权重不同， 偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此，通常只对权重W正则化，而不正则化偏差b。在实际操作中，可以发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当W=0的特殊情况下，才能得到损失值为0。 设置$\Delta$$\Delta$这个超参数在绝大多数情况下被设置为$\Delta=1.0$。超参数$\Delta$和$\lambda$看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重W的大小对于分类分值有直接影响：当我们将W中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如$\Delta=1$或$\Delta=100$）从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度$\lambda$来控制)。 Softmax分类器SVM是最常用的两个分类器之一，而另一个就是Softmax分类器，它的损失函数和SVM的损失函数不同。Softmax分类器可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出$f(x_i,W)$作为每个分类的评分。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释。在Softmax分类器中，函数映射保持不变$f(x_i;W)=Wx_i$,但将这些评分视为每个分类的未归一化的对数概率，并将hinge loss替换为交叉熵损失(cross-entropy loss)。公式如下：$$L_i=-log(\frac {e^{f_{y_i}}}{\sum_j e^{f_j}})$$在上式中，$f_j$表示评分向量f中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值$L_i$的均值与正则化损失R(W)之和。其中函数$f_j(z) = \frac {e^{z_j}}{\sum_k e^{z_k}}$被称作为Softmax函数：其输入值是一个向量，向量中的元素为任意实数的评分值，函数对其进行压缩，输出一个向量，其中每个元素在0到1之间，且所有的元素之和为1。信息理论视角：在”真实”分布p和评估分布q之间的较差熵定义如下：$$H(p,q)=-\sum_x p(x)logq(x)$$因此，Softmax分类器所做的就是最小化在估计分类概率和在“真实”分布之间的较差熵。在这个解释中，“真实”分布就是所有概率密度都分布在正确的类别上（比如：p=[0,…,1,…,0]在某个位置就有一个单独的1）。还有，既然交叉熵可以写成熵和相对熵$H(p,q)=H(p)+D_{KL}(p||q)$，并且delta函数p的熵是0，那么就能等价的看做是对两个分别之间的相对熵做最小化操作。换句话说，交叉熵损失函数“想要”预测分布的所有概率密度都在正确分类上。 概率论解释：先看下面的公式:$$P(y_i|x_i,W)=\frac {e^{f_{y_i}}}{\sum_j e^{f_j}}$$可以解释为是给定图像数据$x_i$，以W为参数，分配给正确分类标签$y_i$的归一化概率。为了理解这点，请回忆一下Softmax分类器将输出向量f中的评分值解释为没有归一化的对数概率。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行了归一化处理，使得这些概率的和为1。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE）。该解释的另一个好处是，损失函数中的正则化部分R(W)可以被看做是权重矩阵W的高斯先验，这里进行的是最大后验估计（MAP）而不是最大似然估计。 SVM 和 Softmax的比较下图有助于区分Softmax和SVM这两类分类器： 针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量f（本节中是通过矩阵乘来实现）。不同之处在于对f中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的对数概率，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。 Softmax分类器为每个分类提供了“可能性”：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的“可能性”是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在“可能性”上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：$$[1,-2,0]\rightarrow[e^1,e^{-2},e^0]=[2.71,0.14,1]\rightarrow[0.7,0.004,0.26]$$现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么Softmax函数的计算就是：$$[0.5,-1,0]\rightarrow[e^{0.5},e^{-1},e^0]=[1.65,0.73,1]\rightarrow[0.55,0.12,0.33]$$现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的置信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释。 在实际使用中，SVM和Softmax经常是相似的:通常来说，这两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加”局部目标化”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（$\Delta =1$）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。 对于Softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。 小结 与kNN分类器不同，参数方法的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重W进行一个矩阵乘法运算。 偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。 损失函数（SVM和Softmax线性分类器最常用的2个损失函数）。损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程笔记(2) 图像分类]]></title>
    <url>%2F2017%2F12%2F02%2FCS231n%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AC%AC2%E8%AF%BE-Image-Calssification%2F</url>
    <content type="text"><![CDATA[本文转自：https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit，原文：http://cs231n.github.io/classification/，并根据原文添加了部分内容。 图像分类图像分类问题，就是对已有的固定的分类标签集合，对于输入的图像，从分类标签集合中找到一个分类标签，最后把分类标签分配给该输入图像。虽然看起来很简单，但是是计算机视觉领域的核心问题之一，并且有着广泛的应用。例如，以下图为例，图像分类模型读取该多，并生成该图片属于{cat,dog,hat,mug}中各个标签的概率。人类可以直接看到这个图像，但对于计算机来说，看到的确是由数字组成的巨大的3维数组。在此例中，图片的大小为248*400，有3个颜色通道，分别是红、绿和蓝。如此，图像就包含了248*400*3=297600个数字，每个数字都在0-255之间，图像分类的任务就是从这些数字中学习到知识，把该图像分类到“猫”。 困难和挑战：对于人类来说，从图片中识别“猫”非常简单，但是对于计算机而言却不是这么简单了。计算机视觉算法在图像识别方面遇到的一些困难： 视角变化（Viewpoint variation） 大小变化（Scale variation） 形变（Deformation) 遮挡（Occlusion) 光照条件（Illumination conditions) 背景干扰（Background clutter） 类内差异（Intra-class variation） 数据驱动方法（Data-driven approach）数据驱动方法类似和教小孩看图识物类似：给计算机很多数据，让计算机进行学习，从而进行分类。该方法的流程如下： 输入：输入包含N个图像的集合，每个图像的标签是k种分类标签中的一种。这个集合称为训练集。 学习：使用训练集来学习每个类到底长什么样。一般称为训练分类器或学习一个模型。 评价：让分类器来预测它未见过的图像的分类标签，以此评价分类器的好坏。分类正确的图像数量越多，则分类器的性能越好。 Nearest Neighbor分类器图像分类数据集：CIFAR-10，包含60000张32*32的小图像，数据集包含10中类别，60000张图片被分为包含50000张图片的训练集和包含10000张图片的测试集。下图就是10类的10张随机图片。 上图的左边是训练样本集，右边：第一列是测试图像，然后第一列的每个图像的右边是使用Nearest Neighbor算法，根据像素的差异，从训练样本集中选出的10张最类似的图片。 Nearest Neighbor算法，假设我们拿到包含50000张图片的训练集，对于一个要预测的图片，Nearest Neighbor算法会拿这张测试的图片和训练集中的每个图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。至于判断两张图片是否相似，以及最相似，在本例中，就是比较32*32*3的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。也就是说，将两张图片先转化为向量$I_1$和$I_2$，然后计算$L_1$距离：$$d_1(I_1,I_2)=\sum_p |I_1^p-I_2^p|$$以图片中的一个颜色通道为例来进行说明，两张图片使用$L_1$距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么$L_1$距离为0，但是如何两张图片很是不同，那么$L_1$值将会非常大。 距离选择：计算向量间的距离方法有很多种，另一个常用的方法为$L_2$距离，从几何的角度看，可以理解为计算两个向量间的欧式距离。 $L_1$和$L_2$比$L_2$的比较：在面对两个向量之间的差异时，$L_2$比$L_1$更不能容忍差异。也就是说，相对于一个巨大的差异，$L_2$距离更倾向于接受多个中等程度的差异。$L_1$和$L_2$都是在p-norm常用的特殊形式。 k-Nearest Neighbor分类器Nearest Neighbor算法使用最相似的1张图片作为最终的预测结果。k-Nearest Neighbor算法则是找到最相似的k张图片，然后让它们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的标签。当k=1时，K-Nearest Neighbor就变为Nearest Neighbor。从直观上可以看到，更高的k值可以让效果更平滑，使得分类器对于异常值更有抵抗力。 上图显示了Nearest Neighbor分类器和5-Nearest Neighbor分类器的区别。图中使用了2维的点来表示，分成3类。不同的颜色区域代表使用$L_2$距离的分类器的决策边界。白色的区域是分类模糊的例子（即图像与两个以上的分类标签绑定）。需要注意的是，在NN分类器中，异常的数据制造出一个不正确预测的孤岛。5—NN分类器将这些不规则都平滑了，使得针对测试数据的泛化能力更好。 用于超参数调优的验证集k-NN分类器需要设定k值，选择哪个k值最合适呢?同样也距离函数也是可选择的，那么选哪个好？这些选择，被称为超参数（hyperparameter)。在基于数据进行学习的机器学习算法设计中，朝参数时非常常见的，但是如何选择这些超参数？ 我们可能会尝试不同的值，看哪个值表现最好就选哪个。但这样做的时候要非常小心，特别注意：决不能使用测试集来进行调优。在训练机器学习模型时，应该把测试集看做非常宝贵的资源，不到最后一步，绝不使用它。如果使用测试集进行调优，而且算法看起来效果不错，但算法实际部署后，性能可能会远低于预期。这种情况，称之为过拟合。从另一个角度来说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实是过于乐观了，实际部署起来效果就会差很多。所以，最终测试的时候再使用测试集，可以很好地近似度量你所设计的分类器的泛化性能。 测试集只能使用一次，即在训练完成后评价最终的模型时使用。 实际在进行参数调优的过程中，是从训练集中抽取一部分数据用来调优，称之为验证集（validation set）。以CIFAP-10数据集为例，我们可以用49000个图像作为训练集，用1000个图像作为验证集。验证集其实就是作为假的测试集来调优。 把训练集分为训练集和验证集，使用验证集来对超参数进行调优，最后只在测试集上对模型进行评价。 交叉验证，有时候训练集较小，就会导致验证集的数量更小，人们会使用一种称为交叉验证的方法。这种方法把训练集评价分为K份，用其中的k-1份来训练，另1份来验证。然后循环着取其中的k-1份来训练，其中1份来验证。最后取所有k次验证的结果的平均值作为算法验证结果。在实际的应用中，一般会直接把训练集按照50%-90%的比例分为训练集和验证集。但这也是根据具体情况来定的：如果超参数数量较多，可能就想用更大的验证集，而验证集的数量不够，最好还是使用交叉验证。 Nearest Neighbor分类器的优劣 易于理解，实现简单； 算法的训练不需要花时间，其训练只需要和所有存储的训练图像进行比较（缺点）； 虽然训练花费很多时间，但是一旦训练完成，对新的测试数据进行分类非常快。 Nearset Neighbor分类器在某些特定的情况下，可能是不错的选择。但是在实际的图像分类工作中，很少使用。 实际应用K-NN的流程 预处理数据：对数据中的特征进行归一化，让其具有0均值和单位方差（unit variance)。 如果数据是高维数据，考虑使用降维方法。 将数据随机分为训练集和测试集。 在验证机上进行调优，尝试足够多的k值，尝试$L_1$和$L_2$两种距离。 如果分类器跑得太慢，尝试使用Approximate Nearest Neighbor库（比如FLANN）来加速这个过程，其代价是降低一些准确率。 对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上运行并再次训练呢？因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。在实践中，不要这样做。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。直接使用测试集来测试用最优参数设置好的最优模型，得到测试集数据的分类准确率，并以此作为你的kNN分类器在该数据上的性能表现。 小结 介绍了图像分类问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。 Nearest Neighbor分类器，分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。 选取超参数的正确方法是：将原始训练集分为训练集和验证集，我们在验证集上尝试不同的超参数，最后保留表现最好那个。 如果训练数据量不够，使用交叉验证方法，它能帮助我们在选取最优超参数的时候减少噪音。一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。 最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。 —end—]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《神经网络与深度学习》读书笔记]]></title>
    <url>%2F2017%2F11%2F30%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第一章 使用神经网络识别手写数字感知器 感知器在20世纪五、六十年代由科学家Frank Rosenblatt发明，其受到Warren McCulloch和Walter Pitts早期的工作影响。一个感知器接受几个二进制输入，$x_1,x_2,…,$,并产生一个二进制的输出： 示例中的感知器有三个输入，$x_1,x_2,x_3$,通常可以有更多或更少的输入，通过引入相应的权重，$w_1,w_2,…$表示相应输入对于输出的重要性的实数。神经元的输出，0或者是1，则由分配权重后的总和$\sum_{j}w_jx_j$小于或大于一些给定的阈值决定。和权重一样，阈值是一个实数，一个神经元的参数。可以用下面的形式来表示： 简化一下感知器的数学描述，用$w·x=\sum_{j}w_jx_j,b=-threshold$,则感知器的规则可以重写为： S型神经元单个感知器上一个权重或偏置的微小改动有时会引起那个感知器的输出完全翻转，如从0变到1。这样的翻可能接下来引起其余网络的行为以极其复杂的方式完全改变。因此，虽然有时可以正确对一类进行分类，但网络在其他图像的行为很可能以一些很难控制的方式被完全改变。这使得逐步修改权重和偏置来让网络接近期望行为变得很困难。 可以引入一种称为S型神经元来克服这个问题。S型神经元和感知器类似，但是被修改为权重和偏置的微小改动只引起输出的微小变化。这对于让神经元网络学习起来是很关键。S型神经元同样有多个输入，$x_1,x_2,…$，对应每个输入有权重，$w_1,w_2,…$和一个总的偏置，b。但是输出不是0和1，而是$\sigma(w·x+b)$,$\sigma$被称为S型函数，定义为：$$\sigma(z)=\frac {1}{1+e^{-z}}$$可以看到：当$z=w·x+b$为很大的正数时，S型神经元的输出近似为1；当$z=w·x+b$为很小的负数时，S型神经元的输出近似为0。这种行为与感知器很像，只有在$w·x+b$取中间值时，和感知器模型有比较大的偏离。利用S型神经元，会得到一个平滑的感知器。$\sigma$是平滑的，利用其特性，权重和偏置的微小变化会输出一个为微小的变化，不会像感知器那样变化剧烈。S型函数的形状如下： 神经网络的架构一个典型的神经网络架构如下图所示：主要包括：输入神经元、隐藏层和输出神经元三个部分。以上一层的输出作为下一层的输入，这种网络被称为前馈神经网络。这意味着网络中是没有回路的-信息总是向前传播，从不反向回馈。 使用梯度下降算法进行学习使用神经网络构建手写体识别分类算法，使用二次代价函数作为损失函数，训练神经网络找到最小化二次代价函数C(w,b)的权重和偏置。： 为什么使用二次代价，而不是直接最大化正确分类图像的数量？因为，在神经网络中，被正确分类的图像数量关联权重和偏置的函数并不是一个平滑的函数。大多数情况下，对权重和偏置做出的微小变动完全不会影响被正确分类的图像的数量。这会导致很难去解决如何改变权重和偏置来取得改进的性能。而用一个类似二次代价的平滑代价函数则能更好地解决如何用权重和偏置中的微小改变来取得更好的效果。梯度下降法假设我们要最小化某些函数，$C(v)$。它可以是任意的多元实值函数，$v=v_1,v_2,…$，用v代替w和b以强调它可能是任意的函数，并不局限于神经网络的环境。为了最小化$C(v)$，想象C是一个只有两个变量$v_1$和$v_2$的函数，如下图所示： 如果在$v_1$和$v_2$方向上分别改变很小的量，即$\Delta v_1$和$\Delta v_2$，微积分告诉我们$C$将会有如下变化：$$\Delta C \approx \frac {\partial C}{\partial v_1}\Delta v_1 + \frac {\partial C}{\partial v_2}\Delta v_2 $$因为，我们要最小化$C$，所以要寻找中选择$\delta v_1$和$\delta v_2$的方法，使得$\Delta C$为负；为了弄明白如何选择，需要定义$\delta v$为v变化的向量，$\Delta v= (\Delta v_1,\Delta v_2)^T$；定义$C$的梯度为偏导数的向量$\nabla C$:$$\nabla C =(\frac {\partial C}{\partial v_1},\frac {\partial C}{\partial v_2})^T$$那么，$$\Delta C \approx \nabla C · \Delta v$$这个表达式解释了为什么$\nabla C$被称为梯度向量：$\nabla C$把v的变化关联为$C$的变化，正如我们期望的用梯度来表示。如果选取：$$\Delta v=-\eta \nabla C$$那么，可以看到$\Delta C \approx -\eta \nabla C · \nabla C = -\eta ||\nabla C||^2$ 是永远小于等于0的，如果按照这种方式去改变v，那么$C$会一直减小，不会增加。因此，可以用如下方式更新$v$:$$v \rightarrow v^{new} = v - \eta \nabla C$$然后用它再次更新规则来计算下一次的更新，如果反复持续这样做，我们将持续减小C直到获得一个全局的最小值。如果$C$是一个具有更多变量的函数，也同样适用。梯度下降法就是通过这种方式重复改变$v$来找到函数$C$的最小值。如何在神经网络中使用梯度下降法去学习呢？其思想就是利用梯度下降算法去寻找能使得代价函数取得最小值的权重$w_k$和偏置$b_l$。我们将权重和偏置代替变量$v_j$，那么可以得到：$$w_k \rightarrow w_k^{new} = w_k - \eta \frac {\partial C}{\partial w_k}\\\\b_l = b_l^{new}=b_l - \eta \frac {\partial C}{\partial b_l}$$随机梯度下降，就是通过随机选取小量的训练输入样本来计算$\nabla C_x$,进而估计梯度$\nabla C$。通过计算少量的样本的平均值可以快速得到一个对于实际梯度$\nabla C$的很好的估算，有助于加速梯度下降，进而加速学习过程。批量梯度下降，随机选取小批量的数据，在这批数据上计算梯度，并更新参数。 第二章 反向传播算法是如何工作的反向传播算法的推导首先，定义神经网络中的权重，我们使用$w_{jk}^l$表示从(l-1)层的第k个神经元到l层的第j个神经元的连接上的权重。如下图所示：同时，我们使用$b_{j}^l$表示在第l层第j个神经元的偏置，使用$a_j^l$表示第l层第j个神经元的激活值，下图清楚地解释了这样表示的含义： 于是，第l层的第j个神经元的激活值$a_j^l$就和第(l-1)层的激活值通过下面的公式联系起来$$a_j^l = \sigma(\sum_k w_{jk}^la_k^{l-1} + b_j^l)$$对每一层l都定义一个权重矩阵$w^l$,表示连接到第l层神经元的权重，同时，每一层，定义一个偏置向量，$b^l$。于是，第l层的激活值向量$a^l$可以写成如下形式：$$a^l = \sigma(w^l a^{l-1} + b^l)$$这个表达式给出了一种更加全局的思考每层的激活值和前一层激活值的关联方式：我们仅仅用权重矩阵作用在激活值上，然后加上一个偏置向量，最后作用于$\sigma$函数。在计算$a^l$的过程中，我们计算了中间量$z^l = w^la^{l-1}+b^l$，我们称其为l层神经元的带权输入，这样$a^l = \sigma(z^l)$。其次，看一下神经网络代价函数的两个假设。上一章使用的是二次代价函数： 其中，n是训练样本总数；求和运算遍历了每个训练样本x；y=y(x)是对应的目标输出；L表示网络的层数；$a^L=a^L(x)$是当输入是x时网络输出的激活值向量。反向传播的目标是计算代价函数$C$分别关于w和b的偏导数，为了让反向传播可行需要作出以两个假设：第一个假设就是代价函数可以被写成一个在每个训练样本x上的点检函数$C_x$的均值$C=\frac {1}{n} \sum_x C_x$。需要这个假设的原因是反向传播实际上是对一个独立的训练样本计算了$\frac {\partial C_x}{\partial w}$和$\frac {\partial C_x}{\partial b}$。然后，我们通过在所有训练样本上进行平均化获得$\frac {\partial C}{\partial w}$和$\frac {\partial C}{\partial b}$。实际上，有了这个假设，我们会认为训练样本x已经被固定住了，丢掉了其下标，将代价函数$C_x$看做$C$。第二个假设就是代价函数可以写成神经网络输出的函数：最后，反向传播算法的推导。反向传播其实是对权重和偏置变化影响代价函数的过程的理解。最终极的含义其实就是计算偏导数$\frac {\partial C}{\partial w_{jk}^j}$和$\frac {\partial C}{\partial b_j^l}$。为了计算这些值，首先引入一个中间量$\delta_j^l$，称为在第l层第j个神经元上的误差。反向传播将给出计算误差$\delta_j^l$的流程，然后将其关联到计算$\frac {\partial C}{\partial w_{jk}^j}$和$\frac {\partial C}{\partial b_j^l}$上。反向传播的4个基本方程：(1) 输出层误差的方程，$\delta^L$,每个元素定义如下：$$\delta_j^L = \frac {\partial C}{\partial a_j^L}\sigma^\prime{(z_j^L)} (BP1)$$(2) 使用下一层的误差$\delta^{l+1}$ 来表示当前的误差$\delta^l$:$$\delta ^l = ((w^{l+1})^T \delta^{l+1})\bigodot\sigma^\prime(z^l) (BP2)$$其中，$(w^{l+1})^T$是第l+1层的权重矩阵$w^{l+1}$的转置。(3) 代价函数关于网络中任意偏置的改变率：$$\frac {\partial C}{\partial b_j^l} = \delta_j^l (BP3)$$(4) 代价函数关于任何一个权重的改变率：$$\frac {\partial C}{\partial w_{jk}^l} = a_k^{l-1}\delta_j^l (BP4)$$现在证明这四个基本的方程，所有的这些都是多元微积分的链式法则的推论。首先，从方程(BP1)开始，它给出了输出误差$\delta^L$的表达式。首先有如下的定义：$$\delta_j^L=\frac {\partial C}{\partial z_j^L}$$表示L层上第j个神经元的误差，应用链式法则，可以用输出激活值的偏导数的形式重新表示上面的偏导数：$$\delta_j^L=\sum_k \frac {\partial C}{\partial a_k^L}\frac {\partial a_k^L}{\partial z_j^L}$$这里求和是在输出层的所有神经元k上运行的。当然，第k个神经元的输出激活值$a_k^L$只依赖于当k=j时第j个神经元的输入权重$z_j^L$。所以，当k不等于j时$\partial a_k^L/\partial z_j^L$消失了。结果我们可以简化上一个方程为：$$\delta_j^L=\frac {\partial C}{\partial a_j^L}\frac {\partial a_j^L}{\partial z_j^L}$$于是，由$a_j^L=\sigma(z_j^L)$,上式右边的第二项可以写成$\sigma^\prime(z_j^L)$，方程就变成：$$\delta_j^L = \frac {\partial C}{\partial a_j^L}\sigma^\prime{(z_j^L)}$$下一步，证明（BP2），它给出了以下一层误差$\delta ^{l+1}$的形式表示误差$\delta^l$。为此，我们想要以$\delta_k^{l+1}=\partial C/ \partial z_k^{l+1}$的形式重写$\delta_j^l=\partial C/\partial z_j^l$。我们可以应用链式法则：$$\delta_j^l = \frac {\partial C}{\partial z_j^l}=\sum_k \frac {\partial C}{\partial z_k^{l+1}} \frac {\partial z_k^{l+1}}{\partial z_j^l}=\sum_k \frac {\partial z_k^{l+1}}{\partial z_j^l}\delta_k^{l+1} $$这里，最后一行交换了右边两项，并用$\delta_k^{l+1}$的定义带入。为了对最后一样的最后一项进行求值，注意：$$z_k^{l+1}=\sum_j w_{kj}^{l+1}a_j^l = \sum_j w_{kj}^{l+1}\sigma(z_j^l)+b_k^{l+1}$$做微分，我们得到：$$\frac {\partial z_k^{l+1}}{\partial z_j^l}=w_{kj}^{l+1}\sigma^\prime(z_j^l)$$带入，就可得到：$$\sigma_j^l = \sum_k w_{kj}^{l+1}\sigma_k^{l+1}\sigma^\prime(z_j^l)$$总结一下 反向传播算法反向传播算法给出了一种计算代价函数梯度的方法。用算法描述出来就是： 第三章 改进神经网络的学习方法交叉熵代价函数使用sigmod作为激活函数，使用二次代价函数作为损失函数时，当神经元的输出接近1的时候，曲线变得相当平，$\sigma^\prime(z)$就很小，相应计算出的梯度也会非常小，学习的速度就会变慢。假设神经元的输出为$a=\sigma(z)$,其中，$z=\sum_j w_jx_j+b$是输入的带权和。我们定义这个神经元的交叉熵代价函数：$$C = -\frac {1}{n}\sum_x[yln a + (1-y)ln(1-a)]$$其中，n是训练样本总数，求和是在所有的训练输入x上进行的，y是对应的目标输出。交叉熵为何能够解释成一个代价函数？将交叉熵看做是代价函数有两点原因。第一，它是非负的，C&gt;0。第二，如果对于所有的训练输入x，神经元实际的输出接近目标值，那么交叉熵接近0，假设在这个例子中，y=0而a接近0，交叉熵的值为接近于0，反之y=1而a接近于1，交叉熵的值也接近于0，所以，在实际输出和目标输出之间的差距越少，最终的较差熵的值就越低。综上所述，交叉熵是非负的，在神经元达到很好的正确率的时候接近0。交叉熵代价函数有一个比二次代价函数更好的特性就是它避免了学习速度下降的问题。首先，看一下交叉熵函数关于权重的偏导数，将$a=\sigma(z)$带入交叉熵损失函数，应用链式法则，得到： 将结果合并一下，简化成： 最终，可以得到：可以看到，权重的学习的速度受到$\sigma(z)-y$，也就是输出中的误差的控制。更大的误差，更快的学习速度。特别地，这个代价函数还避免了像二次代价函数类似方程中$\sigma^\prime(z)$导致学习的缓慢。交叉熵代价函数的学习曲线：特别地，当我们使用二次代价函数时，学习在神经元犯了明显的错误时却比学习快接近真实值的时候缓慢；而使用交叉熵学习正是在神经元犯了明显错误的时候速度更快。特别地，当我们使用二次代价函数时，当神经元在接近正确的输出前犯了明显错误的时候，学习变得更加缓慢；而使用交叉熵，在神经元犯明显错误时学习得更快。这些现象不依赖于如何设置学习速率。### 过度拟合和规范化过度拟合是神经网络的一个主要问题。这在现代网络中特别正常，因为网络权重和偏置数量巨大。检测过度拟合的明显方法是使用上面的方法-跟踪测试数据集合的准确率随训练变化情况。如果我们看到测试数据上的准确率不再提升，那么我们就停止训练。严格地说，这其实并非是过度拟合的一个必要现象，因为测试集和训练集上的准确率可能会同时停止提升。当然，采用这样的策略是可以防止过度拟合的。使用validation_data而不是test_data来防止过度拟合，在每个迭代期的最后都计算在validation_data上的分类准确率。一旦分类准确率已经饱和，就停止训练。这个策略被称为early stopping。在实际应用中，我们不会立即知道什么时候准确率会饱和。相反，我们会一直训练直到我们确信准确率已经饱和。为何要使用validation_data来代替test_data防止过度拟合问题？实际上，这是一个更为一般的策略的一部分，这个一般的策略就是使用validation_data来衡量不同的超参数（如迭代次数、学习速率、最好的网络架构等等）的选择的效果。如果设置超参数是基于test_data的话，可能最终我们得到过度拟合于test_data的超参数，这些超参数符合test_data的特点，但是网络的性能并不能够泛化到其他的数据集合上。我们借助validation_data来克服这个问题，一旦获得了想要的超参数，最终我们就使用test_data进行准确率测量。规范化增加训练样本的数量是一种减轻过度拟合的方法。还有其他的一些方法能够减轻过度拟合，就是规范化，有时被称为权重衰减(weight decay)或者L2规范化。L2规范化的思想就是增加一个额外的项到代价函数上，这个项叫做规范化项。下面是规范化的较差熵:可以看出，规范化的效果是让网络倾向于学习小一点的权重，其他的东西都是一样的。大的权重只有能够给出代价函数第一项足够的提升时才被允许。换而言之，规范化可以当做一种寻找小的权重和最小化原始的代价函数之间的折中。这两部分之前相对的重要性就由$\lambda$的值来控制了：$\lambda$越小，就偏向于最小的原始代价函数，反之，倾向于小的权重。新的权重的学习规则就变成：这正和通常的梯度下降学习规则相同，除了通过一个因子$1-\frac {n\lambda}{n}$重新调整了权重$w_0$。这种调整有时被称为权重衰减，因为它使得权重变小。我们已经把规范化描述为一种减轻过度拟合和提高分类准确率的方法。实际上，这不是仅有的好处。实践表明，在使用不同的权重初始化进行多次MNIST网络训练的时候，我们发现无规范化的网络会偶然被限制住，明显困在了代价函数的局部最优值处。结果就是不同的运行会给出相差很大的结果。对比看来，规范化的网络能够提供更容易复制的结果。为什么会这样子呢？，从经验上来看，如果代价函数是无规范化的，那么权重向量的长度可能会增长，而其他的东西都保持一样。随着时间的推移，这会导致权重向量变得非常大。所以会使得权重向量卡在朝着更多还是更少的方向上变化，因为当长度很大的时候梯度下降带来的变化仅仅会引起在那个方向发生微小的变化。我们相信这个现象让我们的学习算法更难有效地探索权重空间，最终导致很难找到代价函数的最优值。为何规范化可以帮助减轻过度拟合？通常的说法是：小的权重在某种程度上，意味着更低的复杂性，也就对数据给出了一种更简单却更强大的解释，因此应该优先选择。L2规范化没有限制偏置，实际上可以对偏置进行限制，但在某种程度上，对不对偏置进行规范化其实就是一种习惯了。然而，需要注意的是，有一个大的偏置并不会向大的权重那样会让神经元对输入太过敏感。所以，我们不需要对大的偏置带来的学习训练数据的噪声太过担心。同时，允许大的偏置能够让网络更加灵活，因为，大的偏置让神经元更加容易饱和，这有时候是我们所要达到的效果。所以，我们通常不会对偏置进行规范化。规范化的其他技术除了L2外还有很多规范化技术，另外给出三种减轻过拟合的其他方法：L1规范化、弃权和人为增加训练样本。L1规范化：这个方法是在未规范化的代价函数上加上一个权重绝对值的和：$$C=C_0 + \frac{\lambda}{n} \sum_w |w|$$在L1规范化中，权重通过一个常量向0进行缩小。在L2规范化中，权重通过一个和w成比例的量进行缩小。所以，当一个特定的权重绝对值|w|很大时，L1规范化的权重缩小得要远比L2规范化要小的多、相反，当一个特定的权重绝对值|w|很小时，L1规范化的权重缩小得要比L2规范化大的多。最终的结果是：L1规范化倾向于聚集于网络的权重在相对少量的高重要度连接上，而其他权重就会被驱使向0接近。弃权(DropOut)是一种相当激进的技术。和L1、L2规范化不同，弃权技术并不依赖对代价函数的修改。而是，在弃权中，我们改变了网络本身。弃权技术在训练大规模深度网络时尤其有用，这样的网络中过度拟合问题经常特别突出。人为扩展训练数据### 权重初始化在创建了神经网络后，我们需要进行权重和偏置的初始化。之前的方式是根据独立高斯随机变量来选择权重和偏置，其被归一化为均值0，标准差1。假设我们使用一个有大量输入神经元的网络，比如说1000个。假设，我们已经使用归一化的高斯分布初始化了连接的第一个隐藏层的权重。现在我们将注意力集中在这一层的连接权重上，忽略网络的其他部分： 为了简化，假设我们使用训练输入x,其中一半的输入神经元的值为1，另一半为0。以下的论点更普遍适用，让我们考虑隐藏神经元输入的带权和$z=\sum_j w_jx_j+b$。其中，500个项消去了，因为对应的输入$x_j$为0。所有z是遍历总共501个归一化的高斯随机变量的和，包含500个权重项和额外的1个偏置项。因此，z本身是一个均值为0,标准差为22.4的高斯分布。z其实有一个非常宽的高斯分布，完全不是非常尖的形状： 尤其是，我们可以从上图看出|z|会变得非常大，如果这样，隐藏神经元的输出$\sigma(z)$就会接近1或0。也就表示我们的隐藏神经元会饱和。所以，当出现这样的情况时，在权重中进行微小的调整仅仅会给隐藏神经元的激活值带来极其微弱的改变。而这种微弱的改变也会影响网络中剩下的神经元，然后会带来相应的代价函数的改变。结果就是，这些权重在我们进行梯度下降算法时会学习得非常缓慢。我们可以进行更好地初始化，能够避免这种类型的饱和，最终避免学习速度的下降，假设我们有n个输入神经元，我们可以使用均值为0标准差为$1/\sqrt[]{n}$的高斯随机分布初始化这些权重。也就是说，我们会向下挤压高斯分布，让我们的神经元更不可能饱和。 第五章 深度神经网络为何很难训练当神经网络层数加深，先前的层可能学习的比较好，但是后面的层却停滞不变。实际上，我们发现，在深度神经网络中使用基于梯度下降的学习方法本身存在着内在的不稳定性。这种不稳定性使得先前或者后面的层的学习过程阻滞。 消失的梯度问题在某些深度神经网络中，在我们在隐藏层BP的时候梯度倾向于变小。这意味着在前面的隐藏层中的神经元学习速度要慢与后面的隐藏层。在很多神经网络中存在着更加根本的导致这个现象出现的原因。这个现象也被称作是消失的梯度问题（vanishing gradient problem） 更一般地说，在深度神经网络中的梯度是不稳定的，在前面的层中或会消失或会爆炸。这种不稳定性才是神经网络中基于梯度学习的根本问题。下图是一个有三层隐藏层的神经网络： 其中，$w_1,w_2,…$是权重,而$b_1,b_2,…$是偏置，C是某个代价函数。现在看一下关联于第一个隐藏层神经元梯度$\partial C/ \partial b_1$。下图给出了具体的表达式：为何会出现梯度消失：现在把梯度的整个表达式写下来： 除了最后一项，该表达是一系列形如$w_j\sigma^\prime(z_j)$的乘积。如果我们使用标准方法来初始化网络中的权重，那么会使用一个均值为0标准差为1的高斯分布。因此，所有的权重通常会满足$|w_j|&lt;1$。有了这些信息，我们就会发现$w_j\sigma^\prime(z_j)&lt;1/4$。并且在我们进行了所有这些项的乘积时，最终的结果肯定会指数级下降：项越多，乘积下降的越快。这样就能够解释消失的梯度问题。 不稳定的梯度问题：根本问题其实并非是消失的梯度问题或者爆炸的梯度问题，而是在前面的层上的梯度是来自后面的层上的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡。如果没有某种机制或者更加本质的保证来达成平衡，那网络就很容易不稳定了。简而言之，真实的问题就是神经网络受限于不稳定梯度的问题。所以，如果我们使用标准的基于梯度的学习算法，在网络中的不同层会出现按照不同学习速度学习的情况。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识图谱技术综述]]></title>
    <url>%2F2017%2F11%2F11%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[本文主要是对《知识图谱技术综述》 徐增林等人的这篇论文阅读时的一个整理和记录。知识图谱技术是AI技术的重要组成部分，建立的具有语义处理能力与开放互联能力的知识库，可在智能搜索、智能问答、个性化推荐等智能信息服务中产生应用价值。这篇论文全面介绍了知识图谱的定义、架构以及知识图谱的知识抽取、知识表示、知识融合、知识推理四大核心技术的研究进展。 思维导图 知识图谱的定义与架构知识图谱的定义知识图谱是Google用于增强其搜索引擎功能的知识库。本质上，知识图谱是一种揭示实体之间关系的语义网络，可以对现实世界的事物及其相互关系进行形式化地描述。现在知识图谱以被用来泛指各种大规模的知识库。三元组是知识图谱的一种通用表示方式，即G=(E,R,S)，其中： G：表示知识库 E：表示知识库中的所有实际的集合 R：表示知识库中的关系集合 S：表示知识库中的三元组集合，$S \subseteq E \times R \times S $ 三元组的基本形式主要包括实体1、关系、实体2和概念、属性、属性值等，实体是知识库中最基本的元素，不同的实体之间存在不同的关系。概念主要指集合、类别、对象类型、事物的种类，例如人物、地理等；属性主要指对象可能具有的属性、特征、特性、特点及参数，例如国籍、生日等；属性值主要指对象指定属性的值，例如中国、1988-09-08等。每个实体可以用一个全局唯一确定的ID来标识，每个属性-属性值对（atrribute-value pair，AVP）可用来刻画实体的内在特性，而关系可用来连接两个实体，刻画它们之间的关联。 从覆盖范围而言，知识图谱可以分为通用知识图谱和行业知识图谱。通用知识图谱注重广度，强调融合更多的实体，较行业知识图谱而言，其准确度不够高，并且受概念范围的影响，很难借助本体库对公理、规则以及约束条件的支持能力规范其实体、属性、实体间的关系等。通用的知识库主要用于智能搜索等领域。行业知识图谱通常需要依靠特定的行业的数据来构建，具有特定的行业意义。行业知识图谱中，实体的属性与数据模式往往比较丰富，需要考虑到不同的业务场景与使用人员。 知识图谱的架构知识图谱的架构主要包括自身的逻辑结构以及体系架构。 1）知识图谱的逻辑结构知识图谱在逻辑上可以分为模式层与数据层两个层次。数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。可选择图数据库作为存储介质，例如开源的Neo4j、Twitter的FlockDB、seones的GraphDB。模式层构建在数据层之上，主要是通过本体库来规范数据层的一系列事实表达。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度叫较小。 2）知识图谱的体系架构知识图谱的体系架构是指构建模式结构，如下图所示（图引自该论文） 知识图谱主要有自顶向下(top-down)与自底向上(bottom-up)两种构建方式。自顶向下指是先为知识图谱定义好本体与数据模式，再将实体加入到知识库。该构建方式需要利用一些现有的结构化知识库作为其基础的知识库，例如Freebase就是采用这种方式，它的绝大部分数据是从维基百科中得到的。自底向上指的是从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式。目前，大多数知识图谱都采用自底向上的方式进行构建，其中最典型的就是Google的Knowledge Vault。 大规模知识库案例随着语义web资源数量激增、大量的RDF数据被发布和共享、LOD(linked open data)等项目的全面展开，学术界和工业界的研究人员花费了大量的精力构建各种结构化知识库。主要包括开放链接知识库、行业知识库两类。 开放链接知识库在LOD项目的云图中FreeBase、Wikidata、DBpedia、YAGO这4个是比较重要的大规模知识库。它们中不仅包含大量的半结构化、非结构化数据，是知识图谱数据的重要来源。而且具有较高的领域覆盖面，与领域知识库存在大量的关系。 垂直行业知识库行业知识库也可以称为垂直型知识库，这类知识库的描述目标是特定的行业领域，通常需要依靠特定行业的数据才能构建，因此其描述范围极为有限。下面几个就是比较典型的垂直行业知识库。 IMDB，是一个关于电影演员、电影、电视节目、电视明星以及电影制作的治疗库。 MusicBrainz,是一个结构化的音乐维基百科，致力于收藏所有的音乐元数据，并向大众用户开放。 ConceptNet,是一个语义知识网络，主要由一系列代表概念的结点构成，这些概念将主要采用自然语言单词或短语的表达形式，通过相互连接建立语义联系。 知识图谱的关键技术大规模知识库的构建与应用需要多种智能信息处理技术的支持。主要包括以下技术： 知识抽取：从一些公开的半结构化、非结构化的数据中提取出实体、关系、属性等知识要素。 知识融合：消除实体、关系、属性等指称项与事实对象之间的奇异，形成高质量的知识库。 知识推理：在已有的知识库的基础上，进一步挖掘隐含的知识，从而丰富、扩展知识库。 知识表示：以某种方式表示知识 下面就分别介绍相关的技术。 知识抽取知识抽取主要是面向开放的链接数据，通过自动化的技术抽取出可用的知识单元，知识单元主要包括实体、关系以及属性3个知识要素，并以此为基础，形成一系列高质量的事实表达，为上层模式层的构建奠定基础。 (1) 实体抽取实体抽取，也称为命名实体识别，指的是从原始预料自动识别出命名实体。实体抽取主要分为3种方法： 基于规则与词典的方法 基于统计机器学习的方法 面向开放域的抽取方法 (2) 关系抽取关系抽取的目标是解决实体间的语义链接问题，早期的关系抽取主要是通过人工构造语义规则以及模板的方法识别实体关系。随后，实体间的关系模型逐渐代替了人工预定义的语法与规则。但是，仍需要提前定义实体间的关系类型。后来出现面向开放域的信息抽取框架(open information extraction,OIE),这是抽取模式上一个巨大进步。但OIE方法在对实体的隐含关系抽取方面性能低下，因此，部分研究者提出基于马尔科夫逻辑网、基于本体推理的深层隐含关系抽取方法。 (3) 属性抽取属性抽取主要是针对实体而言的，通过属性可形成对实体的完整描述。由于实体的属性可以看成是实体与属性值之间的一种名称关系，因此可以将实体属性的抽取问题转换为关系抽取问题。大量的属性数据主要存在于半结构化、非结构化的大规模开放域数据集中。抽取这些属性的方法，一种是将上述从百科网站上抽取的结构化数据作为可用于属性抽取的训练集，然后再将该模型应用于开放域中的实体属性抽取；另一种，根据实体属性与属性之间的关系模式，直接从开放域数据集上抽取属性。但是，由于属性值附近普遍存在一些限定属性值定义的属性名等，所以该抽取方法的准确率并不高。 知识表示虽然，基于三元组的知识表示形式比较直观，但是其在计算效率、数据稀疏性等方面面临着诸多问题。以Deep Learning为代表的表示学习技术可以将实体的语义信息表示为稠密低维实值向量，进而在低维空间中高效计算实体、关系及其之间的复杂语义关联，对知识库的构建、推理、融合及应用均具有重要的意义。 (1) 应用场景分布式表示旨在用一个综合向量表示实体对象的语义信息，是一种模仿人脑工作的表示机制，通过知识表示而得到的分布式表示形式在知识图谱的计算、补全、推理等方面将起到重要作用： 1）语义相似度计算。由于实体通过分布式表示而形成的是一个个低维的实值向量，所以，可使用熵权系数法、余弦相似性等方法计算它们之间的相似性。这种相似性刻画了实体之间的语义关联程度，为自然语言处理等提供了极大的便利。 2）链接预测。通过分布式表示模型，可以预测图谱中任意两个实体之间的关系，以及实体间存在的关系的正确性。尤其是在大规模知识图谱上下文中，需要不断补充其中的实体关系，所以链接预测又被称为知识图谱的不全。 (2) 代表模型知识表示学习的代表模型主要包括距离模型、双线性模型、神经张量模型、矩阵分解模型、翻译模型(TransE模型)等。 (3) 复杂关系模型知识库中的实体关系类型也可以分为1-to-1、1-to-N、N-to-1、N-to-N四种类型，复杂的关系主要指的是1-to-N、N-to-1、N-to-N的3种关系类型。由于TransE模型不能用在处理复杂关系上，一系列基于它的扩展模型纷纷被提出，主要包括：TransH模型、TransR模型、TransD模型、TransG模型和KG2E模型。 (4) 多源信息融合三元组作为知识库的一种通用的表示形式，通过表示学习，能够以较为直接的方式表说实体、关系及其之间的复杂语义关联。然而，互联网中仍蕴含着大量与知识库实体、关系有关的信息未被考虑或有效利用，如何充分融合、利用这些多源异质的相关信息，将有利于进一步提升现有知识表示模型的区分能力以及性能。目前，多源异质信息融合模型方面的研究尚处于起步阶段，涉及的信息来源也极为有限。 知识融合由于知识图谱中的知识来源广泛，存在知识质量良莠不齐、来自不同数据源的知识重复、知识间的关联不够明确等问题，所以需要进行知识的融合。知识融合是高层次的知识组织，使来自不同的知识源的知识在同一框架规范下进行异构数据整合、消歧、加工、推理验证、更新等步骤，达到数据、信息、方法、经验以及人的思想的融合，形成高质量的知识库。 实体对齐实体对齐(entity alignment)，也称为实体匹配或实体解析，主要用于消除异构数据中实体冲突、指向不明等不一致性问题，可以从顶层创建一个大规模的统一知识库，从而帮助机器理解多源异质的数据，形成高质量的知识。 在大数据环境下，受知识库规模的影响，在进行知识库实体对齐时，主要会面临以下3个方面的挑战：（1）计算复杂度；（2）数据质量，不同知识库的构建目的与方式不同，可能存在知识质量良莠不齐、相似重复数据、孤立数据、数据时间粒度不一致等等问题。（3）先验训练数据，在大规模知识库中想要获得这种先验数据却非常困难。通常情况下，需要手工构造训练数据。 基于上述，知识库实体对齐的主要流程包括： 将待对齐数据分区索引，以降低计算的复杂度； 利用相似度函数或相似性计算算法查找匹配实例； 使用实体对齐算法进行实例融合； 将步骤2和3的结果结合起来，形成最终的对齐结果。 对齐算法可以分为成对实体对齐和集体实体对齐两大类，而集体实体对齐又可分为局部集体对齐和全局集体对齐。 （1）成对实体对齐基于传统概率模型的实体对齐方法，主要是考虑两个实体各自属性的相似性，而不考虑实体之间的关系。基于机器学习的实体对齐方法，主要是将实体对齐问题转化为二分类问题。根据是否使用标注数据可分为有监督学习与无监督学习两类。 （2）局部集体实体对齐方法局部集体实体对齐方法为实体本身的属性以及与它有关联的实体的属性分别设置不同的权重，并通过加权求和计算总体的相似度，还可使用向量空间模型以及余弦相似度来判别大规模知识库中的实体的相似程度。 （3）全局集体实体对齐方法 知识加工通过实体对齐，可以得到一系列的基本事实表达或初步的本体雏形，然而事实并不等于知识，它只是知识的基本单位。要形成高质量的知识，还需要经过知识加工的过程，从层次上形成一个大规模的知识体系，统一对知识进行管理。知识加工主要包括本体构建与质量评估两方面的内容。 （1）本体构建本体是同一领域内不同主体之间进行交流、连通的语义基础，其主要呈现树状结构，相邻的层次结点或概念之间具有严格的”IsA”关系，有利于进行约束、推理等，却不利于表达概念的多样性。本体在知识图谱中的地位相当于知识库的模具，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。 本体可以通过人工编辑的方式手动构建，也可以通过数据驱动自动构建，然后再经质量评估方法与人工审核相结合的方式加以修正和确认。数据驱动的本体自动构建过程主要可以分为以下3个阶段： 纵向概念间的并列关系计算。通过计算任意两个实体间并列关系的相似度，可辨析它们在语义层面是否属于同一个概念。计算方法主要包括模式匹配和分布相似度两种。 实体上下位关系抽取。上下位关系抽取方法包括基本语法的抽取与基本语义的抽取两种方式。 本体生成。对各个层次得到的概念进行聚类，并为每一类的实体指定1个或多个公共上位词。 （2）质量评估对知识库的质量评估任务通常是与实体对齐任务一起进行的，其意义在于，可以对知识的可信度进行量化，保留置信度较高的，舍弃置信度较低的，有效确保知识的质量。 知识更新人类的认知能力、知识储备以及企业需求都会随着时间而不断递增。因此，知识图谱的内容也需要与时俱进，不论是通用的知识图谱，还是行业知识图谱，它们都需要不断地迭代更新，扩展现有的知识，增加新的知识。 根据知识图谱的逻辑结构，其更新主要包括模式层的更新和数据层的更新。模式层的更新是指本体中元素的更新，包括概念的增加、修改、删除，概念属性的更新以及概念间上下位关系的更新。通常来说，模式层的增量更新方式消耗资源较少，但是多数情况下是在人工干预的情况下完成的。例如，需要人工定义规则，人工处理冲突等。数据层的更新指的是实体元素的更新，包括实体的增加、修改、删除，以及实体的基本信息和属性值。由于数据层的更新一般影响面较小，因此，通常以自动的方式完成。 知识推理知识推理则是在已有的知识库基础上进一步挖掘隐含的知识，从而丰富、扩展知识库。在推理的过程中，往往需要关联规则的支持。对于推理规则的挖掘，主要还是依赖于实体以及关系间的丰富同现情况。知识推理的对象可以是实体、实体的属性、实体间的关系、本体库中的概念的层次结构等。知识推理方法主要可以分为基于逻辑的推理和基于图的推理两种类别。 知识图谱的典型应用智能搜索基于知识图谱的智能搜索是一种基于长尾的搜索，搜索引擎以知识卡片的形式将搜索结果展现出来。用户的查询请求将经过查询式语义理解与知识检索两个阶段。 查询式语义理解。知识图谱对查询式的语义分析主要包括：（1）对查询请求文本进行分词、词性标注以及纠错；（2）描述归一化，使其与知识库中的相关知识进行匹配；（3）语境分析，在不同的语境下，用户查询式中的对象会有所差别，因此，知识图谱需要结合用户当时的情感，将用户此时需要的答案及时反馈给用户；（4）查询扩展，明确了用户的查询意图以及相关概念后，需要加入当前语境下的相关概念进行扩展。 知识检索。经过查询式语义分析后，标准查询语境进行知识库检索引擎，引擎会在知识库中检索相应的实体以及与其在类别、关系、相关性等方面匹配度较高的实体。通过对知识库的深层挖掘与提炼后，引擎将给出具有重要性排序的完整知识体系。 智能搜索引擎主要以3种形式展现知识： 集成的语义数据。例如，搜索梵高，搜索引擎将以知识卡片的形式给出梵高的生平，并配合图片等信息。 直接给出用户查询问题的答案。例如，用户搜索“姚明的身高是多少？”，搜索引擎的结果是“226cm”。 根据用户的查询给出推荐的列表等。 深度问答问答系统是信息检索系统的一种高级形式，能够以准确简洁的自然语言为用户提供问题的解答。多数问答系统更倾向于将给定的问题分解为多个小的问题，然后逐一去知识库中抽取匹配的答案，并自动检测其在时间与空间上的吻合度等，最后将答案进行合并，以直观的方式展现给用户。 社交网络知识图谱的挑战(1)知识获取(2)知识表示 复制关系中的知识表示 多源信息融合中的知识表示 (3)知识融合 并行与分布式算法 众包算法 跨语言知识对齐 (4)知识应用]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采样方法]]></title>
    <url>%2F2017%2F11%2F04%2F%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在前两周组内的技术分享中，分享了采样方法。之前在研究生阶段就对采样方法很是疑惑，特别是在看LDA时，用到的Gibbs采样，很多次尝试去学习这一知识点，但都一知半解。所以，借这个机会认真学习一下采样方法相关的知识。本文主要是记录一下自己在学习采样方法时，对不同采样方法原理的理解，主要包括蒙特卡洛方法介绍和5中不同的采样方法。 蒙特卡洛方法首先说一下蒙特卡洛方法，Monte Carlo方法，又称为统计模拟法、随机抽样技术，是一种随机模型方法，以概率和统计理论为基础的一种计算方法。是使用随机数（或伪随机数）来解决很多复杂问题的计算方法。其核心就是通过将所要求解的问题同一定的概率模型相联系，利用计算机进行模拟或抽样，以获得问题的近似解。简单地理解蒙特卡洛方法，其实就是将要解决的采用一定的方式进行转换，转换之后的问题通过利用计算机实现统计模拟或抽样，从而获得问题是近似解。至于为什么叫Monte Carlo方法，可能是和闻名世界的赌城——摩纳哥的一个小山城有关吧！具体是什么原因，不必去深究。 来看一个典型的用Monte Carlo方法解决实际问题的例子。计算圆周率Π的值，如果采用Monte Carlo方法进行计算，可以进行如下的转换： 首先，设置一个边长为1的正方形，其内有一圆，圆的半径为0.5，如下图所示： 随机的向正方形内打点，打的点在圆内的概率等于圆与正方形的面积之比0.25Π 随机产生M个点(x,y)，其中，x和y都是区间[0,1]内的符合均匀分布的随机数 假设落在圆内的点有N个，当M足够大时，根据大数定理，频率等于概率，就有：$$\frac {N} {M} = 0.25Π$$，$$Π=\frac {4N} {M}$$ 实际用代码去模拟一下，得出的圆周率的值为：3.143748，和3.1415926非常接近。 12345678def samplePI(maxCnt = 1000000):accCnt = 0for i in range(maxCnt): x = np.random.random() y = np.random.random() if np.sqrt(x**2 + y**2) &lt; 1: accCnt += 1print "PI=", float(accCnt) / maxCnt *4 通过一个简单的例子，可以直观地理解Monte Carlo方法。采样Monte Carlo方法去解决实际问题一般可以分为如下的步骤： 对复杂的问题进行转换，构造或描述随机过程。例如，将计算圆周率的问题，转换为在一个正方形内进行打点，将圆周率的值和点在圆内的概率联系起来。 从已知的概率分布中进行采样,采样出符合特定分布的样本。例如，从符合[0,1]均匀分布中采样出x和y。 建立估计量进行计算。 Monte Carlo方法是一种通用的计算技术，可以解决如下问题: 随机模拟：从一个pdf产生”典型”的样本 计算积分：在高维空间中的积分 优化问题 学习：MLE:f(x;Θ) 利用Monte Carlo方法去解决实际问题的第2步是从符合特定分布中采样出样本。我们知道，计算机本身无法产生真正的随机数，但可以根据一定的算法产生伪随机数。比如，通过线性同余发生器可以生成伪随机数，我们可以用确定性的算法生成[0,1]之间符合均匀分布的随机数，可以被当成真正的随机数使用。而至于产生符合比较复杂分布的随机数，则是以均匀分布为基础进行采样，以获得符合特定复杂分布的样本，这就是采样方法。不同的采样方法，采样过程不同，但大都基于均匀分布来进行的。下面就分别介绍，几种不同的采样方法。 Inverse Transform Sampling在介绍不同的采样方法之前，先说一下概率分布，概率分布一般分为连续型和离散型两种，离散型的概率分布一般用概率质量分布函数(pmf)表示，而连续分布用概率密度(pdf)表示,对pmf进行累加或对pdf进行积分的函数，对应于累积分布函数(cdf)。所有pmf的取值之和为1，对pdf在其定义域上进行积分，积分的值为1。对于一些简单的分布p(x),我们可以直接进行采样，比如，采样符合p(x)=[0.1,0.3,0.5,0.1]分布的样本 $x=x_1,x_2,x_3,x_4$分布，我们可以直接从Uniform(0,1)采样出一个数，x小于0.1，则采样的值为$x_1$,x等于0.1小于0.4则为$x_2$，以此进行类推。对于比较复杂的分布p(x)，不能直接进行采样。如果要采样的目标分布为p(x),它的累积分布函数F(x)能够求出来，F(x)的反函数也能求出来，那么，Inverse Transform Sampling的采样过程如下：123Inverse Transform Sampling1. 从Uniform(0,1)中随机采样一个点，用u表示。2. 计算$F^-1(u)$的值为x,则x就是服从p(x)的分布的一个样本点。 简单的证明一下：设F(x)为目标采样分布P(x)的累积分布，$x=F^{-1}(u) u\in[0,1]$为F(x)的反函数。因为F(x)是单调递增的(累积函数的性质)，所以$x=F^{-1}(u)$也是单调递增函数，对于如下不等式:$$F^{-1}(u)\leq F^{-1}(F(x)), if u \leq F(x) （1）$$根据反函数的定义有：$$F^{-1}(u) \leq x, if u \leq F(x) （2）$$根据Uniform(0,1)的定义，其累积分布函数如下： 所以，采样出的点的$F^{-1}(u)$累积分布函数如下：$$P(F^{-1}(u) \leq x)=P(u \leq F(x))=H(F(x))=F(x)$$由此可见，采样出的点的累积分布函数为F(x),所以，为符合p(x)分布的样本。虽然，利用这种方法可以采样出符合特定分布的样本，但是这种采样方法存在一个缺陷，即，如果要采样的分布p(x)累积分布函数不能够求出来或者累积分布函数没有反函数，这种方法就失效了 Acceptance-Rejection Sampling对于采样的目标分布p(x),如果其比较复杂，可以采用Acceptance-Rejection Sampling接受-拒绝采样，来进行采样。这种采样方法不直接对p(x)进行采样，而是选择另一个分布q(x)，存在常数M，使得$p(x)\leq Mq(x),\forall x $,称为建议分布(Proposal Density),这个分布容易进行采样，通过对q(x)的采样，来实现对p(x)的采样。其具体的采样过程如下：1234Acceptance-Rejection Sampling的采样过程：1. 从q(x)中随机采样出一个样本点x02. 在从Uniform(0, Mq(x0))中采样出另一个样本点u3. 如果u&gt;p(x0),则拒绝x0,并且重复前面的步骤。否则接受x0为符合p(x)分布的样本 这种采样方法可以用计算圆周率的方法来进行理解。把正方形看做一个分布Mq(x),圆形看做要采样的目标分布q(x),通过对Mq(x)进行随机采样，即打点，如点果落在圆内，表明该点符合圆形这个分布，即可以做符合p(x)的样本。即，如下图所示的圆内的点都是符合圆形这个分布的点。 Acceptance-Rejection Sampling，通过对q(x)的采样，实现了对p(x)的采样。这种采样方法的接受率正比于$\frac {1} {M}$,等于p(x)下面的面积除以Mq(x)下面的面积。可以看出，只有当M尽可能的小时，采样出的点被接受的概率才会大，从而可以提升采样的效率，因此在使用该方法进行采样是M的选择比较重要。同时,可以看出，如果q(x)和p(x)的形相似时，M就会越小，接受率就会越高。然而，对于高维的目标采样分布，q(x)可能不容易寻找，且M也可能会很大，此时，接受率就会变小，采样效率会变差。 Importance SamplingImportance Sampling 这种采样方法，其并不是为了获得符合特定分布p(x)的样本，而是为了解决当p(x)不容易进行采样时，计算E[f(x)]，x符合p(x)分布，的问题。其可以进行如下的转换：首先，根据期望的定义，E[f(x)]的计算公式如下：$$E[f(x)] = \int_x f(x)p(x)dx$$因为，p(x)的样本不容易获取，Importance Sampling同样引入一个建议分布q(x)，比较容易获取符合q(x)分布的样本，进行如下转换（以下内容参考：http://blog.csdn.net/dark_scope/article/details/70992266）：$$\int_x f(x)p(x)dx = \int_x f(x)\frac {p(x)} {q(x)} q(x) dx= \int_x g(x)q(x)dx where g(x)=f(x) \frac {p(x)} {q(x)} = f(x)w(x)$$可以看出，通过上面的转化，可以将计算f(x)在p(x)下的期望转化为求g(x)在q(x)分布下的期望。其中，$w(x) = \frac {p(x)} {q(x)}$被称为Importance Weight。但是，有些时候p(x)也是很难计算的，更常见的情况是比较方便的计算$\hat p (x)$和$\hat q(x)$$$p(x) = \frac {\hat p(x) }{Z_p}$$$$p(x) = \frac {\hat p(x)}{Z_p}$$其中，$Z_{p/q}$是一个标准化项，可以看成是一个常数，是的$\hat p(x)$或者$\hat q(x)$等比例变化为一个概率分布，也可以理解为softmax里的分母。其中：$$Z_p=\int_x \hat p(x)dx$$$$Z_q=\int_x \hat q(x)dx$$在这种情况下，Importance Sampling 可以进行如下的转换：$$ \int_x f(x)p(x)dx=\int_x f(x)\frac {p(x)} {q(x)} q(x)dx\\\\ =\int_x f(x) \frac {\hat p(x)/Z_p} {\hat q(x)/Z_q} q(x)dx\\\\=\frac {Z_q}{Z_p} \int_x f(x) \frac {\hat p(x)}{\hat q(x)}\\\\=\frac {Z_q}{Z_p} \int_x \hat g(x)q(x)dx\\\\其中，\hat g(x) = f(x)\frac {\hat g(x)}{\hat q(x)}=f(x)\hat w(x)$$而$\frac {Z_q}{Z_p}$直接计算不太好计算，而它的倒数：$$\frac {Z_p}{Z_q}=\frac {1}{Z_q}\int_x \hat p(x)dx, Z_q=\frac {\hat q(x)} {q(x)} $$所以：$$\frac {Z_p}{Z_q}=\frac {\hat p(x)}{\hat q(x)}q(x)dx = \int_x \hat w(x)q(x)dx$$这样，假设能方便从q(x)进行采样，所以上式又被转换为一个Monte Carlo可解的问题，也就是说：$$\frac {Z_p}{Z_q}=\frac {1}{m} \sum_{i=1}^m \hat w(x_i), x_i符合q(x)分布。$$最终，求解E[f(x)]的问题可以转换为：$$E[f(x)]=\frac {1}{m} \sum_{i=1}^m \hat w(x_i)f(x_i), 其中，x_i为符合q(x)的样本\\\ \hat w(x_i)=\frac {\hat w(x_i)}{\sum_{i=1}^m \hat w(x_i)}$$所以，我们可以在不用知道q(x)确切值的情况下，就可以近似地计算得到E[f(x)]。其计算过程如下：Importance Sampling采样过程： 首先为p(x)找到一个建议分布q(x),q(x)比较容易采样。 然后从q(x)中采样出m个点x 带入$E[f(x)] = \frac {1}{m} \sum_{i=1}^m \hat w(x_i)f(x_i)$ 计算期望。虽然这种方法能够work,但是在高维空间里找到一个这样合适的q(x)非常难。即使有 Adaptive importance sampling 和 Sampling-Importance-Resampling的出现，要找到一个同时满足容易抽样并且和目标分布相似的建议分布，通常是不可能的！ MCMC: Markov Chain Monte CarloImportance Sampling和Acceptance-Rejection Sampling虽然能够实现对一些分布的采样，但是只有当选取的建议分布q(x)和要进行采样的目标分布p(x)很近似时才表现好，所以选取合适的q(x)是非常关键。当在高维空间进行采样，标准的采样方法会失败，对于Acceptance-Rejection Sampling,当目标分布的维数增高时，拒绝率会趋近于100%，采样的效率会很低。对于Importance Sampling，大多数的样本的权重值会趋近于0。对于高维复杂问题，可以用马尔科夫链（Markov Chain)产生一系列相关样本，实现对目标分布的采样。MCMC是一种用一定范围内的均匀分布的随机数对高维空间概率进行采样的通用技术，其基本思想是设计一个马尔科夫链，使得其稳定概率分布为要采样的目标分布$\pi(x)$ 首先来看一下马尔科夫链的定义及其平稳分布： 马尔科夫性质：某一时刻状态转移的概率只依赖于它前一个状态 定义：假设存在状态序列$… X_{t-2},X_{t-1},X_t,X_{t+1}…$,时刻t+1的状态的条件概率只依赖于t时刻的状态$x_t$,即：$$P(X_{t+1}|…X_{t-2},X_{t-1},X_t)=P(X_{t+1}|X_t)$$ 马尔科夫链：满足马尔科夫性质的随机过程 以天气变化来解释一下上面的定义，假设每天的天气是一个状态的话，状态转移可以看成是天气的变化，比如从晴天变成阴天、从阴天变成雨天等。马尔科夫性质讲的是，今天的天气情况只依赖于昨天的天气，和前天以及之前的天气状况没有任何关系。马尔科夫链可以看成每天的天气按照这个规律进行变化的一个过程。一个马尔科夫链可以由下面的公式定义： 一个马尔科夫链一般由三部分构成： 状态空间：可以理解为天气状况的所有情况 {阴天，晴天，雨天，…} 初始状态：可以理解为第一天的天气情况 状态转移矩阵：可以理解为所有由一种天气状况变为另一种天气状况的概率 马尔科夫链具有一个非常重要的性质：马尔科夫链的平稳分布。来看一个例子，假设一个国家的人口地域分布分为：农村、城镇和城市3种状态。每年人口流动情况如下图： 上图表示每种状态转移到另一种状态的概率。如果定义矩阵P，P的某一位置P(i,j)的值为P(j|i)，表示从状态i转换为状态j的概率，则根据上图可以得到马尔科夫链的状态转移矩阵为： 假设初始状态的人口地域分布为$\pi_0=[\pi_0(1),\pi_0(2),\pi_0(3)]$，每年人口按照状态转移矩阵P进行转移，n年后人口的地域分布为$\pi_n=\pi_{n-1}P=…=\pi P^n$。假设存在如下两种初始的人口分布: $\pi_0=[0.5,0.4,0.1]$ $\pi_0=[0.3,0.4,0.3]$ 按照状态转移矩阵进行转移一定年数后的人口分布情况分布如下图所示： 可以看出，尽管采用了不同的初始化状态，但最终的概率分布都趋近于一个稳定的概率分布[0.167,0.388,0.444]。可以看到，马尔科夫链模型的状态转移矩阵收敛到稳定概率分布和初始状态概率分布无关。也就是说，如果得到了稳定概率分布对应的马尔科夫链模型的状态转移矩阵，我们可以从任意的概率分布样本开始，带入马尔科夫链模型的状态转移矩阵，经过一系列的状态转移，最终样本的分布会趋近于稳定的概率分布。用数学的语言来定义一下马尔科夫链的收敛性质：如果一个非周期的马尔科夫链，其状态转移矩阵为P，并且它的任何两个状态之间是联通的，那么$\lim_{n \rightarrow +\infty} P_{ij}^n$ 与i无关，则有： $\lim_{n \rightarrow +\infty} P^n = \begin{bmatrix}{\pi(1)}&amp;{\pi(2)}&amp;{\cdots}&amp;{\pi(n)}\\\\{\pi(1)}&amp;{\pi(2)}&amp;{\cdots}&amp;{\pi(n)}\\\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\\\{\pi(1)}&amp;{\pi(2)}&amp;{\cdots}&amp;{\pi(n)}\\\\\end{bmatrix}$ $\pi(j)=\sum_{i=0}^\infty P_{ij}$，$p_{ij}$表示状态i转移到状态j的概率 $\pi$是方程$\pi P = \pi$的唯一非负解，其中，$$\pi=[\pi(1),\pi(2),….,\pi(j),…] \sum_{i=0}^\infty \pi(i)=1$$ 上面的性质中有如下的说明： 非周期的马尔科夫链：主要是指马尔科夫链的状态转化不是循环的，如果是循环的则永远不会收敛。我们一般遇到的马尔科夫链一般都是非周期性的。用数学方式可以表述为：对于任意某一状态i,d为集合${n | n\geq 1,P_{ij}^n &gt; 0}$的最大公约数为1，如果d=1，则该状态为非周期的。 任何两个状态是联通的：指从任意一个状态可以通过有限步到达其他的任意一个状态，不会出现条件概率为0导致不可达的情况。 马尔科夫链的状态可以是有限的，也可以是无限的。因此，可以用于连续概率分布和离散概率分布。 $\pi$通常称为马尔科夫链的平稳分布。 基于马尔科夫链的采样方法从马尔科夫链的收敛性质可以看出，如果我们能够得到某个平稳分布对应的马尔科夫链状态转移矩阵，就很容易采样出这个平稳分布的样本。假设任意初始化的概率分布为$\pi_0(x)$，经过第一轮转移后的概率分布是$\pi_0(x)$，经过i轮后的概率分布为$\pi_i(x)$。假设经过n轮后马尔科夫链收敛到平稳分布$\pi(x)$,即：$$\pi_n(x)=\pi_{n+1}(x)=…=\pi(x)$$那么经过n轮之后的，沿着状态转移矩阵进行转移得到的样本都是符合$\pi(x)$分布的样本。也就是说，如果从一个具体的初始状态$x_0$出发，沿着马尔科夫链按照状态转移矩阵进行跳转，假设n次跳转后收敛，那么得到一个转移状态序列$X_0,X-1,…,X_n,X_{n+1},….$，则$X_n,X_{n+1},…$都是符合平稳分布$\pi(x)$的样本。 基于马尔科夫链的采样过程如下： 输入： 状态转移矩阵P，设定状态转移次数$n_1$，需要采样的样本个数$n_2$ 从任意概率分布采样得到一个初始状态值$x_0$ for t=0 to $n_1+n_2 - 1$：从条件概率分布$P(x|x_t)$中采样出样本为$x_{t+1}$ 输出：样本$(x_{n_1},x_{n_1+1},…,x_{n_1+n_2})$即为符合平稳分布$\pi(x)$的样本。 解释一下上面的从条件概率分布$P(x|x_t)$中采样出样本为$x_{t+1}$。为什么要从$P(x|x_t)$中进行采样？因为，当前的状态为$x_t$，从当前状态$x_t$进行转移，可转移到的状态为状态空间集合，所以要以当前状态为条件向其他状态进行转移，所以要从$P(x|x_t)$中进行采样。例如，人口分布的例子，假设当前的状态是农村，那么从农村转移到农村、城镇和城市的概率分别为0.5、0.4和0.1，即$P(x|x_t)=[0.5,0.4,0.1]$，转移时要按照这个分布进行转移，即采样时要按照这个分布进行采样，使用均匀分布可以很容易实现。而状态转移为连续的情况，则$P(x|x_t)$则是一个具体的连续分布，通过对$P(x|x_t)$的采样，实现转移。可以看出，我们要采样的目标分布为$\pi(x)$,如果我们能够构建一个状态转移矩阵P，使得其马氏链上的平稳分布是$\pi(x)$，那么初始化一个状态，按照状态转移矩阵P进行转移，经过n次后收敛，第n+1次后的样本都是符合$\pi(x)$分布的。所以，MCMC采样方法的关键是构建状态转移矩阵P。如何构建这个状态转移矩阵P呢？首先看一下马尔科夫链的细致平稳条件：如果一个非周期的马尔科夫链状态转移矩阵为P和概率分布$\pi(x)$，对于所有的i，j满足：$$\pi(i)P(i,j)=\pi(j)P(j,i)$$其中，P(i,j)表示状态i转移到状态j的概率,则称$\pi(x)$是状态转移矩阵P的平稳分布。证明：$$\sum_{i=1}^\infty \pi(i)P(i,j)=\sum_{i=1}^\infty \pi(j)P(j,i)=\pi(j)\sum_{i=1}^\infty P(j,i) = \pi(j)$$写成矩阵的形式，即：$$\pi P=\pi$$由于$\pi$是$\pi P=\pi$的解，所以$\pi$是一个平稳分布。上式被称为细致平稳条件(detailed balance condition)。其实这个定理是显而易见的，因为细致平稳条件的物理含义就是对于任何两个状态i,j 从i转移出去到j而丢失的概率质量，恰好会被从j转移回i的概率质量补充回来，所以状态i上的概率质量$\pi(i)$是稳定的，从而$\pi(x)$是马尔科夫链的平稳分布。从细致平稳条件可以看出，只要找到可以使概率分布$\pi(x)$满足细致平稳分布的矩阵P即可。假设在进行采样之前已经存在一个状态转移矩阵Q，Q(i,j)表示从状态i转移到状态q的概率，也可以表示为Q(j|i),通常情况下：$$\pi(i)Q(i,j)\neq \pi(j)Q(j,i)$$也就是说不满足细致平稳条件。不过可以对其进行改造，使其满足细致平稳条件。具体的是引入$\alpha$使得上面的公式成立，即：$$\pi(i)Q(i,j)\alpha(i,j)=\pi(j)Q(j,i)\alpha(j,i)$$什么样的$\alpha$能够使上式成立呢？最简单的，按照对称性，可以取如下的值：$$\alpha(i,j)=\pi(j)Q(j,i)\\\\\alpha(j,i)=\pi(i)Q(j,i)$$在改造Q的过程中，引入的$\alpha(i,j)$称为接受率，取值在[0,1]之间，物理意义可以理解为在原来的马尔科夫链上，从状态i以Q(i,j)的概率转跳转到状态j的时候，我们以$\alpha(i,j)$的概率接受这个转移。这样就可以得到新的马尔科夫链的转移概率为$Q(i,j)\alpha(i,j)$。通过这种改造，就能够进行采样了。MCMC采样过程如下：MCMC采样算法 初始化马尔科夫链初始状态$X_0=x_0$ fo t=0,1,2,… do:2.1 在t时刻马尔科夫链状态为$X_t=x_t$，从$Q(x|x_t)$中采样出一个样本y2.2 从均匀分布中采样出u~Uniform(0,1)2.3 如果$u&lt;\alpha(x_t,y)=\pi(y)Q(x_t|y)$，则接受转移$x_{t+1}=y$2.4 否则，不接受转移，$x_{t+1}=x_t$ 上面的MCMC采样算法已经能够很好的工作了，但是它存在一个缺陷:马氏链在转移的过程中的接受率$\alpha(i,j)$可能偏小，这样采样过程中，马氏链不容易转移，一直处于原地，导致收敛到平稳分布的速度偏慢。假设$\alpha(i,j)=0.1,\alpha(j,i)=0.2$，假设在此时满足细致平稳条件，于是有：$$\pi(i)Q(i,j).0.1 = \pi(j)\alpha(j,i).0.2$$将上式两边同时扩大5倍，等式变为：$$\pi(i)Q(i,j).0.5 = \pi(j)\alpha(j,i).1$$可以看到细致平稳条件并没有被打破，而接受率变大了。因此，我们可以把细致平稳条件中的$\alpha(i,j)$和$\alpha(j,i)$同比例放大，使得两个数中较大的那个放大到1，这样就提高了采样中的跳转的接受率。于是$\alpha(i,j)$可以取下面的值:$$\alpha(i,j) = min{\frac {\pi(j)Q(j,i)} {\pi(i)Q(i,j)},1}$$这样，就完成了对MCMC采样算法的改造，这就是Metropolis-Hastings采样算法，其采样过程如下：Metropolis-Hastings采样算法 初始化马尔科夫链初始状态$X_0=x_0$ for t=0,1,2,… do:2.1 在t时刻马尔科夫链状态为$X_t=x_t$，从$Q(x|x_t)$中采样出一个样本y2.2 从均匀分布中采样出u~Uniform(0,1)2.3 如果$u&lt;\alpha(x_t,y)=min{\frac {\pi(y)Q(y,x_t)} {\pi(x_t)Q(x_t,y)},1}$，则接受转移$x_{t+1}=y$2.4 否则，不接受转移，$x_{t+1}=x_t$ Gibbs Sampling虽然MCMC采样和Metropolis-Hasting采样算法已经能够解决蒙特卡罗方法中需要的任意概率分布的样本的问题。但是还是存在一定的缺陷，首先是采样过程中要计算接受率，在高维时，计算量大，可能存在辛辛苦苦计算出的接受率，最终被拒绝，不跳转。并且由于接受率的原因导致算法的收敛时间变长。其次是，对于高维空间，状态的条件概率分布好求解，但是联合分布不好求。针对这一问题，对于高维空间的数据采样，Stuart Geman和Donald Geman这两兄弟于1984年提出来了Gibbs Sampling算法。Gibbs Sampling算法思想是通过构建状态转移矩阵，使得接受率为1，从而提升了接受的效率。对于二维情形，假设存在一个概率分布P(x,y)，对于x坐标相同的两个点$A(x_1,y_1),B(x_1,y_2)$可以得到如下公式：$$p(x_1,y_1)p(y_2|x_1) = p(x_1)p(y_1|x_1)p(y_2|x_1)\\\\p(x_1,y2)p(y_1|x_1)=p(x_1)p(y_2|x_1)p(y_1|x_1)$$上面的转换是利用乘法公式进行转换的。于是可以得到：$$p(x_1,y_1)p(y_2|x_1) = p(x_1,y_2)p(y_1,x_1)\\\\p(A)p(y_2|x_1) = p(B)p(y_1|x_1)$$可以看到，在$x=x_1$这条直线上，如果使用条件概率分布$p(y|x_1)$作为任意两点转移概率，那么任何两点之间的转移概率满足细致平稳条件。同样，在$y=y_1$这条直线上任取两点$A(x_1,y_1),C(x_2,y_1)$，同样有：$$p(A)p(x_2|y_1)=p(C)p(x_1|y_1)$$于是可以构造平面上任意两点之间的转移概率矩阵Q:$$Q(A-&gt;B)=p(y_B|x_1), if x_A=x_B=x_1\\\\Q(A-&gt;C)=P(x_C|y_1), if y_A=y_C=y_1\\\\Q(A-&gt;D)=0，其他$$则对于平面上任意两点X,Y,很容易验证是否满足细致平稳条件：$$p(X)Q(X-&gt;Y)=P(Y)Q(Y-&gt;X)$$于是二维的Gibbs Sampling采样算法，采样过程如下： 随机初始化$X_0=x_0, Y_0=y_0$ 对于t=1,2,…循环采样：2.1 从条件概率分布$p(y|x_0)$中采样得到$y_1$2.2 从条件概率分布$p(x|y_1)$中采样得到$x_1$ 对于多维的情况，算法也是成立的。例如，一个n维的概率分布$\pi(x_1,x_2,…,x_n)$，可以通过在n个坐标轴上轮换进行采样，得到新的样本集。对于轮换到的任意一个坐标轴$x_i$上的转移，马尔科夫链的状态转移概率为$p(x_i|x_1,x_2,…,x_{i-1},x_{i+1},…,x_n)$。即固定n-1个坐标轴，在某一个坐标轴上移动，同样是满足细致平稳条件。多维的Gibbs Sampling算法采样过程如下： 随机初始化${x_i:i=1,…,n}$ 对于t=0,1,2,…循环采样：2.1 $X_{1}^{t+1} ~p(x_1|x_{2}^{(t)},x_{3}^{(t)},…,x_n^{(t)}$2.2 $X_{2}^{t+1} ~p(x_2|x_{1}^{(t+1)},x_{3}^{(t)},…,x_n^{(t)}$2.3 …2.4 $X_{j}^{t+1} ~p(x_j|x_{1}^{(t+1)},x_{2}^{(t+1)},…,x_{j-1}^{(t+1)},x_{j+1}^{(t)},…,x_n^{(t)}$2.5 …2.5 $X_{n}^{t+1} ~p(x_1|x_{1}^{(t+1)},x_{2}^{(t+1)},…,x_{n-1}^{(t+1)}$ 同样的，轮换坐标轴不是必须的，可以随机选择某一个坐标轴进行状态转移，只不过常用的Gibbs采样的实现都是基于坐标轴轮换的。 Reference Pattern Recognition and Machine Learning， Christopher Bishop，Chapter 11 LDA数学八卦 http://blog.csdn.net/dark_scope/article/details/70992266 http://www.cnblogs.com/pinard/p/6625739.html https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling http://www.jdl.ac.cn/user/lyqing/StatLearning/10_08_MonteCarlo-blue.pdf http://www.jdl.ac.cn/user/lyqing/StatLearning/10_13_MonteCarlo2.pdf]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Sampling Methods</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编写可读代码的艺术-读书笔记]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%BC%96%E5%86%99%E5%8F%AF%E8%AF%BB%E4%BB%A3%E7%A0%81%E7%9A%84%E8%89%BA%E6%9C%AF-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第二章 把信息装到名字里​ 主题是：把信息塞入名字中。即，读者仅通过读到名字就可以获得大量的信息。主要有以下几点： 使用专业的单词–例如，不用Get，而用Fetch或者Download可能会更好，这由上下文决定。 避免空泛的名字，像tmp和retval，除非使用它们有特殊的理由。 使用具体的名字来更细致地描述事物——ServerCanStart() 这个名字就比CanListenOnPort更不清楚。 给变量名带上重要的细节——例如，在值为毫秒的变量后加上_ms，或者在还需要转义的、未处理的变量前加上raw_。 为作用域大的名字采用更长的名字——不要用让人费解的一个或两个字母的名字来命名在几屏之间都可见的变量。对于只存在于几行之间的变量用短一点的名字更好。 有目的地使用大小写、下划线等——例如，可以在类成员和局部变量后面加上“_”来区分它们。第三章 不会误解的名字​ 不要误解的名字是最好的名字——阅读你代码的人应该理解你的本意，并且不会有其他的理解。 在决定使用一个名字以前，要想象一下你的名字被误解成什么。最好的名字是不会误解的。 当要定义一个值的上限或下限时，max_和min_是很好的前缀。对弈包含的范围，first和last是好的选择。对于包含/排除范围，begin和end是好的选择。 当为布尔值命名时，使用is和has这样的词来明确表示它是一个布尔值，避免使用反义的词。 要小心用户对特定词的期望。例如，用户会期望get()或者size()是轻量的方法。第四章 审美​ 通过把代码用一致的、有意义的方式“格式化”，可以把代码变得更容易读，并且可以读得更快。下面是讨论过的一些具体技巧: 如果多个代码块做相似的事情，尝试让他们有同样的剪影。 把代码按“列”对齐可以让代码更容易浏览。 如果在一段代码中提到A、B和C，那么不要在另一段中说B、C和A。选择一个有意义的顺序，并始终用这样的顺序。 用空行来把大块分成逻辑上的“段落”。第五章 该写出什么样的注释 注释的目的是帮助读者了解作者在写代码时已经知道的那些事情。本章主要介绍如何发现所有的不那么明显的信息块并把它们写下来。什么地方不需要注释: 能从代码本身中迅速地推断的事实。 用来粉饰烂代码的“拐杖式注释”——应该把代码改好。应该记录下来的想法包括: 对于为什么代码写成这样而不是那样的内在理由（“指导性批注”）。 代码中的缺陷，使用像TODO:或者XXX:这样的标记。（TODO:还没有处理的事情；FIXME：已知的无法运行的代码；HACK：对一个问题不得不采用的比较粗暴的解决方案；XXX：危险！这里有重要的问题） 常量背后的故事，为什么是这个值。站在读者的立场上思考: 预料到代码中哪些部分会让读者产生疑问，并且给它们加上注释。 为普通读者意料之外的行为加上注释。 在文件/类的级别上使用“全局观”注释来解释所有的部分是如何一起工作的。 用注释来总结代码块，使读者不致迷失在细节中。第六章 如何写出言简意赅的注释 当像“it”和“this”这样的代词可能指代多个事物时，避免使用它们。 尽量精确地描述函数的行为。 在注释中用精心挑选的输入/输出例子进行说明。 声明代码的高层次意图，而非明显的细节。 用嵌入的注释来解释难以理解的函数参数。 用含义丰富的词来使注释简洁。第七章 把控制流变得易读有几种方法可以让代码的控制流更易读 在写一个比较时（while （bytes_expected &gt; bytes_received）），把改变的值写在左边，并且把更稳定的值写在右边更好一些（while （bytes_received &lt; bytes_expected））。 可以重新排列if/else语句中的语句块。通常来讲，先处理正确的/简单的/有趣的情况。有时这种准则会冲突，但是当不冲突时，这是要遵守的经验法则。 某些编程结构，像三目运算符(:?)、do/while循环，以及goto经常会导致代码的可读性变差。最好不要使用它们，因为总是有更整洁的代替方式。 嵌套的代码块需要更加集中精力去理解。每层新的嵌套都需要读者把更多的上下文“压入栈”。应该把它们改写成更加“线性”的代码来避免深嵌套。 通常来讲提早返回可以减少嵌套并让代码整洁。“保护语句”（在函数顶部处理简单的情况时）尤其有用。第八章 拆分超长的表达式 关键思想：把超长的表达式拆分为更容易理解的小块 引入“解释变量”来代表较长的子表达式。这种方式有三个好处： 它把巨大的表达式拆成小段； 它通过用简单的名字描述子表达式来让代码文档化； 它帮助读者识别代码中的主要概念。 用德摩根定理来操作逻辑表达式——这个技术可以把布尔表达式用更整洁的方式重写； 任何复杂逻辑的地方都可以进行拆分。第九章 变量与可读性减少变量 删除没有价值的临时变量 减少中间结果 减少控制变量缩小变量的作用域 关键思想:让你的变量对尽量少的代码行可见。 只写一次的变量更好总结 本章是关于程序中的变量是如何快速累积而变得难以跟踪的。你可以通过减少变量的数量和让它们尽量“轻量级”来让代码更有可读性。具体有： 减少变量。 减少每个变量的作用域，越小越好。把变量移到一个有最少代码可以看到的地方。 只写一次的变量更好。那些只设置一次的变量（或者const、final、常量）使得代码更容易理解。第十章 抽取不相关的子问题​ 本章一个简单的总结就是“把一般代码和项目专有的代码分开“。其结果是，大部分的代码都是一般代码。通过建立一大组库和辅助函数来解决一般问题，剩下的只是让你的程序与众不同的核心部分。​ 这个技巧有帮助的原因是它使程序员关注小而定义良好的问题，这些问题已经同项目的其他部分脱离。其结果是，对于这些子问题的解决方案倾向于更加完整和正确。你也可以再以后重用它们。第十一章 一次只做一件事 关键思想：应该把代码组织得一次只做一件事情 ​ 如果你有很难读的代码，尝试把它所做的所有任务列出来。其中一些任务可以很容易地变成单独的函数。其他的可以简单地变成为一个函数中的逻辑”段落”。具体如何拆分这些任务没有它们已经分开这个事实那样重要。难的是要准确地面描述你的程序所做的所有这些小事情。 第十二章 把想法变成代码​ 本章讨论了一个简单的技巧，用自然语言描述程序然后用这个描述来帮助你写出更自然的代码。这个技巧出人意料地简单，但很强大。看到你在描述中所用的词和短语还可以帮助你发现哪些子问题可以拆分出来。但是这个“用自然语言说事情”的过程不仅可以用于写代码。另一个看待这个问题的角度是：如果你不能把问题说明白或者用词语来做设计，估计是缺少什么东西或者什么东西缺少定义。把一个我那天变成语言可以让它变得更具体。 第十三章 少写代码 关键思想:最好的代码就是没有代码。 质疑和拆分你的需求 保持小的代码库 ​ 随着项目的增长，项目加进来的越来越多的源文件。项目很大，没有一个人自己全部理解它。增加新功能会变得很痛苦，而且使用这些代码还很费力还令人不快。最好的解决办法就是”让你的代码库越小，越轻量级越好“，可以尝试如下方法： 1. 创建越多越好的”工具“代码来减少重复代码； 2. 减少无用代码或没用的功能； 3. 让你的项目保持分开的子项目状态； 4. 总的来说，要小心代码的”重量“。让它保持又轻又灵。 熟悉你周边的库 总结 ​ 本章是关于写越少代码越好的。每行新的代码都需要测试、写文档和维护。另外，代码库中的代码越多，它就越”重“，而且在其上开发就越难。可以通过以下方法来避免重新编写新代码： 从项目中消除不必要的功能，不要过度设计； 重新考虑需求，解决版本最简单的问题，只要能完成工作就行； 经常性地通读标准库的整个API，保持对他们的熟悉程度。 第十四章 测试与可读性在测试代码中，可读性仍然很重要。如果测试的可读性很好，其结果是他们也会变得很容易写，因此大家会写更多的测试。并且，如果你把事实代码设计得容易测试，代码设计会变得更好。以下是如何改进测试的具体要点： 每个测试的最高一次应该越简明越好。最好每个测试的输入/输出可以用一行代码描述； 如果测试失败了，它所发出的错误消息应该能让你容易跟踪并修正这个bug； 使用最简单的并且能够完整运用代码的测试输入； 给测试函数取一个有完整描述性的名字，以使每个测试所测到的东西很明确。不要用Test1()，而要像Test___这样的名字。 最重要的是，要使它易于改动和增加新的测试。]]></content>
      <categories>
        <category>程序猿的自我修养</category>
      </categories>
      <tags>
        <tag>程序猿的自我修养</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第7章 类]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%AC%AC7%E7%AB%A0-%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[7.1 定义抽象数据类型构造函数 每个类都分别定义了它的对象初始化的方式，类通过一个或几个特殊的成员函数来控制其对象的初始化过程，这些函数叫做构造函数。构造函数的任务是初始化类对象的数据成员，无论何时只要类的对象被创建，就会执行构造函数。构造函数的名字和类名相同，和其他函数不一样的是，构造函数没有返回类型；除此之外类似于其他的函数，构造函数也有一个参数列表和一个函数体。类可以包含多个构造函数，和其他重载函数差不多，不同的构造函数之间必须在参数数量或参数类型上有所区别。 不同于其他成员函数，构造函数不能被声明为const的。当我们创建类的一个const对象时，直到构造函数完成初始化过程，对象才能真正取得其“常量”属性。因此，构造函数在const对象的构造过程中可以向其写值。 默认构造函数 类通过一个特殊的构造函数来控制默认初始化的过程，这个函数叫做默认构造函数。默认构造函数无须任何实参。如果我们的类没有显式地定义构造函数，那么编译器就会为我们隐式地定义一个默认构造函数。编译器创建的构造函数又被称为合成的默认构造函数。对于大多数来说，这个合成的默认构造函数按照如下规则初始化类的数据成员： 如果存在类内的初始化值，用它类初始化成员。 否则，默认初始化该成员。 某些类不能依赖于合成的默认构造函数 合成的默认构造函数只适合非常简单的类。对于一个普通的类来说，必须定义它自己的默认构造函数。其原因如下： 编译器只有在发现类不包含任何构造函数的情况下才会替我们生成一个默认构造函数。一旦我们定义了一些其他的构造函数，那么除非我们再定义一个默认的构造函数，否则类将没有默认构造函数。 对于某些类来说，合成的默认构造函数可能执行错误的操作。 有时候编译器不能为某些类合成默认的构造函数。例如，如果类中包含一个其他类类型的成员且这个成员的类型没有默认构造函数，那么编译器将无法初始化该成员。对于这样的类，我们必须要自定义默认构造函数，否则该类将没有可用的默认构造函数。 构造函数初始值列表如果编译器不支持内置初始值，那么默认的构造函数就应该使用构造函数初始值列表来初始化类的每个成员。例如，12Sales_data(const std::string &amp;s):bookNo(s)&#123;&#125;Sales_data(const std::string &amp;s, unsigned n, double p):bookNo(s),units_sold(n), revenue(p*n) &#123;&#125; 我们把括号里的部分称为构造函数初始值列表，它负责为新创建的对象的一个或几个数据成员赋值。构造函数初始值是成员名字的一个列表，每个名字后面紧跟括号括起来的成员初始值。不同成员的初始化通过逗号分隔开来。 通常情况下，构造函数使用类内初始值不失为一种好的选择，因为只要这样的初始值存在我们就能确保为成员赋予一个正确的值。不过如果编译器不支持类内初始值，则所有的构造函数都应该显式地初始化每个内置类型的成员。 构造函数不应该轻易覆盖掉类内的初始值，除非新赋的值与原值不同。如果你不能使用类内初始值，则所有构造函数都应该显式地初始化每个内置类型的成员。 7.2 访问控制与封装在C++语言中，我们使用访问说明符加强类的封装性： 定义在public说明符之后的成员在整个程序内可被访问，public成员定义类的接口。 定义在private说明符之后的成员可以被类的成员函数访问，但是不能被使用该类的代码访问，private部分封装了类的实现细节。 使用class和struct定义类的唯一区别就是默认的访问权限。 7.2.1 友元类可以允许其他类或者函数访问它的非公有成员，方法是令其他类或者函数称为它的友元。如果想把一个函数作为它的友元，只需增加一条以friend关键字开始的函数声明语句即可：12345678910class Sales_data&#123;//为Sales_data的非成员函数所做的友元声明 friend Sales_data add(const Sales_data&amp;, const Sales_data&amp;); friend std::istream &amp;read(std::istream&amp;, Sales_data&amp; ); friend std::ostream &amp;print(std::ostream&amp; ,const Sales_data&amp;); public： //... private: //...&#125; 友元的声明只能出现在类定义的内部，但是在类内出现的具体位置不限。友元不是类的成员也不受它所在区域访问控制级别的约束。一般来说，最好在类定义开始或结束前的位置集中声明友元。 封装的益处 封装有两个重要的优点： 确保用户代码不会无意间破坏封装对象的状态。 被封装的类的具体实现细节可以随时改变，而无须调整用户级别的代码。 7.3 类的其他特性可变数据成员 有时（但并不频繁）会发生这样一种情况，我们希望能修改类的某个数据成员，即使是在一个const成员函数内。可以通过在变量的声明中加入mutable关键字做到这一点。一个可变数据成员永远不会是const，即使它是const对象成员。因此，一个const成员函数可以改变一个可变成员的值。例如：12345678910class Screen&#123; public: void some_member() const; private: mutable size_t access_ctr; //即使在一个const对象内也能被修改&#125;void Screen::some_member() const&#123; ++access_ctr; //保存一个计数值，用于记录成员函数被调用的次数&#125; 尽管some_member是一个const成员函数，它仍然能够改变access_ctr的值。该成员是个可变成员，因此任何成员函数，包括const函数在内都能改变它的值。 类类型每个类定义了唯一的类型，对于两个类来说，即使它们的成员完全一样，这两个类也是两个不同的类型。对于一个类来说，它的成员和其他任何类的成员都不是一回事。就像可以把函数的声明和定义分离开来一样，我们也能仅声明类而暂时不定义它：1class Screen; //Screen类的声明 这种声明有时被称作前向声明，它向程序中引入了名字Screen并且指明Screen是一种类类型。对于类型Screen来说，在它声明之后定义之前是一个不完全类型，也就是说，此时我们已知Screen是一个类类型，但是不清楚它到底包含哪些成员。不完全类型只能在非常有限的情景下使用：可以定义指向这种类型的指针或引用，也可以声明以不完全类型作为参数或者返回类型的函数。对于一个类来说，在我们创建它的对象之前该类必须被定义过，而不能仅仅被声明。否则，编译器就无法了解这样的对象需要多少存储空间。类似的，类也必须首先被定义，然后才能用引用或者指针访问其成员。毕竟，如果类尚未定义，编译器也就不清楚该类到底有哪些成员。 7.3.4 友元再探类之间的友元关系 如果一个类指定了友元类，则友元类的成员函数可以访问此类包括非公有成员在内的所有成员。必须要注意的一点是，友元关系不存在传递性。每个类负责控制自己的友元类或友元函数。 令成员函数作为友元 除了令整个windo_mgr作为友元之外，Screen还可以只为clear挺访问权限。当把一个成员函数声明为友元时，我们必须明确指出该成员函数属于哪个类：1234class Screen&#123;//Window_mgr::clear必须在Screen类之前被声明 friend void Window_mrg::clear(ScreenIndex);&#125; 要想令某个成员函数作为友元，我们必须仔细组织程序的结构，以满足声明和定义的彼此依赖关系。在这个例子中，我们必须按照如下方式设计程序： 首先定义Window_mgr类，其中声明clear函数，但是不能定义它。在clear使用Screen的成员函数之前必须先声明Screen。 接下来定义Screen，包括对于clear的友元声明。 最后定义clear，此时它才可以使用Screen的成员。 7.4 类的作用域每个类都会定义它自己的作用域。在类的作用域之外，普通的数据和函数成员只能由对象、引用或指针使用成员访问运算符来访问。对于类类型成员则使用作用域运算符访问。不论哪种情况，跟在运算符之后的名字都必须是对应类的成员。 名字查找与类的作用域在编写的程序中，名字查找（寻找与所用名字最匹配的声明的过程）的过程比较直接了当： 首先，在名字所在的块中寻找其声明语句，只考虑在名字的使用之前出现的声明。 如果没有找到，继续查找外层作用域。 如果最终没有找到匹配的声明，则程序报错。 对于定义在类内部的成员函数来说，解析其中名字的方式与上述的查找规则有所区别，不过在当前的例子中体现的不太明显。类的定义分为两步处理： 首选，编译成员的声明。 直到类全部可见后才编译函数体。 一般来说，内层作用域可以重新定义外层作用域中的名字，即使该名字已经在内层作用域中使用过。然而，在类中，如果成员使用了外层作用域中的某个名字，而该名字代表一种类型，则类不能再之后重新定义改名字。 成员定义中普通块作用域的名字查找成员函数中使用的名字按照如下方式解析： 首先，在成员函数内查找该名字的声明。和前面一样，只有在函数使用之前出现的声明才被考虑。 如果在成员函数内没有找到，则在类内进行查找，这时类的所有成员都可以被考虑。 如果类内也没有找到该名字的声明，在成员函数定义之前的作用域内继续查找。 7.5 构造函数再探 如果成员是const、引用、或者属于某种未提供默认构造函数的类类型，我们必须通过构造函数初始值列表为这些成员提供初值。 成员函数的初始化顺序 成员的初始化顺序与它们在类定义中的出现顺序一致：第一个成员先被初始化，然后第二个，以此类推。构造函数初始值列表中初始值的前后位置关系不会影响实际的初始化顺序。 一般来说，初始化的顺序没有什么特别要求。不过如果一个成员是用另一个成员来初始化的，那么这两个成员的初始化顺序就很关键了。 默认实参和构造函数 如果一个构造函数为所有的参数都提供了默认实参，则它实际上也定义了默认构造函数。 委托构造函数 C++11新标准扩展了构造函数的初始值的功能，使得我们可以定义所谓的委托构造函数。一个委托构造函数使用它所属类的其他构造函数执行它自己的初始化过程，或者说它把它自己的一些职责委托给了其他构造函数。举个例子： 1234567891011class Sales_data&#123; public: //非委托构造函数使用对应的实参初始化成员 Sales_data(std::string s, unsigned cnt, double price): bookNo(s),units_sold(cnt),revenue(cnt* price)&#123;&#125; //其余构造函数全都委托给另一个构造函数 Sales_data():Sales_data("", 0 , 0) &#123;&#125; Sales_data(std::string a):Sales_data(s, 0, 0)&#123;&#125; Sales_data(std::istream &amp;is):Sales_data()&#123; reda(is, *this);&#125;&#125; 在这个Sales_data类中，除了一个构造函数外其他的都委托了它们的工作给另一个函数。 默认构造函数的作用 当对象被默认初始化或值初始化时自动执行默认构造函数。默认初始化在以下情况下发生： 当我们在块作用域内不是要任何初始值定义一个非静态变量或者数组时。 当一个类本身含有类类型的成员且使用合成的默认构造函数时。 当类类型的成员没有在构造函数初始值列表中显示地初始化时。 值初始化在以下情况发生： 在数组初始化的过程中如果我们提供的初始值数量少于数组的大小时。 当我们不使用初始值定义一个局部静态变量时。 当我们通过书写形如T()的表达式显式地请求值初始化时，其中T是类型名。 类必须包含一个默认构造函数以便在上述情况下使用，其中大多数情况非常容易判断。 隐式的类类型转换 抑制构造函数定义的隐式转换，在要求隐式转换的程序上下文中，我们可以通过将构造函数声明为explicit加以阻止。explicit构造函数只能用于直接初始化。发生隐式转换的一种情况是当我们执行拷贝形式的初始化时，此时我们只能使用直接初始化而不能使用explicit构造函数。 聚合类聚合类使得用户可以直接访问其成员，并且具有特殊的初始化语法形式。当一个类满足如下条件时，我们说它是聚合的： 所有的成员都是public的。 没有定义任何构造函数。 没有类内初始值。 没有基类，也没有virtual函数。 7.6 类的静态成员声明静态成员 我们通过在成员的声明之前加上关键字static使得其与类关联在一起。和其他成员一样，静态成员可以是public的或private的。静态数据成员的类型可以是常量、引用、指针、类类型等。举个例子：1234567891011class Account&#123; public: void calculate()&#123; amount += amount * interestrate;&#125; static double rate() &#123; return interestrate;&#125; static void rate(double); private: std::string owner; double amount; static double interestrate; static double initRate();&#125;; 类的静态成员存在于任何对象之外，对象中不包含任何与静态数据成员有关的数据。因此，每个Account对象将包含两个数据成员：owner和amount。只存在一个interestRate对象而且它被所有的Account对象共享。 静态成员不与任何对象绑定在一起，它们不包含this指针。作为结果，静态成员函数不能声明为const的，而且我们也不能再static函数体内使用this指针。 定义静态成员 和其他成员函数一样，我们既可以在类的内部也可以在类的外部定义静态成员函数。当在类的外部定义静态成员时，不能重复static关键字，该关键字只能出现在类内部的声明语句：1234void Account::rate(double newRate)&#123; interestRate = newRate;&#125; 一般来说，我们不能在类的内部初始化静态成员。相反的，必须在类的外部定义和初始化每个静态成员。和其他对象一样，一个静态数据成员只能定义一次。类似全局变量，静态数据成员定义在任何函数之外。因此，一旦它被定义，就将一直存在于程序的整个生命周期中。 要想确保对象只定义一次，最好的办法是把静态数据成员的定义与其他非内联函数的定义放在同一个文件中。 静态成员的类内初始化 通常情况下，类的静态成员不应该在类的内部初始化。然而，我们可以为静态成员提供const整数类型的类内初始值，不过要求静态成员必须是字面值常量类型的constexpr。初始值必须是常量表达式，因为这些成员本身就是常量表达式，所以他们能用在所有适合于常量表达式的地方。例如，我们可以用一个初始化了的静态成员指定数组成员的维度：12345678class Account&#123; public: static double rate() &#123;return interestRate;&#125; static void rate(double); private: static constexpr int period = 30; //period是常量表达式 double daily_tbl[period];&#125; 如果某个静态成员的应用场景仅限于编译器可以替换它的值的情况，则一个初始化的const或constexpr static 不需要分别定义。相反，如果我们将它用于值不能替换的场景中，则该成员必须要有一条定义语句。 即使一个常量静态数据成员在类内部被初始化了，通常情况下也应该在类的外部定义一下该成员。]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vi编辑器使用]]></title>
    <url>%2F2017%2F10%2F10%2FVi%E7%BC%96%E8%BE%91%E5%99%A8%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1、vi编辑器概述1.1 启动vi编辑器启动vi编辑器的方式如下： 命令 说明 vi filename，vim filename 从filename文件的第1行开始编辑，即光标默认停留在文件的第1行第一个字符处。 vi/vim + n filename 从filename文件的第n行开始编辑，即光标默认停留在文件的第n行的第一个字符处 vi/vim + filename 从filename文件的最后1行开始编辑，即光标默认停留在文件的最后一行第一个字符处。 vi/vim +/.pattern filename 从filename文件的第1行包含字符串“pattern”的行开始编辑 vi/vim -r filename 在系统崩溃后恢复filename vi/vim -R filename 以只读的方式编辑filename 当用vim打开并编辑一个文件时，该文件会被自动锁定。如果此时另一用户也希望打开该文件进行编辑将会出现一些提醒： 以只读方式打开文件：按字母键o，用户以只读方式打开文件，不会影响其他用户对该文件的同时操作。 直接编辑：按字母键e，可以直接编辑该文件，但与其他同时操作的文件的用户在存盘时可能会冲突。 恢复：按字母键r。如果出现在该界面是由上一次编辑此文件时崩溃引起的，选择该选项可以恢复修改内容。 退出：按字母键q，直接退出vi编辑器。 1.2退出vi退出vi可按下Esc键，然后输入q即可退出vi，返回Shell提示符。 1.3 命令模式用户可以输入命令，通过命令实现移动光标、删除或修改部分文本以及如复制、粘贴、退出等操作。命令一经输入就立即执行，不需要按回车键。不管在什么模式下，只要按一下Esc键，即可将vi切换到命令模式下： 命令 说明 i 在当前字符前插入文本 I 在当前行的行首插入文本 a 在当前字符之后追加文本 A 在当前行的行尾追加文本 o 在当前行的下面添加一新行 O 在当前行的上面添加一新行 1.4 末行模式在命令模式下按下冒号键即可切换到末行模式。此时光标停留在最后一行上，并等待用户输入所需执行的末行命令。当用户输入命令后，按回车键，命令即可被执行。例如，输入命令set number并按回车键后，使得光标跳到指定的行。 1.2 vi中常用命令1.2.1 插入命令：i和I1.2.2 附加文本命令：a和A1.2.3 字符与块删除命令在命令行模式下，删除命令分两种，一种是字符删除命令，一种是块删除命令删除字符使用x/X命令，x命令可以删除当前字符，X命令可以删除当前光标左侧字符。如果在使用命令前指定了重复的次数，可连续删除多个字符。例如，命令3w表示删除从当前光标位置开始向后的3个字符，命令3W表示删除当前光标之前的3个字符。命令d/D可以把指定的文本块从工作缓冲区中删除。命令D表示删除从当前光标开始到本行未尾一段字符。命令d还需配合其他命令组合来确定删除文本块的大小。具体的使用如下： 命令 说明 x 删除当前字符 X 删除当前光标左侧字符 dl 与命令x相同 d0 从行的开始处删除 d$ 或 D 删除到行的末尾 dw 删除到单词的末尾 d) 删除到句子末尾 dd 删除一行 dL 删除到当前屏幕的最后一行（包括最后一行） 3dd 删除当前行开始的3 dH 从当前屏幕的第1行开始删除 1.2.4 行合并命令：J在命令行模式 下，合并命令可将当前行的末尾与下一行连接起来，并在两行之间插入一个空格，合并后光标定位到该空格处。如果当前行以句号结束，则会在两行之间插入两个空格。 1.2.5 文本替换命名在命令行模式下，替换命令s/S可以删除指定的文本，并使vi进行插入模式。命令s只替换当前字符，即删除当前光标所在位置的字符并切换到插入模式。命令S与命令cc类似，可以删除当前行并切换到插入模式。编辑完文本后，按esc键可以切换到命令模式，完成替换操作。在命令s前加入数字可以指定要替换字符的数目。例如，3s表示删除从当前光标所在字符开始的3个字符，并且切换到插入模式。如果指定的数目超出了当前行的行尾，则删除到行尾结束。 1.2.6 句点命令在命令行模式下，句点命令将重复执行最近一次的修改命令。例如，如果刚刚用命令dd删除了当前一行，那么命令.将会继续删除接下来的一行。 1.2.7 撤销修改命令：u/Uvi包括一个通用缓冲区和26个命名缓冲区，vi会将最近的修改保存在通用缓冲区中，撤销命令实际上是从通用缓冲区取出文本并进行恢复操作。在编辑修改文本之后，在命令行模式下直接使用撤销命令可以恢复到文本修改之前的状态。撤销修改命令包括u和U命令命令u可以撤销上一次的编辑操作。执行一次u命令只能撤销上一次对文本的操作。如果删除一行后又插入了某些字符，则执行一次u命令只能删除插入的字符。如果希望恢复删除的行，则需要再执行一次u命令。命令U可以撤销当前的所有修改，将文本恢复到启动修改之前的状态。 1.2.8 复制命令：y/Y赋值命令实际是把指定的文本内容复制到通用缓冲区。复制命令包括y和Y命令，需要在命令模式下使用。命令yy可以复制一行，默认是当前行。如果需要复制多行，可以在命令yy前指定行数，例如3yy可以复制当前开始的3行文本。命令Y与yy类似。 1.2.9 粘贴命令：p/P命令p把通用缓冲区的内容插入到当前字符之后。命令P把通用缓冲区的内容插入到当前字符之前。 1.2.10 重复执行 ctrl+r如果撤销某个命令后又想重新执行该命令，可以在命令模式下使用组合键Ctrl+R，也可以在末行模式下输入命令：redo并按回车键，vi会重新执行已被撤销的命令。重复命令也可以连续执行多次。 1.3 vi中字符与文件操作vi可以在整个缓冲区查找与正则表达式匹配的字符串。在命令模式下，键入斜杠(/)后跟待查找的字符串，按回车键，vi即可开始搜索。状态行将同步显示插入的斜杠和搜索字符串。如果搜索成功，光标会停留在首次匹配该字符串的第1个字符处。按n键可以向后重复上一次搜索，按N键可以向前重复上次的搜索。 1.3.1 查找指定字符命令在命令行模式下，命令f可以在当前行当前光标出开始查找指定的字符，并将光标移动到该字符出现的位置。如果没有找到，则不移动光标。F命令与f类似，只是在本行开始的位置到光标所在位置之间查找。 命令t在查找字符时，会将光标定位在指定字符出现位置前一个字符位置，而命令T在查找字符时会将光标定位在指定字符出现的后一字符位置。 1.3.2 替换指定字符串在末行模式下的替换命令s具有查找和修改功能。替换命令首先查找某个字符串，然后修改该字符串。末行模式下的替换命令s语法格式如下：1:[g] [address] s/ search/replacement [/option] 其中，g表示在所有的行进行查找和替换，否则只对第一次查找的结果进行替换。address表示查找的范围，如果不指定address，默认只搜索当前行。search表示搜索的字符串，replace表示替换字符串，/为分隔符。 分隔符可以使用除字母、数字、空白符和反斜杠之外的任意其他字符，但应保持search前后的分隔符相同。 address示例 说明 3 第3行 22,100 从22行到100行之间，包括22行和100行 5, . 从第5行开始至当前行 5,$ 从第5行值末尾 % 整个缓冲区 g/pattern 包含字符串pattern的所有行 /pattern/ 搜索到首次包含字符串pattern 的行 .,. +20 从当前行开始向后的连续20行 .,. -10 从当前行开始向前的连续10行 命令 说明 :s/string1/string2 将当前行中第1次搜索到的字符串string1替换为string2 :1,.s/string1/string2/g 将当前行之前的所有行中的字符串string1替换为string2，其中g表示在每一行中都允许进行多次替换 :1,$s/string1/string2/g 将所有的行中出现字符串string1替换为string2 :%s/string1/string2/g 将所有的行中出现字符串string1替换为string2 :.,.+10s/string1/string2/g 将从当前行开始连续10行内出现的字符串string1替换为string2 :/string/s/string1/string2/g 将首次串string的行中字符串string1替换为string2 :g /string/s/string1/string2 在所有行中将包含string字符串的行中的第一次出现string1的字符串替换为string2 :g /string/s/string1/string2/g 在所有行中将包含string字符串的行中的string1的字符串替换为string2 5,.s/string1/string2 将第5行到当前行之间所有行中出现的string1替换为string2 1.3.3 更改大小写的命令在命令模式下，代字符（~）可以用来更改字母的大小写，可以将小写字母改为大写，也可以将大写字母改为小写。 1.3.4 定位到指定行命令在命令模式下，输入命令“行号+G”可以将当前光标定位到缓冲区中指定的行， 如果没有指定行号，默认将光标定位到缓冲区的最后一行。命令G不需要缓冲区显式地给出行号，即即使不显示行号，也可以定位到指定的行。例如，在命令行模式下，输入3G可以将光标定位到缓冲区第3行。 1.3.5 显示状态信息命令显示文件状态信息通常有 两种方式：在命令模式下，按ctrl+G组合键和在末行模式下使用命令f 1.3.6 保存和退出命令 命令 说明 :w 保存缓冲区，但不退出。保存后磁盘中的文件被修改，不能恢复 :w filename 将缓冲区的内容保存到指定的filename中 ZZ 保存并退出，在命令下使用 :q 退出，但如果缓冲区已经被修改将提示错误信息，不能退出 :wq 保存并退出 q! 直接退出，系统不进行警告 1.4 窗口操作1.4.1 窗口操作的快捷方式 组合键ctrl+W+S：用于打开编辑同一个文件的另一个窗口，即用于实现窗口的水平切分。 组合键ctrl+W+N：可以用于打开一个新的窗口编辑一个新的文件。 组合键Ctrl+W+W：可以实现多个窗口之间的切换。 1.4.2 窗口垂直拆分：vsplit1.4.3 窗口水平拆分：split1.4.4 屏幕滚动当一个文件的行数超出了屏幕所能显示的最大行数时，需要上下滚动文本才能观察到全部内容。滚动的方向是相对文本而言的，不是相对屏幕。向下滚动意味着向文件的结尾处移动。具体滚动命令如下： 命令 说明 ctrl+Y 向上滚动一行 ctrl+E 向下滚动一行 ctrl+U 向上滚动半屏 ctrl+D 向下滚动半屏 ctrl+F 向下滚动整屏 ctrl+B 向上滚动整屏]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第13章 拷贝控制]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%AC%AC13%E7%AB%A0-%E6%8B%B7%E8%B4%9D%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本章主要学习类如何控制该类型对象拷贝、赋值、移动或销毁时做什么。类通过一些特殊的成员函数控制这些操作，包括：拷贝构造函数、移动构造函数、拷贝赋值运算符、移动赋值运算符以及析构函数。 当定义一个类时，我们显示地或隐式地指定在此类型的对象拷贝、移动、赋值和销毁时做什么。一个类通过定义五中特殊的成员函数来控制这些操作。包括：拷贝构造函数、拷贝赋值运算符、移动构造函数、移动赋值运算符和析构函数。拷贝和移动构造函数定义了当用同类型的同一个对象初始化对象时做什么。拷贝和移动赋值运算符定义了将一个对象赋予同类型的另一个对象时做什么。析构函数定义了当此类型对象销毁时做什么。我们称这些操作为拷贝控制操作。 13.1 拷贝、赋值与销毁13.1.1 拷贝构造函数如果一个构造函数的第一个参数是自身类类型的引用，且任何额外参数都有默认值，则此构造函数是拷贝构造函数。123456class Foo&#123; public: Foo(); //默认构造函数 Foo(const Foo&amp;); //拷贝构造函数&#125; 拷贝构造函数的第一个参数必须是一个引用类型。拷贝构造函数通常不应该是explicit的。 合成拷贝构造函数 如果我们没有为一个类定义拷贝构造函数，编译器会为我们定义一个。与合成默认构造函数不同，即使我们定义了其他构造函数，编译器也会为我们合成一个拷贝构造函数。 对某些类来说，合成拷贝构造函数用来阻止我们拷贝该类类型的对象。而一般情况，合成的拷贝构造函数将其参数的成员逐个拷贝到正在创建的对象中。编译器从给定对象中依次将每个非static成员拷贝到正在创建的对象中。 每个成员的类型决定了它如何拷贝：对类类型的成员，会使用其拷贝构造函数来拷贝；内置类型的成员则直接拷贝。 拷贝初始化12345string dots(10,'.'); //直接初始化string s(dots); //直接初始化string s2 = dots; //拷贝初始化string null_book = "9-9999-9999";//拷贝初始化 当使用直接初始化时，我们实际上是要求编译器使用普通的函数匹配来与我们提供的参数最匹配的构造函数。当我们使用拷贝初始化时，我们要求编译器将右侧运算对象拷贝到正在创建的对象中，如果需要的话还要进行类型转换。 拷贝初始化发生的情况 使用=号定义变量时。 将一个对象作为实参传递给一个非引用类型的实参。 用花括号列表初始化一个数组中的元素或一个聚合类的成员。 参数和返回值 在函数调用过程中，具有非引用类型的参数要进行拷贝初始化。类似的，当一个函数具有非引用的返回类型时，返回值会被用来初始化调用方的结果。 拷贝构造函数被用来初始化非引用类类型参数，这一特性解释了为什么拷贝构造函数自己的参数必须是引用类型。如果其参数不是引用类型，则调用永远也不会成功-为了调用拷贝构造函数，我们必须拷贝它的实参，但为了拷贝实参，我们又需要调用拷贝构造函数，如此无限循环。 13.1.2 拷贝赋值运算符与拷贝构造函数一样，如果类未定义自己的拷贝赋值运算符，编译器会为它合成一个。 13.1.3 析构函数在一个构造函数中，成员的初始化是在函数体执行之前完成的，且按照他们在类中出现的顺序进行初始化。在一个析构函数中，首先执行函数体，然后按照成员初始化顺序的逆序销毁成员。 什么时候会调用析构函数无论何时一个对象被销毁，就会自动调用其析构函数： 变量在离开其作用域时被销毁。 当一个对象被销毁时，其成员被销毁。 容器（无论是标准库容器还是数组）被销毁时，其元素被销毁。 对于动态分配的对象，当对指向它的指针应用delete运算符时被销毁。 对于临时对象，当创建它的完整表达式结束时被销毁。 由于析构函数自动运行，我们的程序可以按需分配需要的资源，而无须担心何时释放这些资源。 13.1.5 使用=default我们可以通过将拷贝控制成员定义为=default来显式地要求编译器生成合成的版本。123456789class Sales_data&#123; public: //拷贝控制成员：使用default Sales_data() = default; Sales_data(const Sales_data &amp;) = default; Sales_data&amp; operator =(const Sales_data &amp;); ~Sales_data() = default; //其他成员的定义，&#125; 当我们在类内用=default修饰成员的声明时，合成的函数将隐式地声明为内联的。如果我们不希望合成的成员是内联函数，应该只对成员的类外定义使用=default，就像对拷贝赋值运算符所做的那样。 13.1.6 阻止拷贝 大多数类应该定义默认构造函数、拷贝构造函数和拷贝赋值运算符，无论是隐式还是显式地。 虽然，大多数的类应该定义拷贝构造函数和拷贝赋值运算符，但对某些类来说，这些操作没有合理的意义。在此情况下，定义类时必须采用某种机制阻止拷贝或赋值。例如，iostream类阻止了拷贝，以避免多个对象写入或读取相同的IO缓冲。 定义删除的函数 在新的标准下，我们可以通过将拷贝构造函数和拷贝赋值运算符定义为删除的函数来阻止拷贝。删除的函数是这样一种函数：我们虽然声明了它们，但不能以任何方式使用它们。在函数的列表后面加上=delete来指出我们希望它地定义为删除的。12345struct NoCopy&#123; NoCopy() = default; //使用合成的默认构造函数 NoCopy(const NoCopy&amp;)=delete; //阻止拷贝 NoCopy &amp;operator=(const NoCopy&amp;) = delete;//阻止赋值&#125; =delete通知编译器，我们不需要定义这些成员。 与default不同，=delete必须出现在函数第一次声明的时候，这个差异与这些声明的含义在逻辑上是吻合的。一个默认的成员只影响为这个成员而生成的代码，因此=default直到编译器生成代码时才需要。另一方面，编译器需要直到一个函数是删除的，以便禁止试图使用它的操作。 与=defalut的另外一个不同之处是，我们可以对任何函数指定=delete（我们只能对编译器可以合成的默认构造函数或拷贝控制成员使用=default）。虽然删除函数的主要用途是禁止拷贝控制成员，但当我们希望引导函数匹配过程时，删除函数有时也是有用的。 析构函数不能是删除的成员 合成的拷贝控制成员可能是删除的如前所述，如果我们未定义拷贝控制成员，编译器会为我们定义合成的版本。类似地，如果一个类未定义构造函数，编译器会为其合成一个默认的构造函数。对某些类来说，编译器将这些合成的成员定义为删除的函数： 如果类的某个成员的析构函数是删除的或不可访问的（例如，是private的），则类的合成析构函数被定义为删除的。 如果类的某个成员的拷贝构造函数是删除的或不可访问的，则类的合成拷贝构造函数被定义为删除的。如果类的某个成员的析构函数是删除的或不可访问的，则类合成的拷贝构造函数也被定义为删除的。 如果类的某个成员的拷贝赋值运算符时删除的或不可访问的，或是类有一个const的或引用成员，则类的合成拷贝赋值运算符被定义为删除的。 如果类的某个成员的析构函数是删除的或不可访问的，或是有一个引用成员，它没有类内初始化器、或是类有一个const成员，它没有类初始化器且其类型未显式定义默认构造函数，则该类的默认构造函数被定义为删除的。 本质上，这些规则的含义是：如果一个类有数据成员不能默认构造、拷贝、复制或销毁，则对应的成员函数将被定义为删除的。 13.2 拷贝控制和资源管理通常管理类外资源的类必须定义拷贝控制成员。一旦一个类需要析构函数，那么它几乎肯定也需要一个拷贝构造函数和一个拷贝赋值运算符。为了定义这些成员，我们首先必须确定此类型对象的拷贝语义。一般来说，有两种选择：可以定义拷贝的操作，使类的行为看起来像一个值或像一个指针。 类的行为像一个值，意味着它应该也有自己的状态。当我们拷贝一个像值的对象的时候，副本和原对象是完全独立的。改变副本不会对原对象有任何影响，反之亦然。 行为像指针的类则共享状态。当我们拷贝一个这类的对象时，副本和原对象使用相同的底层数据。改变副本也会改变原对象，反之亦然。 13.3 交换操作除了定义拷贝控制成员，管理资源的类通常还定义一个名为swap的函数。对于那些与重排元素顺序的算法一起使用的类，定义swap是非常重要的。如果一个类定义了自己的swap，那么算法将使用类自定义版本。否则，算法将使用标准库定义的swap函数。 13.5 动态内存管理类某些类需要在运行时分配可变大小的内存空间。这种类通常可以使用标准库容器来保存它们的数据。例如，我们的StrBlob类使用一个vector来管理其元素的底层内存。但这一策略并不是对每个类都适用：某些类需要自己进行内存分配。这些类一般来说必须定义自己的拷贝控制成员来管理所分配的内存。 13.6 对象移动新标准的一个最主要的特性是可以移动而非拷贝对象的能力。在某些情况下，对象拷贝后就立即被销毁了。在这些情况下， 移动而非拷贝对象会大幅度提升性能。在旧C++版本中，没有直接的方法移动对象。因此，即使不必拷贝对象的情况下，我们也不得不拷贝。如果对象较大，或者是对象本身要求分配内存空间，进行不必要的拷贝代价是非常大的。但在新的标准中，我们可以用容器保存不可拷贝的类型，只要他们能被移动即可。 标准库容器、string和shared_ptr类既支持移动页支持拷贝。IO类和unique_ptr类可以移动但不能拷贝。 13.6.1 右值引用为了支持移动操作，新的标准引入了一种新的引用类型-右值引用。所谓的右值引用就是必须绑定到右值的引用。我们通过&amp;&amp;而不是&amp;来获得右值引用。如我们将要看到的，右值引用有一个重要的性质-只能绑定到一个将要销毁的对象。因此，我们可以自由地将一个右值引用的资源“移动”到另一个对象中。一般而言，一个左值表达式表示的是一个对象的身份，而一个右值表达式表示的是对象的值。类似任何引用，一个右值引用也不过是某个对象的另一个名字而已。右值引用有着完全相反的绑定特性：我们可以将一个右值引用绑定到这类表达式上，但不能将一个右值引用直接绑定到一个左值上。123456int i=42;int &amp;r = i; //正确：r引用iint &amp;&amp;rr = i; //错误：不能将一个右值引用绑定到一个左值上int &amp;r2 = i*42; //错误：i*42是一个右值const int &amp;r3 = i*42; //正确：我们可以将一个const的引用绑定到一个左值上int &amp;&amp;rr2 = i*42; //正确：将rr2绑定到乘法结果上 左值持久：右值短暂左值由持久的状态，而右值要么是字面常量，要么是在表达式求值过程中创建的临时对象。由于右值引用只能绑定到临时对象，我们得知： 所引用的对象将要被销毁。 该对象没有其他用户。 这两个特性意味着：使用右值引用的代码可以自由地接管所引用的对象的资源。 右值引用指向将要被销毁的对象。因此，我们可以从绑定到右值引用的对象“窃取”状态。 变量是左值变量可以看作只有一个预算对象而没有运算符的表达式，虽然我们很少这样看待变量。类似其他任何表达式，变量表达式也有左值、右值属性。变量表达式都是左值。带来的结果就是，我们不能将一个右值引用绑定到一个右值引用类型的变量上。 标准库move函数虽然不能将一个右值引用直接绑定到一个左值上，但我们可以显式地将一个左值转换为对应的右值引用类型。我们还可以通过调用一个名为move的新标准库函数获得绑定到左值上的右值引用。此函数定义在头文件utility中。 13.6.2 移动构造函数和移动赋值运算符类似string类，如果我们自己的类也同时支持移动和拷贝，那么也能从中受益。为了让我们自己的类型支持移动操作，需要为其定义移动构造函数和移动赋值运算符。这两个成员类似对应的拷贝操作，但他们从给定对象“窃取”资源而不是拷贝资源。移动构造函数的第一个参数是该类类型的一个引用。不同于拷贝构造函数的是，这个引用参数在移动构造函数中是一个右值引用。与拷贝构造函数一样，任何额外的参数都必须有默认实参。除了完成资源移动，移动构造函数还必须确保移动后资源对象处于这样一个状态-销毁它是无害的。特别是，一旦资源完成移动，资源对象必须不再指向被移动的资源-这些资源的所有权已经归属新创建的对象。123456StrVce::StrVec(StrVec &amp;&amp;s)noexcept : elements(s.elements),first_free(s.first_free),cap(s.cap)&#123; s.elements = s.first_free = s.cap = nullptr; &#125;]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第11章 关联容器]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%AC%AC11%E7%AB%A0-%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[关联容器支持高效的关键字查找和访问。两个主要的关联容器类型是map和set。map中的元素时一些关键字-值（key-value）对：关键字起到索引的作用，值则表示索引相关连的数据。set中每个元素只包含一个关键字；set支持高效的关键字查询操作-检查一个给定的关键字是否在set中。标准容器提供8个关联容器，如下图所示：类型map和multimap定义在头文件map中；set和multiset定义在头文件set中；无序容器则定义在头文件unordered_map和unordered_set中。 11.1 使用关联容器类似顺序容器，关联容器也是模板。为了定义一个map，我们必须指定关键字和值的类型。 11.2 关联容器概述当定义一个map时，必须既指明关键字类型又指明值类型；而定义一个set时，只需指明关键字的类型，因为set中没有值。每个关联容器都定义了一个默认构造函数，它创建一个指定类型的空容器。我们也可以将关联容器初始化为另一个同类型容器的拷贝，或是从一个值范围来初始化关联容器，只要这些值可以转化为容器所需的类型就可以。例如，123map&lt;string,size_t&gt; word_count;set&lt;string&gt; exclude = &#123;"the","but","and","or"&#125;;map&lt;string,string&gt; authors = &#123;&#123;"Joyce","James"&#125;,&#123;"Austen","Jane"&#125;,&#123;"Dickens","Charles"&#125;&#125;; multimap或multiset 一个map或set中的关键字必须是唯一的，即，对一个给定的关键字，只能有一个元素的关键字等于它。容器multimap和multiset没有此限制，它们都允许多个元素有相同的关键字。 11.2.2 关键字类型的要求关联容器对其关键字类型有一些限制。对有序容器-map、multimap、set以及multiset，关键字类型必须定义元素的比较的方法。默认情况下，标准库使用关键字类型的&lt;运算符来比较两个关键字。在集合类型中，关键字就是元素类型；在映射类型中，关键字类型是元素的第一部分的类型。 11.2.3 pair类型pair标准库类型，它定义在头文件utility中，一个pair保存两个数据成员。类似容器，pair是一个用来生成特定类型的模板。当创建一个pair时，我们必须提供两个类型名，pair的数据成员将其具有对应的类型。两个类型不要求一样：123pair&lt;string,string&gt; anon;pair&lt;string, size_t&gt; word_count;pair&lt;string, vector&lt;int&gt;&gt; line; pair的默认构造函数对数据成员进行值初始化。与其他标准库类型不同，pair的数据成员是public的。两个成员分别命名为first和second。我们用普通的成员访问符号可以来访问它们。pair上的操作，如下图所示： 11.3 关联容器操作关联容器还定义了如下所示的类型，这些类型表示容器关键字和值的类型。 对于set类型，key_type和value_type是一样的；set中保存的值就是关键字。 11.3.1 关联容器迭代器当解引用一个关联容器迭代器时，我们会得到一个类型为容器的value_type的值的引用。对map而言，value_type是一个pair类型，其first成员保存const的关键字，second保存值：123456auto map_it = word_count.begin();// *map_it指向一个pait&lt;const string, size_t&gt;对象的引用count &lt;&lt;map_it-&gt;first; //打印此元素的关键字cout &lt;&lt; map_it-&gt;second; //打印此元素的值map_it-&gt;first = "new key"; //错误：关键字是const的++map_it-&gt;second; //正确 set的迭代器是const的1234567set&lt;int&gt; iset = &#123;0,1,2,3,4,5,6,7,8,9&#125;;set&lt;int&gt;::iterator set_it = iset.begin();if(set_it!=iset.end())&#123;*set_it = 42; //错误：set中关键字是只读的cout &lt;&lt; *set_it &lt;&lt; endl;&#125; 11.3.2 添加元素关联容器的insert成员向容器中添加一个元素或一个元素范围。由于map和set（以及对应的无序类型）包含不重复的关键字，因此插入一个已存在的元素对容器没有任何影响。关联容器的insert操作如下图所示： 检测insert的返回值 insert（或emplace）返回的值依赖于容器类型和参数。对于不包含重复关键字的容器，添加单一元素的insert和emplace版本返回一个pair，告诉我们插入操作是否成功。pair的first成员是一个迭代器，指向具有给定关键字的元素；second成员是一个bool值，指出元素是插入成功还是已经存在于容器中。如果关键字已经在容器中，则insert什么事业不做，且返回值中bool部分为false。吐过关键字不存在，元素被插入到容器中，且bool值为true。 11.3.3 删除元素关联容器定义了三个版本的erase，如下图所示。与顺序容器一样，我们可以通过传递给erase一个迭代器或一个迭代器对来删除一个元素或者一个元素范围。者两个版本的erase与对应的顺序容器的操作非常相似；指定的元素被删除，函数返回void。 11.3.4 map的下标操作map和unordered_map容器提供了下标运算符和一个对应at函数。set类型不支持下标运算符。map下标预算符接受一个索引，获取与此关键字 相关联的值。但是，与其他下标运算符不同的是，如果关键字不在map中，会为它创建一个元素并插入到map中，关联值将进行值初始化。123map&lt;string, size_t&gt; word_count; //empty map//插入一个关键字为Anna的元素，关联值进行值初始化；然后将1赋予它word_count["Anna"] = 1; 11.3.5 访问元素关联容器提供多种查找一个指定元素的方法。如果我们关心的是一个特定的元素是否在容器中，可能find是最佳选择。对于不允许重复关键字的容器，可能使用find还是count没有区别。但是对于允许重复关键字的容器，count还会做更多工作。如果元素在容器中，它还会统计有多少个元素有相同的关键字。如果不需要计数，最好使用find。 lower_bound和upper_bound这两个操作都接受一个关键字，返回一个迭代器。如果关键字在容器中，lower_bound返回的迭代器将指向第一个具有给定关键字的元素，而upper_bound返回的迭代器则指向最后一个匹配的给定关键字的元素之后的位置。如果元素不在mulitmap中，则lower_bound和upper_bound会返回相等的迭代器-指向一个不影响排序的关键字插入位置。因此，相同的关键字调用lower_bound和upper_bound会得到一个迭代器范围，表示具有该关键字的元素的范围。如果没有元素与给定的关键字匹配，则lower_bound和upper_bound会返回相等的迭代器，都指向给定关键字的插入点，能保持容器中元素顺序的插入位置。 equal_range函数，接受一个关键字，返回一个迭代器pair，如果关键字存在，则第一个迭代器指向第一个与关键字匹配的元素，第二个迭代器指向最后一个匹配元素之后的位置。如果未找到匹配元素，则两个迭代器都指向关键字可以插入的位置。 11.4 无序容器新标准定义了4个无序关联容器，这些容器不是使用比较运算符来组织元素的，而是使用一个哈希函数和关键字类型的==运算符。在关键字类型的元素没有明显的序关系的情况下，无序容器是非常有用的。在某些应用中，维护元素的序代价非常高，此时无序容器也很有用。 除了哈希管理操作之外，无序容器还提供了与有序容器相同的操作（find、insert）。这意味着我们曾用于map和set的操作也能用于unordered_map和unordered_set。类似的，无序容器也有允许重复关键字的版本。 无序容器管理的操作如下图所示： 无序容器对关键字类型的要求默认情况下，无序容器使用关键字类型的==运算符来比较元素，它们还使用一个hash类型的对象来生成每个元素的哈希值。标准库为内置类型（包括指针）提供了hash模板。我们可以直接定义关键字是内置类型（包括指针类型）、string还是只能指针类型的无序容器。 但是，我们不能直接定义关键字类型为自定义类类型的无序容器。与容器不同，不能直接使用哈希模板，而必须提供我们自己的hash模板版本。]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第9章 顺序容器]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%AC%AC9%E7%AB%A0-%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[一个容器是一些特定类型的对象的集合。顺序容器为程序员提供了控制元素存储和访问顺序的能力。这种顺序不依赖与元素的值，而是与容器加入元素时的位置相对应。标准库还提供了三种容器适配器，分别为容器操作定义了不同的接口，来与容器类型适配。 9.1 顺序容器概述下表列出了标准库中的顺序容器，所有顺序容器都提供了快速顺序访问元素的能力。但是，这些容器在以下方面都有不同的性能折中： 向容器添加或从容器中删除元素的代价。 非顺序访问容器中元素的代价。 确定使用哪种顺序容器以下是一些选择容器的基本原则： 除非有很好的理由选择其他容器，否则应使用vector。 如果你的程序有很多小的元素，且空间的额外开销很重要，则不要使用list或forward_list 程序要求随机访问元素，应使用vector或deque。 如果程序要求在容器中间插入或删除元素，应使用list或forward_list。 如果程序需要在头尾位置插入或删除元素，但不会在中间位置进行插入或删除操作，则使用deque。 如果程序只有在读取输入时才需要在容器中间位置插入元素，随后需要随机访问元素，则 ​首先，确定是否真的需要在容器中间位置添加元素。当处理输入数据时，通常可以很容易地向vector追加数据，然后再调用标准库的sort函数来重排容器中的元素，从而避免在中间位置添加元素。 如果必须在中间位置插入元素，考虑在输入阶段使用list，一旦输入完成，将list中的内容拷贝到vector中。 9.2 容器库概览本节中，介绍的所有容器都适用的操作。一般来说，每个容器都定义在一个头文件中，文件名与类型名相同。即，deque定义在头文件deque中，list定义在头文件list中，以此类推。容器均定义为模板类。必须提供额外信息来生成特定的容器类型。 9.2.1 迭代器迭代器范围 一个迭代器范围由一对迭代器表示，两个迭代器分别指向同一个容器中的元素或者尾元素之后的位置。这两个迭代器通常被称为beign和end，或者是first和last，它们标记了容器中元素的一个范围。beign和end是一个左闭合区间，其标准数学描述为：[beign, end)表示范围自begin开始，于end之前结束。迭代器begin和end必须指向相同的容器。end可以与begin指向相同的位置，但不能指向begin之前的位置。假定begin和end构成一个合法的迭代器范围，则： 如果begin和end相等，则范围为空。 如果begin与end不相等，则范围至少包含一个元素，且begin指向该范围中的第一个元素。 我们可以对begin递增若干次，使得begin==end。 9.2.2 容器类型成员类型别名，通过类型别名，我们可以在不了解容器中元素类型的情况下使用它。如果需要元素类型，可以使用容器的value_type。如果需要元素类型的一个引用，可以使用reference或const_reference。这些元素相关的类型别名在泛型编程中非常有用。 9.2.4 容器定义和初始化每个容器类型都定义了一个默认构造函数，除array之外，其他容器的默认构造函数都会创建一个指定类型的空容器，且都可以接受指定容器大小和元素初始值的参数。 将一个容器初始化为另一个容器的拷贝 将一个新容器创建为另一个容器的拷贝方法有两种： 直接拷贝整个容器。 （array除外）拷贝有一个迭代器对指定的元素范围。12345678list&lt;string&gt; authors = &#123;"Milton","Shakespeare","Austen"&#125;;vector&lt;const char*&gt; articles = &#123;"a","an","the"&#125;;list&lt;string&gt; list2(authors); //正确：类型匹配deque&lt;string&gt; authList(authors); //错误：容器类型不匹配vector&lt;string&gt; words(articles); //错误：容器类型不匹配//正确：可以将const char * 元素转换为stringforward_list&lt;string&gt; words(articles.begin(), articles.end()); 当将一个容器初始化为另一个容器的拷贝时，两个容器的容器类型和元素类型都必须相同。 与顺序容器大小相关的构造函数 处理与关联容器相同的构造函数外，顺序容器（array）还提供了另一个构造函数，它接受一个容器大小和一个可选元素初始值。如果我们不提供元素初始值，则标准库会创建一个值初始化器：1234vector&lt;int&gt; ivec(10,-1); //10个int元素，每个都初始化为-1；list&lt;string&gt; svec(10, "hi!");forward_list&lt;int&gt; ivec(10); //10个元素，每个都初始化为0deque&lt;string&gt; svec(10); //10个元素，每个都是空string 只有顺序容器的构造函数才接受大小参数，关联容器并不支持 9.2.5 赋值和swap下图列出了与赋值相关的运算符可以用于所有容器。赋值运算符将其左边容器中的全部元素替换为右边容器中元素的拷贝：12c1 = c2; //将c1的内容替换为c2中元素的拷贝c1 = &#123;a,b,c&#125;; //赋值后，c1大小为3 使用assign（仅顺序容器） 顺序容器（array除外）还定义了一个名为assign的成员，允许我们从一个不同但相容的类型赋值，或者从容器的一个子序列赋值。assign操作用参数所指定的元素（的拷贝）替换左边容器中的所有元素。例如，可以使用assign实现将一个vector中的一段char*值赋予一个list中的string：12345list&lt;string&gt; names;vector&lt;const char*&gt; oldstyle;names = oldstyle; //错误：容器类型不匹配//正确：可以将const char* 转换为stringnames.assign(oldstyle.cbegin(), oldstyle.cend()); assign的第二个版本接受一个整型值和一个元素值。它用指定数目且具有相同给定值的元素替换容器中的原有的元素：12list&lt;string&gt; slist1(1); //1个元素，为空stringlist.assign(10,"Hiya!"); //10个元素，每个都是"Hiya!" 使用swap swap操作交换两个相同类型的容器的内容，调用swap之后，两个容器中的元素将会交换：123vector&lt;string&gt; svec1(10); //10个元素的vectorvector&lt;string&gt; svec2(24); //24个元素的vectorswap(svec1,svec2); 调用swap后，svec1将包含24个元素，svec2将包含10个元素。除了array外，交换两个容器内容的操作保证会很快–元素本身并未交换，swap只是交换了两个容器内部的数据结构。与其他容器不同，swap两个array会真正交换它们的元素。因此，交换两个array所需的时间与array中元素的数目成正比。 9.3 顺序容器的操作9.3.1 向顺序容器添加元素除了array外，所有的标准容器都提供灵活的内存管理。在运行时可以动态添加或删除元素来改变容器的大小。下图列出了向顺序容器添加元素的操作。 使用push_back push_back将一个元素追加到一个vector的尾部。除了array和forward_list之外，每个顺序容器（包括string类型）都支持push_back。 关键概念：容器元素时拷贝 当我们用一个对象来初始化容器时，或将一个对象插入到容器中时，实际上放入到容器中的是对象值的一个拷贝，而不是对象本身。就像我们将一个对象传递给非引用参数一样，容器中的元素与提供值的对象之间没有任何关联。随后，对容器中元素的任何改变都不会影响到原始对象，反之依然。 使用push_front除了push_back、list、forward_list和deque容器还支持名为push_front的类似操作。此操作将元素插入到容器头部：123list&lt;int&gt; ilist;for(size_t ix = 0; ix!=4; ++ix) ilist.push_front(ix); 在容器中特定位置添加元素 insert成员提供了更一般的添加功能，它允许我们在容器中任意位置插入0个或多个元素。vector、deque、list和string都支持insert成员。forward_list提供了特殊版本的insert成员。每个insert函数都接受一个迭代器作为其第一个参数。迭代器指出了在容器中什么位置放置新元素。它可以指向容器中任何位置，包括容器尾部之后的下一个位置。由于迭代器可能指向容器尾部之后不存在的元素位置，而且在容器开始位置插入元素是很有用的功能，所以insert函数将元素插入到迭代器所指定的位置之前。 使用emplace操作新标准引用三个新成员-emplace_front、emplace和emplace_back，这些操作构造而不是拷贝元素。这些操作分别对应push_front、insert和push_back，允许我们将元素放置在容器头部、一个指定位置之前或容器尾部。 9.3.2 访问元素 访问成员函数返回的是引用 在容器中访问元素的成员函数（即，front、back、下标和at）返回的都是引用。如果容器是一个const对象，则返回值是const的引用。如果容器不是const的，则返回值是普通引用，我们可以用来改变元素的值。12345678if(!c.empty())&#123; c.front() = 42; //将42赋予c中的第一个元素 auto &amp;v = c.back(); //获得指向最后一个元素的引用 v = 1024; //改变c中的元素 auto v2 = c.back(); //v2不是一个引用，他是c.back()的一个拷贝 v2 = 0; //未改变c中的元素。&#125; 下标操作和安全的随机访问 提供快速随机访问的容器（string、vector、deque和array）也都提供下标运算符。下标运算符接受一个下标参数，返回容器中该位置元素的引用。给定下标必须在“在范围内”。如果我们希望确保下标是合法的，可以使用at成员函数。at成员函数类似下标运算符，但如果下标越界，at会抛出一个out_of_range异常。 9.3.3 删除元素与添加元素的多种方式类似，（非array）容器也有多种删除元素的方式。如下图所示： 删除元素的成员函数并不检查其参数。在删除元素之前，程序员必须要确保它们是存在的。 pop_front和pop_back成员函数 pop_fron和pop_back成员函数分别删除首元素和尾元素。与vector和string不支持push_front一样，这些类型也不支持pop_front。类似地，forward_list不支持pop_back。与元素访问成员函数类似，不能对一个空容器执行弹出操作。 从容器内部删除一个元素 成员函数erase从容器中指定位置删除元素。我们可以删除由一个迭代器指定的单个元素，也可以删除由一对迭代器指定范围内的所有元素。两种形式的erase都返回指向删除的元素之后的位置的迭代器。即，若j是i之后的元素，那么erase（i）将返回指向j的迭代器。1234567list&lt;int&gt; lst = &#123;0,1,2,3,4,5,6,7,8,9&#125;;auto it = lst.begin();while (it!=lst.end()) if(*it%2) //若元素为奇数 it = lst.erase(it); //删除此元素 else ++it; 删除多个元素接受一对迭代器的erase版本允许我们删除一个范围内的元素：1elem1 = slist.erase(elem1, elem2); //调用后，elem1 == elem2 9.3.5 改变容器的大小如下图所示，我们可以用resize来增大或缩小容器，与往常一样，array不支持resize。如果当前大小大于所要求的大小，容器后部的元素都会被删除；如果当前大小小于新大小，会将新元素添加到容器后部。 9.3.6 容器操作可能使迭代器失效向容器中添加元素和从容器中删除元素的操作可能会使指向容器元素的指针、引用或者迭代器失效。在向容器添加元素后： 如果容器是vector或string，且存储空间被重新分配，则指向容器的迭代器、指针和引用都会失效。如果存储空间未重现分配，指向插入位置之前的元素迭代器、指针和引用仍会有效，但指向插入位置之后元素的迭代器，指针和引用将会失效。 对于deque，插入到除首尾位置之外的任何位置都会导致迭代器、指针、引用失效。如果在首尾位置添加元素，迭代器会失效，但指向存在的元素的引用和指针不会失效。 对于list和forward_list，指向容器的迭代器、指针和引用仍有效。 当我们删除一个元素后： 对于list和forward_list，指向容器其他位置的迭代器、引用和指针仍有效。 对于deque，如果在首尾之外的任何位置删除元素，那么指向被删除元素之外其他元素的迭代器、引用或指针也会失效。如果是删除deque的尾元素，则尾后迭代器也会失效，但其他迭代器、引用和指针不会受影响；如果是删除首元素，这些页不会受影响。 对于vector和string，指向被删元素之前的迭代器、引用和指针仍有效注意：当我们删除元素时，尾后迭代器总是会失效。 9.4 vector 对象是如何增长的标准库实现者采用了可以减少容器空间重新分配次数的策略。当不得不获取新的内存空间时，vector和string的实现通常会分配比新的空间需求更大的内存空间。容器预留这些空间作为备用，可用来保存更多的元素。这样，就不需要每次添加新元素都重新分配容器的内存空间了。 管理容器的成员函数 vector和string类型提供了一些成员函数，允许我们与它的实现中内存分配部分互动。capacity操作告诉我们容器在不扩展内存空间的情况下可以容纳多个元素。reverse操作允许我们通知容器它应该准备保存多少个元素。reserve并不改变容器中元素的数量，它仅影响vector预先分配多大的内存空间。只有当需要的内存空间超过当前容量时，reverse调用才会改变vector的容量。如果需求大小大于当前容量，reverse至少分配与需求一样大的内存空间。如果需求大小小于或等于当前容量，reverse什么也不做。特别是，当需求大小小于当前容量时，容器不会退回内存空间。因此，在调用reverse之后，capacity将会大于或等于传递给reverse的参数。 capacity和size容器的size是指它已经保存的元素的数目；而capacity则是在不分配新的内存空间的前提下它最多可以保存多少元素。 每个vector实现都可以选择自己的内存分配策略。但是必须遵守的一条原则是：只有当迫不得已时才可以分配新的内存空间。 9.5 string的额外操作除了顺序容器共同的操作之外，string类型还提供了一些额外的操作。这些操作中的大部分要么提供string类和C风格字符数组之间的相互转换，要么是增加了允许我们用下标代替迭代器的版本。标准库string类型定义了大量函数。幸运的是，这些函数使用了重复的模式。 9.5.1 构造string的其他方法 9.5.2 改变string的其他方法 9.5.3 string的搜索操作 9.5.4 compare函数 9.5.5 数值转换 9.6 容器适配器除了顺序容器外，标准库还定义了三个顺序容器适配器：stack、queue和priority_queue。适配器是标准库中的一个通用概念。容器、迭代器和函数都有适配器。本质上，一个适配器是一种机制，能使某种事物的行为看起来像另外一种事物一样。一个容器适配器接受一种已有的容器类型，使其行为看起来像一种不同的类型。 栈适配器 队列适配器]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第6章 函数]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%AC%AC6%E7%AB%A0-%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[6.1 函数基础一个典型的函数定义包括以下部分：返回类型、函数名字、由0个或多个形参组成的列表以及函数体。其中，形参以逗号隔开，形参的列表位于一对括号之内。 调用函数 函数的调用完成两项工作：一是用实参初始化函数对应的形参，二是将控制权转移给被调函数。此时，主调函数的执行被暂时中断，被调函数开始执行。形参和实参 实参是形参的初始值。第一个实参初始化第一个形参，第二个实参初始化第二个形参，以此类推。尽管实参和形参存在对应关系，但是并没有规定实参的求值顺序。实参的类型必须与对应的形参类型匹配，这一点与之前的规则是一致的。函数有几个形参，我们就必须提供相同数量的实参。 函数的形参列表 函数的形参列表可以为空，但是不能省略。想要定义一个不带形参的函数，最常用的办法是书写一个空的形参列表。不过为了与C语言兼容，也可以使用关键字void表示函数没有形参：12void f1()&#123;/*...*/&#125; //隐式地定义空形参列表void f2(void) &#123;/*...*/&#125; //显示地定义空形参列表 任意两个形参都不能同名，而且函数最外层作用域中的局部变量也不能使用与函数形参一样的名字。 6.1.1 局部对象在c++语言中，名字有作用域，对象有声明周期。理解这两个概念非常重要。 名字的作用域是程序文本的一部分，名字在其中可见。 对象的生命周期是程序执行过程中该对象存在的一段时间。形参和函数体内部定义的变量统称为局部变量。 自动对象 对于普通局部变量对应的对象来说，当函数的控制路径经过变量定义语句时创建该对象，当到达定义所在的块末尾时销毁它。我们把只存在于块执行期间的对象称为自动对象。当块的执行结束之后，块中创建的自动对象的值就变成未定义的了。形参是一种自动对象。函数开始时为形参申请存储空间，因为形参定义在函数体作用域之内，所以一旦函数终止，形参页就被销毁。 局部静态对象 某些时候，有必要令局部变量的声明周期贯穿函数调用及之后的时间。可以将局部变量定义成static类型从而获得这样的对象。局部静态对象在程序的执行路径第一次经过对象定义语句时初始化，并且直到程序终止才被销毁，在此期间即使对象所在的函数结束执行也不会对它有影响。 6.1.2 函数声明函数的声明和函数的定义非常类似，唯一的区别是函数声明无须函数体，用一个分号替代即可。 6.1.3 分离式编译随着程序越来越复杂，我们希望把程序的各个部分分别存储在不同的文件中。为了允许编写程序时按照逻辑关系将其划分开来，C++语言支持分离式编译，分离式编译允许我们把程序分割到几个文件中去，每个文件独立编译。 6.2 参数传递和其他变量一样，形参的类型决定了形参和实参交互的方式。如果形参是引用类型，它将绑定到对应的实参上；否则，将实参的值拷贝后赋值给形参。当形参是引用类型时，我们说它对应的实参被引用传递或者函数被传引用调用。当实参的值被拷贝给形参时，形参和实参是两个相互独立的对象。我们说这样的实参被值传递或者函数被传值调用。 指针形参 指针的行为和其他引用类型一样。当执行指针拷贝操作时，拷贝的是指针的值，拷贝后，两个指针是不同的指针。因为指针使我们可以间接地访问它所指的对象，所以通过指针可以修改它所指对象的值。 使用引用避免拷贝 拷贝大的类类型对象或者容器对象比较低效，甚至有的类类型根本就不支持拷贝操作。当某种类型不支持拷贝操作时，函数只能通过引用形参访问该类型的对象。 如果函数无须改变引用形参的值，最好将其声明为常量引用。 6.2.3 const形参和实参顶层const作用于对象本身：1234const int ci = 42; //不能改变ci，const是顶层的int i = ci； //正确：当拷贝ci时，忽略了它的顶层constint * const p = &amp;i; //const是顶层的，不能给p赋值*p = 0； //正确：通过p改变对象的内容是允许的，现在i变成了0 和其他初始化过程一样，当用实参初始化形参时会忽略掉顶层的const。换句话说，形参的顶层const被忽略掉了。当形参有顶层const时，传给它常量对象或者非常量对象都是可以的。想要调用引用版本的reset函数，只能使用int类型的对象，而不能使用字面值、求值结果为int的表达式、需要转换的对象或者const int类型的对象。类似的，要想调用指针版的reset只能使用int*。另一方面，我们能传递一个字符串字面值作为find_char的第一个实参，这是因为该函数的引用形参是常量引用，而c++允许我们用字面值初始化常量引用。 6.2.4 数组形参数组的两个特殊性质对我们定义和使用作用在数组上的函数有影响。这两个性质分别是：不允许拷贝数组以及使用数组时会将其转换成指针，所以当我们为函数传递一个数组时，实际上传递的是指向数组元素的指针。尽管不能以值传递的方式传递数组，但是我们可以把形参写成类似数组的形式：1234//尽管形式不同，但这三个print函数是等价的，每个函数都有一个const int* 类型的形参void print(const int *);void print(const int []);void print(const int[10]); 因为数组是以指针的形式传递给函数的，所有一开始函数并不知道数组的确切尺寸，调用者应该为此提供一些额外的信息。管理指针形参有三种常用的技术。 使用标记指定数组长度管理数组实参的第一种方法是要求数组本身包含一个结束标记，使用这种方法的典型示例是C风格字符串。 使用标准库规范管理数组实参的第二种技术是传递指向数组首元素和尾元素的指针。 显示传递一个表示数组大小的形参第三种管理数组实参的方法是专门定义第一个表示数组大小的形参。 数组引用形参 C++语言允许将变量定义成数组的引用，基于同样的道理，形参也可以是数组的引用。此时，引用形参绑定到对应的实参上，也就是绑定到数组上：123456//正确：形参是数组的引用，维度是类型的一部分void print(int (&amp;arr)[10])&#123; for(auto elem: arr) cout &lt;&lt;elem&lt;&lt;endl;&#125; 传递多维数组 C++中实际上没有真正的多维数组，所谓的多维数组其实就是数组的数组。和所有数组一样，将多维数组传递给函数时，真正传递的是指向数组首元素的指针。因为我们处理的是数组的数组，所以首元素本身就是一个数组，指针就是一个指向数组的指针。数组第二维（以及后面的所有维度）的大小都是数组类型的一部分，不能省略。 ###6.2.6 含有可变形参的函数 为了编写能处理不同数量实参的函数，C++11新标准提供了两种主要的方法：如果所有的实参类型相同，可以传递一个名为initializer_list的标准库类型；如果实参的类型不同，我们可以编写一种特殊的函数，也就是所谓的可变参数模板。C++还有一种特殊的形参类型（即省略符），可以用它传递可变数量的实参。 initializer_list形参 如果函数的实参数量未知，但是全部实参的类型都相同，我们可以使用initializer_list类型的形参。initializer_list是一种标准库类型，用于表示某种特定类型的值的数组。initializer_list类型定义在同名头文件中，它提供的操作如表6.1所示： 省略形参 省略形参是为了便于C++程序访问某些特殊的C代码而设置的，这些代码使用了名为varargs的C标准库功能。通常，省略符形参不应用于其他目的。省略符形参只能出现在形参列表的最后一个位置，它的形式无外乎一下两种：12void foo(param_listm, ...);void foo(...); 第一种形式指定了foo函数的部分形参的类型，对应于这些形参的实参将会执行正常的类型检查。省略符形参对应的实参无须类型检查。在第一种形式中，形参声明后面的逗号是可选的。 6.3 返回类型和return语句return语句终止当前正在执行的函数并将控制权返回到调用该函数的地方。return语句有两种形式：12return ;return expression; 6.3.1 无返回值函数没有返回值的return语句只能用在返回类型是void的函数中。返回void的函数不要求非得有return语句，因为在这类函数的最后一句后面会隐式地执行return。一个返回类型的void的函数也能使用return语句的第二种形式，不过此时return语句的expression必须是另一个返回的void函数。强行令void函数返回其他类型的表达式将产生编译错误。 6.3.2 有返回值函数return语句的第二种形式提供了函数的结果，只要函数的返回类型不是void，则该函数内的每条return语句必须返回一个值。return语句的返回值的类型必须与函数的返回类型相同，或者能隐式地转换成函数的返回类型。 不要返回局部对象的引用或指针函数完成之后，它所占用的存储空间也随之被释放掉，因此，函数终止意味着局部变量的引用将指向不再有效的内存区域。 返回类类型的函数和调用运算符 和其他运算符一样，调用运算符也有优先级和结合律，调用运算符的优先级与点运算符和箭头运算符相同，并且也符合左结合律。因此，如果函数返回指针、引用或类的对象，我们就能够适应函数调用的结果访问结果的对象的成员。 引用返回左值 函数的返回类型决定函数调用是否是左值，调用一个返回引用的函数得到左值，其他返回类型得到右值。可以像使用其他左值那样来使用返回引用的函数的调用，特别是，我们能为返回类型是非常量引用的得结果赋值。 列表初始化返回值 C++11新标准规定，函数可以返回花括号包围的值的列表。类似于其他返回结果，此处的列表页用来对表示函数返回的临时量进行初始化。如果列表为空，临时量执行值初始化；否则，返回的值由函数的返回类型决定。 6.3.3 返回函数指针因为数组不能被拷贝，所以函数不能返回数组。不过，函数可以返回数组的指针或引用。虽然从语法上来说，要想定义一个返回数组的指针或引用的函数比较繁琐，但是有一些方法可以简化这一任务，其中最直接的方法是使用类型别名：123typedef int arrT[10]; //arrT是一个类型别名，它表示类型是含有10个整数的数组using arrT = int[10]; //arrT的等价声明arrT* fun(int i); //func 返回一个指向含有10个整数的数组的指针 返回数组指针的函数的形式如下所示：1Type (*function(parameter_list))[dimension] 例如，下面这个func函数的声明没有使用类型别名：1int (*func(int i)) [10]; 可以按照以下的顺序来逐层理解该声明的含义： func(int i) 表示调用func函数时需要一个int类型的实参。 (* func(int i)) 意味着我们可以对函数调用的结果执行解引用操作。 (*func(int i))[10] 表示解引用func的调用将得到一个大小是10的数组。 int (* func(int i))[10] 表示数组中的元素类型是int类型。 使用尾置返回类型 在C++11新标准种还有一种可以简化上述func声明的方法，就是使用使用尾置返回类型。任何函数的定义都能使用尾置返回，但这种形式对于返回类型比较复杂的函数比较有效。比如，返回类型是数组的指针或者数组的引用。尾置返回类型跟在形参列表后面并以一个-&gt;符号开头。为了表示函数真正的返回类型跟在形参列表之后，我们在本该出现返回类型的地方放置一个auto：12//func接受一个int类型的实参，返回一个指针，该指针指向含有10个整数的数组auto func(int i) -&gt; int(*)[10]; 6.4 函数重载如果同一作用域内的几个函数名字相同但形参列表不同，我们称之为重载（overload）函数。这些函数接收的形参类型不一样，但是执行出的操作非常相似。当调用这些函数时，编译器会根据传递的实参类型推断想要的是哪个函数。函数的名字仅仅是让编译器知道它调用的是哪个函数，而函数重载可以在一定程度上减轻程序员起名字、记名字的负担。 main函数不能重载 对于重载函数来说，它们应该在形参数量或形参类型上有所不同。不允许两个函数除了返回类型外，其他的所有要素都相同。 重载和const形参顶层const不影响传入函数的对象。一个拥有顶层const的形参无法和另外一个没有顶层const的形参区分开来：12345Record lookup(Phone);Record lookup(const Phone); //重复声明了Record lookup（Phone）Record lookup(Phone *);Recode lookup(Phone* const); //重复声明了Record lookup（Phone *） 另一方面，如果形参是某种类型的指针或引用，则通过区分其指向的常量对象还是非常量对象可以实现函数重载，此时的const是底层的：12345Record lookup(Account &amp;);Record lookup(const Account&amp;); //新函数，作用于常量引用Record lookup(Account *); //新函数，作用于指向Account的指针Record lookup(const Account *); //新函数，作用于指向常量的指针 调用重载的函数 函数匹配是指一个过程，在这个过程，我们把函数调用与一组重载函数中的某一个关联起来，函数匹配也叫做重载确定。编译器首先将调用的实参与重载集合中每一个函数的形参进行比较，然后根据比较的结果决定到底调用哪个函数。当调用重载函数时有三种可能的结果： 编译器找到一个与实参最佳匹配的函数，并生成调用该函数的代码。 找不到任何一个函数与调用的实参匹配，此时编译器发出无匹配的错误信息。 有多个一个函数可以匹配，但是每一个都不是明显的最佳选择。此时，也将发生错误，称为二义性调用。 6.5 特殊用途语言特性6.5.1 默认实参某些函数有这样一种形参，在函数的很多次调用中它们都被赋予一个相同的值，此时，我们把这个反复出现的值称为函数的默认实参。调用含有默认实参的函数时，可以包含该实参，也可以省略该实参。在定义函数时，我们可以为一个或多个形参定义默认值，不过需要注意的是，一旦某个形参被赋予了默认值，它后面的所有形参都必须有默认值。当设计含有默认实参的函数时，其中一项任务是合理设置形参的顺序，尽量让不怎么使用默认值的形参出现在前面，而让那些经常使用默认值的形参出现在后面。 默认实参声明 对于函数的声明来说，通常的习惯是将其放在头文件中，并且一个函数只声明一次，但是多次声明同一个函数也是合法的。不过有一点需要注意的是，在给定的作用域中一个形参只能被赋予一次默认实参。换句话说，函数的后续声明只能为那些没有默认值的形参添加默认实参，而且该形参右侧的所有形参必须都有默认值。 6.5.2 内联函数和constexpr函数内联函数可以避免函数调用的开销将函数指定为内联函数（inline），通常就是将它再每个调用点上“内联地”展开。一般来说，内联机制用于优化规模较小、流程直接、频繁调用的函数。 constexpr函数constexpr函数是指能用于常量表达式的函数。定义constexpr函数的方法与其他函数类似，不过要遵循几项约定：函数的返回类型及所有形参的类型都得是字面值类型，而且函数体中必须有且只有一条return语句：12constexpr int new_sz() &#123;return 42;&#125;constexpr int foo = new_sz(); //正确：foo是一个常量表达式 constexpr 函数不一定返回常量表达式 把内联函数和constexpr函数放在头文件内 和其他函数不一样，内联函数和constexpr函数可以在程序中多次定义。毕竟编译器想要展开函数仅有函数声明是不够的，还需要函数的定义。不过，对于某个给定的内联函数或者constexpr函数来说，它的多个定义必须完全一致。基于这个原因，内联函数和constexpr函数通常定义在头文件。 6.6 函数匹配确定候选函数和可行函数 函数匹配的第一步就是选定本次调用对应的重载函数集，集合中的函数称为候选函数。候选函数具备两个特征：一是与被调用的函数同名，二是其声明在调用点可见。第二步是考察本次调用提供的实参，然后从候选函数中选出能被这组实参调用的函数，这些新选出的函数称为“可行函数”。可行函数也有两个特征：一是其形参数量与本次调用提供的实参数量相等，二是每个实参的类型与对应的形参类型相同，或者能够转换成形参的类型。 寻找最佳匹配 函数匹配的第三步是从可行函数中选择与本次调用最匹配的函数。在这一过程中，逐一检查函数调用提供的实参，寻找形参类型与实参类型最匹配的那个可行的函数。基本思想是，实参类型与形参类型越接近，它们匹配得越好。 为了确定最佳匹配，编译器将实参类型到形参类型的转换划分为几个等级，具体排序如下所示： 精确匹配，包括以下情况： 实参类型和形参类型相同。 实参从数组类型或函数类型转换成对应的指针类型。 向实参添加顶层const或者从实参中删除顶层const。 通过const转换实现的匹配。 通过类型提升实现的匹配。 通过算术类型转换或者指针转换实现的匹配。 通过类类型转换实现的匹配。]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第3章 字符串、向量和数组]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%AC%AC3%E7%AB%A0-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[3.1 命名空间的using声明需要注意的事项： 每个名字都需要独立的using声明 头文件不应包含using声明 3.2 标准库类型string标准库类型string表示可变长的字符序列，使用string类型必须首先包含string头文件。作为标准库的一部分，string定义在命名空间std中。包含头文件的代码如下：12#include &lt;string&gt;using std::string; 3.2.1 定义和初始化string对象 初始string对象的方式 string s1 默认初始化，s1是一个空串 string s2(s1) s2是s1的副本 string s3(“value”) s3是字面值“value”的副本，除了字面值最后的空字符外 string s3=“value” 等价于s3(“value”)，s3是字面值”value”的副本 string s4(n, ‘c’) 把s4初始化为连续n个字符c组成的串 直接初始化和拷贝初始化 c++语言有几种不同的初始化方式，通过string我们可以清楚地看到在这些初始化方式之间到底有什么区别和联系。如果使用等号（=）初始化一个变量，实际上执行的是拷贝初始化，编译器把等号右侧的初始值拷贝到新创建的对象中去。与之相反，如果不使用等号，则执行的是直接初始化。 3.2.2 string对象上的操作string对象上的操作如下图所示： 读取未知数量的string对象 下面的代码用于读取未知数量的string对象：12345678int main()&#123; string word; while(cin&gt;&gt;word) //反复读取，直到到达文件末尾 cout &lt;&lt; word&lt;&lt;endl; return 0; &#125; 使用getline读取一整行 有时我们希望在最终得到的字符串中保留输入的空白字符，这时应该使用getline函数代替原来的&gt;&gt;运算符。getline函数的参数是一个输入流和一个string对象，函数从给定的输入流中读入内容，直到遇到换行符为止，然后把所读的内容存入到那个string对象中去（注意不存换行符）。getline只要一遇到换行符就结束读取操作并返回结果，哪怕输入的一开始就是换行符也是如此。如果输入真的一开始就是换行符，那么所得的结果是个空string。下面是一段示例代码：1234567int main()&#123; string line; while(getline(cin,line)) cout &lt;&lt;line&lt;&lt;endl; return 0;&#125; 3.2.3 处理string对象中的字符cctype头文件定义了一组标准库函数处理字符串，其具体的函数如下图说所示：处理每个字符？使用基于范围的for语句如果想对string对象中的每个字符做点什么操作，目前最好的办法是使用C++11新标准提供的语句：范围for（range for）语句。这种语句遍历给定序列中的每个元素，并对序列中的每个值执行某种操作，其语法形式是：12for(declaration: expression) statement 其中，expression部分是一个对象，用于表示一个序列。declaration部分负责定义一个变量，该变量将被用于访问序列中的基础元素。每次迭代，declaration部分的变量会被初始化为expression部分的下一个元素值。下面是遍历string对象的实例：123string str("some string");for(auto c: str) cout&lt;&lt;c&lt;&lt;endl; 3.3 标准库类型vector标准库类型vector表示对象的集合，其中所有对象的类型都相同。集合中的每个对象都有一个与之对应的索引，索引用于访问对象。想要使用vector，必须包含适当的头文件。12#include &lt;vector&gt;using std::vector; vector是模板而非类型，由vector生成的类型必须包含vector中元素的类型，例如，vector。vector能容纳绝大多数类型的对象作为其元素，但是因为引用不是对象，所有不存在包含引用的vector。除此之外，其他大多数内置类型和类类型都可以构成vector对象，甚至组成vector的元素也可以是vector。 3.3.1 定义和初始化vector对象主要类型可以分为如下几类： 默认初始化 列表初始化vector对象 创建指定数量的元素初始化，例如：vector ivec(10, -1); 值初始化，例如，vector ivec(10)，10个元素，每个都初始化为0 3.3.2 向vector对象中添加元素使用push_back()方法可以向vector对象中添加元素。 3.3.3 vector的其他操作 3.4 迭代器介绍迭代器的运算符下表列举了迭代器支持的一些运算。使用==和!=来比较两个合适的迭代器是否相等，如果两个迭代器指向的元素相同或者都是同一个容器的尾后迭代器，则他们相等；否则就说两个迭代器不相等。 标准容器迭代器的运算符 *iter 返回迭代器iter所指元素的引用 iter-&gt;mem 解引用iter并获取该元素的名为mem的成员，等价于(*iter).mem ++iter 令iter指示容器中的下一个元素 –iter 令iter指示容器的上一个元素 iter1 == iter2 判断两个迭代器是否相等，如果两个迭代器指示的同一个元素 iter1!=iter2 或者它们是同一个容器的尾后迭代器，则相等；反之，不相等 begin和end运算符 begin和end返回的具体类型由对象是否是常量决定，如果对象是常量，begin和end返回const_iterator；如果对象不是常量，返回iterator； 结合解引用和成员访问操作 解引用迭代器可获得迭代器所指的对象，如果该对象的类型恰好是类，就可以进一步访问它的成员。例如，对于一个由字符串组成的vector对象来说，要想检查其元素是否为空，令it为vector对象的迭代器，只需检查it所指字符串是否为空就可以了，其代码如下：1(*it).empty() 为了简化上述表达式，C++语言定义了箭头运算符-&gt;。箭头运算符把解引用和成员访问操作结合在一起，也就是说，iter-&gt;mem和(*it).mem表达的意思相同。 3.4.2 迭代器运算string和vector的迭代器提供了很多额外的运算符，一方面可使得迭代器的每次移动跨过对多个元素，另外也支持迭代器进行关系运算，具体如下图： 3.5 数组与vector不同的是，数组的大小确定不变，不能随意向数组中增加元素。因为，数组的大小固定，因此对某些特殊的应用来说程序的运行时性能比较好，但是相应的损失了一些灵活性。 指针和数组在C++语言中，指针和数组有非常紧密的联系，使用数组的时候编译器一般会把它转换为指针。通常情况下，使用取地址符来获取指向某个对象的指针，取地址符可以用于任何对象。数组的元素也是对象，对数组使用下标运算符得到该数组指定位置的元素。因此，像其他元素一样，对数组的元素使用取地址符就能的得到指向该元素的指针：12string nums[] = &#123;"one", "two", "three"&#125;;string *p = &amp;nums[0]; //p指向numsde 第一个元素 数组还有一个特性：在很多用到数组名字的地方，编译器都会自动地将其替换为一个指向数组首元素的指针：1string *p2 = nums; //等价于p2 = &amp;nums[0] 由此，可知在一些情况系数组的操作实际上是指针的操作，当使用数组作为auto变量的初始值时，推断得到的类型是指针而非数组。 指针也是迭代器指向数组元素的指针拥有更多功能。vector和string的迭代器支持的运算，数组的指针全部支持。尽管能够计算得到尾后指针，但这种用法易出错。C++11标准引入了两个名为begin和end的函数。这两个函数与容器中的两个同名成员的功能类似，不过数组毕竟不是类类型，因此这两个函数不是成员函数。正确的使用形式是将数组作为他们的参数：123int ia[] = &#123;0,1,2,3,4,5,6,7,8,9&#125;;int *beg = begin(ia); //指向ia首元素的指针int *last = end(ia); //指向ia尾元素的的下一个位置的指针 begin函数返回指向ia首元素的指针，end函数返回指向ia尾元素下一位置的指针，这两个函数定义在iterator头文件中。 解引用和指针运算的交互指针加上一个整数所得的结果还是一个指针。假设结果指针指向一个元素，则允许解引用该结果指针：12int ia[] = &#123;0,2,4,5,8&#125;;int last = *(ia+4); //正确：把last初始化为8，也就是ia[4]的值 最好在必要的地方加上括号，类似的，上例中指针加法的圆括号也必不可少。如果写成下面的形式：1last = *ia + 4; // 正确：last=4等价于ia[0]+4]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第2章 变量和基本类型]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%AC%AC2%E7%AB%A0-%E5%8F%98%E9%87%8F%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[2.1 基本内置类型 C++提供了一套包括算术类型和空类型在内的基本数据类型。其中，算术类型包括字符、整型数、布尔值和浮点数。空类型不对应具体的值，仅用于一些特殊的场合。 2.1.1 算术类型算术类型分为两类：整型（包括字符和布尔类型）和浮点型。C++算数类型如下表所示： 含义 类型 最小尺寸 布尔类型 bool 未定义 字符 char 8位 宽字符 wchar_t 16位 Unicode字符 char16_t 16位 Unicode字符 char32_t 32位 短整型 short 16位 整型 int 16位 长整型 long 32位 长整型 long long 64位 单精度浮点数 float 6位有效数字 双精度浮点数 double 10位有效数字 扩展精度浮点数 long double 10位有效数字 除去布尔型和扩展的字符型之外，其他整型可以划分为带符号的（singed）和无符号（unsigned）的两种。带符号类型可以表示正数、负数和0，无符号的类型仅能表示大于等于0的值。int、short、long和long long都是带符号的，通过在这些类型前添加unsigned就可以得到无符号类型。与其他整型不同，字符型被分为了三种：char、signed char和unsigned char。尽管字符型有三种，但是字符型的表现形式只有两种：带符号的和无符号的。 如何选择合适的类型 当明确知晓数值不可能为负时，选择无符号类型。 使用int执行整数运算。在实际使用中short常常显得太小，long一般和int有一样的尺寸。如果数值超过了int的表示范围，选用long long。 在算术表达式中不要使用char或bool，只有在存放字符或布尔值时才使用他们。因为类型char在一些机器上是由符号的，而在另一些机器上又是无符号的，所有如果使用char进行运算特别容易出现问题。如果使用的整数为一个不大的整数，那么明确指定它的类型是singed char或unsigned char。 执行浮点数运算选用double，因为float通常精度不够而且双精度浮点数和单精度浮点数的计算代价相差无几。 ###2.1.2 类型转换C++类型转换时的处理操作： 将非布尔类型的算术值赋值给布尔类型时，初始值为0则结果为false，否则结果为true。 将布尔值赋给非布尔类型时，初始值为false结果为0，初始值为true结果为1。 将浮点数赋值给整数类型时，进行近似处理，结果值将仅保留浮点数中小数点之前的部分。 将整数值赋值给浮点数时，小数部分记为0。如果该整数所占的空间超过了浮点类型的容量，精度可能有损失。 当赋值给无符号类型一个超过它表示范围的值时，结果是初始值对无符号类型表示数值总数取模后的余数。 当赋值给带符号类型一个超出它表示范围的值时，结果是未定义的。 2.2 变量变量定义 变量定义的基本形式是：首先是类型说明符，随后紧跟由一个或多个变量名组成的列表，其中变量名以逗号分隔，最后以分号结果结束。1int sum = 0, value, uints_sold= 0; 变量初始化 变量初始化不是赋值，初始化的含义是创建变量时赋予其一个初始值，而赋值的含义是把对象当前值擦除，而以一个新的值来代替。 列表初始化 C++语言定义了初始化的好几种不同的形式，也是初始化问题复杂性的一个体现。例如，定义一个名为units_sold的int变量并初始化为0，以下的4条语句都可以做到这一点：1234int units_sold=0;int units_sold = &#123;0&#125;;int units_sold&#123;0&#125;;int units_sold(0); 默认初始化 如果定义变量时没有指定初始值，则变量被默认初始化，此时变量被赋予了“默认值”。默认值是由变量类型决定，同时定义变量的位置也会对此有影响。 变量的声明和定义的关系 为了允许把程序拆分为多个逻辑部分来编写，C++语言支持分离式编译机制，该机制允许将程序分割为若干个文件，每个文件可被独立编译。为了支持分离式编译，C++语言将声明和定义区分开来。声明使得名字为程序所知，一个文件如果想使用别处定义的名字必须包含对那个名字的生明。而定义负责创建与名字关联的实体。变量的声明规定了变量的类型和名字，在这一点定义与之相同。但是除此之外，定义还申请存储空间，也可能会为一个变量赋初始值。如果想声明一个变量而非定义它，就在变量名前添加关键字extern，而不是显示初始化变量：12extern int i; //声明而非定义iint j; //声明并定义j 任何包含了显示初始化的声明即成为定义。在函数体内部，如果试图初始化一个由extern关键字标记的变量，将引发错误。 变量能且只能被定义一次，但是可以被多次声明。 变量的作用域 全局作用域，全局作用域内的变量在整个程序的范围内都可使用。如果函数有可能用到全局变量，则不宜再定义一个同名的局部变量。 2.3 复合类型2.3.1 引用引用（reference）为对象起了另外一个名字，引用类型引用另外一种类型。通过将声明符号写成&amp;d的形式来定义引用类型，其中d是声明的变量名：123int ival = 1024;int &amp;refVal = ival; //refVal指向ival（是ival的另一个名字）int &amp;refVal2; //报错:引用必须要被初始化 一般在初始化变量时，初始值会被拷贝到新建的对象中。然而，定义引用时，程序把引用和它的初始值绑定在一起，而不是将初始值拷贝给引用。一旦初始化完成，引用将和它的初始值对象一直绑定在一起。因为无法令引用重新绑定到另外一个对象，因此引用必须初始化。 引用并非对象，相反的，它只是为一个已经存在的对象所起的另外一个名字。 为引用赋值，实际上是把值赋给了与引用绑定的对象。因为引用本身不是一个对象，所有不能定义引用的引用。 引用的定义 允许在一条语句中定义多个引用，其中每个引用标识符都必须以符号&amp;开头：123int i = 1024, i2 = 2048;int &amp;r = i, r2 = i2; //r是一个引用，与i绑定在一起，r2是intint i3 = 1024, &amp;ri = i3;//i3是int，ri是一个引用，与i3绑定在一起 2.3.2 指针指针是指向另外一种类型的复合类型。与引用类似，指针也实现了对其他对象的间接访问。然而指针与引用相比又有很多不同点。其一，指针本身就是一个对象，允许对指针赋值和拷贝，而且在指针的生命周期内它可以先后指向几个不同的对象。其二，指针无须在定义时赋初值。和其他内置类型一样，在块作用域内定义的指针如果没有初始化，也将拥有一个不确定的值。定义指针类型的方法是将声明符写成*d的形式，其中d是变量名。如果在一条语句中定义了几个指针变量，每个变量前面都必须有符号*:12int *ip1, *ip2; //ip1和ip2都是指向int型对象的指针double dp, *dp2; //dp2是指向double型对象的指针，dp是double类型对象 获取对象的地址 指针存放某个对象的地址，要想获取该地址，需要使用取地址符（操作符&amp;）：12int ival = 42;int *p = &amp;ival; //p存放变量ival的地址，或者说p是指向变量ival的指针 指针的类型都要和它所指向的对象严格匹配。 指针值 指针的值（即地址）应属于下列4中状态之一： 指向一个对象。 指向紧邻对象所占空间的下一个位置。 空指针，意味着指针没有指向任何对象。 无效指针，也就是上述情况之外的其他值。 利用指针访问对象 如果指针指向了一个对象，则允许使用解引用符（操作符*）来访问该对象：123int ival = 42;int *p = &amp;ival; //p存放着变量ival的地址cout &lt;&lt; *p ; //由符号*得到指针p所指的对象，输出42 对指针解引用会得出所指的对象，因此如果给解引用的结果赋值，实际上也就是给指针所指的对象赋值：12*p = 0; //由符号*得到指针p所指的对象，即可由p为变量ival赋值cout &lt;&lt; *p;//输出0 空指针 空指针不指向任何对象，在试图使用一个指针之前代码可以首先检查它是否为空。以下列出几个生成空指针的方法：123int *p1 = nullptr;int *p2 = 0;int *p3 = NULL; 建议：初始化所有指针 赋值和指针 给指针赋值就是令它存放一个新的地址，从而指向一个新的对象：1234567int i=42;int *pi = 0;int *pi2 = &amp;i; //pi2初始化，存有i的地址int *pi3; //如果pi3定义于块内，则pi3的值是无法确定的pi3 = pi2; //pi3和pi2指向同一个对象ipi2 = 0; //pi2不指向任何对象了 有时太想搞清楚一条赋值语句到底是改变了指针的值还是改变了指针所指向的对象的值不太容易，最好的办法就是记住赋值永远改变等号左侧的对象。当写出如下语句时：1pi = &amp;ival; //pi的值被改变，现在pi指向了ival 意思是为pi赋一个新的值，也就是改变了那个存放在pi内的地址值。相反的，如果写出如语句：1*pi = 0; //ival的值被改变，指针pi并没有被改变 则*pi（也就是指针pi指向的那个对象）发生改变。void*指针 void*是一种特殊的指针类型，可以用于存放任意对象的地址。一个void*指针存放着一个地址，这一点和其他指针类似。我们对该地址中到底是个什么类型的对象不了解。 指向指针的指针 通过*的个数可以区分指针的级别。也就是说，**表示指向指针的指针，***表示指向指针的指针的指针，以此类推： 123int ival = 1024;int *pi = &amp;ival; //pi指向一个int型的数int **ppi = &amp;pi; //ppi指向一个int型的指针 2.4 const限定符当希望定义一种变量，它的值不能被改变，可以使用关键字const对变量类型加以限定。使其成为const对象，const对象一旦创建后其值就不能再改变，所以const对象必须初始化。 如果想在多个文件之间共享const对象，必须在变量的定义之前添加extern关键字 2.4.1 const的引用可以把引用绑定在conster对象上，就像绑定到其他对象上一样，称之为对常量的引用。与普通引用不同的是，对常量的引用不能被用作修改它所绑定的对象：1234const int ci = 1024;const int &amp;r1 = ci; //正确：引用及其对应的对象都是常量r1 = 42; //错误：r1是对常量的引用int &amp;r2 = ci; //错误：试图让一个非常量音引用指向一个常量对象 初始化和对const的引用 引用的类型必须与其所引用的对象的类型一致，但是有个例外：在初始化常量引用时允许用任意表达式作为初始值，只要该表达式的结果能转换成引用的类型即可。尤其，允许为一个常量引用绑定非常量的对象、字面值。对const的引用可能引用一个并非const的对象。必须认识到，常量引用仅对引用可参与的操作做出了限定，对于引用的对象本身是不是一个常量未作限定。因为对象也可能是一个非常量，所有允许通过其他途径改变它的值： 12345int i=42;int &amp;r1 = i; //引用r1绑定对象iconst int &amp;r2 = i; //r2也绑定对象i，但是不允许通过r2修改i的值r1 = 0; //r1并非常量，i的值修改为0r2 = 0; //错误：r2是一个常量引用 r2绑定整数i是合法的行为。然而，不允许通过r2修改i的值。尽管如此，i的值仍然允许通过其他途径修改，既可以直接给i赋值，也可以通过像r1一样绑定到i的其他引用来修改。 2.4.2 指针和const指向常量的指针不能用于改变其所指对象的值。要想存放常量对象地址，只能使用指向常量的指针：1234const double pi = 3.14; //pi是个常量double *ptr = &amp;pi; //错误：ptr是一个普通的指针const double *cptr = &amp;pi; //正确：cptr可以指向一个双精度常量*cptr = 42; //错误：不能给*cptr赋值 指针的类型必须与其所指对象的类型一致，例外情况是：允许令一个指向常量的指针指向一个非常量对象：12double deval = 3.14;cptr = &amp;deval; //正确：但是不能通过cptr改变deval的值 const指针 指针是对象而引用不是，因此就像其他对象类型一样，允许把指针本身定为常量。常量指针必须初始化，而且一旦初始化完成，则它的值就不能改变了。把*放在const关键字之前用以说明指针是一个常量，即不变的是指针本身的值，而非指向的那个值：1234int errNumb = 0;int * const curErr = &amp;errNumb; //curErr将一直指向errNumbconst double pi = 3.14159;const double *const pip = &amp;pi; //pip是一个指向常量对象常量指针 要想弄清楚这些声明的含义最有效的方法的是从右向左阅读。 2.5 处理类型2.5.1 类型别名类型别名是一个名字，它是某种类型的同义词。使用类型别名有很多好处，它让复杂的类型名字变得简单明了、易于理解和使用。有两种方法可以用于定义类型别名。传统的方法是使用关键字typedef：12typedef double wages; //wages是double的同义词typedef wages base,*p; //base是double的同义词，p是double*的同义词 新标准规定了一种新的方法，使用别名声明来定义类型的别名：1using SI = Sales_item; //SI是Sales_item的同义词 2.5.2 auto类型说明符在编程时常常需要把表达式的值赋值给变量，这就要求在声明变量的时候清楚地知道表达式的类型。然而，有时根本做不到。为了解决这个问题，C++11新标准引用了auto类型说明符，用它就能让编译器替我们去分析表达式所属的类型。和原来那些只对应一种特定类型的说明符（如double）不同，auto让编译器通过初始值来推算变量的类型。显然，auto定义的变量需要有初始值：1auto item = val1 + val2; //item初始化为val1和val2相加的结果 2.5.3 decltype类型指示符有时会遇到这种情况：希望从表达式的类型推断出要定义的变量的类型，但是不想用该表达式的值初始化变量。为了满足这一要求，C++ 11新标准引入了第二种类型说明符decltype，它的作用是选择并返回操作数的数据类型。在此过程中，编译器分析表达式并得到它的类型，却不实际计算表达式的值：1decltype(f()) sum = x; //sum的类型就是函数f的返回类型]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
</search>
